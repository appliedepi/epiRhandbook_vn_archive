---
knit: "bookdown::render_book"
title: "Cẩm nang dịch tễ học với R"  
description: "The Epi R Handbook is a R reference manual for applied epidemiology and public health."
author: "the handbook team"
date: "`r Sys.Date()`"
#url: 'https://github.com/nsbatra/Epi_R_handbook'
#twitter-handle: 
#cover-image: images/R_Handbook_Logo.png
site: bookdown::bookdown_site
# output: bookdown::gitbook:
#      config:
#           sharing:
#                twitter: yes
#                facebook: yes
#                whatsapp: yes
#                github: yes
documentclass: book
---

#  {.unnumbered}

```{r, out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "Epi R Handbook banner beige 1500x500.png"))
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<meta name="description" content="The Epi R Handbook is an R reference manual for applied epidemiology and public health.">

<meta http-equiv="Content-Type" content="text/html; charset=utf-8">

<!-- <span style="color: red;">**THIS IS A DRAFT.  REVIEWERS GIVE FEEDBACK AT THIS [LINK](https://forms.gle/4RNdRRLGx67xW9yq9)**.</span> -->

<!-- <span style="color: darkgreen;">**DO YOU LIKE THIS HANDBOOK? SHOULD SOMETHING BE CHANGED? PLEASE TELL US!**</span> -->

<!-- <form target="_blank" action="https://forms.gle/A5SnRVws7tPD15Js9"> -->

<!--     <input type="submit" value="FEEDBACK" /> -->

<!-- </form> -->

<!-- ======================================================= -->

<!-- ## An R reference manual for applied epidemiology and public health {.unnumbered} -->

<!-- <span style="color: brown;">**The Epi R Handbook is an R reference manual for applied epidemiology and public health.**</span> -->

<!-- ## About this handbook   -->

## Dịch tễ học ứng dụng và y tế công cộng với R {.unnumbered}

**Sổ tay này hướng tới:**

-   Là một tài liệu tham khảo R một cách nhanh chóng\
-   Cung cấp các ví dụ tập trung vào nhiệm vụ giải quyết các vấn đề dịch tễ học phổ biến\
-   Hỗ trợ các nhà dịch tễ học chuyển sang sử dụng R\
-   Có thể sử dụng trong các tình huống có kết nối internet thấp thông qua **[phiên bản ngoại tuyến][Tải sách và dữ liệu]**

<!-- * Use practical epi examples - cleaning case linelists, making transmission chains and epidemic curves, automated reports and dashboards, modeling incidence and making projections, demographic pyramids and rate standardization, record matching, outbreak detection, survey analysis, survival analysis, GIS basics, contact tracing, phylogenetic trees...   -->

<!-- **How is this different than other R books?**   -->

<!-- * It is community-driven - *written for epidemiologists by epidemiologists* in their spare time and leveraging experience in local, national, academic, and emergency settings   -->

<!-- Dual-column created based on the rmarkdown cookbook here: https://bookdown.org/yihui/rmarkdown-cookbook/multi-column.html -->

<br>

::: {style="display: flex;"}
<div>

```{r, out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "epiRhandbook_HexSticker_500x500.png"))
```

</div>

::: {.col data-latex="{0.05\\textwidth}"}
  <!-- an empty Div (with a white space), serving as
a column separator -->
:::

<div>

[**Được viết bởi các nhà dịch tễ học, dành cho các nhà dịch tễ học**]{style="color: black;"}

Chúng tôi là những nhà dịch tễ học đến từ khắp mọi nơi trên thế giới, viết trong thời gian rảnh của mình để cung cấp tài liệu này tới cộng đồng. Sự động viên và góp ý của bạn luôn được chào đón thông qua:

-   Gửi [**biểu mẫu phản hồi**](https://forms.gle/A5SnRVws7tPD15Js9)\
-   Email tới [**epiRhandbook\@gmail.com**](mailto:epiRhandbook@gmail.com){.email} hoặc tweet [**\@epiRhandbook**](https://twitter.com/epirhandbook)\
-   Gửi các vấn đề cho chúng tôi tại [**Github repository**](https://github.com/epirhandbook/Epi_R_handbook)

</div>
:::

<!-- ======================================================= -->

## Sổ tay này được sử dụng như thế nào {.unnumbered}

-   Truy cập các trang trong phần Mục lục, hoặc sử dụng ô tìm kiếm
-   Nhấn biểu tượng "copy" để sao chép code\
-   Kết hợp theo dõi cùng với các bộ [dữ liệu minh họa][Tải sách và dữ liệu]\
-   Xem phần "Tài nguyên" trong từng chương để tìm thêm tài liệu

**Phiên bản ngoại tuyến**

Xem hướng dẫn tại trang [Tải sách và dữ liệu].

<!-- ======================================================= -->

## Lời cảm ơn {.unnumbered}

Sổ tay này được tạo ra bởi sự hợp tác của các nhà dịch tễ học từ khắp nơi trên thế giới, đúc kết kinh nghiệm cùng với các tổ chức khác bao gồm các cơ quan y tế địa phương, tiểu bang, tỉnh và quốc gia, Tổ chức Y tế Thế giới (WHO), Tổ chức Bác sỹ không biên giới (MSF), hệ thống các bệnh viện, và các đơn vị nghiên cứu.

Sổ tay này **không phải** là sản phẩm đã được phê duyệt của bất kỳ tổ chức cụ thể nào. Mặc dù chúng tôi cố gắng đảm bảo tính chính xác, nhưng chúng tôi không chịu trách nhiệm về nội dung trong cuốn sách này.

### Những người đóng góp {.unnumbered}

**Chủ biên:** [Neale Batra](https://www.linkedin.com/in/neale-batra/)

**Nhóm nòng cốt dự án:** [Neale Batra](https://www.linkedin.com/in/neale-batra/), [Alex Spina](https://github.com/aspina7), [Amrish Baidjoe](https://twitter.com/Ammer_B), Pat Keating, [Henry Laurenson-Schafer](https://github.com/henryls1), [Finlay Campbell](https://github.com/finlaycampbell)

**Nhóm tác giả**: [Neale Batra](https://www.linkedin.com/in/neale-batra/), [Alex Spina](https://github.com/aspina7), [Paula Blomquist](https://www.linkedin.com/in/paula-bianca-blomquist-53188186/), [Finlay Campbell](https://github.com/finlaycampbell), [Henry Laurenson-Schafer](https://github.com/henryls1), [Isaac Florence](www.Twitter.com/isaacatflorence), [Natalie Fischer](https://www.linkedin.com/in/nataliefischer211/), [Aminata Ndiaye](https://twitter.com/aminata_fadl), [Liza Coyer](https://www.linkedin.com/in/liza-coyer-86022040/), [Jonathan Polonsky](https://twitter.com/jonny_polonsky), [Yurie Izawa](https://ch.linkedin.com/in/yurie-izawa-a1590319), [Chris Bailey](https://twitter.com/cbailey_58?lang=en), [Daniel Molling](https://www.linkedin.com/in/daniel-molling-4005716a/), [Isha Berry](https://twitter.com/ishaberry2), [Emma Buajitti](https://twitter.com/buajitti), [Mathilde Mousset](https://mathildemousset.wordpress.com/research/), [Sara Hollis](https://www.linkedin.com/in/saramhollis/), Wen Lin

**Nhóm dịch giả**: [Nguyễn Thanh Lương](https://www.linkedin.com/in/ntluong95/), [Nguyễn Thị Khánh Huyền](https://www.linkedin.com/in/huyen-nguyen-3920b51a6/), Võ Hữu Thuận, Nguyễn Trung Thành, Vũ Thu Hà, [Hồ Hoàng Dung](https://www.linkedin.com/in/dzunggg/) 

**Nhóm phản biện**: Pat Keating, Annick Lenglet, Margot Charette, Danielly Xavier, Esther Kukielka, Michelle Sloan, Aybüke Koyuncu, Rachel Burke, Kate Kelsey, [Berhe Etsay](https://www.linkedin.com/in/berhe-etsay-5752b1154/), John Rossow, Mackenzie Zendt, James Wright, Laura Haskins, [Flavio Finger](ffinger.github.io), Tim Taylor, [Jae Hyoung Tim Lee](https://www.linkedin.com/in/jaehyoungtlee/), [Brianna Bradley](https://www.linkedin.com/in/brianna-bradley-bb8658155), [Wayne Enanoria](https://www.linkedin.com/in/wenanoria), Manual Albela Miranda, [Molly Mantus](https://www.linkedin.com/in/molly-mantus-174550150/), Pattama Ulrich, Joseph Timothy, Adam Vaughan, Olivia Varsaneux, Lionel Monteiro, Joao Muianga

**Hình minh họa**: Calder Fong

<!-- **Editor-in-Chief:** Neale Batra  -->

<!-- **Project core team:** Neale Batra, Alex Spina, Amrish Baidjoe, Pat Keating, Henry Laurenson-Schafer, Finlay Campbell   -->

<!-- **Authors**: Neale Batra, Alex Spina, Paula Blomquist, Finlay Campbell, Henry Laurenson-Schafer, [Isaac Florence](www.Twitter.com/isaacatflorence), Natalie Fischer, Aminata Ndiaye, Liza Coyer, Jonathan Polonsky, Yurie Izawa, Chris Bailey, Daniel Molling, Isha Berry, Emma Buajitti, Mathilde Mousset, Sara Hollis, Wen Lin   -->

<!-- **Reviewers**: Pat Keating, Mathilde Mousset, Annick Lenglet, Margot Charette, Isha Berry, Paula Blomquist, Natalie Fischer, Daniely Xavier, Esther Kukielka, Michelle Sloan, Aybüke Koyuncu, Rachel Burke, Daniel Molling, Kate Kelsey, Berhe Etsay, John Rossow, Mackenzie Zendt, James Wright, Wayne Enanoria, Laura Haskins, Flavio Finger, Tim Taylor, Jae Hyoung Tim Lee, Brianna Bradley, Manual Albela Miranda, Molly Mantus, Priscilla Spencer, Pattama Ulrich, Joseph Timothy, Adam Vaughan, Olivia Varsaneux, Lionel Monteiro, Joao Muianga   -->

### Tài trợ và hỗ trợ {.unnumbered}

Sổ tay này nhận được tài trợ thông qua kinh phí hỗ trợ trợ xây dựng năng lực khẩn cấp COVID-19 từ [TEPHINET](https://www.tephinet.org/), mạng lưới toàn cầu của các Chương trình Đào tạo Dịch tễ học Thực địa (FETPs).

Các hỗ trợ hành chính được cung cấp bởi mạng lưới cựu sinh EPIET ([EAN](https://epietalumni.net/)), với lời cảm ơn đặc biệt tới Annika Wendland. EPIET là Chương trình đào tạo Dịch tễ học can thiệp tại Châu Âu.

Đặc biệt gửi lời cảm ơn tới Trung tâm Điều hành Amsterdam (OCA) của Tổ chức Bác sỹ không biên giới (MSF) cho những sự hỗ trợ của họ trong quá trình phát triển cuốn sổ tay này.

*Ấn phẩm này được hỗ trợ bởi Hợp đồng Hợp tác số NU2GGH001873, được tài trợ bởi Trung tâm Kiểm soát và Phòng ngừa Dịch bệnh thông qua TEPHINET, một chương trình của Lực lượng đặc nhiệm về sức khỏe toàn cầu. Nội dung của sổ tay hoàn toàn do tác giả chịu trách nhiệm và đại diện cho quan điểm chính thức của Trung tâm Kiểm soát và Phòng ngừa Dịch bệnh, Bộ Y tế và Dịch vụ Nhân sinh, Lực lượng Đặc nhiệm về Sức khỏe Toàn cầu, hoặc TEPHINET*

### Cảm hứng {.unnumbered}

Rất nhiều các hướng dẫn và tóm tắt cung cấp kiến thức sử dụng để phát triển nội dung sổ tay này được tham khảo trong các trang nội dung tương ứng.

Một cách tổng quát hơn, các nguồn sau đây đã truyền nguồn cảm hứng cho cuốn sổ tay này:\
[The "R4Epis" project](https://r4epis.netlify.app/) (một sự hợp tác giữa MSF và RECON)\
[R Epidemics Consortium (RECON)](https://www.repidemicsconsortium.org/)\
[R for Data Science book (R4DS)](https://r4ds.had.co.nz/)\
[bookdown: Authoring Books and Technical Documents with R Markdown](https://bookdown.org/yihui/bookdown/)\
[Netlify](https://www.netlify.com) để lưu trữ trang web này

<!-- ### Image credits {-}   -->

<!-- Images in logo from US CDC Public Health Image Library) include [2013 Yemen looking for mosquito breeding sites](https://phil.cdc.gov/Details.aspx?pid=19623), [Ebola virus](https://phil.cdc.gov/Details.aspx?pid=23186), and [Survey in Rajasthan](https://phil.cdc.gov/Details.aspx?pid=19838).   -->

## Điều khoản sử dụng và đóng góp {.unnumbered}

### Giấy phép {.unnumbered}

<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" alt="Creative Commons License" style="border-width:0"/></a><br />Sổ tay này được cấp phép theo <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.

Chúng tôi khuyến khích các khóa học và các chương trình đào tạo dịch tễ sử dụng cuốn sổ tay này cho sinh viên của mình. Nếu bạn có thắc mắc về mục đích sử dụng của mình, hãy gửi email tới [**epiRhandbook\@gmail.com**](mailto:epiRhandbook@gmail.com){.email}.

### Trích dẫn {.unnumbered}

Neale Batra và cộng sự, Cẩm nang Dịch tễ học với R. <a rel="license" href="https://zenodo.org/badge/231610102.svg"><img src="https://zenodo.org/badge/231610102.svg" alt="DOI" style="border-width:0"/></a><br />

### Đóng góp {.unnumbered}

Nếu bạn muốn đóng góp nội dung, vui lòng liên hệ với chúng tôi thông qua Github hoặc email. Chúng tôi đang triển khai lịch trình cập nhật cho cuốn sách cũng như xây dựng hướng dẫn dành cho cộng tác viên.

Xin lưu ý rằng dự án epiRhandbook được phát hành cùng với bộ [Quy tắc ứng xử của cộng tác viên](https://contributor-covenant.org/version/2/0/CODE_OF_CONDUCT.html). Bằng cách đóng góp cho dự án này, bạn đồng ý tuân theo các điều khoản của nó.
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:index.Rmd-->

# (PART) Về cuốn sách này {.unnumbered}
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/cat_about_book.Rmd-->

# Biên tập và ghi chú kỹ thuật {#editorial-style}

Trong chương này, chúng tôi sẽ mô tả triết lý và phong cách viết code, cũng như các quyết định biên tập cụ thể được thực hiện trong việc tạo ra cuốn sổ tay này.

## Cách tiếp cận và phong cách

Độc giả tiềm năng của cuốn sách này là rất lớn, bao gồm những người hoàn toàn mới đối với R, và cả những người dùng R có kinh nghiệm đang tìm kiếm cho mình các phương pháp và mẹo hay nhất. Vì vậy, cuốn sách cần phải vừa dễ tiếp cận vừa ngắn gọn. Do đó, cách tiếp cận của chúng tôi là cung cấp lượng văn bản giải thích *vừa đủ* để một người mới sử dụng R cũng có thể áp dụng code và hiểu code đang làm gì.

Một vài điểm lưu ý:

-   Đây là cuốn sách tham khảo về code đi kèm với những ví dụ tương đối ngắn gọn - *không phải* một cuốn sách giáo khoa về R hay khoa học dữ liệu\
-   Đây là một cuốn *sổ tay về R* sử dụng trong dịch tễ học ứng dụng - không phải là một hướng dẫn về các phương pháp của dịch tễ học ứng dụng\
-   Cuốn sách dự kiến sẽ luôn được thay đổi và cập nhập do các R packages tối ưu cho một nhiệm vụ luôn được thay đổi thường xuyên, vì vậy chúng tôi hoan nghênh những thảo luận về những điều được nhấn mạnh trong cuốn sách này

### R packages {.unnumbered}

**Quá nhiều lựa chọn**

Một trong những khía cạnh thách thức nhất của việc học R là bạn biết package nào được sử dụng trong trường hợp nào. Việc vật lộn với một công việc mà chỉ sau này bạn nhận ra là có một package R giúp bạn thực hiện tất cả những điều đó trong một dòng lệnh là điều không hề hiếm gặp!

Trong sổ tay này, chúng tôi cố gắng cung cấp cho bạn ít nhất hai cách để hoàn thành công việc: một phương pháp đã thử và đúng (có thể là **base** R hoặc **tidyverse**) và một R package đặc biệt được thiết kế riêng cho mục đích đó. Chúng tôi muốn bạn có một số tùy chọn trong trường hợp bạn không thể tải xuống một package nhất định hoặc package đó không hoạt động với bạn.

Khi lựa chọn package để làm việc, chúng tôi ưu tiên các R package và phương pháp tiếp cận đã được cộng đồng thử nghiệm và hiệu chỉnh, giảm thiểu số lượng package được sử dụng trong một phiên làm việc điển hình, bao gồm sự ổn định (không thay đổi thường xuyên) và giúp hoàn thành nhiệm vụ một cách đơn giản và gọn gàng

Cuốn sách này ưu tiện các package và câu lệnh từ thư viện **tidyverse**. Tidyverse là một tuyển tập các R package được thiết kế dành riêng cho khoa học dữ liệu, trong đó các package này chia sẻ nền tảng ngữ pháp và cấu trúc dữ liệu chung. Tất cả các package từ thư viện tidyverse có thể được cài đặt hoặc gọi thông qua thư viện **tidyverse**. Đọc thêm tại [tidyverse website](https://www.tidyverse.org/).

Khi thích hợp, chúng tôi cũng cung cấp các tùy chọn code sử dụng **base** R - là các packages và hàm có sẵn của R khi cài đặt. Điều này là do chúng tôi nhận thấy rằng một số độc giả của cuốn sách này có thể không có Internet tốt để tải xuống các package bổ sung.

**Liên kết các hàm và packages một cách rõ ràng**

Trong các hướng dẫn về R thường rất khó chịu khi một hàm được hiển thị trong code, nhưng bạn không biết hàm đó đến từ package nào! Chúng tôi cố gắng tránh tình trạng này.

Trong các đoạn văn bản trần thuật, tên các package được viết in đậm (ví dụ: **dplyr**) và các hàm được viết như sau: `mutate()`. Chúng tôi cố gắng nói rõ ràng về một hàm đến từ package nào, bằng cách tham chiếu package đó trong đoạn văn bản gần đó hoặc nhấn mạnh package đó một cách rõ ràng trong đoạn code như sau: `dplyr::mutate()`. Điều này nhìn có vẻ thừa thãi, nhưng chúng tôi làm điều đó là có mục đích.

Tham khảo thêm chương [R cơ bản] để hiểu thêm về package và hàm.

### Phong cách viết code {.unnumbered}

Trong sổ tay này, chúng tôi thường viết theo phong cách "thêm dòng mới", điều này làm cho code trông có vẻ "dài hơn". Chúng tôi làm vậy vì một vài lý do sau đây:

-   Chúng tôi có thể viết các giải thích bằng `#` bên cạnh mỗi phần nhỏ của code\
-   Nhìn chung, code dài hơn (theo chiều dọc) thì dễ đọc hơn\
-   Nó cũng dễ đọc hơn trong một diện tích màn hình hẹp (không cần kéo thanh điều hướng trái phải)\
-   Từ việc thụt lề, có thể dễ dàng hơn để biết arguments nào thuộc về hàm nào

Kết quả là, code *lẽ ra* sẽ được viết trông như thế này:

```{r, eval=F}
linelist %>% 
  group_by(hospital) %>%  # group rows by hospital
  slice_max(date, n = 1, with_ties = F) # if there's a tie (of date), take the first row
```

...bây giờ sẽ được viết như thế này:

```{r, eval=F}
linelist %>% 
  group_by(hospital) %>% # group rows by hospital
  slice_max(
    date,                # keep row per group with maximum date value 
    n = 1,               # keep only the single highest row 
    with_ties = F)       # if there's a tie (of date), take the first row
```

Code R thường không bị ảnh hưởng bởi thêm các dòng mới hoặc thụt lề. Khi viết code, nếu bạn xuống dòng ngay sau dấu phẩy thì R sẽ tự động thụt lề cho bạn.

Chúng tôi cũng sử dụng rất nhiều những khoảng cách (ví dụ `n = 1` thay vì `n=1`) vì nó giúp dễ đọc hơn. Hãy văn minh với những người đang đọc code của bạn!

### Danh pháp {.unnumbered}

Trong sổ tay này, chúng tôi thường đề cập đến "cột" và "hàng" thay vì dùng "biến" và "quan sát". Như đã giải thích trong phần sơ lược về ["tidy data"](https://tidyr.tidyverse.org/articles/tidy-data.html), hầu hết các bộ dữ liệu thống kê dịch tễ học bao gồm các hàng, cột và giá trị theo cấu trúc

*Biến số* chứa các giá trị đo lường của cùng một thuộc tính (như nhóm tuổi, kết cục hoặc ngày khởi phát). *Các quan sát* bao gồm tất cả các giá trị được đo trên cùng một đơn vị (ví dụ: người, địa điểm hoặc mẫu phòng thí nghiệm). Vì vậy, những khía cạnh này có thể khó được định nghĩa một cách cụ thể.

Trong một bộ dữ liệu "tidy", mỗi cột là một biến số, mỗi hàng là một quan sát và mỗi ô là một giá trị duy nhất. Tuy nhiên, bạn có thể gặp một số bộ dữ liệu không phù hợp với quy luật này - bộ dữ liệu định dạng "ngang" có thể có một biến số được chia thành nhiều cột (xem ví dụ trong chương Pivoting dữ liệu). Tương tự như vậy, các quan sát có thể được trải thành nhiều hàng.

Phần lớn cuốn sách này tập trung vào quản lý và biến đổi dữ liệu, vì vậy việc đề cập đến cấu trúc dữ liệu cụ thể của các hàng và cột sẽ liên quan hơn là đề cập tới các khái niệm trừu tượng như các quan sát và biến. Các trường hợp ngoại lệ chủ yếu xảy ra trong các chương về phân tích dữ liệu, ở đó chúng tôi đề cập nhiều hơn đến các biến số và quan sát.

### Lưu ý {.unnumbered}

Dưới đây là một vài lưu ý bạn có thể gặp trong cuốn sách:

[***GHI CHÚ:*** Đây là ghi chú]{style="color: black;"}\
[***MẸO:*** Đây là mẹo.]{style="color: darkgreen;"}\
[***CẨN TRỌNG:*** Đây là ghi chú cẩn trọng.]{style="color: orange;"}\
[***NGUY HIỂM:*** Đây là một cảnh báo.]{style="color: red;"}

## Quyết định biên tập

Dưới đây, chúng tôi ghi lại các quyết định biên tập quan trọng về việc lựa chọn package và hàm. Nếu bạn không đồng ý hoặc muốn đưa ra một công cụ mới để xem xét, vui lòng tham gia/bắt đầu cuộc thảo luận trên [Trang Github](https://github.com/epirhandbook/Epi_R_handbook) của chúng tôi.

**Bảng các package, hàm, và các quyết định biên tập khác**

+---------------------------+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+
| Chủ đề                    | Cân nhắc                                                                       | Lựa chọn                                                                                                           | Lý do ngắn gọn                                                                       |
+===========================+================================================================================+====================================================================================================================+======================================================================================+
| Phương pháp code chung    | **tidyverse**, **data.table**, **base**                                        | **tidyverse**, với 1 chương về **data.table**, các giải pháp thay thế từ **base** R cho người đọc không có internet | **tidyverse** dễ đọc, phổ biến, được dạy nhiều nhất                                  |
+---------------------------+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+
| Gọi Package               | `library()`,`install.packages()`, `require()`, **pacman**                      | **pacman**                                                                                                         | Rút ngắn và đơn giản hóa code cho hầu hết các trường hợp cài đặt / tải nhiều package |
+---------------------------+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+
| Nhập và xuất              | **rio**, và các package khác                                                   | **rio**                                                                                                            | Dễ dàng cho nhiều kiểu file                                                          |
+---------------------------+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+
| Nhóm để tổng hợp thống kê | **dplyr** `group_by()`, **stats** `aggregate()`                                | **dplyr** `group_by()`                                                                                             | Thống nhất với **tidyverse**                                                         |
+---------------------------+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+
| Pivoting                  | **tidyr** (các hàm pivot), **reshape2** (melt/cast), **tidyr** (spread/gather) | **tidyr** (các hàm pivot)                                                                                          | **reshape2** đã nghỉ hưu **tidyr** sử dụng các hàm pivot ở phiên bản v1.0.0          |
+---------------------------+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+
| Làm sạch tên cột          | **linelist**, **janitor**                                                      | **janitor**                                                                                                        | Hợp nhất các package được nhắc đến                                                   |
+---------------------------+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+
| Epiweeks                  | **lubridate**, **aweek**, **tsibble**, **zoo**                                 | thông thường là **lubridate** ,các package khác tùy trường hợp cụ thể                                             | **lubridate** dễ đọc, có tính nhất quán, và triển vọng bảo trì gói                  |
+---------------------------+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+
| Nhãn ggplot               | `labs()`, `ggtitle()`/`ylab()`/`xlab()`                                        | `labs()`                                                                                                           | tất cả các nhãn ở một nơi, đơn giản                                                  |
+---------------------------+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+
| Chuyển sang factor        | `factor()`, **forcats**                                                        | **forcats**                                                                                                        | các hàm khác nhau của nó cũng chuyển đổi thành factor trong cùng một lệnh            |
+---------------------------+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+
| Đường cong dịch bệnh      | **incidence**, **ggplot2**, **EpiCurve**                                       | **incidence2** thì nhanh, **ggplot2** thì chi tiết                                                                 | tùy theo                                                                             |
+---------------------------+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+
| Sự kết hợp                | `paste()`, `paste0()`, `str_glue()`, `glue()`                                  | `str_glue()`                                                                                                       | Nhiều cú pháp đơn giản hơn hàm paste; nằm bên trong **stringr**                      |
+---------------------------+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+

## Các bản sửa đổi chính

| Ngày              | Thay đổi chính            |
|-------------------|---------------------------|
| 10 Tháng Năm 2021 | Phát hành phiên bản 1.0.0 |

## Thông tin phiên làm việc (R, RStudio, packages)

Dưới đây là thông tin về các phiên bản của các R package, RStudio và R được sử dụng trong quá trình rendering cuốn sách này.

```{r}
sessioninfo::session_info()
```
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/editorial_style.Rmd-->

# Tải sách và dữ liệu {#data-used}

<!-- Note to self: If you want to create a download link to Github, right-click the "View Raw" button on Github, copy the address, and use that in the HTML below. -->

## Tải sách ngoại tuyến

Bạn có thể tải xuống phiên bản ngoại tuyến của sổ tay này dưới dạng tệp HTML để có thể xem trong trình duyệt web của mình ngay cả khi bạn không có kết nối internet. Nếu bạn đang cân nhắc việc sử dụng ngoại tuyến Sổ tay Epi R, dưới đây là một số điều bạn cần cân nhắc:

-   Khi bạn mở tệp, có thể mất một đến hai phút để tải các hình ảnh và mục lục\
-   Phiên bản ngoại tuyến có bố cục hơi khác so với phiên bản trực tuyến - là một trang rất dài với Mục lục ở phía bên trái. Để tìm kiếm các cụm từ cụ thể, hãy sử dụng Ctrl + F (Cmd-f)\
-   Xem chương [Package đề xuất] để hỗ trợ bạn cài đặt các R package thích hợp trước khi bạn mất kết nối internet\
-   Cài đặt package **epirhandbook** của chúng tôi trong đó chứa tất cả các dữ liệu minh họa (quy trình cài đặt được mô tả bên dưới)

**Có hai cách bạn có thể tải xuống sổ tay:**

### Sử dụng link download {.unnumbered}

Để truy cập nhanh, **nháy phải chuột** vào [link này](https://github.com/epirhandbook/Epi_R_handbook/raw/master/offline_long/Epi_R_Handbook_offline.html) **và lựa chọn "Save link as"**.

Nếu trên máy Mac, hãy sử dụng Cmd + Nhấp chuột. Nếu trên điện thoại di động, hãy bấm và giữ liên kết và chọn "Save link". Sổ tay sẽ tải xuống thiết bị của bạn. Nếu trên màn hình xuất hiện mã HTML gốc, hãy đảm bảo bạn đã làm đúng theo các hướng dẫn bên trên hoặc thử Phương án 2.

### Sử dụng package của chúng tôi {.unnumbered}

Chúng tôi cung cấp một R package có tên là **epirhandbook**. Nó bao gồm một hàm có tên `download_book()` giúp bạn tải xuống sổ tay này từ kho Github của chúng tôi vào máy tính của bạn.

Package này cũng chứa hàm `get_data()` giúp tải xuống toàn bộ các dữ liệu minh họa vào máy tính của bạn.

Chạy dòng code sau để cài đặt package **epirhandbook** từ [Github repository *appliedepi*](https://github.com/appliedepi/epirhandbook). Đây không phải là package thuộc CRAN, do đó cần sử dụng hàm đặc biệt `p_install_gh()` để cài đặt nó từ Github.

```{r, eval=F}
# install the latest version of the Epi R Handbook package
pacman::p_install_gh("appliedepi/epirhandbook")
```

Bây giờ, bạn gọi package để sử dụng cho phiên làm việc R hiện tại:

```{r, eval=F}
# load the package for use
pacman::p_load(epirhandbook)
```

Tiếp theo, bạn chạy hàm `download_book()` (phần trong ngoặc bỏ trống) để tải sổ tay vào máy tính của bạn. Nếu bạn sử dụng RStudio, một cửa sổ sẽ xuất hiện cho phép bạn lựa chọn thư mục lưu trữ.

```{r, eval=F}
# download the offline handbook to your computer
download_book()
```

## Tải dữ liệu xuống để tiện theo dõi

Để "tiện theo dõi" cùng với sổ tay này, bạn có thể tải xuống các bộ dữ liệu minh họa và các kết quả.

### Sử dụng package của chúng tôi {.unnumbered}

Cách dễ nhất để tải xuống tất cả dữ liệu là cài đặt package **epirhandbook** của chúng tôi. Nó chứa hàm `get_data()` giúp lưu toàn bộ dữ liệu minh họa vào một thư mục bạn chọn trên máy tính của mình.

Để cài đặt package **epirhandbook**, bạn chạy theo code dưới đây. Lưu ý là package này không từ CRAN, do đó cần sử dụng hàm `p_install_gh()` để cài đặt. Thông tin đầu vào sẽ được chuyển tới trang Github của chúng tôi ("*appliedepi*") và package **epirhandbook**.

```{r, eval=F}
# install the latest version of the Epi R Handbook package
pacman::p_install_gh("appliedepi/epirhandbook")
```

Bây giờ, bạn gọi package để sử dụng cho phiên làm việc hiện tại:

```{r, eval=F}
# load the package for use
pacman::p_load(epirhandbook)
```

Tiếp theo, sử dụng hàm `get_data()` trong package để tải dữ liệu minh họa và máy tính của bạn. Chạy hàm `get_data("all")` để tải *toàn bộ* dữ liệu minh họa, hoặc bạn có thể nêu tên một tệp cụ thể và phần mở rộng bên trong dấu ngoặc kép để tải một tệp duy nhất.

Dữ liệu sẽ được tải xuống cùng với package và bạn đơn giản chỉ cần lưu nó vào một thư mục trên máy tính của bạn. Một cửa sổ sẽ xuất hiện, cho phép bạn chọn vị trí lưu thư mục. Chúng tôi khuyên bạn nên tạo một thư mục mới tên là "data" vì có khoảng 30 tệp (bao gồm các bộ dữ liệu minh họa và kết quả).

```{r, eval=F}
# download all the example data into a folder on your computer
get_data("all")

# download only the linelist example data into a folder on your computer
get_data(file = "linelist_cleaned.rds")

```

```{r, eval=F}
# download a specific file into a folder on your computer
get_data("linelist_cleaned.rds")
```

Khi bạn dùng hàm `get_data()` để lưu tệp dữ liệu vào máy tính của mình, bạn sẽ vẫn cần nhập dữ liệu vào R. Xem chương [Nhập xuất dữ liệu] để biết thêm chi tiết.

Nếu bạn muốn, bạn có thể xem toàn bộ dữ liệu sử dụng trong cuốn sách này ở [**thư mục "dữ liệu"**](https://github.com/epirhandbook/Epi_R_handbook/tree/master/data) trong kho Github của chúng tôi.

### Tải từng thứ một {.unnumbered}

Tùy chọn này liên quan đến việc tải xuống từng tệp dữ liệu từ kho lưu trữ Github của chúng tôi thông qua liên kết hoặc lệnh R dành riêng cho từng tệp. Một số loại tệp cho phép nút tải xuống, trong khi những loại khác có thể được tải xuống thông qua lệnh R.

#### Dữ liệu linelist {.unnumbered}

Đây là số liệu bùng phát Ebola giả định, được nhóm tác giả cẩm nang mở rộng từ bộ dữ liệu thực hành `ebola_sim` trong package **outbreaks**.

-   <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_raw.xlsx' class='download-button'>Bấm để tải xuống dữ liệu "thô" linelist (.xlsx)</span></a>. Bộ dữ liệu "thô" là một trang tính Excel với dữ liệu lộn xộn. Sử dụng số liệu này trong chương [Làm sạch số liệu và các hàm quan trọng].

-   <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>Bấm để tải xuống dữ liệu "đã làm sạch" linelist (.rds)</a>. Sử dụng tệp này cho tất cả các chương khác trong sổ tay có sử dụng bộ dữ liệu linelist. Tệp mở rộng .rds là một kiểu file của R có khả năng lưu trữ các thông tin cột. Điều này đảm bảo bạn sẽ có ít việc phải làm khi làm sạch số liệu sau khi nhập số liệu vào R.

*Các tệp liên quan khác:*

-   <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.xlsx' class='download-button'>Bấm để tải xuống dữ liệu "đã làm sạch" linelist dưới dạng tệp Excel</a>

-   Một phần của chương làm sạch sử dụng "từ điển làm sạch" (tệp .csv). Bạn có thể tải nó trực tiếp vào R bằng cách chạy các lệnh sau:

```{r, eval=F}
pacman::p_load(rio) # install/load the rio package

# import the file directly from Github
cleaning_dict <- import("https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/cleaning_dict.csv")
```

#### Dữ liệu số trường hợp sốt rét {#data_malaria .unnumbered}

Đây là số liệu giả định về số lượng trường hợp sốt rét theo nhóm tuổi, cơ sở điều trị và ngày. Tệp mở rộng .rds là một kiểu file của R có khả năng lưu trữ các thông tin cột. Điều này đảm bảo bạn sẽ có ít việc phải làm khi làm sạch số liệu sau khi nhập số liệu vào R.

<a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/malaria_facility_count_data.rds' class='download-button'> Bấm để tải file dữ liệu sốt rét (.rds file) </a>

#### Dữ liệu thang đo Likert {.unnumbered}

Đây là dữ liệu giả định từ một cuộc khảo sát sử dụng thang đo Likert, được sử dụng trong chương [Tháp dân số và thang đo Likert]. Bạn có thể tải những dữ liệu này trực tiếp vào R bằng cách chạy các lệnh sau:

```{r, eval=F}
pacman::p_load(rio) # install/load the rio package

# import the file directly from Github
likert_data <- import("https://raw.githubusercontent.com/nsbatra/Epi_R_handbook/master/data/likert_data.csv")
```

#### Flexdashboard {.unnumbered}

Dưới đây là các liên kết đến tệp được dùng trong chương [Dashboards với R Markdown]:

-   Để tải xuống R Markdown dashboard về một đợt bùng phát dịch, bấm phải chuột vào [link](https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/flexdashboard/outbreak_dashboard.Rmd) này (Cmd+click đối với Mac) và chọn "Save link as".\
-   Để tải xuống HTML dashboard, bấm phải chuột vào [link](https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/flexdashboard/outbreak_dashboard_test.html) này (Cmd+click đối với Mac) và chọn "Save link as".

#### Truy vết tiếp xúc {.unnumbered}

Chương [Truy vết tiếp xúc] trình bày phân tích dữ liệu truy vết tiếp xúc, sử dụng dữ liệu minh họa từ [Go.Data](https://github.com/WorldHealthOrganization/godata/tree/master/analytics/r-reporting). Dữ liệu được sử dụng trong chương này có thể được tải xuống dưới dạng tệp .rds bằng cách bấm vào các liên kết sau:

<a href='https://github.com/WorldHealthOrganization/godata/blob/master/analytics/r-reporting/data/cases_clean.rds?raw=true' class='download-button'> Bấm để tải xuống dữ liệu điều tra trường hợp (.rds file) </a>

<a href='https://github.com/WorldHealthOrganization/godata/blob/master/analytics/r-reporting/data/contacts_clean.rds?raw=true' class='download-button'> Bấm để tải xuống dữ liệu ghi nhận tiếp xúc (.rds file) </a>

<a href='https://github.com/WorldHealthOrganization/godata/blob/master/analytics/r-reporting/data/followups_clean.rds?raw=true' class='download-button'> Bấm để tải xuống dữ liệu theo dõi liên hệ (.rds file) </a>

[***LƯU Ý:*** Dữ liệu truy vết tiếp xýc có cấu trúc từ phần mềm khác (ví dụ: KoBo, DHIS2 Tracker, CommCare) có thể sẽ khác. Nếu bạn muốn đóng góp dữ liệu hoặc nội dung mẫu thay thế cho trang này, vui lòng [liên hệ chúng tôi](#contact_us).]{style="color: black;"}

[***MẸO:*** Nếu bạn đang triển khai Go.Data và muốn kết nối với API phiên bản của bạn, vui lòng xem chương Nhập xuất dữ liệu [(mục API)](#import_api) và [Go.Data Cộng đồng thực hành](https://community-godata.who.int/).]{style="color: darkgreen;"}

#### GIS {.unnumbered}

Shapefiles có nhiều tệp thành phần phụ, mỗi tệp có một phần mở rộng tệp khác nhau. Một tệp sẽ có phần mở rộng ".shp", nhưng những tệp khác có thể là ".dbf", ".prj", v.v.

Chương [GIS cơ bản] cung cấp các liên kết đến trang web *Humanitarian Data Exchange*, nơi bạn có thể tải xuống trực tiếp các shapefiles dưới dạng tệp nén.

Ví dụ, dữ liệu phân bố của các cơ sở y tế có thể được tải xuống [tại đây](https://data.humdata.org/dataset/hotosm_sierra_leone_health_facilities). Bạn tải tệp "hotosm_sierra_leone_health_facilities_points_shp.zip". Sau khi được lưu vào máy tính của bạn, hãy “giải nén” thư mục. Bạn sẽ thấy một số tệp có các phần mở rộng khác nhau (ví dụ: “.shp”, “.prj”, “.shx”) - tất cả những tệp này phải được lưu vào cùng một thư mục trên máy tính của bạn. Sau đó, để nhập vào R, hãy cung cấp đường dẫn đến tệp và tên của tệp “.shp” bằng hàm `st_read()` từ package **sf** (đã được mô tả trong chương [GIS cơ bản]).

Nếu bạn làm theo Cách 1 để tải xuống tất cả dữ liệu minh họa (thông qua package **epirhandbook** của chúng tôi), tất cả các shapefiles đã được bao gồm.

Ngoài ra, bạn có thể tải xuống các shapefiles từ thư mục "data" trên trang R Handbook Github (xem thư mục con "gis"). Tuy nhiên, cần lưu ý rằng bạn sẽ phải tải *từng* tệp con xuống máy tính của mình. Trong Github, nhấp vào từng tệp riêng lẻ và tải chúng xuống bằng cách nhấp vào nút “Download”. Xem hình minh họa dưới đây, bạn có thể thấy shapefile “sl_adm3” bao gồm nhiều tệp con như thế nào - và mỗi tệp đều cần được tải xuống từ Github.

```{r out.height = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "download_shp.png"))
```

#### Cây phả hệ {.unnumbered}

Xem chương [Cây phả hệ]. Tệp có tên Newick về cây phả hệ được xây dựng từ việc giải trình tự toàn bộ bộ gen của 299 mẫu Shigella sonnei và dữ liệu mẫu tương ứng (được chuyển đổi thành tệp văn bản). Các mẫu và kết quả từ nước Bỉ được cung cấp thông qua Trung tâm tham khảo quốc gia về Salmonella và Shigella (NRC Bỉ) trong phạm vi dự án do EUPHEM Fellow của ECDC thực hiện, và cũng sẽ được xuất bản dưới dạng bản thảo. Dữ liệu quốc tế được cung cấp công khai trên cơ sở dữ liệu công cộng (ncbi) và đã được xuất bản trước đó.

-   Để tải xuống file cây phả hệ “Shigella_tree.txt”, nhấn chuột phải vào [link này](https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/phylo/Shigella_tree.txt) (Cmd+click đối với Mac) và chọn "Save link as".\
-   Để tải xuống file "sample_data_Shigella_tree.csv" với thông tin bổ sung cho từng mẫu, nhấn chuột phải vào [link này](https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/phylo/sample_data_Shigella_tree.csv) (Cmd+click đối với Mac) và chọn "Save link as".\
-   Để xem subset-tree mới được tạo, nhấn chuột phải vào [link này](https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/phylo/Shigella_subtree_2.txt) (Cmd+click đối với Mac) và chọn "Save link as". Tệp .txt sẽ được tải xuống máy tính của bạn.

Sau đó bạn có thể nhập tệp .txt files bằng hàm `read.tree()` từ **ape** package, như đã được trình bày trong chương này.

```{r, eval=F}
ape::read.tree("Shigella_tree.txt")
```

#### Chuẩn hóa {.unnumbered}

Xem trong chương [Tỷ lệ chuẩn hóa]. Bạn có thể tải dữ liệu trực tiếp từ kho lưu trữ Github của chúng tôi trên internet vào phiên làm việc R của bạn bằng các lệnh sau :

```{r, eval=F}
# install/load the rio package
pacman::p_load(rio) 

##############
# Country A
##############
# import demographics for country A directly from Github
A_demo <- import("https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/standardization/country_demographics.csv")

# import deaths for country A directly from Github
A_deaths <- import("https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/standardization/deaths_countryA.csv")

##############
# Country B
##############
# import demographics for country B directly from Github
B_demo <- import("https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/standardization/country_demographics_2.csv")

# import deaths for country B directly from Github
B_deaths <- import("https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/standardization/deaths_countryB.csv")


###############
# Reference Pop
###############
# import demographics for country B directly from Github
standard_pop_data <- import("https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/standardization/world_standard_population_by_sex.csv")
```

#### Chuỗi thời gian và phát hiện ổ dịch {#data_outbreak .unnumbered}

Xem trong chương [Chuỗi thời gian và phát hiện ổ dịch](#data_outbreak). Chúng tôi sử dụng các trường hợp campylobacter được báo cáo ở Đức từ 2002-2011, có sẵn từ package **surveillance** của R. (*lưu ý.* tập dữ liệu này đã được điều chỉnh từ bản gốc, trong đó 3 tháng dữ liệu cuối năm 2011 đã bị xóa để dùng với mục đích minh họa)

<a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/time_series/campylobacter_germany.xlsx' class='download-button'> Bấm để tải xuống dữ liệu Campylobacter ở Đức (.xlsx) </a>

Chúng tôi cũng sử dụng dữ liệu khí hậu ở Đức từ 2002-2011 (nhiệt độ tính bằng độ C và lượng mưa tính bằng milimet). Dữ liệu được tải xuống từ tập dữ liệu phân tích vệ tinh Copernicus của EU bằng cách sử dụng package **ecmwfr** . Bạn sẽ cần tải xuống tất cả những thứ này và nhập chúng vào R bằng hàm `stars::read_stars()` như đã được giải thích trong chương chuỗi thời gian.

<a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/time_series/weather/germany_weather2002.nc' class='download-button'> Bấm để tải dữ liệu thời tiết ở Đức 2002 (.nc file) </a>

<a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/time_series/weather/germany_weather2003.nc' class='download-button'> Bấm để tải dữ liệu thời tiết ở Đức 2003 (.nc file) </a>

<a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/time_series/weather/germany_weather2004.nc' class='download-button'> Bấm để tải dữ liệu thời tiết ở Đức 2004 (.nc file) </a>

<a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/time_series/weather/germany_weather2005.nc' class='download-button'> Bấm để tải dữ liệu thời tiết ở Đức 2005 (.nc file) </a>

<a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/time_series/weather/germany_weather2006.nc' class='download-button'> Bấm để tải dữ liệu thời tiết ở Đức 2006 (.nc file) </a>

<a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/time_series/weather/germany_weather2007.nc' class='download-button'> Bấm để tải dữ liệu thời tiết ở Đức 2007 (.nc file) </a>

<a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/time_series/weather/germany_weather2008.nc' class='download-button'> Bấm để tải dữ liệu thời tiết ở Đức 2008 (.nc file) </a>

<a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/time_series/weather/germany_weather2009.nc' class='download-button'> Bấm để tải dữ liệu thời tiết ở Đức 2009 (.nc file) </a>

<a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/time_series/weather/germany_weather2010.nc' class='download-button'> Bấm để tải dữ liệu thời tiết ở Đức 2010 (.nc file) </a>

<a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/time_series/weather/germany_weather2011.nc' class='download-button'> Bấm để tải dữ liệu thời tiết ở Đức 2011 (.nc file) </a>

#### Phân tích sống còn {#data_survey .unnumbered}

Đối với chương [phân tích sống còn](https://epirhandbook.com/survey-analysis.html), chúng tôi sử dụng dữ liệu khảo sát tử vong giả định dựa trên mẫu khảo sát của MSF OCA. Dữ liệu giả định này là một phần của [Dự án "R4Epis"](https://r4epis.netlify.app/).

<a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/surveys/survey_data.xlsx' class='download-button'> Bấm để tài xuống dữ liệu khảo sát giả định (.xlsx) </a>

<a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/surveys/survey_dict.xlsx' class='download-button'> Bấm để tài xuống từ điển dữ liệu khảo sát giả định (.xlsx) </a>

<a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/surveys/population.xlsx' class='download-button'> Bấm để tài xuống dữ liệu khảo sát quần thể giả định (.xlsx) </a>

#### Shiny {#data_shiny .unnumbered}

Chương [Dashboards với Shiny] trình diễn việc xây dựng một ứng dụng đơn giản để hiển thị dữ liệu bệnh sốt rét.

Để tải xuống các tệp R dùng để tạo thành ứng dụng Shiny:

Bạn có thể <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/malaria_app/app.R' class='download-button'> bấm vào đây để tải xuống tệp app.R trong đó chứa code của cả UI và Server của ứng dụng Shiny.</a>

Bạn có thể <a href='https://github.com/epirhandbook/Epi_R_handbook/blob/master/data/malaria_app/data/facility_count_data.rds' class='download-button'> bấm vào đây để tải tệp facility_count_data.rds<span></a> có chứa dữ liệu sốt rét cho ứng dụng Shiny. Lưu ý rằng bạn có thể cần phải lưu trữ nó trong thư mục “data” để các đường dẫn tệp here () hoạt động chính xác.

Bạn có thể <a href='https://github.com/epirhandbook/Epi_R_handbook/blob/master/data/malaria_app/global.R' class='download-button'> bấm vào đây để tải tệp global.R<span></a> mà sẽ được chạy trước khi mở ứng dụng, như đã được giải thích trong chương.

Bạn có thể <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/malaria_app/funcs/plot_epicurve.R' class='download-button'> bấm vào đây để tải tệp plot_epicurve.R<span></a> có nguồn từ tệp global.R. Lưu ý rằng bạn có thể cần phải lưu trữ nó trong thư mục “funcs” để các đường dẫn tệp here () hoạt động chính xác.
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/data_used.Rmd-->

# (PART) Nhập môn về R {.unnumbered}
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/cat_basics.Rmd-->

# R Cơ bản {#basics}

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "basics_header_close.png"))
```

Chào mừng bạn!

Chương này sẽ mô tả những kỹ thuật cơ bản của R. Đây không phải là hướng dẫn sử dụng toàn diện, nhưng sẽ cung cấp những kiến thức cơ bản và có thể hữu ích trong việc làm mới hiểu biết của bạn. Phần [Tài nguyên học liệu](#learning) sẽ liên kết đến những hướng dẫn sử dụng R bao quát hơn.

Các phần của chương này đã được điều chỉnh dưới sự cho phép của [Dự án R4Epis](https://r4epis.netlify.app/).

Xem chương [Chuyển đổi sang R] để biết các mẹo khi chuyển đổi từ STATA, SAS hoặc Excel sang R.

```{r, echo=F}
# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
pacman::p_load(apyramid)
```

<!-- ======================================================= -->

## Tại sao sử dụng R?

Như đã được công bố trên [trang web dự án R](https://www.r-project.org/about.html), R là một ngôn ngữ và môi trường lập trình cho tính toán và đồ họa thống kê. Nó rất linh hoạt, có khả năng mở rộng và hướng tới cộng đồng.

**Chi phí**

R được sử dụng miễn phí! Có sự tồn tại mạnh mẽ về đạo đức trong cộng đồng người sử dụng nguồn tài nguyên mở và miễn phí.

**Khả năng tái lập**

Thực hiện quản lý và phân tích dữ liệu của bạn thông qua một ngôn ngữ lập trình (so sánh với Excel hoặc công cụ khác mà thao tác chính là nhấp chuột/thao tác thủ công) giúp nâng cao **khả năng tái lập**, giúp **phát hiện lỗi** dễ dàng hơn và giảm bớt khối lượng công việc của bạn.

**Cộng đồng**

R có cộng đồng người dùng khổng lồ và có tính hợp tác. Các package và công cụ mới nhằm giải quyết các vấn đề thực tế được phát triển hàng ngày và được kiểm tra bởi cộng đồng người dùng. Ví dụ, [R-Ladies](https://rladies.org/) là một tổ chức toàn cầu có sứ mệnh thúc đẩy sự đa dạng giới trong cộng đồng R và là một trong những tổ chức người dùng R lớn nhất. Thậm chí có thể có một phần của nhóm này đang ở gần bạn!

## Các thuật ngữ chính

**RStudio** - RStudio là Giao diện đồ họa người dùng (GUI) giúp sử dụng **R** dễ dàng hơn. Đọc thêm [trong mục RStudio](#rstudio).

**Đối tượng** - Bao gồm mọi thứ bạn lưu trữ trong R - bộ dữ liệu, biến, danh sách tên làng, quy mô dân số, thậm chí cả các kết quả đầu ra như đồ thị - là các *đối tượng* được *gán tên* và *có thể được tham chiếu* trong các lệnh sau này. Đọc thêm [trong mục Đối tượng](#objects).

**Hàm** - Mỗi hàm là một code hoạt động mà chấp nhận dữ liệu đầu vào và trả về kết quả đầu ra đã được biến đổi. Đọc thêm [trong mục Các hàm](#functions).

**Packages** - Mỗi package R là một gói câu lệnh có khả năng chia sẻ. Đọc thêm [trong mục Packages](#packages).

**Scripts** - Mỗi script là một tệp tài liệu chứa các lệnh của bạn. Đọc thêm [trong mục Scripts](#scripts)

## Tài nguyên học liệu {#learning}

### Tài nguyên trong RStudio {.unnumbered}

**Tài liệu trợ giúp**

Tìm kiếm tab "Help" của RStudio về tài liệu liên quan đến package R và các hàm cụ thể. Tab này nằm trong cửa sổ chứa các tab Files, Plots và Packages (thường ở cửa sổ phía dưới bên phải). Như một lối tắt, bạn cũng có thể nhập tên của một package hoặc câu lệnh vào R Console sau dấu hỏi chấm và không bao gồm dấu ngoặc đơn để mở trang trợ giúp liên quan.

Ví dụ: `?filter` hoặc `?diagrammeR`.

**Các hướng dẫn có sự tương tác**

R có thể được học thông qua một số tương tác *trong* RStudio.

RStudio cung cấp một cửa sổ Tutorial được hỗ trợ bởi package [**learnr**](https://blog.rstudio.com/2020/02/25/rstudio-1-3-integrated-tutorials/). Chỉ cần cài đặt package này và mở hướng dẫn qua tab "Tutorial" trong cửa sổ RStudio phía trên bên phải (cũng chứa các tab Environment và History).

Package [**swirl**](https://swirlstats.com/) cung cấp các nội dung học tương tác trong R Console. Cài đặt và tải package này, rồi chạy lệnh `swirl()` (dấu ngoặc đơn trống) trong R Console. Bạn sẽ thấy các thông báo xuất hiện trong cửa sổ Console. Phản hồi bằng cách nhập vào Console. Nó sẽ hướng dẫn bạn qua một nội dung học do bạn lựa chọn.

### Cheatsheets {.unnumbered}

Có rất nhiều "cheatsheets" PDF có sẵn trên [trang web của RStudio](https://rstudio.com/resources/cheatsheets/), ví dụ như:

-   Factors với package **forcats**\
-   Ngày và thời gian với package **lubridate**\
-   Chuỗi với package **stringr**\
-   Các vòng lặp với package **purrr**\
-   Nhập dữ liệu\
-   Cheatsheet biến đổi dữ liệu với package **dplyr**\
-   R Markdown (để tạo các tài liệu như PDF, Word, Powerpoint...)\
-   Shiny (để xây dựng các ứng dụng web tương tác)\
-   Trực quan hóa dữ liệu với package **ggplot2**\
-   Bản đồ học (GIS)\
-   Package **leaflet** (bản đồ tương tác)\
-   Python với R (package **reticulate**)

Đây là tài nguyên R trực tuyến dành riêng cho [Người dùng Excel](https://jules32.github.io/r-for-excel-users/)

### Twitter {.unnumbered}

R có một cộng đồng twitter sôi động, nơi bạn có thể tìm hiểu các mẹo, lối tắt và tin tức - hãy theo dõi các tài khoản sau:

-   Theo dõi chúng tôi: [\@epiRhandbook](https://twitter.com/epirhandbook)\
-   R Function A Day [\@rfuntionaday](https://twitter.com/rfunctionaday) là một nguồn tài nguyên *tuyệt vời*
-   R for Data Science [\@rstats4ds](https://twitter.com/rstats4ds?lang=en)\
-   RStudio [\@RStudio](https://twitter.com/rstudio?lang=en)\
-   RStudio Tips [\@rstudiotips](https://twitter.com/rstudiotips)\
-   R-Bloggers [\@Rbloggers](https://twitter.com/Rbloggers)\
-   R-ladies [\@RLadiesGlobal](https://twitter.com/RLadiesGlobal)\
-   Hadley Wickham [\@hadleywickham](https://twitter.com/hadleywickham?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor)

Cũng như:

**\#epitwitter** và **\#rstats**

### Nguồn tài nguyên trực tuyến miễn phí {.unnumbered}

Cuốn sách [R for Data Science](https://r4ds.had.co.nz/) (R dành cho Khoa học Dữ liệu) của Garrett Grolemund và Hadley Wickham

Trang web của dự án [R4Epis](https://r4epis.netlify.app/) nhằm mục đích "phát triển các công cụ làm sạch, phân tích và báo cáo dữ liệu được chuẩn hóa dùng trong các trường hợp bùng phát dịch phổ biến và các cuộc điều tra dựa trên dân số mà sẽ được tiến hành trong các ứng phó khẩn cấp của Tổ chức Bác sỹ không biên giới" Bạn có thể tìm thấy tài liệu đào tạo cơ bản về R, các mẫu báo cáo RMarkdown và khảo sát về các đợt bùng phát dịch, cũng như các hướng dẫn để giúp bạn thiết lập chúng.

### Các ngôn ngữ khác ngoài Tiếng Anh {.unnumbered}

[Tài liệu RStudio bằng tiếng Tây Ban Nha](https://www.rstudio.com/collections/espanol/)

[Giới thiệu cơ bản về R (tiếng Pháp)](https://juba.github.io/tidyverse/index.html)

<!-- ======================================================= -->

## Cài đặt

### R và RStudio {.unnumbered}

**Làm thế nào để cài đặt R**

Truy cập vào trang web <https://www.r-project.org/> và tải phiên bản mới nhất của R phù hợp với máy tính của bạn.

**Làm thế nào để cài đặt RStudio**

Truy cập vào trang web <https://rstudio.com/products/rstudio/download/> và tải phiên bản Desktop mới nhất của RStudio phù hợp với máy tính của bạn.

**Quyền truy cập**\
Lưu ý rằng bạn nên cài đặt R và RStudio vào một ổ đĩa mà bạn có quyền đọc và ghi lại. Nếu không, khả năng cài đặt các package R (thường xuyên xảy ra) của bạn sẽ bị ảnh hưởng. Nếu bạn gặp sự cố, hãy thử mở RStudio bằng cách nhấp chuột phải vào biểu tượng và chọn "Run as administrator". Các mẹo khác có thể được tìm thấy trong chương [R trên ổ cứng mạng].

**Làm thế nào để cập nhật R và RStudio**

Phiên bản R của bạn được in ra R Console khi khởi động. Bạn cũng có thể chạy lệnh `sessionInfo()`.

Để cập nhật R, truy cập đến trang web được nhắc đến ở trên và cài đặt lại R. Ngoài ra, bạn có thể sử dụng package **installr** (trên Windows) bằng cách chạy câu lệnh `installr::updateR()`. Thao tác này sẽ mở ra các hộp thoại giúp bạn tải xuống phiên bản R mới nhất và cập nhật các package của bạn lên phiên bản R mới. Có thể tìm thấy thêm chi tiết trong [tài liệu](https://www.r-project.org/nosvn/pandoc/installr.html) **installr** .

Lưu ý rằng phiên bản R cũ sẽ vẫn tồn tại trong máy tính của bạn. Bạn có thể tạm thời chạy phiên bản cũ hơn ("installation" cũ hơn) của R bằng cách nhấp vào "Tools" -\> "Global Options" trong RStudio và chọn một phiên bản R. Điều này có thể hữu ích nếu bạn muốn sử dụng một package chưa được cập nhật để hoạt động trên phiên bản R mới nhất.

Để cập nhật RStudio, truy cập đến trang web được nhắc đến ở trên và cài đặt lại RStudio. Một tùy chọn khác là nhấp vào "Help" -\> "Check for Updates" trong RStudio, nhưng điều này có thể dẫn đến việc không hiển thị các bản cập nhật mới nhất.

Để xem phiên bản R, RStudio hoặc package nào đã được sử dụng khi viết Sổ tay này, hãy xem chương [Biên tập và ghi chú kỹ thuật].

### Những phần mềm khác bạn *có thể* cần cài đặt {.unnumbered}

-   TinyTeX (*để biên dịch tài liệu RMarkdown sang PDF*)\
-   Pandoc (*để biên dịch tài liệu RMarkdown*)\
-   RTools (*để xây dựng các package cho R*)\
-   phantomjs (*để lưu ảnh tĩnh của mạng động, chẳng hạn như chuỗi lây truyền*)

#### TinyTex {.unnumbered}

TinyTex là một bản phân phối LaTeX tùy chỉnh, hữu ích khi tạo các tệp PDF từ R.\
Truy cập <https://yihui.org/tinytex/> để tìm hiểu thêm thông tin.

Để cài đặt TinyTex từ R:

```{r, eval=F}
install.packages('tinytex')
tinytex::install_tinytex()
# to uninstall TinyTeX, run tinytex::uninstall_tinytex()
```

#### Pandoc {.unnumbered}

Pandoc là một công cụ chuyển đổi văn bản, một phần mềm tách biệt với R. **Nó đi kèm với RStudio và không cần phải tải xuống.** Nó hỗ trợ quá trình chuyển đổi văn bản từ Rmarkdown sang các định dạng như .pdf và có bổ sung thêm một số tính năng phức tạp.

#### RTools {.unnumbered}

RTools là một phần mềm được sử dụng để xây dựng package cho R

Cài đặt từ trang web: <https://cran.r-project.org/bin/windows/Rtools/>

#### phantomjs {.unnumbered}

Phần mềm này thường được sử dụng để chụp "chụp ảnh màn hình" trang web. Ví dụ khi bạn tạo một chuỗi lây truyền với package **epicontacts**, một tệp HTML có thể tương tác và chuyển động được tạo ra. Nếu bạn muốn có hình ảnh tĩnh, sử dụng package [**webshot**](https://wch.github.io/webshot/articles/intro.html) để tự động hóa quá trình này. Việc này sẽ yêu cầu chương trình bên ngoài "phantomjs". Bạn có thể cài đặt phantomjs thông qua package **webshot** bằng lệnh `webshot::install_phantomjs()`.

<!-- ======================================================= -->

## RStudio {#rstudio}

### Làm quen {.unnumbered}

**Đầu tiên, mở RStudio.** Biểu tượng của chúng có sự tương đồng, hãy chắc chắn bạn đang mở *RStudio* chứ không phải R.

Để RStudio hoạt động bạn cũng cần phải cài đặt R trên máy tính (xem hướng dẫn cài đặt ở bên trên).

**RStudio** là một giao diện người dùng (GUI) giúp sử dụng **R** dễ dàng hơn. YBạn có thể coi R như một động cơ đang đảm đương công việc chính của một phương tiện và RStudio là phần thân của phương tiện (với ghế ngồi, các phụ kiện,...) giúp bạn sử dụng động cơ tiến về phía trước. Bạn có thể xem toàn bộ cheatsheet giao diện người dùng của RStudio (PDF) tại [đây](https://www.rstudio.com/wp-content/uploads/2016/01/rstudio-IDE-cheatsheet.pdf)

RStudio mặc định hiển thị bốn cửa sổ hình chữ nhật.

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "RStudio_overview.png"))
```

[***MẸO:*** Nếu RStudio của bạn chỉ hiển thị một cửa sổ bên trái thì đó là do bạn chưa mở scripts nào.]{style="color: black;"}

**Cửa sổ mã nguồn**\
Cửa sổ này, mặc định hiển thị phía trên bên trái, là một khoảng trống để chỉnh sửa, chạy và lưu các [scripts](#scripts) của bạn. Script chứa các lệnh mà bạn muốn chạy. Cửa sổ này cũng có thể hiển thị thông tin dữ liệu (data frames).

Đối với người dùng Stata, cửa sổ này tương tự với các cửa sổ Do-file và Data Editor.

**Cửa sổ R Console**

R Console, mặc định ở cửa sổ bên trái hoặc phía dưới bên trái của RStudio, là ngôi nhà của "động cơ" R. Đây là nơi các lệnh thực sự được chạy, các kết quả đầu ra không phải là đồ họa và các thông báo lỗi/cảnh báo sẽ xuất hiện. Bạn có thể nhập và chạy các lệnh trực tiếp trong R Console, nhưng sẽ sớm nhận ra các lệnh này không được lưu như khi chạy lệnh từ một script.

Nếu bạn đã quen thuộc với Stata, R Console giống như cửa sổ Command Window và Results Window.

**Cửa sổ Environment**\
Cửa sổ này, mặc định ở phía trên bên phải, thường được sử dụng để xem tóm tắt ngắn gọn về các [đối tượng](#objects) itrong R Environment ở phiên hiện tại. Các đối tượng này có thể bao gồm các tập dữ liệu đã được nhập, chỉnh sửa hoặc tạo mới, các tham số bạn đã xác định (ví dụ: một tuần dịch tễ cụ thể để phân tích), vectơ hoặc các danh sách bạn đã xác định trong quá trình phân tích (ví dụ: tên các vùng). Bạn có thể nhấp vào mũi tên bên cạnh tên của data frames để xem các biến số của nó.

Cửa sổ này gần giống với cửa sổ Variables Manager trong Stata.

Cửa sổ này cũng chứa *History* - nơi mà bạn có thể xem các lệnh đã làm trước đó. Nó cũng có một tab "Tutorial" - là nơi mà bạn có thể hoàn thành các hướng dẫn tương tác với R nếu bạn đã cài đặt package **learnr**. Nó cũng chứa một tab "Connections" cho phép các kết nối bên ngoài và có thể có cửa sổ "Git" nếu bạn chọn giao diện với Github.

**Cửa sổ Plots, Viewer, Packages, và Help**\
Cửa sổ phía dưới bên phải bao gồm một số tab quan trọng. Các đồ họa chính điển hình bao gồm bản đồ sẽ được hiển thị trong Cửa sổ Plot. Các kết quả đầu ra tương tác hoặc HTML sẽ được hiển thị trong cửa sổ Viewer. Cửa sổ File là một trình duyệt có thể được sử dụng để mở hoặc xóa tệp. Cửa sổ Packages cho phép bạn xem, cài đặt, cập nhật, xóa, tải/dỡ các package R và xem bạn có phiên bản package nào. Để tìm hiểu thêm về các package hãy xem [mục packages](#packages) bên dưới.

Cửa sổ này chứa các nội dung tương đương với các cửa sổ Plots Manager và Project Manager trong Stata.

### Các cài đặt của RStudio {.unnumbered}

Thay đổi các cài đặt và giao diện của RStudio trong thanh menu thả xuống *Tools*, bằng cách chọn *Global Options*. Ở đó, bạn có thể thay đổi cài đặt mặc định, bao gồm cả màu giao diện/nền.

```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "RStudio_tools_options_1.png"))

knitr::include_graphics(here::here("images", "RStudio_tools_options.png"))
```

**Khởi động lại**

Nếu R của bạn bị treo, bạn có thể khởi động lại R bằng cách di chuột đến menu Session và nhấp vào "Restart R". Thao tác này giúp tránh rắc rối khi đóng và mở RStudio. Mọi thứ trong môi trường R của bạn sẽ bị xóa khi thực hiện thao tác này.

### Các phím tắt {.unnumbered}

Dưới đây là một vài phím tắt rất hữu dụng. Tất cả các phím tắt cho Windows, Max và Linux nằm ở chương 2 [cheatsheet giao diện người dùng](https://www.rstudio.com/wp-content/uploads/2016/01/rstudio-IDE-cheatsheet.pdf) của RStudio.

+----------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------+
| Windows/Linux                    | Mac                    | Hành động                                                                                                                             |
+==================================+========================+=======================================================================================================================================+
| Esc                              | Esc                    | Ngắt lệnh hiện tại (hữu ích nếu bạn vô tình chạy một lệnh chưa hoàn chỉnh và không thể thoát khỏi việc nhìn thấy "+" trong R console) |
+----------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------+
| Ctrl+s                           | Cmd+s                  | Lưu (script)                                                                                                                          |
+----------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------+
| Tab                              | Tab                    | Tự động điền                                                                                                                          |
+----------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------+
| Ctrl + Enter                     | Cmd + Enter            | Chạy (các) dòng/phần code đang được chọn                                                                                              |
+----------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------+
| Ctrl + Shift + C                 | Cmd + Shift + c        | Bình luận/bỏ bình luận các dòng được đánh dấu                                                                                         |
+----------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------+
| Alt + -                          | Option + -             | Chèn `<-`                                                                                                                             |
+----------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------+
| Ctrl + Shift + m                 | Cmd + Shift + m        | Chèn `%>%`                                                                                                                            |
+----------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------+
| Ctrl + l                         | Cmd + l                | Dọn dẹp R console                                                                                                                     |
+----------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------+
| Ctrl + Alt + b                   | Cmd + Option + b       | Chạy từ đầu đến dòng hiện tại                                                                                                         |
+----------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------+
| Ctrl + Alt + t                   | Cmd + Option + t       | Chạy code chunk hiện tại (R Markdown)                                                                                                 |
+----------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------+
| Ctrl + Alt + i                   | Cmd + Shift + r        | Chèn code chunk (vào R Markdown)                                                                                                      |
+----------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------+
| Ctrl + Alt + c                   | Cmd + Option + c       | Chạy code chunk hiện tại (R Markdown)                                                                                                 |
+----------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------+
| up/down arrows in R console      | Tương tự               | Chuyển đổi qua các lệnh đã chạy gần đây                                                                                               |
+----------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------+
| Shift + up/down arrows in script | Tương tự               | Chọn nhiều dòng code                                                                                                                  |
+----------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------+
| Ctrl + f                         | Cmd + f                | Tìm và thay thế trong script hiện tại                                                                                                 |
+----------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------+
| Ctrl + Shift + f                 | Cmd + Shift + f        | Tìm trong các tệp (tìm/thay thế trên nhiều script)                                                                                    |
+----------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------+
| Alt + l                          | Cmd + Option + l       | Thu gọn code đã chọn                                                                                                                  |
+----------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------+
| Shift + Alt + l                  | Cmd + Shift + Option+l | Mở rộng code đã chọn                                                                                                                  |
+----------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------+

[***MẸO:*** Sử dụng phím Tab của bạn khi nhập để sử dụng chức năng tự động hoàn thành của RStudio. Điều này có thể giúp ngăn ngừa các lỗi chính tả. Nhấn Tab trong khi nhập để hiện ra menu thả xuống gồm các hàm và đối tượng có thể có, dựa trên những gì bạn đã nhập.]{style="color: darkgreen;"}

<!-- ======================================================= -->

## Hàm {#functions}

Các hàm là phần cốt lõi của việc sử dụng R. Hàm là cách bạn thực hiện các tác vụ và hoạt động. Nhiều hàm được cài đặt sẵn với R, nhiều hàm khác sẵn sàng để tải xuống trong các *packages* (giải thích trong phần [packages](#packages)), và bạn thậm chí có thể viết các hàm tùy chỉnh của riêng mình!

Phần khái niệm cơ bản của hàm giải thích:

-   Thế nào là một hàm và cách mà chúng hoạt động\
-   Thế nào là *đối số* của hàm\
-   Làm cách nào để nhận được sự trợ giúp khi tìm hiểu một hàm

*Lưu ý nhanh về cú pháp:* Trong cuốn sổ tay này, các hàm được viết dưới dạng code văn bản với dấu mở ngoặc đơn như sau: `filter()`. Như đã giải thích trong phần [packages](#packages), các hàm được tải xuống có sẵn trong các *packages*. Trong sổ tay này, tên các package được **in đậm**, ví dụ như **dplyr**. Đôi khi trong code ví dụ, bạn có thể thấy tên hàm được liên kết rõ ràng với tên package của chính hàm đó bằng hai dấu hai chấm (`::`) như thế này: `dplyr::filter()`. Mục đích của việc liên kết này sẽ được giải thích trong phần package.

<!-- ======================================================= -->

### Các hàm cơ bản {.unnumbered}

**Một hàm giống như một cỗ máy nhận các dữ liệu đầu vào, thực hiện một số thao tác với dữ liệu đó và sản xuất kết quả đầu ra.** Kết quả đầu ra như thế nào phụ thuộc vào hàm mà bạn sử dụng.

**Các hàm thường hoạt động dựa trên các đối tượng được đặt trong dấu ngoặc đơn của hàm**. Ví dụ, hàm `sqrt()` tính căn bậc hai của một số:

```{r basics_function_sqrt}
sqrt(49)
```

Đối tượng được dùng cho một hàm cũng có thể là một cột trong tập dữ liệu (xem phần [Đối tượng](#objects) để biết chi tiết về tất cả các loại đối tượng). Vì R có thể lưu trữ nhiều tập dữ liệu, bạn sẽ cần xác định cả tập dữ liệu và cột. Một cách để làm điều này là sử dụng ký hiệu `$` để liên kết tên của tập dữ liệu và tên của cột (`dataset$column`). Trong ví dụ dưới đây, hàm `summary()` được áp dụng cho cột `age` trong tập dữ liệu `linelist`, và kết quả đầu ra là bản tóm tắt các giá trị số và giá trị missing của cột.

```{r basics_functions_summary}
# Print summary statistics of column 'age' in the dataset 'linelist'
summary(linelist$age)
```

[***LƯU Ý:*** Đằng sau một hàm là hệ thống code bổ sung phức tạp đã được gói gọn cho người dùng thành một lệnh đơn giản.]{style="color: black;"}

<!-- ======================================================= -->

### Hàm với nhiều đối số {.unnumbered}

Các hàm thường yêu cầu một số dữ liệu đầu vào, được gọi là ***đối số***, đặt trong dấu ngoặc đơn của hàm, thường được phân tách bằng dấu phẩy.

-   Một vài đối số là bắt buộc để hàm hoạt động chính xác, những đối số khác là tùy chọn\
-   Những đối số tùy chọn có thiết lập mặc định\
-   Các đối số có thể nhận ký tự, số, logic (TRUE / FALSE) và các dữ liệu đầu vào khác

Dưới đây là một hàm giả định thú vị, được gọi là `oven_bake()`, là ví dụ về một hàm điển hình. Hàm này nhận một đối tượng đầu vào (ví dụ: một tập dữ liệu, hoặc trong ví dụ này là "bột") và thực hiện các hoạt động được xác định bởi các đối số bổ sung (`minutes =` and `temperature =`). Kết quả đầu ra có thể được in ra cửa sổ console hoặc được lưu dưới dạng một đối tượng bằng cách sử dụng toán tử gán `<-`.

```{r basics_functions_image, echo=F, out.width = "75%", fig.align = "center"}
knitr::include_graphics(here::here("images", "Function_Bread_Example.png"))
```

**Trong một ví dụ thực tế hơn**, hàm `age_pyramid()` dưới đây tạo một biểu đồ tháp tuổi dựa trên nhóm tuổi đã xác định và cột phân tách nhị phân, ví dụ như `giới tính`. Hàm được cung cấp bởi ba đối số trong dấu ngoặc đơn và được phân tách nhau bằng dấu phẩy. Các giá trị được cung cấp cho các đối số thiết lập `linelist` là dataframe được sử dụng, `age_cat5` là cột để đếm và `giới tính` là cột nhị phân để sử dụng chia kim tự tháp theo màu.

```{r basics_functions_arguments, include=FALSE, results='hide', message=FALSE, warning=FALSE,}
## create an age group variable by specifying categorical breaks
linelist$age_group <- cut(linelist$age, breaks = c(0, 5, 10, 15, 20, 30, 45, 60))
```

```{r message=FALSE, warning=FALSE,  out.width = "75%", out.height="75%"}
# Create an age pyramid
age_pyramid(data = linelist, age_group = "age_cat5", split_by = "gender")
```

Lệnh trên có thể được viết tương tự như bên dưới, theo cách dài hơn với một dòng mới cho mỗi đối số. Phong cách này có thể dễ đọc và dễ viết "bình luận" hơn với `#` để giải thích từng phần (bình luận mở rộng là một thực hành tốt!). Để chạy lệnh dài hơn này, bạn có thể bôi đen toàn bộ lệnh và nhấp vào "Run" hoặc chỉ cần đặt con trỏ vào dòng đầu tiên rồi nhấn đồng thời phím Ctrl và phím Enter.

```{r message=FALSE, warning=FALSE,  out.width = "75%", out.height="75%"}
# Create an age pyramid
age_pyramid(
  data = linelist,        # use case linelist
  age_group = "age_cat5", # provide age group column
  split_by = "gender"     # use gender column for two sides of pyramid
  )
```

Không cần xác định nửa đầu của phép gán đối số (ví dụ: `data =`) nếu các đối số được viết theo một thứ tự cụ thể (được chỉ định trong tài liệu của hàm). Đoạn code dưới đây tạo ra cùng một kim tự tháp như ở trên, bởi vì hàm kì vọng thứ tự đối số là: data frame, biến `age_group`, biến `split_by`.

```{r, basics_functions_pyramid2, eval = FALSE, warning=FALSE, message=FALSE, , out.width = "75%", out.height="75%", eval=F}
# This command will produce the exact same graphic as above
age_pyramid(linelist, "age_cat5", "gender")
```

**Một lệnh `age_pyramid()` phức tạp hơn có thể bao gồm các đối số *tùy chọn* để:**

-   Hiển thị tỷ lệ thay vì số lượng (đặt `proportional = TRUE` khi giá trị mặc định là `FALSE`)\
-   Chỉ định hai màu để sử dụng (`pal =` là viết tắt của "bảng màu" và được cung cấp với một vectơ gồm hai tên màu. Xem chương [đối tượng](#objectstructure) để biết cách hàm `c()` tạo ra một vectơ)

[***LƯU Ý:*** Đối với các đối số mà bạn xác định với cả hai phần của đối số (ví dụ: `proportional = TRUE`), thứ tự của chúng trong tất cả các đối số không quan trọng.]{style="color: black;"}

```{r message=FALSE, warning=FALSE, out.width = "75%", out.height="75%"}
age_pyramid(
  linelist,                    # use case linelist
  "age_cat5",                  # age group column
  "gender",                    # split by gender
  proportional = TRUE,         # percents instead of counts
  pal = c("orange", "purple")  # colors
  )
```

<!-- ======================================================= -->

### Viết hàm {.unnumbered}

R là một ngôn ngữ được định hướng xung quanh hàm, vì thế bạn nên cảm thấy được trao quyền để tự viết các hàm của riêng mình. Việc tạo ra hàm mang đến một vài lợi thế:

-   Tạo điều kiện thuận lợi cho lập trình mô-đun - tách code thành các phần độc lập và có thể quản lý\
-   Thay thế việc copy-and-paste lặp đi lặp lại, điều mà có thể dễ xảy ra lỗi\
-   Đặt tên dễ nhớ cho các đoạn code

Cách viết một hàm được trình bày cụ thể trong chương [Viết hàm].

<!-- A function is given a name and defined with the assignment operator `<-` to a special **base** R function called `function()`. Within the parentheses, the arguments that the function will accept are defined. This is followed by curly brackets `{ }`, within which the actual code of the function is written.     -->

```{r, eval=F, echo=F}
my_function <- function( ARGUMENTS HERE ){ CODE HERE }
```

<!-- The arguments should be provided in the syntax `argument = default`, separated by commas.   -->

<!-- Here is an example where we create a function `staff_calc()` to serve as a staffing calculator for COVID-19 case investigation and contact tracing calls.   -->

<!-- The arguments (inputs) and their default values will be:   -->

<!-- * `daily_cases = NULL` The number of new COVID-19 cases per day   -->

<!-- * `contacts_each = 5` The number contacts enumerated for each case   -->

<!-- * `time_case = 0.5`  Number of hours to complete a case investigaton by phone   -->

<!-- * `time_contact = 0.25`  Number of hours to complete a contact follow-up by phone   -->

<!-- * `time_day = 8` The number of hours one staff works per day   -->

<!-- Below, the function is created. The code ends with the special function `return()`, which is what the function produces.    -->

<!-- ```{r message=FALSE, warning=FALSE, out.width = "75%", out.height="75%"} -->

<!-- staff_calc <- function(daily_cases = NULL, contacts_each = 5, -->

<!--                        time_case = 0.5, time_contact = 0.25, time_day = 8){ -->

<!--   # Define total daily hours for calling cases -->

<!--   case_hours <- daily_cases * time_case  -->

<!--   # Define total daily hours for calling contacts -->

<!--   contact_hours <- daily_cases * contacts_each * time_contact -->

<!--   # Calculate number of staff required -->

<!--   staff_required <- (case_hours + contact_hours)/time_day -->

<!--   return(staff_required) -->

<!-- } -->

<!-- ``` -->

<!-- Once this code is run, the function will be defined and will appear in the R Environment. We can run the function. Below all the default values are used and the `daily_cases = ` is set to 150.   -->

```{r eval=F, echo=F, message=FALSE, warning=FALSE, out.width = "75%", out.height="75%"}
staff_calc(daily_cases = 150)
```

```{r, eval=F, echo=F}
case_incidence <- tibble(
  dates = seq.Date(from = as.Date("2020-05-01"), to = as.Date("2020-05-21"), by = 1),
  projected_incidence = c(102,110,50,37,106,190,146,138,135,111,60,43,189,184,185,80,44,97,254,291,288),
  staff_needed = staff_calc(projected_incidence)
)

ggplot(case_incidence, aes(x = dates))+
  geom_line(aes(y = projected_incidence))+
  geom_line(aes(y = staff_needed))
```

<!-- There are many other nuances to understand when writing functions, as discussed in the page [Writing functions].   -->

<!-- ======================================================= -->

<!-- ======================================================= -->

## Packages {#packages}

**Packages chứa các hàm.**

Một package trong phần mềm R là một gói code và các tài liệu hướng dẫn có thể chia sẻ được chứa các hàm được định nghĩa trước. Cộng đồng người dùng phần mềm R luôn phát triển những package giúp giải quyết các vấn đề cụ thể, điều này có thể sẽ giúp bạn trong công việc của mình! Bạn sẽ có thể cần cài đặt và sử dụng hàng trăm package trong quá trình sử dụng phần mềm R.

Khi cài đặt, R đã có sẵn các package và hàm **"cơ bản"** giúp thực hiện các nhiệm vụ đơn giản. Nhưng nhiều người dùng R tạo ra các hàm chuyên biệt, được cộng đồng R kiểm chứng và bạn có thể tải xuống dưới dạng **package** để sử dụng theo cách của riêng mình. Trong sách này, tên package được viết **in đậm**. Một trong những khía cạnh thách thức hơn cả của R đó là thường có nhiều hàm hoặc package để lựa chọn nhằm hoàn thành một nhiệm vụ nhất định.

### Cài đặt và Gọi {.unnumbered}

*Các hàm* được chứa trong **packages** có thể được tải ("cài đặt") về máy tính của bạn từ internet. Khi một package được tải xuống, package đó sẽ được lưu trữ trong "thư viện" của bạn. Sau đó, bạn có thể truy cập các hàm mà nó chứa trong phiên làm việc hiện tại trên R của bạn bằng cách "Gọi" package.

*Hãy coi R là thư viện cá nhân của bạn*: Khi bạn tải xuống một package, thư viện của bạn nhận được một cuốn sách mới gồm các hàm, nhưng mỗi lần bạn muốn sử dụng một hàm trong cuốn sách đó, bạn phải mượn ("gọi") cuốn sách đó từ thư viện của mình.

Tóm lại: để sử dụng các hàm có sẵn trong package R, phải thực hiện 2 bước:

1)  Package phải được **cài đặt** (một lần), *và*\
2)  Package phải được **gọi** (trong mỗi phiên làm việc của R)

#### Thư viện của bạn {.unnumbered}

"Thư viện" của bạn thực ra là một thư mục trên máy tính của bạn, bao gồm các thư mục chứa các package đã được cài đặt. Hãy tìm nơi R được cài đặt trong máy tính của bạn và tìm kiếm một thư mục có tên "win-library". Ví dụ: `R\win-library\4.0` (4.0 là phiên bản R - bạn sẽ có các thư viện khác nhau tùy theo phiên bản R mà bạn đã tải xuống).

Bạn có thể in ra đường dẫn tệp đến thư viện của mình bằng cách gõ lệnh `.libPaths()` (dấu ngoặc bỏ trống). Điều này trở nên đặc biệt quan trọng nếu làm việc với [R trên ổ cứng mạng].

#### Cài đặt từ CRAN {.unnumbered}

Thông thường, người dùng R tải các package xuống từ CRAN. CRAN (Comprehensive R Archive Network - Mạng lưu trữ R toàn diện) là một kho công cộng trực tuyến gồm các package R đã được xuất bản bởi các thành viên cộng đồng R.

Bạn có cần lo lắng về vi-rút và bảo mật khi tải xuống một package từ CRAN? Đọc [bài viết sau](https://support.rstudio.com/hc/en-us/articles/360042593974-R-and-R-Package-Security) để hiểm thêm về chủ đề này.

#### Làm thế nào để cài đặt và gọi {.unnumbered}

Trong sách này, chúng tôi khuyên bạn nên sử dụng package **pacman** (viết tắt của "package manager"). Nó cung cấp một hàm thuận tiện `p_load()` mà sẽ cài đặt một package nếu cần *và* gọi nó để sử dụng trong phiên làm việc hiện tại.

Cú pháp khá đơn giản. Chỉ cần liệt kê tên của các package trong dấu ngoặc đơn của hàm `p_load()` và phân tách chúng bằng dấu phẩy. Lệnh dưới đây sẽ cài đặt các package sau **rio**, **tidyverse**, và **here** nếu chúng chưa được cài đặt và sẽ gọi chúng ra để sử dụng. Điều này làm cho cách tiếp cận `p_load()` trở nên thuận tiện và ngắn gọn nếu chia sẻ scripts với người khác. Lưu ý rằng tên package có phân biệt chữ hoa chữ thường.

```{r}
# Install (if necessary) and load packages for use
pacman::p_load(rio, tidyverse, here)
```

Lưu ý rằng chúng ta đã sử dụng cú pháp `pacman::p_load()` để viết rõ ràng tên package (**pacman**) trước tên hàm (`p_load()`), được nối với nhau bằng hai dấu hai chấm `::`. Cú pháp này tiện dụng vì nó cũng gọi package **pacman** (giả sử package này đã được cài đặt).

Ngoài ra còn có các hàm **base** R thay thế mà bạn sẽ gặp thường xuyên. Hàm **base** R để cài đặt một package là `install.packages()`. Tên của package muốn cài đặt phải được đặt trong dấu ngoặc đơn bên trong *dấu ngoặc kép*. Nếu bạn muốn cài đặt nhiều package trong một lệnh, chúng phải được liệt kê trong một vectơ dạng ký tự `c()`.

Lưu ý: lệnh này *cài đặt* một package, nhưng *không* gọi nó ra để sử dụng trong phiên làm việc hiện tại.

```{r, eval=F}
# install a single package with base R
install.packages("tidyverse")

# install multiple packages with base R
install.packages(c("tidyverse", "rio", "here"))
```

Việc cài đặt cũng có thể được thực hiện bằng cách chọn và nhấp chuột vào cửa sổ RStudio "Package" và chọn "Install", sau đó tìm kiếm tên package mong muốn cài đặt.

Hàm **base** R để **gọi** một package ra sử dụng (sau khi nó đã được cài đặt) là `library()`. Hàm này chỉ có thể gọi một package tại một thời điểm (cách khác của lệnh `p_load()`). Bạn có thể nhập tên package có hoặc không có dấu ngoặc kép.

```{r, eval=F}
# load packages for use, with base R
library(tidyverse)
library(rio)
library(here)
```

Để kiểm tra xem một package đã được cài đặt và/hoặc đã được gọi hay chưa, bạn có thể xem Cửa số Package trong RStudio. Nếu package được cài đặt, nó sẽ hiển thị ở đó với số phiên bản. Nếu checkbox của nó được đánh dấu nghĩa là nó đã được gọi cho phiên làm việc hiện tại.

**Cài đặt từ Github**

Đôi khi, bạn cần cài đặt một package chưa có sẵn từ CRAN. Hoặc có lẽ package đã có sẵn trên CRAN nhưng bạn muốn *phiên bản mới hơn* với các tính năng mới chưa được cung cấp trong phiên bản cũ. Chúng thường được lưu trữ trên trang web [github.com](https://github.com/) trong một "kho lưu trữ (repository)" code công khai và miễn phí. Đọc thêm về Github trong chương [Version control với Git và Github].

Để download packages R từ Github, bạn có thể dụng hàm `p_load_gh()` từ **pacman**, hàm này sẽ cài đặt package nếu cần và gọi nó để sử dụng cho phiên làm việc R hiện tại. Cách khác để cài đặt bao gồm sử dụng package **remotes** hoặc **devtools**. Đọc thêm về các hàm của **pacman** tại [Tài liệu về package](https://cran.r-project.org/web/packages/pacman/pacman.pdf).

Để cài đặt từ Github, bạn phải cung cấp thêm các thông tin sau:

1)  Github ID của chủ sở hữu
2)  Tên của repository chứa package\
3)  *(Tùy chọn) Tên của "nhánh" (phiên bản phát triển cụ thể) mà bạn muốn tải xuống*

Trong các ví dụ dưới đây, từ đầu tiên trong dấu ngoặc kép là Github ID của chủ sở hữu kho lưu trữ, sau dấu gạch chéo là tên của kho lưu trữ (tên của package).

```{r, eval=F}
# install/load the epicontacts package from its Github repository
p_load_gh("reconhub/epicontacts")
```

Nếu bạn muốn cài đặt từ một "nhánh" (phiên bản) khác với nhánh chính, hãy thêm tên nhánh sau dấu "\@", được đặt phía sau tên kho lưu trữ.

```{r, eval=F}
# install the "timeline" branch of the epicontacts package from Github
p_load_gh("reconhub/epicontacts@timeline")
```

Nếu không có sự khác biệt giữa phiên bản Github và phiên bản trên máy tính của bạn, bạn không cần thực hiện thao tác này. Thay vào đó, bạn có thể "buộc" phải cài đặt lại bằng cách sử dụng `p_load_current_gh()` với đối số `update = TRUE`. Đọc thêm về **pacman** tại [Minh họa trực tuyến](http://trinker.github.io/pacman/vignettes/Introduction_to_pacman.html)

**Cài đặt từ ZIP hoặc TAR**

Bạn có thể cài đặt package từ một URL:

```{r, eval=F}
packageurl <- "https://cran.r-project.org/src/contrib/Archive/dsr/dsr_0.2.2.tar.gz"
install.packages(packageurl, repos=NULL, type="source")
```

Hoặc, tải xuống máy tính của bạn dưới dạng tệp nén:

Cách 1: sử dụng lệnh `install_local()` từ package **remotes**

```{r, eval=F}
remotes::install_local("~/Downloads/dplyr-master.zip")
```

Cách 2: sử dụng lệnh `install.packages()` của **base** R, cung cấp đường dẫn tệp đến tệp ZIP và thiết lập `type = "source` và `repos = NULL`.

```{r, eval=F}
install.packages("~/Downloads/dplyr-master.zip", repos=NULL, type="source")
```

### Cú pháp code {.unnumbered}

Để tăng sự tường minh trong cuốn sách này, các hàm đôi khi sẽ được đặt sau tên package của chúng bằng cách sử dụng ký hiệu `::` theo cách sau: `package_name::function_name()`

Khi một package được gọi cho một phiên làm việc, việc làm này là không cần thiết. Bạn chỉ cần sử dụng `function_name()`. Tuy nhiên, việc viết tên package sẽ hữu ích khi một tên hàm phổ biến và có thể tồn tại trong nhiều package (ví dụ: `plot()`). Việc viết tên package sẽ giúp gọi package trong trường hợp nó chưa được gọi ra.

```{r eval=FALSE}
# This command uses the package "rio" and its function "import()" to import a dataset
linelist <- rio::import("linelist.xlsx", which = "Sheet1")
```

### Trợ giúp về hàm {.unnumbered}

Để đọc thêm thông tin về một hàm, bạn có thể tìm kiếm hàm đó trong cửa sổ Help của RStudio nằm ở góc dưới bên phải. Hoặc bạn cũng có thể chạy một lệnh chẳng hạn như `?thefunctionname` (đặt tên của hàm sau dấu chấm hỏi) và trang Trợ giúp sẽ xuất hiện trong cửa sổ Help. Cuối cùng, hãy thử tìm kiếm trên internet.

### Cập nhật packages {.unnumbered}

Bạn có thể cập nhật các packages bằng cách cài đặt lại chúng. Bạn cũng có thể bấm vào nút "Update" màu xanh lá cây trong cửa sổ packages của RStudio để xem packages nào có phiên bản mới để cài đặt. Lưu ý rằng code cũ của bạn có thể cần được cập nhật nếu có một bản sửa đổi lớn về cách hoạt động của một hàm!

### Xóa packages {.unnumbered}

Sử dụng `p_delete()` từ **pacman**, hoặc `remove.packages()` từ **base** R. Ngoài ra, hãy tìm thư mục chứa thư viện của bạn và xóa thư mục theo cách thủ công.

### Sự phụ thuộc {.unnumbered}

Các packages thường phụ thuộc vào các packages khác để hoạt động. Chúng được gọi là sự phụ thuộc. Nếu một package không cài đặt được, thì package phụ thuộc vào nó có khả năng cũng không thể cài đặt được.

Xem sự phụ thuộc của một package với lệnh `p_depends()`, và xem package nào phụ thuộc vào nó với `p_depends_reverse()`

### Hàm bị che giấu {.unnumbered}

Không có gì lạ nếu hai hoặc nhiều packages chứa cùng một tên hàm. Ví dụ: packages **dplyr** có hàm `filter()`, nhưng package **stats** cũng vậy. Hàm `filter()` mặc định phụ thuộc vào thứ tự các package này được gọi lên trong phiên làm việc R - packages được gọi ra sau sẽ là mặc định cho hàm `filter()`.

Bạn có thể kiểm tra thứ tự của chúng trong cửa sổ Environment của R Studio - nhấp vào menu thả xuống "Global Environment" và xem thứ tự của các packages. Các hàm thuộc các packages *ở vị trí thấp hơn* trong danh sách thả xuống đó sẽ che giấu các hàm cùng tên trong các packages xuất hiện ở vị trí bên trên trong danh sách thả xuống. Khi bạn vừa gọi một package, R sẽ cảnh báo bạn trong bảng điều khiển nếu xảy ra hiện tượng này, nhưng điều này rất hay bị bỏ quên.

```{r out.width = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "masking_functions.png"))
```

Dưới đây là những cách bạn có thể sửa lỗi hàm bị che giấu:

1)  Ghi rõ tên package trong lệnh. Ví dụ, sử dụng `dplyr::filter()`\
2)  Sắp xếp lại thứ tự mà các package được tải (ví dụ trong `p_load()`), và **bắt đầu một phiên làm việc R mới**

### Gỡ package {.unnumbered}

Để gỡ (detach) một package, hãy sử dụng lệnh dưới đây, với tên package chính xác và chỉ có một dấu hai chấm. Lưu ý rằng điều này có thể không giải quyết được việc hàm bị che giấu.

```{r, eval=F}
detach(package:PACKAGE_NAME_HERE, unload=TRUE)
```

### Cài đặt phiên bản cũ hơn {.unnumbered}

Xem [hướng dẫn](https://support.rstudio.com/hc/en-us/articles/219949047-Installing-older-versions-of-packages) này để cài đặt phiên bản cũ hơn của một package cụ thể.

### Packages đề xuất {.unnumbered}

Xem chương [Package đề xuất] để biết danh sách các packages thường được sử dụng trong dịch tễ học.

<!-- ======================================================= -->

## Scripts {#scripts}

Scripts là một phần cơ bản của lập trình. Chúng là các tài liệu chứa các câu lệnh của bạn (ví dụ: các hàm để tạo và chỉnh sửa bộ số liệu, các hàm để in các biểu đồ trực quan hóa số liệu, v.v.). Bạn có thể lưu một scripts và chạy lại sau này. Có nhiều lợi ích để lưu trữ và chạy các lệnh của bạn từ một scripts (so với nhập "từng lệnh" vào R console):

-   Tiện dụng - bạn có thể chia sẻ công việc của mình với người khác bằng cách gửi cho họ các tập lệnh của bạn\
-   Khả năng tái lập - để bạn và những người khác biết chính xác những gì bạn đã làm\
-   Kiểm soát phiên bản - để bạn có thể theo dõi các thay đổi do chính bạn hoặc đồng nghiệp thực hiện\
-   Dễ dàng nhận xét/chú thích - để giải thích cho đồng nghiệp của bạn những gì bạn đã làm

### Bình luận {.unnumbered}

Khi viết script, bạn có thể thêm các chú thích ("bình luận") xung quanh code R của bạn. Bình luận là cần thiết để giải thích cho chính bạn và những người đọc khác hiểu những gì bạn đang làm. Bạn có thể thêm bình luận bằng cách nhập dấu thăng (\#) và viết bình luận của bạn sau đó. Nội dung bình luận sẽ xuất hiện với màu khác với code R.

Bất kỳ code nào được viết sau dấu \# sẽ không được chạy. Do đó, đặt dấu \# trước dòng code cũng là một cách hữu ích để tạm thời vô hiệu hóa một dòng code ("comment out") nếu bạn không muốn xóa nó). Bạn có thể comment out/in nhiều dòng cùng một lúc bằng cách bôi đen chúng và nhấn Ctrl + Shift + c (Cmd + Shift + c trong Mac).

```{r, eval = F}
# A comment can be on a line by itself
# import data
linelist <- import("linelist_raw.xlsx") %>%   # a comment can also come after code
# filter(age > 50)                          # It can also be used to deactivate / remove a line of code
  count()

```

-   Bình luận *những gì* bạn đang làm *và **tại sao** bạn làm như vậy*.\
-   Chia code của bạn thành các phần hợp lý\
-   Kèm theo code của bạn với mô tả từng bước về những gì đang được thực hiện (ví dụ: các bước được đánh số)

### Phong cách viết code {.unnumbered}

Phong cách viết code của bạn rất quan trọng - đặc biệt là khi làm việc theo nhóm. Chúng tôi khuyên bạn nên tuân theo [hướng dẫn](https://style.tidyverse.org/) phong cách viết code **tidyverse** . Bên cạnh đó còn có các packages khác như **styler** và **lintr** để giúp bạn tuân theo phong cách này.

Một vài điểm rất cơ bản để làm cho code của bạn dễ dàng đọc được đối với người khác:\
\* Khi đặt tên cho các đối tượng, chỉ sử dụng các chữ cái viết thường, số và dấu gạch dưới `_`, ví dụ: `my_data`\
\* Thường xuyên sử dụng dấu cách, bao gồm cả xung quanh các toán tử, ví dụ: `n = 1` và `age_new <- age_old + 3`

### Ví dụ về Script {.unnumbered}

Dưới đây là một ví dụ về một đoạn ngắn R script. Hãy nhớ rằng, bạn càng giải thích ngắn gọn lệnh code của mình trong phần bình luận, thì đồng nghiệp của bạn sẽ càng thích bạn!

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "example_script.png"))
```

<!-- ======================================================= -->

### R markdown {.unnumbered}

R markdown là một dạng của R script và có khả năng *xuất* thành các tệp đầu ra (PDF, Word, HTML, Powerpoint, v.v.). Đây là những công cụ vô cùng hữu ích và linh hoạt thường được sử dụng để tạo các báo cáo tự động. Ngay cả trang web và cuốn sách này cũng được viết bằng R markdown!

Bạn cần biết rằng những người mới bắt đầu dùng R cũng có thể sử dụng R Markdown - do đó đừng sợ! Để tìm hiểu thêm, hãy xem chương [Báo cáo với R Markdown] trong cuốn sách này.

<!-- ======================================================= -->

### R notebooks {.unnumbered}

Không có sự khác biệt trong cách viết giữa R markdown và R notebook. Tuy nhiên, việc thực thi của hai loại file này hơi khác nhau một chút. Xem trang [Web](http://uc-r.github.io/r_notebook) này để biết thêm chi tiết.

<!-- ======================================================= -->

### Shiny {.unnumbered}

Shiny apps/websites được chứa trong một script có tên `app.R`. Tệp này có ba thành phần:

1)  Giao diện người dùng (ui)\
2)  Một hàm máy chú\
3)  Một lệng gọi hàm `shinyApp`

Xem thêm trong cuốn sách này tại chương [Dashboards với Shiny], hoặc hướng dẫn trực tuyến này: [Shiny tutorial](https://shiny.rstudio.com/tutorial/written-tutorial/lesson1/)

*Trước đây, tệp bên trên được chia thành 2 tệp con (`ui.R` và `server.R`)*

### Thu gọn Code {.unnumbered}

Bạn có thể thu gọn các đoạn code để làm cho script của bạn dễ đọc hơn.

Để làm điều này, hãy tạo tiêu đề văn bản bằng dấu thăng \#, viết tiêu đề của bạn và thêm vào phía sau nó ít nhất hoặc là 4 dấu gạch ngang (-), dấu thăng (\#) hoặc dấu bằng (=). Ngay sau đó, một mũi tên nhỏ sẽ xuất bên cạnh ở bên phải số thự tự của dòng lệnh. Bạn có thể nhấp vào mũi tên này và phần code bên dưới sẽ được thu gọn cho đến trước tiêu đề tiếp theo và một mũi tên hai chiều xuất hiện ở đây.

Để mở rộng lại đoạn code đã thu gọn, hãy nhấp lại vào mũi tên hoặc biểu tượng mũi tên hai chiều. Ngoài ra còn có thể sử dụng thêm các phím tắt như đã được giải thích trong [mục RStudio](#rstudio) của chương này.

Bằng cách tạo tiêu đề bằng \#, bạn cũng sẽ kích hoạt Mục lục ở cuối tập lệnh của mình (xem bên dưới) mà bạn có thể sử dụng để điều hướng tập lệnh của mình. Bạn có thể tạo tiêu đề phụ bằng cách thêm các ký hiệu \#, ví dụ: \# \# cho tiêu đề chính, \#\# fcho tiêu đề thứ hai, và \#\#\# cho tiêu đề thứ ba.

Dưới đây là hai phiên bản của một ví dụ cho script. Ở bên trái là bản gốc với các tiêu đề được chú thích. Ở bên phải, bốn dấu gạch ngang đã được viết sau mỗi tiêu đề, làm cho chúng có thể thu gọn được. Hai trong số chúng đã được thu gọn và bạn có thể thấy Mục lục ở dưới cùng hiện hiển thị từng phần.

```{r, out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "code_folding1.png"))
knitr::include_graphics(here::here("images", "code_folding2.png"))
```

Các phần code khác mặc định đủ điều kiện để thu gọn bao gồm các phần "nằm giữa" hai dấu ngoặc nhọn `{ }`, chẳng hạn như định nghĩa hàm hoặc các khối điều kiện (câu lệnh if else). Bạn có thể đọc thêm về cách thu gọn code tại [trang của Rstudio](https://support.rstudio.com/hc/en-us/articles/200484568-Code-Folding-and-Sections).

<!-- ======================================================= -->

<!-- ======================================================= -->

<!-- ======================================================= -->

## Thư mục làm việc

Thư mục làm việc là vị trí thư mục gốc được R sử dụng cho công việc của bạn - nơi R tìm kiếm và lưu các tệp theo mặc định. Mặc định là, R sẽ lưu các tệp mới và xuất tệp vào vị trí này, đồng thời sẽ tìm kiếm các tệp để nhập dữ liệu (ví dụ: bộ dữ liệu) tại đây.

Thư mục làm việc xuất hiện bằng dòng chữ màu xám ở phía trên cửa sổ Rstudio Console. Bạn cũng có thể in thư mục làm việc hiện tại bằng cách chạy lệnh `getwd()` (để trống dấu ngoặc đơn).

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "working_directory_1.png"))
```

### Gợi ý cách tiếp cận {.unnumbered}

**Xem thêm tại chương [Dự án R] để biết chi tiết về gợi ý các cách tiếp cận của chúng tôi trong việc quản lý thư mục làm việc của bạn.**\
Một cách phổ biến, hiệu quả và không gặp sự cố khi quản lý thư mục làm việc và đường dẫn tệp là kết hợp 3 yếu tố này trong một quy trình làm việc với [Dự án R][R projects] có định hướng :

1)  Một dự án R để lưu trữ tất cả tệp của bạn (xem tại chương [Dự án R])\
2)  Package **here** để định vị tệp (xem tại [Import and export])\
3)  The **rio** package to import/export files (see page on [Nhập xuất dữ liệu])

<!-- ======================================================= -->

### Thiết lập bằng lệnh {.unnumbered}

Cho tới gần đây, nhiều người học R đã được dạy để bắt đầu script của họ bằng lệnh `setwd()`. Hãy thay đổi thói quen đó bằng cách sử dụng [Dự án R][R projects] và đọc thêm tài liệu sau để hiểu [lý do không nên sử dụng `setwd()`](https://www.tidyverse.org/blog/2017/12/workflow-vs-script/). Một cách ngắn gọn là, công việc của bạn chỉ thực hiện được trên máy tính của bạn, các đường dẫn tệp được sử dụng để nhập và xuất tệp trở nên "dễ lỗi" và điều này cản trở nghiêm trọng đến việc cộng tác và sử dụng code của bạn trên bất kỳ máy tính nào khác. Bạn có những lựa chọn khác dễ dàng hơn!

Như đã nói ở trên, mặc dù chúng tôi không khuyến nghị phương pháp này trong hầu hết các trường hợp, bạn vẫn có thể sử dụng lệnh `setwd()` với đường dẫn tệp thư mục mong muốn trong dấu ngoặc kép, ví dụ:

```{r, eval=F}
setwd("C:/Documents/R Files/My analysis")
```

[***NGUY HIỂM:*** thiết lập một thư mục làm việc với `setwd()` *có thể* dẫn đến "lỗi" nếu đường dẫn tệp dành riêng cho một máy tính. Thay vào đó, hãy sử dụng đường dẫn tệp liên quan đến thư mục gốc Dự án R (với package **here**).]{style="color: red;"}

<!-- ======================================================= -->

### Thiết lập thủ công {.unnumbered}

Để thiết lập thư mục làm việc một cách thủ công (trỏ và nhấp tương đương với `setwd()`), hãy chọn mục Session trên thanh công cụ và chọn "Set Working Directory", sau đó chọn "Choose Directory". Chú ý: nếu sử dụng phương pháp này, bạn sẽ phải thực hiện việc này theo cách thủ công mỗi khi mở RStudio.

<!-- ======================================================= -->

### Thiết lập bên trong một dự án R {.unnumbered}

Nếu bạn đang mở một dự án R, thư mục làm việc sẽ mặc định là thư mục gốc của dự án R có chứa tệp ".rproj". Điều này sẽ áp dụng nếu bạn mở RStudio bằng cách nhấp vào mở R Project (tệp có phần mở rộng ".rproj").

<!-- ======================================================= -->

### Thư mục làm việc với R markdown {.unnumbered}

Trong script ở R markdown, thư mục làm việc mặc định là thư mục chứa tệp R markdown (`.Rmd`). Nếu sử dụng dự án R và package **here**, điều này sẽ không được áp dụng. Để biết thư mục làm việc là gì, sử dụng lệnh `here()` như đã được giải thích tại chương [Dự án R].

Nếu bạn muốn thay đổi thư mục làm việc của một tệp độc lập ở R markdown (không phải ở dự án R), nếu bạn sử dụng `setwd()` điều này sẽ chỉ áp dụng chỉ cho đoạn code đó. Để thực hiện thay đổi cho tất cả các đoạn code trong R markdown, hãy điều chỉnh ở bước thiết lập để thêm tham số `root.dir =`, như bên dưới:

```{r, eval=F}
knitr::opts_knit$set(root.dir = 'desired/directorypath')
```

Cách này dễ hơn nhiều so với chỉ sử dụng R markdown bên trong một dự án R và sử dụng package **here**.

<!-- ======================================================= -->

### Cung cấp đường dẫn tệp {.unnumbered}

Có lẽ điều khiến những người mới bắt đầu với R cảm thấy nản nhất (ít nhất là với người dùng máy tính Windows) đó là gõ đường dẫn tệp để nhập xuất dữ liệu. Chúng tôi có giải thích cặn kẽ về cách tạo đường dẫn tệp đầu vào tốt nhất trong chương [Nhập xuất dữ liệu], nhưng dưới đây là một số điểm chính:

**Đường dẫn tệp bị lỗi**

Dưới đây là ví dụ về đường dẫn tệp "tuyệt đối" hoặc "địa chỉ đầy đủ". Chúng có thể bị lỗi nếu được sử dụng bởi một máy tính khác. Một ngoại lệ là nếu bạn đang sử dụng ổ đĩa chia sẻ/mạng.

    C:/Users/Name/Document/Analytic Software/R/Projects/Analysis2019/data/March2019.csv  

**Đường dẫn với dấu gạch chéo**

*Nếu nhập đường dẫn tệp, hãy lưu ý hướng của các dấu gạch chéo.* Sử dụng dấu *gạch chéo xuôi* (`/`) để tách các thành phần ("data/provincial.csv"). Đối với người dùng Windows, cách mặc định mà đường dẫn tệp được hiển thị là dấu *gạch chéo ngược* (\\) - vì vậy bạn sẽ cần phải thay đổi hướng của mỗi dấu gạch chéo. Nếu bạn sử dụng package **here** được miêu tả ở [Dự án R] thì dấu gạch chéo không còn là vấn đề với bạn nữa.

**Đường dẫn tệp tương đối**

Nói chung, chúng tôi khuyên bạn nên cung cấp các đường dẫn tệp theo cách "tương đối" - nghĩa là, đường dẫn *liên quan đến* thư mục gốc Dự án R của bạn. Bạn có thể thực hiện việc này bằng cách sử dụng package **here** như được giải thích trong chương [Dự án R]. Một đường dẫn tệp tương đối sẽ trông như thế này:

```{r, eval=F}
# Import csv linelist from the data/linelist/clean/ sub-folders of an R project
linelist <- import(here("data", "clean", "linelists", "marin_country.csv"))
```

Ngay cả khi sử dụng đường dẫn tệp tương đối trong dự án R, bạn vẫn có thể sử dụng đường dẫn tuyệt đối để nhập/xuất dữ liệu ở bên ngoài dự án R của bạn.

<!-- ======================================================= -->

## Đối tượng {#objects}

Mọi thứ trong R đều là một đối tượng, và R là một ngôn ngữ "lập trình hướng đối tượng". Các phần dưới đây sẽ giải thích:

-   Cách tạo ra các đối tượng (`<-`)
-   Các loại đối tượng (ví dụ: data frames, vectors..)\
-   Cách truy cập các tập con của đối tượng (ví dụ: các biến số trong một bộ dữ liệu)\
-   Các loại đối tượng (ví dụ: numeric, logical, integer, double, character, factor)

<!-- ======================================================= -->

### Mọi thứ đều là một đối tượng {.unnumbered}

*Phần này được dựa theo sách [R4Epis project](https://r4epis.netlify.app/training/r_basics/objects/).*\
Mọi thứ bạn lưu trữ trong R - bao gồm bộ dữ liệu, biến số, danh sách tên làng, tổng số dân, thậm chí cả kết quả đầu ra như biểu đồ - đều là **các đối tượng**, được **gán tên** và **có thể được tham chiếu** trong các lệnh sau đó.

Một đối tượng tồn tại khi bạn đã gán giá trị cho nó (xem phần gán bên dưới). Khi nó được gán một giá trị, đối tượng sẽ xuất hiện trong cửa sổ Environment (xem cửa sổ phía trên bên phải của RStudio). Sau đó, nó có thể được sử dụng, thao tác, thay đổi và định nghĩa lại.

<!-- ======================================================= -->

### Định nghĩa một đối tượng (`<-`) {.unnumbered}

**Tạo ra một đối tượng *bằng cách gán cho chúng một giá trị* bằng toán tử \<-.**\
Bạn có thể nghĩ về toán tử gán `<-` tương đương với từ "được định nghĩa là". Các lệnh gán thường tuân theo một trật tự quy định:

**tên đối tượng** \<- **giá trị của đối tượng** (hoặc quy trình / tính toán tạo ra giá trị)

Ví dụ: bạn muốn ghi nhận một báo cáo tuần dịch tễ học hiện tại dưới dạng một đối tượng để tham chiếu tới code của bạn sau này. Trong ví dụ này, đối tượng `current_week` được tạo khi nó được gán giá trị `"2018-W10"` (dấu ngoặc kép sẽ quy định đây là giá trị dạng chữ). Đối tượng `current_week` sẽ xuất hiện trong cửa sổ RStudio Environment (phía trên bên phải) và có thể được tham chiếu tới các lệnh sau này.

Xem các lệnh R và kết quả của chúng như dưới đây.

```{r basics_objects_assignment}
current_week <- "2018-W10"   # this command creates the object current_week by assigning it a value
current_week                 # this command prints the current value of current_week object in the console
```

[***CHÚ Ý:*** Lưu ý rằng số `[1]` trong kết quả ở R console đơn giản là chỉ ra rằng bạn đang xem mục đầu tiên của đầu ra]{style="color: black;"}

[***THẬN TRỌNG:*** **Giá trị của một đối tượng có thể bị ghi đè** bất kỳ lúc nào bằng cách chạy lệnh gán để định nghĩa lại giá trị của nó. Do đó, **thứ tự của các lệnh được chạy rất quan trọng**.]{style="color: orange;"}

Lệnh sau sẽ định nghĩa lại giá trị của đối tượng `current_week`:

```{r basics_objects_reassignment}
current_week <- "2018-W51"   # assigns a NEW value to the object current_week
current_week                 # prints the current value of current_week in the console
```

**Dấu bằng `=`**

Bạn cũng sẽ thấy các dấu bằng trong R code:

-   Hai dấu bằng `==` giữa hai đối tượng hoặc giá trị dùng để đặt một *câu hỏi* logic: "cái này có bằng cái kia không?".\
-   Bạn cũng sẽ thấy các dấu bằng trong các hàm được sử dụng để xác định giá trị của các đối số của hàm (đọc thêm ở các phần bên dưới), ví dụ: `max(age, na.rm = TRUE)`.\
-   Bạn *có thể* sử dụng một dấu bằng `=` thay cho dấu `<-` để tạo và định nghĩa các đối tượng, nhưng điều này không được khuyến khích. Bạn có thể đọc về lý do tại sao điều này không được khuyến khích ở [đây](https://renkun.me/2014/01/28/difference-between-assignment-operators-in-r/).

**Bộ dữ liệu**

Bộ dữ liệu (datasets) cũng là một đối tượng (thường là một "dataframes") và phải được gán tên khi chúng được nhập. Trong đoạn mã dưới đây, đối tượng `linelist` được tạo và gán giá trị từ tệp CSV, tệp này được nhập bằng package **rio** và hàm `import()` của package này.

```{r basics_objects_dataframes, eval=FALSE}
# linelist is created and assigned the value of the imported CSV file
linelist <- import("my_linelist.csv")
```

Bạn có thể đọc thêm về nhập và xuất dữ liệu trong chương [Nhập xuất dữ liệu].

[***THẬN TRỌNG:*** Lưu ý nhanh về cách đặt tên đối tượng:]{style="color: orange;"}

-   Tên đối tượng không được chứa dấu cách, nhưng bạn nên sử dụng dấu gạch dưới (\_) hoặc dấu chấm (.) thay vì dấu cách.\
-   Tên đối tượng phân biệt chữ hoa và chữ thường (nghĩa là Dataset_A khác với dataset_A).
-   Tên đối tượng phải bắt đầu bằng chữ cái (không được bắt đầu bằng số như 1, 2 hoặc 3).

**Kết quả đầu ra**

Các kết quả đầu ra như bảng và biểu đồ cung cấp một ví dụ về cách các kết quả đầu ra có thể được lưu dưới dạng đối tượng hoặc chỉ được in ra mà không cần lưu. Ví dụ, một bảng chéo giữa giới tính và biến kết cục được tạo ra bởi hàm `table()` trong **base** R, có thể được in trực tiếp vào R console (*mà không* cần lưu).

```{r}
# printed to R console only
table(linelist$gender, linelist$outcome)
```

Nhưng bảng này cũng có thể được lưu dưới dạng một đối tượng được đặt tên. Sau đó, bạn có thể in nó ra.

```{r}
# save
gen_out_table <- table(linelist$gender, linelist$outcome)

# print
gen_out_table
```

**Cột**

Các cột trong tập dữ liệu cũng là các đối tượng và có thể được định nghĩa, ghi đè và tạo như được mô tả bên dưới trong phần Cột.

Bạn có thể sử dụng toán tử gán từ **base** R để tạo một cột mới. Dưới đây, cột mới `bmi` (Body Mass Index) được tạo, và giá trị mới ứng với mỗi hàng là kết quả của một phép toán trên giá trị của các hàng trong cột `wt_kg` và cột `ht_cm`.

```{r, eval=F}
# create new "bmi" column using base R syntax
linelist$bmi <- linelist$wt_kg / (linelist$ht_cm/100)^2
```

Tuy nhiên, trong cuốn sách này, chúng tôi tập trung vào một cách tiếp cận khác để định nghĩa cột, sử dụng hàm `mutate()` trong package **dplyr** và *piping* với toán tử pipe (`%>%`). Cú pháp dễ đọc hơn và có những ưu điểm khác đã được giải thích trong cuốn sách này ở chương [Làm sạch số liệu và các hàm quan trọng]. Bạn có thể đọc thêm về *piping* trong phần Piping phía bên dưới.

```{r, eval=F}
# create new "bmi" column using dplyr syntax
linelist <- linelist %>% 
  mutate(bmi = wt_kg / (ht_cm/100)^2)
```

<!-- ======================================================= -->

### Cấu trúc đối tượng {#objectstructure}

**Các đối tượng có thể là một phần dữ liệu đơn lẻ (ví dụ: `my_number <- 24`), hoặc chúng có thể bao gồm dữ liệu có cấu trúc.**

Hình ảnh dưới đây được tham khảo từ [hướng dẫn R trực tuyến này](http://venus.ifca.unican.es/Rintro/dataStruct.html). Nó cho thấy một số cấu trúc dữ liệu phổ biến và tên của chúng. Hình ảnh này không bao gồm dữ liệu không gian. Bạn có thể xem thêm về dữ liệu không gian tại chương [GIS cơ bản].

```{r basics_objects_structures, echo=F, out.width = "75%", out.height="50%", fig.align = "center"}
knitr::include_graphics(here::here("images", "R_data_structures.png"))
```

Trong dịch tễ học (và đặc biệt là dịch tễ học thực địa), bạn sẽ *thường xuyên* phải tiếp xúc với data frames và vectors:

+--------------------+--------------------------------------------------------------------------------------------------+---------------------------------------------------------------------+
| Cấu trúc thường gặ | p \| Giái thích                                                                                  | \| Ví dụ                                                            |
+====================+==================================================================================================+=====================================================================+
| Vectors            | Một vùng chứa cho một chuỗi các đối tượng đơn lẻ, tất cả đều thuộc cùng một loại (e.g. số, chữ). | **"Biến" (Cột) trong dữ liệu là vectors** (ví dụ: cột `age_years`). |
+--------------------+--------------------------------------------------------------------------------------------------+---------------------------------------------------------------------+
| Data Frames        | Vectors (ví dụ: các cột) được liên kết với nhau mà tất cả đều có cùng số hàng. \| \`linel        | ist\` là một data frame.                                            |
+--------------------+--------------------------------------------------------------------------------------------------+---------------------------------------------------------------------+

Lưu ý rằng để tạo một vectơ "độc lập" (mà không phải là một phần của data frame), hàm `c()` được sử dụng để kết hợp các phần tử khác nhau. Ví dụ: nếu tạo một vectơ màu sắc thang màu của biểu đồ: `vector_of_colors <- c("blue", "red2", "orange", "grey")`

<!-- ======================================================= -->

### Kiểu đối tượng {.unnumbered}

Tất cả các đối tượng được lưu trữ trong R đều có một *kiểu* dữ liệu cho biết cách nó được xử lý. Có nhiều kiểu đối tượng, nhưng những kiểu phổ biến bao gồm:

+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+
| Kiểu       | Giải thích                                                                                                                                                                                                    | Ví dụ                                                                                                              |
+============+===============================================================================================================================================================================================================+====================================================================================================================+
| Ký tự      | Là các chữ/từ/câu được đặt **"trong dấu ngoặc kép"**. Đối tượng kiểu ký tự thì không thể tính toán                                                                                                            | "Những ký tự nằm trong dấu ngoặc kép"                                                                              |
+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+
| Số nguyên  | Các số **nguyên** (không có phần thập phân)                                                                                                                                                                   | -5, 14, hoặc 2000                                                                                                  |
+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+
| Số thực    | Bao gồm các số nguyên và **có thể bao gồm phần thập phân**. Nếu trong dấu ngoặc kép, chúng sẽ được coi là dạng ký tự                                                                                          | 23.1 hoặc 14                                                                                                       |
+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+
| Factor     | Đây là các vector có **trật tự xác định** hoặc các giá trị có nhiều danh mục                                                                                                                                  | Biến số của tình trạng kinh tế với các giá trị theo tứ tự                                                          |
+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+
| Ngày tháng | **Khi R được thông báo rằng một dữ liệu ở dạng ngày tháng R**, những dữ liệu này có thể được thao tác và hiển thị theo những cách đặc biệt. Xem thêm về biến ngày tháng tại chương [Làm việc với ngày tháng]. | 2018-04-12 hoặc 15/3/1954 hoặc Wed 4 Jan 1980                                                                      |
+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+
| Logic      | Giá trị của biến chỉ có thể là hai giá trị đặc biệt sau TRUE hoặc FALSE (lưu ý đây **không phải** là "TRUE" và"FALSE" trong dấu ngoặc kép)                                                                    | TRUE hoặc FALSE                                                                                                    |
+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+
| data.frame | Một data.frame trong R lưu trữ một **tập dữ liệu điển hình**. Data.frame bao gồm các vector (cột) dữ liệu được liên kết với nhau, tất cả chúng đều có cùng một số lượng quan sát (hàng).                      | Tập dữ liệu mẫu AJS có tên là `linelist_raw`chứa 68 biến với 300 quan sát (mỗi hàng)                               |
+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+
| tibble     | tibbles là một dạng khác của data.frame, sự khác biệt chính là tibble in đẹp hơn trong R console (hiển thị 10 hàng đầu tiên và chỉ các cột vừa với màn hình)                                                  | data frame, danh sách, hoặc ma trận có thể chuyển đổi thành tibble bằng hàm `as_tibble()`                          |
+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+
| Danh sách  | Một danh sách giống như một vector, nhưng chứa các đối tượng có kiểu khác nhau                                                                                                                                | Một danh sách có thể chứa một số duy nhất, một data.frame, một vector và thậm chí một danh sách khác bên trong nó! |
+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+

**Bạn có thể kiểm tra kiểu của một đối tượng bằng cách cung cấp tên của nó tới hàm `class()`**. Lưu ý: bạn có thể tham chiếu một cột cụ thể trong tập dữ liệu bằng cách sử dụng ký hiệu `$` để phân tách tên của tập dữ liệu và tên của cột.

```{r, echo=TRUE,}
class(linelist)         # class should be a data frame or tibble

class(linelist$age)     # class should be numeric

class(linelist$gender)  # class should be character
```

Đôi khi, một cột sẽ được tự động chuyển đổi thành một kiểu khác bởi R. Hãy coi chừng điều này! Ví dụ: nếu bạn có một vectơ hoặc cột kiểu số, nhưng một giá trị ký tự được chèn vào ... thì toàn bộ cột sẽ thay đổi thành kiểu ký tự.

```{r}
num_vector <- c(1,2,3,4,5) # define vector as all numbers
class(num_vector)          # vector is numeric class
num_vector[3] <- "three"   # convert the third element to a character
class(num_vector)          # vector is now character class
```

Một ví dụ phổ biến của điều này là khi thao tác với một data frame để in bảng - nếu bạn tạo một hàng tính tổng và cố gắng dán /gắn phần trăm với số trong cùng một ô (ví dụ: `23 (40%)`), toàn bộ cột dạng số ở trên sẽ chuyển đổi thành ký tự và không còn có thể được sử dụng cho các phép tính toán học nữa.

**Đôi khi, bạn sẽ cần chuyển đổi các đối tượng hoặc cột sang một loại khác.**

+------------------+-----------------------------------------------------------------------------------------------------------------------------------+
| Hàm              | Chức năng                                                                                                                         |
+==================+===================================================================================================================================+
| `as.character()` | Chuyển sang kiểu ký tự                                                                                                            |
+------------------+-----------------------------------------------------------------------------------------------------------------------------------+
| `as.numeric()`   | Chuyển sang kiểu số                                                                                                               |
+------------------+-----------------------------------------------------------------------------------------------------------------------------------+
| `as.integer()`   | Chuyển sang kiểu số nguyên                                                                                                        |
+------------------+-----------------------------------------------------------------------------------------------------------------------------------+
| `as.Date()`      | Chuyển sang kiểu ngày tháng - Chú ý: Xem mục [Ngày tháng](#dates) để biết thêm chi tiết                                           |
+------------------+-----------------------------------------------------------------------------------------------------------------------------------+
| `factor()`       | Chuyển sang kiểu factor - Chú ý: nếu muốn định nghĩa lại thứ tự của các giá trị trong biến factor thì cần thêm các đối số bổ sung |
+------------------+-----------------------------------------------------------------------------------------------------------------------------------+

Tương tự như vậy, một số hàm **base** R có thể kiểm tra xem một đối tượng CÓ thuộc của một kiểu dữ liệu cụ thể nào hay không, chẳng hạn như `is.numeric()`, `is.character()`, `is.double()`, `is.factor()`, `is.integer()`

Bạn có thể tham khảo [một tài liệu trực tuyến về các kiểu và cấu trúc dữ liệu trong R](https://swcarpentry.github.io/r-novice-inflammation/13-supp-data-structures/) tại đây.

<!-- ======================================================= -->

### Cột / Biến số (`$`) {.unnumbered}

**Một cột trong data frame về mặt kỹ thuật là một "vector" (xem bảng ở trên)** - bao gồm một chuỗi các giá trị cùng loại (ký tự, số, lôgic, v.v.).

Một vectơ có thể tồn tại độc lập với một data frame, ví dụ: vectơ tên cột mà bạn muốn đưa vào làm biến giải thích trong mô hình. Để tạo một vectơ "độc lập", hãy sử dụng hàm `c()` như dưới đây:

```{r, warning=F, message=F}
# define the stand-alone vector of character values
explanatory_vars <- c("gender", "fever", "chills", "cough", "aches", "vomit")

# print the values in this named vector
explanatory_vars
```

**Các cột trong data frame cũng là vectơ và có thể được gọi, tham chiếu, trích xuất hoặc tạo bằng ký hiệu `$`.** Ký hiệu `$` kết nối tên của cột với tên của data frame tương ứng. Trong cuốn sách này, chúng tôi cố gắng sử dụng từ "cột" thay vì "biến số".

```{r basics_objects_call, eval=F}
# Retrieve the length of the vector age_years
length(linelist$age) # (age is a column in the linelist data frame)

```

Bằng cách nhập tên của một dataframe, theo sau bởi ký tự `$`, bạn sẽ thấy menu gợi ý của tất cả các tên cột trong dataframe. Bạn có thể di chuyển giữa các cột bằng phím mũi tên, chọn cột bằng phím Enter để tránh lỗi chính tả!

```{r echo=F, out.width = "100%", fig.align = "center"}
knitr::include_graphics(here::here("images", "Calling_Names.gif"))
```

[***MẸO NÂNG CAO:*** Một số đối tượng phức tạp hơn (ví dụ: một danh sách hoặc đối tượng `epicontacts`) có thể có nhiều cấp độ có thể được truy cập thông qua nhiều ký tự đô la. Ví dụ: `epicontacts$linelist$date_onset`]{style="color: darkgreen;"}

<!-- ======================================================= -->

### Truy cập / indexing đối tượng bằng dấu ngoặc vuông (`[ ]`) {.unnumbered}

Khi cần xem một phần của đối tượng, còn được gọi là "indexing", bạn có thể sử dụng dấu ngoặc vuông `[ ]`. Sử dụng `$` trên dataframe để truy cập một cột cũng là một kiểu indexing.

```{r}
my_vector <- c("a", "b", "c", "d", "e", "f")  # define the vector
my_vector[5]                                  # print the 5th element
```

Dấu ngoặc vuông cũng hoạt động để xem các phần cụ thể trong kết quả đầu ra, chẳng hạn như đầu ra của hàm `summary()`:

```{r}
# All of the summary
summary(linelist$age)

# Just the second element of the summary, with name (using only single brackets)
summary(linelist$age)[2]

# Just the second element, without name (using double brackets)
summary(linelist$age)[[2]]

# Extract an element by name, without showing the name
summary(linelist$age)[["Median"]]

```

Dấu ngoặc vuông cũng hoạt động trên data frames để xem các hàng và cột cụ thể. Bạn có thể thực hiện việc này bằng cú pháp `dataframe[rows, columns]`:

```{r basics_objects_access, eval=F}
# View a specific row (2) from dataset, with all columns (don't forget the comma!)
linelist[2,]

# View all rows, but just one column
linelist[, "date_onset"]

# View values from row 2 and columns 5 through 10
linelist[2, 5:10] 

# View values from row 2 and columns 5 through 10 and 18
linelist[2, c(5:10, 18)] 

# View rows 2 through 20, and specific columns
linelist[2:20, c("date_onset", "outcome", "age")]

# View rows and columns based on criteria
# *** Note the dataframe must still be named in the criteria!
linelist[linelist$age > 25 , c("date_onset", "outcome", "age")]

# Use View() to see the outputs in the RStudio Viewer pane (easier to read) 
# *** Note the capital "V" in View() function
View(linelist[2:20, "date_onset"])

# Save as a new object
new_table <- linelist[2:20, c("date_onset")] 
```

Lưu ý rằng bạn cũng có thể indexing hàng / cột trong một data frames và tibbles bằng cách sử dụng cú pháp của package **dplyr** (hàm `filter()` đối với hàng, và `select()` đối với cột). Đọc thêm về các hàm quan trọng này trong chương [Làm sạch số liệu và các hàm quan trọng].

Để lọc dựa trên "số thứ tự hàng", bạn có thể sử dụng hàm `row_number()`trong package **dplyr** với dấu ngoặc đơn mở như một phần của biểu thức lọc logic. Thường thì bạn sẽ sử dụng toán tử `%in%` và một khoảng giá trị số như một phần của câu lệnh logic đó, như được trình bày dưới đây. Để xem N hàng *đầu tiên*, bạn cũng có thể sử dụng hàm `head()` của package **dplyr**.

```{r, eval=F}
# View first 100 rows
linelist %>% head(100)

# Show row 5 only
linelist %>% filter(row_number() == 5)

# View rows 2 through 20, and three specific columns (note no quotes necessary on column names)
linelist %>% filter(row_number() %in% 2:20) %>% select(date_onset, outcome, age)
```

Khi indexing một đối tượng chứa một **danh sách** bằng một dấu ngoặc vuông sẽ luôn trả về kiểu danh sách, ngay cả khi chỉ một đối tượng được trả về . Tuy nhiên, hai dấu ngoặc vuông có thể được sử dụng để truy cập một phần tử đơn lẻ đối và trả về một kiểu không phải là một danh sách.\
Dấu ngoặc vuông cũng có thể được viết sau nhau, như được minh họa bên dưới.

Bạn có thể xem một [giải thích trực quan về việc indexing với ví dụ về hộp lắc hạt tiêu](https://r4ds.had.co.nz/vectors.html#lists-of-condiments) tại đây, rất hài hước và hữu ích.

```{r}
# define demo list
my_list <- list(
  # First element in the list is a character vector
  hospitals = c("Central", "Empire", "Santa Anna"),
  
  # second element in the list is a data frame of addresses
  addresses   = data.frame(
    street = c("145 Medical Way", "1048 Brown Ave", "999 El Camino"),
    city   = c("Andover", "Hamilton", "El Paso")
    )
  )
```

Đây là cách mà một danh sách được in trong R console. Có hai phần tử được đặt tên:

-   `hospitals`, một vector chứa ký tự\
-   `addresses`, một data frame chứa các địa chỉ

```{r}
my_list
```

Dưới đây là các phương pháp indexing mà bạn có thể sử dụng:

```{r}
my_list[1] # this returns the element in class "list" - the element name is still displayed

my_list[[1]] # this returns only the (unnamed) character vector

my_list[["hospitals"]] # you can also index by name of the list element

my_list[[1]][3] # this returns the third element of the "hospitals" character vector

my_list[[2]][1] # This returns the first column ("street") of the address data frame

```

<!-- ======================================================= -->

### Xóa đối tượng {.unnumbered}

Bạn có thể xóa từng đối tượng riêng lẻ khỏi cửa sổ R environment bằng cách để tên của đối tượng cần xóa vào trong hàm `rm()` (không có dấu ngoặc kép):

```{r, eval=F}
rm(object_name)
```

Bạn có thể xóa tất cả các đối tượng (xóa không gian làm việc của bạn) bằng cách chạy:

```{r, eval=F}
rm(list = ls(all = TRUE))
```

<!-- ======================================================= -->

<!-- ======================================================= -->

<!-- ======================================================= -->

## Piping (`%>%`)

**Hai cách tiếp cận chung để làm việc với các đối tượng là:**

1)  **Pipes/tidyverse** - pipes chuyển một đối tượng từ hàm này sang hàm khác - tập trung vào *hành động* chứ không phải đối tượng\
2)  **Xác định đối tượng trung gian** - một đối tượng được xác định lại nhiều lần - tập trung vào đối tượng

<!-- ======================================================= -->

### **Pipes** {.unnumbered}

**Giải thích một cách đơn giản, toán tử pipe (`%>%`) chuyển một đầu ra trung gian từ hàm này sang hàm tiếp theo.**\
Hiểu đơn giản pipe nghĩa là "sau đó". Nhiều hàm có thể được liên kết với nhau bằng toán tử `%>%`.

-   **Piping nhấn mạnh một chuỗi các hành động, không phải đối tượng mà các hành động đang áp dụng**\
-   Pipes được áp dụng tốt nhất khi một chuỗi hành động phải được thực hiện trên một đối tượng\
-   Pipes đến từ package **magrittr**, và đã tự động được thêm vào packages **dplyr** và **tidyverse**
-   Pipes làm cho code sạch hơn, dễ đọc hơn và trực quan hơn

Đọc thêm về cách tiếp cận này trong package tidyverse tại đây [Hướng dẫn](https://style.tidyverse.org/pipes.html)

Đây là một ví dụ mô phỏng dùng để so sánh, sử dụng các hàm hư cấu để "nướng bánh". Đầu tiên, phương pháp pipe:

```{r piping_example_pipe, eval=F}
# A fake example of how to bake a cake using piping syntax

cake <- flour %>%       # to define cake, start with flour, and then...
  add(eggs) %>%   # add eggs
  add(oil) %>%    # add oil
  add(water) %>%  # add water
  mix_together(         # mix together
    utensil = spoon,
    minutes = 2) %>%    
  bake(degrees = 350,   # bake
       system = "fahrenheit",
       minutes = 35) %>%  
  let_cool()            # let it cool down
```

Đây là một [link](https://cfss.uchicago.edu/notes/pipes/#:~:text=Pipes%20are%20an%20extremely%20useful,code%20and%20combine%20multiple%20operations) khác mô tả công dụng của pipe.

Piping không phải là một hàm trong **base** R. Để sử dụng piping, package **magrittr** phải được cài đặt và gọi ra trong phiên làm việc hiện tại (điều này thường được thực hiện bằng cách gọi package **tidyverse** hoặc **dplyr**). Bạn có thể [đọc thêm về piping trong tài liệu magrittr](https://magrittr.tidyverse.org/).

Lưu ý rằng cũng giống như các lệnh R khác, các pipes có thể được sử dụng để hiển thị kết quả hoặc lưu/lưu lại một đối tượng, tùy thuộc vào toán tử `<-` được code như thế nào. Xem hai ví dụ dưới đây:

```{r, eval=F}
# Create or overwrite object, defining as aggregate counts by age category (not printed)
linelist_summary <- linelist %>% 
  count(age_cat)
```

```{r}
# Print the table of counts in the console, but don't save it
linelist %>% 
  count(age_cat)
```

**`%<>%`**\

Đây là một "assignment pipe (pipe dùng để gán" từ package **magrittr**, package này sẽ *pipe một đối tượng theo chiều tiến lên và cũng tái định nghĩa lại đối tượng*. Đối tượng cần đứng đầu trong chuỗi pipe. Nó nhanh hơn sử dụng pipe thông thường. Hai lệnh dưới đây là tương đương với nhau:

```{r, eval=F}
linelist <- linelist %>%
  filter(age > 50)

linelist %<>% filter(age > 50)
```

<!-- ======================================================= -->

### Định nghĩa đối tượng trung gian {.unnumbered}

Cách tiếp cận này dùng để thay đổi đối tượng/dataframes sẽ phát huy hiệu quả nếu:

-   Bạn cần thao tác trên nhiều đối tượng\
-   Các bước trung gian có ý nghĩa cụ thể và xứng đáng tạo các tên đối tượng riêng biệt

**Các nguy cơ:**

-   Tạo đối tượng mới cho mỗi bước có nghĩa là bạn sẽ tạo thêm rất nhiều đối tượng. Nếu bạn sử dụng không cẩn thận, bạn có thể dễ dàng bị nhầm lẫn!\
-   Đặt thêm nhiều tên cho nhiều đối tượng có thể gây nhầm lẫn\
-   Nếu có lỗi thì không dễ để phát hiện

Đặt tên cho từng đối tượng trung gian hoặc ghi đè lên đối tượng gốc hoặc kết hợp tất cả hàm với nhau đều đi kèm với những rủi ro.

Dưới đây vẫn là ví dụ mô phỏng quy trình làm "bánh" tương tự như trên, nhưng sử dụng phong cách này:

```{r piping_example_redefine, eval=F}
# a fake example of how to bake a cake using this method (defining intermediate objects)
batter_1 <- left_join(flour, eggs)
batter_2 <- left_join(batter_1, oil)
batter_3 <- left_join(batter_2, water)

batter_4 <- mix_together(object = batter_3, utensil = spoon, minutes = 2)

cake <- bake(batter_4, degrees = 350, system = "fahrenheit", minutes = 35)

cake <- let_cool(cake)
```

Kết hợp tất cả các hàm với nhau - câu lệnh rất khó đọc:

```{r eval=F}
# an example of combining/nesting mutliple functions together - difficult to read
cake <- let_cool(bake(mix_together(batter_3, utensil = spoon, minutes = 2), degrees = 350, system = "fahrenheit", minutes = 35))
```

<!-- ======================================================= -->

## Các toán tử và hàm chính {#operators}

Mục này sẽ trình bày chi tiết các toán tử trong R, chẳng hạn như :

-   Toán tử định nghĩa\
-   Toán tử quan hệ (nhỏ hơn, bằng nhau..)\
-   Toán tử logic (và, hoặc..)\
-   Xử lý missing\
-   Các toán tử và hàm toán học (+/-, \>, sum(), median(), ...)\
-   Toán tử `%in%`

<!-- ======================================================= -->

### Toán tử gán {.unnumbered}

**`<-`**

Toán tử gán cơ bản trong R là `<-`. Chẳng hạn như `object_name <- value`.\
Toán tử gán này cũng có thể được viết là `=`. Chúng tôi khuyên bạn nên sử dụng `<-`.\
Bạn nên sử dụng dấu cách trong khi viết code với toán tử gán để dễ đọc hơn.

**`<<-`**

Khi [Viết hàm], hoặc khi sử dụng R với scipt nguồn, thì bạn có thể cần sử dụng toán tử gán này `<<-` (từ **base** R). Toán tử này được sử dụng để định nghĩa một đối tượng trong một hàm lồng trong một hàm khác. Xem thêm tại [nguồn tham khảo online này](https://stat.ethz.ch/R-manual/R-devel/library/base/html/assignOps.html).

**`%<>%`**

Đây là một "pipe gán" từ package **magrittr**, package này sẽ *gán một đối tượng theo chiều tiến lên và cũng định nghĩa lại đối tượng*. Pipe gán phải là toán tử đầu tiên trong chuỗi pipe code. Đây là cách viết ngắn gọn, như được trình bày dưới đây là hai ví dụ tương đương với nhau:

```{r, eval=F}
linelist <- linelist %>% 
  mutate(age_months = age_years * 12)
```

Đoạn code bên trên tương đương với code dưới đây:

```{r, eval=F}
linelist %<>% mutate(age_months = age_years * 12)
```

**`%<+%`**

Toán tử này được sử dụng dể thêm dữ liệu vào Cây phả hệ với package **ggtree**. Xem thêm chương [Cây phả hệ] hoặc [Sách online này](https://yulab-smu.top/treedata-book/).

<!-- ======================================================= -->

### Toán tử quan hệ và logic {.unnumbered}

Toán tử quan hệ so sánh các giá trị và thường được sử dụng khi định nghĩa các biến mới và tập con của bộ dữ liệu. Dưới đây là các toán tử quan hệ phổ biến trong R:

+-----------------------+------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Ý nghĩa               | Toán tử    | Ví dụ        | Kết quả đầu ra của ví dụ                                                                                                                                   |
+=======================+============+==============+============================================================================================================================================================+
| Bằng                  | `==`       | `"A" == "a"` | `FALSE` (vì R phân biệt chữ hoa chữ thường) Lưu ý rằng *== (hai dấu bằng) khác với = (một dấu bằng), một dấu bằng hoạt động tương tự với toán tử gán `<-`* |
+-----------------------+------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Không bằng            | `!=`       | `2 != 0`     | `TRUE`                                                                                                                                                     |
+-----------------------+------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Lớn hơn               | `>`        | `4 > 2`      | `TRUE`                                                                                                                                                     |
+-----------------------+------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Nhỏ hơn               | `<`        | `4 < 2`      | `FALSE`                                                                                                                                                    |
+-----------------------+------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Lớn hơn hoặc bằng     | `>=`       | `6 >= 4`     | `TRUE`                                                                                                                                                     |
+-----------------------+------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Nhỏ hơn hoặc bằng     | `<=`       | `6 <= 4`     | `FALSE`                                                                                                                                                    |
+-----------------------+------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Giá trị bị missing    | `is.na()`  | `is.na(7)`   | `FALSE` (Xem thêm chương [Dữ liệu Missing])                                                                                                                |
+-----------------------+------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Giá trị không missing | `!is.na()` | `!is.na(7)`  | `TRUE`                                                                                                                                                     |
+-----------------------+------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------+

Các toán tử logic, chẳng hạn như AND và OR, thường được sử dụng để kết nối các quan hệ và tạo ra các điều kiện phức tạp hơn. Các biểu thức phức tạp có thể yêu cầu dấu ngoặc đơn () để phân nhóm và thứ tự áp dụng.

+---------------+---------------------------------------------------------------------------------+
| Ý nghĩa       | Toán tử                                                                         |
+===============+=================================================================================+
| AND           | `&`                                                                             |
+---------------+---------------------------------------------------------------------------------+
| OR            | `|` (thanh dọc)                                                                 |
+---------------+---------------------------------------------------------------------------------+
| Dấu ngoặc đơn | `( )` Được sử dụng để nhóm các tiêu chí lại với nhau và làm rõ thứ tự hoạt động |
+---------------+---------------------------------------------------------------------------------+

Ví dụ: chúng ta có một số liệu có tên linelist với hai biến mà chúng tôi muốn sử dụng để minh họa, `hep_e_rdt`: kết quả xét nghiệm và `other_cases_in_hh`: những trường hợp khác trong gia đình. Lệnh dưới đây sử dụng hàm `case_when()` để tạo biến mới `case_def` như sau:

```{r eval=FALSE}
linelist_cleaned <- linelist %>%
  mutate(case_def = case_when(
    is.na(rdt_result) & is.na(other_case_in_home)            ~ NA_character_,
    rdt_result == "Positive"                                 ~ "Confirmed",
    rdt_result != "Positive" & other_cases_in_home == "Yes"  ~ "Probable",
    TRUE                                                     ~ "Suspected"
  ))
```

+---------------------------------------------------------------------------------------------------------+-------------------------------------------+
| Điều kiện trong ví dụ ở trên                                                                            | Giá trị kết quả trong biến mới "case_def" |
+=========================================================================================================+===========================================+
| Nếu giá trị của biến `rdt_result` và `other_cases_in_home` bị missing                                   | `NA` (missing)                            |
+---------------------------------------------------------------------------------------------------------+-------------------------------------------+
| Nếu giá trị trong `rdt_result` là "Positive"                                                            | "Confirmed"                               |
+---------------------------------------------------------------------------------------------------------+-------------------------------------------+
| Nếu giá trị trong `rdt_result` KHÔNG phải là "Positive" VÀ giá trị trong `other_cases_in_home` là "Yes" | "Probable"                                |
+---------------------------------------------------------------------------------------------------------+-------------------------------------------+
| Nếu một trong các tiêu chí trên không được đáp ứng                                                      | "Suspected"                               |
+---------------------------------------------------------------------------------------------------------+-------------------------------------------+

Lưu ý rằng R có phân biệt chữ hoa chữ thường, vì vậy *"Positive" khác với "positive"...*

<!-- ======================================================= -->

### Giá trị Missing {.unnumbered}

Trong R, giá trị missing được biểu diễn bằng giá trị đặc biệt `NA` (giá trị "dành riêng cho missing") (chữ N và A viết hoa - không nằm trong dấu ngoặc kép). Nếu dữ liệu bạn nhập vào R bị missing theo cách khác (ví dụ: 99, "Missing", or .), bạn có thể sẽ cần phải mã hóa lại các giá trị đó thành `NA`. Cách thực hiện việc này được đề cập trong chương [Nhập xuất dữ liệu].

**Để kiểm tra xem một giá trị có phải là `NA` hay không, sử dụng hàm đặc biệt `is.na()`**, kết quả sẽ trả về `TRUE` hoặc `FALSE`.

```{r basics_operators_missing}
rdt_result <- c("Positive", "Suspected", "Positive", NA)   # two positive cases, one suspected, and one unknown
is.na(rdt_result)  # Tests whether the value of rdt_result is NA
```

Đọc thêm về missing, vô hạn, `NULL`, và các giá trị không thể trong chương [Dữ liệu Missing]. Tìm hiểu thêm cách chuyển đổi các giá trị bị missing khi nhập dữ liệu trong chương [Nhập xuất dữ liệu].

<!-- ======================================================= -->

### Toán học và thống kê

{.unnumbered}

Tất cả các toán tử và hàm trong chương này đều có sẵn bằng cách sử dụng **base** R.

#### Toán tử toán học {.unnumbered}

Chúng thường được sử dụng để thực hiện phép cộng, phép chia, để tạo cột mới, v.v. Dưới đây là các toán tử toán học phổ biến trong R. Việc bạn có đặt dấu cách xung quanh các toán tử hay không là không quan trọng.

| Mục đích         | Ví dụ trong R |
|------------------|---------------|
| phép cộng        | 2 + 3         |
| phép trừ         | 2 - 3         |
| phép nhân        | 2 \* 3        |
| phép chia        | 30 / 5        |
| số mũ            | 2\^3          |
| thứ tự hoạt động | ( )           |

#### Các hàm toán học {.unnumbered}

| Mục tiêu          | Hàm                                   |
|-------------------|---------------------------------------|
| làm tròn          | round(x, digits = n)                  |
| làm tròn          | janitor::round_half_up(x, digits = n) |
| làm tròn lên      | ceiling(x)                            |
| làm tròn xuống    | floor(x)                              |
| giá trị tuyệt đối | abs(x)                                |
| căn bậc hai       | sqrt(x)                               |
| số mũ             | exponent(x)                           |
| logarit tự nhiên  | log(x)                                |
| logarit cơ số 10  | log10(x)                              |
| logarit cơ số 2   | log2(x)                               |

lưu ý: sử dụng hàm `round()` và `digits =` để xác định số chữ số thập phân được hiển thị. Sử dụng hàm `signif()` để làm tròn đến số chữ số nhất định.

#### Ký hiệu khoa học {.unnumbered}

Khả năng ký hiệu khoa học được sử dụng phụ thuộc vào giá trị của `scipen`.

Từ tài liệu hướng dẫn của `?options`: scipen được áp dụng khi quyết định in các giá trị số theo ký hiệu cố định hoặc hàm mũ. Giá trị dương thuộc về ký hiệu cố định còn giá trị âm thuộc về ký hiệu khoa học: ký hiệu cố định sẽ luôn được ưu tiên trừ khi có nhiều chữ số 'scipen'.

Nếu như có rất nhiều số bé cần hiển thị (vd: số 0), mặc định tính năng này sẽ "được bật". Để "tắt" tính năng ký hiệu khoa học trong phiên làm việc của bạn, hãy thiết lập nó với một số rất lớn, ví dụ:

```{r, eval=F}
# turn off scientific notation
options(scipen=999)
```

#### Làm tròn {.unnumbered}

[***NGUY HIỂM:*** Hàm `round()` sử dụng "cách làm tròn của ngân hàng" nghĩa là chỉ làm tròn với số .5 nếu số được làm tròn lên là số chẵn. Sử dụng hàm `round_half_up()` từ package **janitor** để thống nhất cách làm tròn với giá trị .5. Xem thêm [giải thích sau đây](https://cran.r-project.org/web/packages/janitor/vignettes/janitor.html#explore-records-with-duplicated-values-for-specific-combinations-of-variables-with-get_dupes)]{style="color: red;"}

```{r}
# use the appropriate rounding function for your work
round(c(2.5, 3.5))

janitor::round_half_up(c(2.5, 3.5))
```

#### Các hàm thống kê {.unnumbered}

[***CẨN TRỌNG:*** Các hàm sau đây sẽ mặc định bao gồm cả giá trị missing khi tính toán. Giá trị missing sẽ trả về kết quả đầu ra chứa `NA`, trừ khi đối số `na.rm = TRUE` được xác định khi viết hàm. Nó cũng có thể viết ngắn gọn thành `na.rm = T`.]{style="color: orange;"}

| Mục đích          | Hàm                |
|-------------------|--------------------|
| trung bình        | mean(x, na.rm=T)   |
| trung vị          | median(x, na.rm=T) |
| độ lệch chuẩn     | sd(x, na.rm=T)     |
| phân vị\*         | quantile(x, probs) |
| tổng              | sum(x, na.rm=T)    |
| giá trị nhỏ nhất  | min(x, na.rm=T)    |
| giá trị lớn nhất  | max(x, na.rm=T)    |
| khoảng giá trị số | range(x, na.rm=T)  |
| tóm tắt\*\*       | summary(x)         |

Notes:

-   `*quantile()`: `x` là vectơ số cần khảo sát, và `probs =` là một vectơ số với các xác suất nằm giữa 0 và 1.0, ví dụ `c(0.5, 0.8, 0.85)`
-   `**summary()`: trả về tóm tắt một vectơ số bao gồm giá trị trung bình, trung vị, và các khoảng phân vị thường gặp

[***NGUY HIỂM:*** Nếu cung cấp một vectơ số cho một trong các hàm trên, hãy đảm bảo các số được đặt trong hàm `c()` .]{style="color: red;"}

```{r}
# If supplying raw numbers to a function, wrap them in c()
mean(1, 6, 12, 10, 5, 0)    # !!! INCORRECT !!!  

mean(c(1, 6, 12, 10, 5, 0)) # CORRECT
```

#### Một số hàm hữu ích khác {.unnumbered}

+------------------------+-------------------+-------------------------------------------------+
| Mục đích               | Hàm               | Ví dụ                                           |
+========================+===================+=================================================+
| Tạo chuỗi số liên tục  | seq(from, to, by) | `seq(1, 10, 2)`                                 |
+------------------------+-------------------+-------------------------------------------------+
| lặp x, n lần           | rep(x, ntimes)    | `rep(1:3, 2)` or `rep(c("a", "b", "c"), 3)`     |
+------------------------+-------------------+-------------------------------------------------+
| chia nhỏ một vectơ số  | cut(x, n)         | `cut(linelist$age, 5)`                          |
+------------------------+-------------------+-------------------------------------------------+
| lấy một mẫu ngẫu nhiên | sample(x, size)   | `sample(linelist$id, size = 5, replace = TRUE)` |
+------------------------+-------------------+-------------------------------------------------+

<!-- ======================================================= -->

### `%in%` {.unnumbered}

Một toán tử rất hữu ích để nhanh chóng đánh giá xem một giá trị có nằm trong một vectơ hoặc một dataframe hay không.

```{r}
my_vector <- c("a", "b", "c", "d")
```

```{r}
"a" %in% my_vector
"h" %in% my_vector
```

Để truy vấn một giá trị **không** `%in%` một vectơ, hãy đặt dấu chấm than (!) **phía trước** biểu thức logic:

```{r}
# to negate, put an exclamation in front
!"a" %in% my_vector
!"h" %in% my_vector
```

`%in%` sẽ rất hữu dụng khi dùng hàm `case_when()` của package **dplyr**. Bạn có thể định nghĩa một vectơ trước đó, sau đó tham chiếu đến nó. ví dụ:

```{r eval=F}
affirmative <- c("1", "Yes", "YES", "yes", "y", "Y", "oui", "Oui", "Si")

linelist <- linelist %>% 
  mutate(child_hospitaled = case_when(
    hospitalized %in% affirmative & age < 18 ~ "Hospitalized Child",
    TRUE                                      ~ "Not"))
```

Lưu ý: Nếu bạn muốn phát hiện một phần của chuỗi, có lẽ việc sử dụng hàm `str_detect()` từ package **stringr**, sẽ không chấp nhận một vectơ ký tự kiểu như `c("1", "Yes", "yes", "y")`. Thay vào đó, nó cần được cung cấp dưới dạng một *biểu thức chính quy* - một chuối cô đọng với thanh dọc cho phép so sánh OR, chẳng hạn như "1\|Yes\|yes\|y". Ví dụ, `str_detect(hospitalized, "1|Yes|yes|y")`. Xem thêm chương [Ký tự và chuỗi] để biết thêm chi tiết.

Bạn có thể chuyển đổi một vectơ ký tự thành một biểu thức chính quy được đặt tên bằng lệnh này:

```{r}
affirmative <- c("1", "Yes", "YES", "yes", "y", "Y", "oui", "Oui", "Si")
affirmative

# condense to 
affirmative_str_search <- paste0(affirmative, collapse = "|")  # option with base R
affirmative_str_search <- str_c(affirmative, collapse = "|")   # option with stringr package

affirmative_str_search
```

<!-- ======================================================= -->

<!-- ======================================================= -->

<!-- ======================================================= -->

## Lỗi và cảnh báo

Phần này giải thích:

-   Sự khác biệt giữa lỗi và cảnh báo\
-   Mẹo cú pháp chung để viết code R\
-   Trợ giúp viết code

Các lỗi thường gặp và cảnh báo cũng như mẹo khắc phục sự cố có thể được tìm thấy trong chương [Các lỗi thường gặp].

<!-- ======================================================= -->

### Lỗi và Cảnh báo {.unnumbered}

Khi một lệnh được thực thi, cửa sổ R Console có thể hiển thị cho bạn cảnh báo hoặc thông báo lỗi bằng văn bản màu đỏ.

-   Một **cảnh báo** nghĩa là R đã hoàn thành lệnh của bạn, nhưng phải thực hiện các bước bổ sung hoặc tạo ra kết quả bất thường mà bạn cần lưu ý.

-   Một **lỗi** nghĩa là R không thể hoàn thành lệnh của bạn.

Tìm manh mối:

-   Thông báo lỗi/cảnh báo thường sẽ bao gồm số dòng xảy ra sự cố.

-   Nếu một đối tượng “không xác định được (is unknown)” hoặc “không tìm thấy (not found)”, có lẽ bạn đã viết sai chính tả, quên gọi một package bằng hàm library(), hoặc quên chạy lại tập lệnh của bạn sau khi thực hiện các thay đổi.

Nếu vẫn thất bại, hãy sao chép thông báo lỗi vào Google cùng với một số từ khóa chính - rất có thể ai đó cũng đã gặp lỗi này rồi!

<!-- ======================================================= -->

### Mẹo cú pháp chung {.unnumbered}

Một số điều cần nhớ khi viết lệnh trong R, để tránh lỗi và cảnh báo:

-   Luôn đóng dấu ngoặc đơn - mẹo: đếm số lần mở dấu ngoặc đơn “(” và đóng dấu ngoặc đơn “)” cho mỗi đoạn mã
-   Tránh để khoảng trắng trong tên cột và đối tượng. Thay vào đó, hãy sử dụng dấu gạch dưới (_) hoặc dấu chấm (.)
-   Theo dõi và nhớ tách các đối số của hàm bằng dấu phẩy
-   R phân biệt chữ hoa và chữ thường, nghĩa là `Variable_A` *khác* với `variable_A`

<!-- ======================================================= -->

### Trợ giúp viết code {.unnumbered}

Bất kỳ tập lệnh nào (RMarkdown hoặc những cái khác) sẽ cung cấp manh mối khi bạn mắc lỗi. Ví dụ: nếu bạn quên viết dấu phẩy ở vị trí cần thiết hoặc quên đóng dấu ngoặc đơn, RStudio sẽ treo cờ trên dòng đó, ở phía bên trái của script, để cảnh báo bạn.
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/basics.Rmd-->

# Chuyển đổi sang R {#transition-to-R}

Dưới đây, chúng tôi cung cấp một số lời khuyên và tài nguyên nếu bạn đang chuyển đổi sang R.

R được giới thiệu vào cuối những năm 1990 và kể từ đó đã phát triển quy mô mạnh mẽ. Khả năng của nó rộng đến mức các lựa chọn thương mại thay thế đã phản ứng với sự phát triển của R để duy trì tính cạnh tranh! ([đọc bài viết so sánh R, SPSS, SAS, STATA và Python](https://www.inwt-statistics.com/read-blog/comparison-of-r-python-sas-spss-and-stata.html)).

Hơn thế nữa, R đã dễ học hơn nhiều so với 10 năm trước. Trước đây, R nổi tiếng là khó sử dụng cho những người mới bắt đầu. Giờ đây việc này trở nên dễ dàng hơn nhiều với giao diện người dùng thân thiện như RStudio, code trực quan như **tidyverse** và có nhiều tài nguyên hướng dẫn.

[**Đừng ngần ngại - hãy đến khám phá thế giới của R!**]{style="color: darkgreen;"}

```{r, echo=F, out.width = "75%", out.height="75%", fig.align = "center"}
knitr::include_graphics(here::here("images", "transition_door.png"))
```

## Từ Excel

Chuyển đổi từ Excel trực tiếp sang R hoàn toàn là mục tiêu có thể đạt được. Nó dường như có vẻ khó khăn, nhưng bạn có thể làm được!

Sự thật là một người có kỹ năng Excel tốt có thể thực hiện các thao tác nâng cao chỉ trong Excel - ngay cả khi sử dụng các công cụ tạo script như VBA. Excel được sử dụng trên toàn thế giới và là một công cụ cần thiết cho một nhà dịch tễ học. Tuy nhiên, kết hợp nó với R có thể cải thiện và mở rộng đáng kể quy trình công việc của bạn.

### Lợi ích {.unnumbered}

Bạn sẽ thấy rằng việc sử dụng R mang lại những lợi ích to lớn trong việc tiết kiệm thời gian, giúp phân tích nhất quán và chính xác hơn, có khả năng tái lập, khả năng chia sẻ và sửa lỗi nhanh hơn. Giống như bất kỳ phần mềm mới nào, bạn phải đầu tư một "đường cong" thời gian học tập để trở nên quen thuộc. Bạn sẽ được mở ra những kĩ năng mới trên một phạm vi rộng đáng kể với R.

Excel là một phần mềm phổ biến mà người dùng mới bắt đầu có thể dễ dàng sử dụng để tạo ra các phân tích và sơ đồ hóa đơn giản với các thao tác "trỏ và nhấp". Trong khi đó, có thể mất vài tuần để trở nên quen thuộc với các chức năng và giao diện của R. Tuy nhiên, R đã phát triển trong những năm gần đây để trở nên thân thiện hơn với người mới bắt đầu.

Nhiều quy trình làm việc của Excel phụ thuộc vào trí nhớ và sự lặp lại - do đó, có nhiều khả năng xảy ra lỗi. Hơn nữa, nhìn chung, việc làm sạch dữ liệu, phương pháp phân tích và các phương trình được sử dụng đều bị ẩn đi trong trang tính. Có thể cần đến một khoảng thời gian đáng kể để những người mới bắt đầu hiểu trang tính Excel đang làm gì và cách khắc phục sự cố. Với R, tất cả các bước được viết rõ ràng trong script và có thể dễ dàng xem, chỉnh sửa, sửa lỗi và áp dụng cho các bộ dữ liệu khác.

**Để bắt đầu chuyển đổi từ Excel sang R, bạn phải điều chỉnh tư duy của mình theo một số hướng quan trọng:**

### Tidy data {.unnumbered}

Sử dụng "tidy" data để máy có thể đọc được thay vì dữ liệu lộn xộn "con người có thể đọc được". Dưới đây là ba yêu cầu chính đối với "tidy" data", đã được giải thích trong hướng dẫn này về ["tidy" data trong R](https://r4ds.had.co.nz/tidy-data.html):

-   Mỗi biến số nằm trên một cột
-   Mỗi quan sát phải nằm trên một hàng
-   Mỗi giá trị phải có ô riêng

Đối với người dùng Excel - hãy nghĩ đến vai trò của ["bảng" Excel](https://exceljet.net/excel-tables) trong việc chuẩn hóa dữ liệu và làm cho định dạng trở nên dễ đoán hơn.

Một ví dụ về "tidy" data là trường hợp bộ dữ liệu linelist được sử dụng trong cuốn sổ tay này - mỗi biến được chứa trong một cột, mỗi quan sát (mỗi trường hợp) có hàng riêng và mọi giá trị chỉ nằm trong một ô. Dưới đây, bạn có thể xem 50 hàng đầu tiên của bộ dữ liệu linelist:

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

*Nguyên nhân chính khiến người dùng gặp phải tình trạng non-tidy data là do nhiều bảng tính Excel được thiết kế để ưu tiên con người dễ đọc chứ không phải máy móc/phần mềm dễ đọc.*

Để giúp bạn thấy sự khác biệt, dưới đây là một số ví dụ mô phỏng về **non-tidy data** mà ưu tiên khả năng đọc của *con người* hơn khả năng đọc của *máy*:

```{r, echo=F, out.width = "100%", out.height="75%", fig.align = "center"}
knitr::include_graphics(here::here("images", "Excel_nonTidy_1.png"))
```

*Vấn đề:* Trong bảng tính ở trên, một số ô *đã được ghép với nhau* - khiến chúng trở nên khó đọc bởi R. Hàng nào nên được coi là "tiêu đề" không rõ ràng. Từ điển dựa trên màu sắc nằm ở phía bên phải và các giá trị ô được biểu thị bằng màu sắc - điều này cũng không dễ dàng được giải thích bởi R (cũng như những người bị mù màu!). Hơn nữa, các phần thông tin khác nhau được kết hợp thành một ô (nhiều tổ chức đối tác hoạt động trong cùng một khu hoặc trạng thái "TBC" trong cùng ô với "Partner D").

```{r, echo=F, out.width = "100%", out.height="100%", fig.align = "center"}
knitr::include_graphics(here::here("images", "Excel_nonTidy_2.png"))
```

*Vấn đề:* Trong bảng tính ở trên, có rất nhiều hàng và cột trống trong bộ dữ liệu - điều này sẽ gây phiền toái khi làm sạch trong R. Hơn nữa, tọa độ GPS được trải rộng trên hai hàng cho một trung tâm điều trị nhất định. Một lưu ý nhỏ - tọa độ GPS có hai định dạng khác nhau!

Các bộ dữ liệu "tidy" có thể không đọc được bằng mắt người, nhưng chúng giúp việc làm sạch và phân tích dữ liệu dễ dàng hơn rất nhiều! Tidy data có thể được lưu trữ ở nhiều định dạng khác nhau, chẳng hạn như dạng "dọc" hoặc "ngang""(xem chương về [Xoay trục dữ liệu]), tuy nhiên các nguyên tắc trên vẫn được tuân thủ.

### Hàm {.unnumbered}

Từ "hàm (function)" trong R có thể mới, nhưng khái niệm này cũng tồn tại trong Excel dưới dạng *công thức (formulas)*. Công thức trong Excel cũng yêu cầu cú pháp chính xác (ví dụ: vị trí của dấu chấm phẩy và dấu ngoặc đơn). Tất cả những gì bạn cần làm là tìm hiểu một vài hàm mới và cách chúng hoạt động cùng nhau trong R.

### Script {.unnumbered}

Thay vì nhấp vào các biểu tượng và kéo các ô, bạn sẽ viết *mọi* bước và quy trình thành một "script". Người dùng Excel có thể quen thuộc với "VBA macros", thứ mà cũng sử dụng cách tiếp cận script.

*R script bao gồm các hướng dẫn từng bước.* Điều này cho phép bất kỳ đồng nghiệp nào cũng có thể đọc script và dễ dàng xem các bước bạn đã thực hiện. Điều này cũng giúp loại bỏ lỗi hoặc các tính toán không chính xác. Xem phần [R cơ bản] về script để có thêm các ví dụ.

Dưới đây là một ví dụ của một R script:

```{r, echo=F, out.width = "75%", out.height="75%", fig.align = "center"}
knitr::include_graphics(here::here("images", "example_script.png"))
```

### Tài liệu liên quan đến chuyển đổi từ Excel-sang-R {.unnumbered}

Dưới đây là một vài đường link hướng dẫn giúp bạn chuyển đổi sang R từ Excel:

-   [R vs. Excel](https://www.northeastern.edu/graduate/blog/r-vs-excel/)\
-   [Các khóa RStudio trong R cho người dùng Excel](https://rstudio-conf-2020.github.io/r-for-excel/)

### Tương tác giữa R và Excel {.unnumbered}

R có khả năng mạnh trong việc nhập các Excel workbook, làm việc với dữ liệu, xuất/lưu tệp Excel và làm việc với các sắc thái của các trang tính Excel.

Đúng là một số định dạng Excel có tính thẩm mỹ hơn có thể bị mất trong quá trình chuyển đổi (ví dụ: chữ nghiêng, chữ nằm ngang, v.v.). Nếu quy trình công việc của bạn yêu cầu chuyển tài liệu qua lại giữa R và Excel trong khi vẫn giữ nguyên định dạng Excel ban đầu, hãy thử các package như **openxlsx**.

## Từ Stata

**Chuyển đến R từ Stata**

Nhiều nhà dịch tễ học được dạy cách sử dụng Stata ngay từ đầu, và có vẻ khó khăn khi chuyển sang R. Tuy nhiên, nếu bạn là một người dùng quen Stata thì việc chuyển sang R chắc chắn sẽ dễ quản lý hơn bạn nghĩ. Mặc dù có một số khác biệt chính giữa Stata và R về cách tạo và sửa đổi dữ liệu, cũng như cách triển khai các chức năng phân tích -- sau khi tìm hiểu những khác biệt chính này, bạn sẽ có thể chuyển đổi các kỹ năng của mình.

Dưới đây là một số cách chuyển đổi chính giữa Stata và R, điều mà có thể hữu ích khi bạn xem lại hướng dẫn này.

**Những lưu ý chung**

+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **STATA**                                                                                                                                                         | **R**                                                                                                                                                                                  |
+===================================================================================================================================================================+========================================================================================================================================================================================+
| Bạn chỉ có thể xem và thao tác với một bộ dữ liệu tại một thời điểm                                                                                               | Bạn có thể xem và thao tác với nhiều bộ dữ liệu cùng một lúc, do đó, bạn sẽ thường xuyên phải xác định bộ dữ liệu của mình trong code                                                  |
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Cộng đồng trực tuyến có sẵn trên <https://www.statalist.org/>                                                                                                     | Cộng đồng trực tuyến có sẵn trên [RStudio](https://community.rstudio.com/), [StackOverFlow](https://stackoverflow.com/questions/tagged/r) và [R-bloggers](https://www.r-bloggers.com/) |
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Thao tác trỏ và nhấp chuột như một tùy chọn                                                                                                                       | Thao tác trỏ và nhấp chuột được giảm tối thiểu                                                                                                                                         |
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trợ giúp cho các lệnh có sẵn trong `help [command]`                                                                                                               | Trợ giúp có sẵn trong `[function]?` hoặc tìm kiếm trong cửa sổ Help                                                                                                                    |
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Bình luận code sử dụng \* hoặc /// hoặc /\* VĂN BẢN \*/                                                                                                           | Bình luận code sử dụng \#                                                                                                                                                              |
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Hầu hết tất cả các lệnh đều được tích hợp sẵn trong Stata. Các lệnh mới do người dùng viết có thể được cài đặt như file **ado** sử dụng **ssc install** [package] | R được cài đặt sẵn các lệnh **cơ bản**, nhưng quá trình sử dụng thông thường cần cài đặt các package khác từ CRAN (xem chương [R cơ bản])                                              |
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Phân tích thường được viết trong **do** file                                                                                                                      | Phân tích được viết trong R script ở cửa sổ chính của RStudio. Các script của R markdown là một giải pháp thay thế.                                                                    |
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

**Thư mục làm việc**

+--------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **STATA**                                                                                        | **R**                                                                                                                                                       |
+==================================================================================================+=============================================================================================================================================================+
| Thư mục làm việc bao gồm các đường dẫn tuyệt đối (Ví dụ: "C:/usename/documents/projects/data/")\ | Thư mục làm việc có thể là đường dẫn tuyệt đối hoặc tương đối đến thư mục gốc của dự án bằng cách sử dụng package **here** (xem chương [Nhập xuất dữ liệu]) |
+--------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Xem thư mục làm việc hiện tại với **pwd**                                                        | Sử dụng `getwd()` hoặc `here()` (nếu sử dụng package **here**), với dấu ngoặc đơn trống                                                                     |
+--------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Cài đặt thư mục làm việc với **cd** "folder location"                                            | Sử dụng `setwd(“folder location”)` hoặc `set_here("folder location)` (nếu sử dụng package **here**)                                                         |
+--------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------+

**Nhập và xem dữ liệu**

+----------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **STATA**                                                                                                                                          | **R**                                                                                                                                                                                                           |
+====================================================================================================================================================+=================================================================================================================================================================================================================+
| Các lệnh cụ thể cho mỗi loại tệp                                                                                                                   | Sử dụng `import()` từ package **rio** cho hầu hết tất cả các loại tệp. Các chức năng cụ thể tồn tại dưới dạng lựa chọn thay thế (xem chương [Nhập xuất dữ liệu])                                                |
+----------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Đọc file csv được thực hiện bằng cách sử dụng **import delimited** "filename.csv"                                                                  | Sử dụng `import("filename.csv")`                                                                                                                                                                                |
+----------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Đọc file xslx được thực hiện bằng cách sử dụng **import excel** "filename.xlsx"                                                                    | Sử dụng `import("filename.xlsx")`                                                                                                                                                                               |
+----------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Duyệt dữ liệu của bạn trong một cửa sổ mới bằng lệnh **browse**                                                                                    | Xem bộ dữ liệu trong cửa sổ nguồn RStudio sử dụng `View(dataset)`. *Bạn cần xác định tên bộ dữ liệu của mình cho hàm trong R vì nhiều bộ dữ liệu có thể được mở cùng một lúc. Lưu ý viết hoa "V" trong hàm này* |
+----------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Có cái nhìn tổng quan hơn về bộ dữ liệu của bạn bằng cách sử dụng **summarize**, lệnh này cung cấp tên biến và các thông tin cơ bản của bộ dữ liệu | Có cái nhìn tổng quan hơn về bộ dữ liệu của bạn bằng cách sử dụng `summary(dataset)`                                                                                                                            |
+----------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

**Thao tác dữ liệu cơ bản**

+-------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **STATA**                                                                                 | **R**                                                                                                                                                                                     |
+===========================================================================================+===========================================================================================================================================================================================+
| Các cột của bộ dữ liệu thường được gọi là "các biến (variables)"                          | Thường được gọi là "các cột (columns)" hoặc thỉnh thoảng là "các véctơ (vectors)" hoặc "các biến (variables)"                                                                             |
+-------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Không cần xác định bộ dữ liệu                                                             | Trong mỗi lệnh dưới đây, bạn cần xác định bộ dữ liệu - xem ví dụ trong chương [Làm sạch số liệu và các hàm quan trọng]                                                                    |
+-------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Các biến mới được tạo bằng lệnh **generate** *varname* =                                  | Tạo các biến mới bằng cách sử dụng lệnh `mutate(varname = )`. Xem chương [Làm sạch số liệu và các hàm quan trọng] để biết tất cả chi tiết về câu lệnh **dplyr** bên dưới                  |
+-------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Các biến được đổi tên bằng cách sử dụng **rename** *old_name new_name*                    | Các cột có thể được đổi tên bằng cách sử dụng lệnh `rename(new_name = old_name)`                                                                                                          |
+-------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Các biến được lược bỏ sử dụng **drop** *varname*                                          | Có thể lược bỏ các cột bằng cách sử dụng lệnh `select()` với tên cột trong ngoặc đơn sau dấu trừ                                                                                          |
+-------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Các biến factor có thể được gán nhãn bằng cách sử dụng một loạt lệnh như **label define** | Việc gán nhãn các giá trị có thể được thực hiện bằng cách chuyển đổi cột thành nhóm Factor và chỉ định thứ bậc. Xem chương [Factors]. Tên cột thường không được gán nhãn như trong Stata. |
+-------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

**Phân tích mô tả**

+-------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **STATA**                                                                                 | **R**                                                                                                                                                                                                |
+===========================================================================================+======================================================================================================================================================================================================+
| Đếm số lượng bảng của một biến sử dụng **tab** *varname*                                  | Cung cấp bộ dữ liệu và tên cột cho `table()` ví dụ như `table(dataset$colname)`. Ngoài ra, có thể sử dụng lệnh `count(varname)` từ package **dplyr**, đã được giải thích trong chương [Nhóm dữ liệu] |
+-------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Lập bảng chéo của hai biến trong bảng 2x2 được thực hiện bằng **tab** *varname1 varname2* | Sử dụng `table(dataset$varname1, dataset$varname2` hoặc `count(varname1, varname2)`                                                                                                                  |
+-------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

Mặc dù danh sách này cung cấp một cái nhìn tổng quan về những điều cơ bản trong việc chuyển các lệnh Stata sang R, nhưng nó vẫn chưa đầy đủ. Bạn có thể quan tâm tới nhiều nguồn tài nguyên tuyệt vời khác dành cho người dùng Stata chuyển sang R:

-   <https://dss.princeton.edu/training/RStata.pdf>
-   <https://clanfear.github.io/Stata_R_Equivalency/docs/r_stata_commands.html>
-   <http://r4stats.com/books/r4stata/>

## Từ SAS

**Chuyển từ SAS sang R**

SAS thường được sử dụng tại các cơ quan y tế công cộng và các lĩnh vực nghiên cứu học thuật. Mặc dù chuyển đổi sang một ngôn ngữ mới hiếm khi là một quá trình đơn giản, nhưng hiểu được những điểm khác biệt chính giữa SAS và R có thể giúp bạn bắt đầu chuyển hướng ngôn ngữ mới bằng ngôn ngữ mẹ đẻ của mình. Dưới đây là phác thảo các bước chuyển đổi chính trong quản lý dữ liệu và phân tích mô tả giữa SAS và R.

**Những lưu ý chung**

+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+
| **SAS**                                                                                                                                                                                                               | **R**                                                                                                                                         |
+=======================================================================================================================================================================================================================+===============================================================================================================================================+
| Cộng đồng trực tuyến có sẵn trên [SAS Customer Support](https://support.sas.com/en/support-home.html)                                                                                                                 | Cộng đồng trực tuyến có sẵn trên RStudio, StackOverFlow và R-bloggers                                                                         |
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+
| Trợ giúp cho các lệnh có sẵn trong `help [command]`                                                                                                                                                                   | Trợ giúp có sẵn trong `[function]?` hoặc tìm kiếm trong cửa sổ Help                                                                           |
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+
| Bình luận code sử dụng `* VĂN BẢN ;` hoặc `/* VĂN BẢN */`                                                                                                                                                             | Bình luận code sử dụng \#                                                                                                                     |
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+
| Hầu hết tất cả các lệnh đều được tích hợp sẵn. Người dùng có thể viết lệnh mới bằng cách sử dụng SAS macro, SAS/IML, SAS Component Language (SCL) và mới đây nhất là, được thực hiện bằng `Proc Fcmp` và `Proc Proto` | R được cài đặt sẵn các lệnh \*\*cơ bản\*\*, nhưng quá trình sử dụng thông thường cần cài đặt các package khác từ CRAN (xem chương [R cơ bản]) |
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+
| Phân tích thường được viết trong chương trình SAS ở cửa sổ Editor.                                                                                                                                                    | Phân tích được viết trong R script trong cửa sổ chính của RStudio. Các script của R markdown là một giải pháp thay thế.                       |
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+

**Thư mục làm việc**

+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **SAS**                                                                                                                                                                         | **R**                                                                                                                                                       |
+=================================================================================================================================================================================+=============================================================================================================================================================+
| Thư mục làm việc có thể là đường dẫn tuyệt đối hoặc tương đối đến thư mục gốc của dự án bằng cách sử dụng `%let rootdir=/root path; %include “&rootdir/subfoldername/filename”` | Thư mục làm việc có thể là đường dẫn tuyệt đối hoặc tương đối đến thư mục gốc của dự án bằng cách sử dụng package **here** (xem chương [Nhập xuất dữ liệu]) |
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Xem thư mục làm việc hiện tại với `%put %sysfunc(getoption(work));`                                                                                                             | Sử dụng `getwd()` hoặc `here()` (nếu sử dụng package **here**), với dấu ngoặc đơn trống                                                                     |
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Cài đặt thư mục làm việc với `libname “folder location”`                                                                                                                        | Sử dụng `setwd(“folder location”)` hoặc `set_here("folder location)` (nếu sử dụng package **here**)                                                         |
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------+

**Nhập và xem dữ liệu**

+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **SAS**                                                                                                                                                                                                                 | **R**                                                                                                                                                                                                       |
+=========================================================================================================================================================================================================================+=============================================================================================================================================================================================================+
| Sử dụng lệnh `Proc Import` hoặc sử dụng lệnh `Data Step Infile`                                                                                                                                                         | Sử dụng `import()` từ package **rio** cho hầu hết tất cả các loại tệp. Các chức năng cụ thể tồn tại dưới dạng lựa chọn thay thế (xem chương [Nhập xuất dữ liệu])                                            |
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Đọc file csv được thực hiện bằng cách sử dụng `Proc Import datafile=”filename.csv” out=work.filename dbms=CSV; run;` HOẶC sử dụng [Data Step Infile statement](http://support.sas.com/techsup/technote/ts673.pdf)       | Sử dụng `import("filename.csv")`                                                                                                                                                                            |
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Đọc các tệp xslx được thực hiện bằng cách sử dụng `Proc Import datafile=”filename.xlsx” out=work.filename dbms=xlsx; run;` HOẶC sử dụng [Data Step Infile statement](http://support.sas.com/techsup/technote/ts673.pdf) | Sử dụng `import("filename.xlsx")`                                                                                                                                                                           |
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Duyệt bộ dữ liệu của bạn trong cửa sổ mới bằng cách mở cửa sổ Explorer và chọn thư viện và tập dữ liệu mong muốn.                                                                                                       | Xem bộ dữ liệu trong cửa sổ nguồn RStudio sử dụng View(dataset). *Bạn cần xác định tên bộ dữ liệu của mình cho hàm trong R vì nhiều bộ dữ liệu có thể được mở cùng một lúc. Lưu ý viết hóa "V" trong hàm này* |
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

**Thao tác dữ liệu cơ bản**

+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **SAS**                                                                                                                                                                   | **R**                                                                                                                                                                     |
+===========================================================================================================================================================================+===========================================================================================================================================================================+
| Các cột của bộ dữ liệu thường được gọi là "các biến (variables)"                                                                                                          | Thường được gọi là "các cột (columns)" hoặc thỉnh thoảng là "các véctơ (vectors)" hoặc "các biến (variables)"                                                             |
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Không cần thao tác đặc biệt để tạo ra một biến. Các biến mới được tạo đơn giản bằng cách nhập tên biến mới, theo sau là dấu bằng, sau đó là biểu thức cho giá trị         | Tạo các biến mới bằng cách sử dụng hàm `mutate(varname = )`. Xem chương [Làm sạch số liệu và các hàm quan trọng] để biết tất cả chi tiết về câu lệnh **dplyr** bên dưới   |
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Các biến được đổi tên bằng cách sử dụng `rename *old_name=new_name*`                                                                                                      | Các cột có thể được đổi tên bằng cách sử dụng ệnh `rename(new_name = old_name)`                                                                                           |
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Các biến được giữ bằng cách sử dụng `**keep**=varname`                                                                                                                    | Các cột có thể được chọn bằng cách sử dụng lệnh `select()` với tên cột trong ngoặc đơn                                                                                    |
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Các biến được lược bỏ sử dụng `**drop**=varname`                                                                                                                          | Có thể lược bỏ các cột bằng cách sử dụng lệnh `select()` với tên cột trong ngoặc đơn sau dấu trừ                                                                          |
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Các biến Factor có thể được gán nhãn trong Data Step bằng cách sử dụng lệnh `Label`                                                                                       | Việc gán nhãn các giá trị có thể được thực hiện bằng cách chuyển đổi cột thành nhóm Factor và chỉ định thứ bậc. Xem chương [Factors]. Tên cột thường không được gán nhãn. |
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Các bản ghi được chọn bằng cách sử dụng lệnh `Where` hoặc `If` trong Data Step. Nhiều điều kiện lựa chọn được phân tách bằng lệnh "and".                                  | Các bản ghi được chọn bằng cách sử dụng lệnh `filter()` với nhiều điều kiện lựa chọn được phân tách bằng toán tử AND (&) hoặc dấu phẩy                                    |
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Bộ dữ liệu được hợp nhất bằng cách sử dụng lệnh `Merge` trong Data Step. Các bộ dữ liệu được hợp nhất cần phải được sắp xếp trước bằng cách sử dụng thao tác `Proc Sort`. | Package **dplyr** cung cấp một số chức năng để hợp nhất các tập dữ liệu. Xem chi tiết trong chương [Nối dữ liệu].                                                         |
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

**Phân tích mô tả**

+------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **SAS**                                                                                                                                        | **R**                                                                                                                                                                                                                          |
+================================================================================================================================================+================================================================================================================================================================================================================================+
| Có cái nhìn tổng quan hơn về bộ dữ liệu của bạn bằng cách sử dụng thao tác `Proc Summary`, thao tác mà cung cấp tên biến và các thống kê mô tả | Có cái nhìn tổng quan hơn về bộ dữ liệu của bạn bằng cách sử dụng `summary(dataset)` hoặc `skim(dataset)` từ package **skimr** package                                                                                         |
+------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Đếm số lượng bảng của một biến sử dụng `proc freq data=Dataset; Tables varname; Run;`                                                          | Xem chương [Bảng mô tả]. Các tùy chọn trong số tất cả các tùy chọn khác bao gồm `table()` từ **base** R và `tabyl()` từ package **janitor**. Lưu ý rằng bạn sẽ cần xác định bộ dữ liệu và tên cột vì R chứa nhiều tập dữ liệu. |
+------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Lập bảng chéo của hai biến trong bảng 2x2 được thực hiện bằng `proc freq data=Dataset; Tables rowvar*colvar; Run;`                             | Một lần nữa, bạn có thể sử dụng `table()`, `tabyl()` hoặc những cách khác đã được mô tả trong chương [Bảng mô tả].                                                                                                             |
+------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

**Một số tài nguyên hữu ích:**

[R for SAS and SPSS Users (2011)](https://www.amazon.com/SAS-SPSS-Users-Statistics-Computing/dp/1461406846/ref=sr_1_1?dchild=1&gclid=EAIaIQobChMIoqLOvf6u7wIVAhLnCh1c9w_DEAMYASAAEgJLIfD_BwE&hvadid=241675955927&hvdev=c&hvlocphy=9032185&hvnetw=g&hvqmt=e&hvrand=16854847287059617468&hvtargid=kwd-44746119007&hydadcr=16374_10302157&keywords=r+for+sas+users&qid=1615698213&sr=8-1)

[SAS and R, Second Edition (2014)](https://www.amazon.com/SAS-Management-Statistical-Analysis-Graphics-dp-1466584491/dp/1466584491/ref=dp_ob_title_bk)

## Khả năng tương tác dữ liệu

Xem chương [Nhập xuất dữ liệu] để biết chi tiết về cách R package **rio** có thể nhập và xuất các file như file STATA .dta, file SAS .xpt và .sas7bdat, file SPSS .por và .sav và nhiều file khác.
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/transition_to_R.Rmd-->

# Package đề xuất {#packages-suggested}

Dưới đây là danh sách các package được đề xuất dành cho các công việc dịch tễ học phổ biến trong R. Bạn có thể sao chép code này, chạy nó và tất cả các package này sẽ cài đặt từ CRAN và tải để sử dụng trong phiên làm việc hiện tại. Nếu một package đã được cài đặt, nó sẽ chỉ được gọi ra để sử dụng.  

Bạn có thể sửa đổi code với ký hiệu `#` để loại bỏ bất kỳ packages nào bạn không muốn.  

Chú ý:  

* Đầu tiên, cần cài đặt package **pacman** trước khi chạy đoạn code dưới đây. Bạn có thể thực hiện việc này với lệnh `install.packages("pacman")`. Trong sổ tay này, chúng tôi nhấn mạnh đến hàm `p_load()` từ package **pacman**, sẽ có thể vừa cài đặt package nếu cần *và* gọi chúng ra để sử dụng trong phiên làm việc. Bạn cũng có thể gọi package đã được cài đặt với lệnh `library()` từ **base** R.  
* Trong đoạn code dưới đây, các packages được bao gồm khi cài đặt/gọi thông qua một package khác được trình bày bằng cách thụt lề và dấu thăng. Ví dụ: **ggplot2** được liệt kê bên dưới **tidyverse**.  
* Nếu nhiều package có các hàm cùng tên, việc *đè lên nhau* đè lên nhau có thể xảy ra khi hàm từ package được gọi ra sau sẽ được ưu tiên hơn. Đọc thêm trong chương [R cơ bản]. Cân nhắc sử dụng package **conflicted** để quản lý các xung đột tương tự.  
* Xem chương [R cơ bản] mục packages để biết thêm về **pacman** và ghi đè.  

Để xem các phiên bản của R, RStudio và R packages được sử dụng trong quá trình viết cuốn sổ tay này, xem chương [Biên tập và ghi chú kỹ thuật].  

## Packages từ CRAN  

```{r, eval=F}

##########################################
# List of useful epidemiology R packages #
##########################################

# This script uses the p_load() function from pacman R package, 
# which installs if package is absent, and loads for use if already installed


# Ensures the package "pacman" is installed
if (!require("pacman")) install.packages("pacman")


# Packages available from CRAN
##############################
pacman::p_load(
     
     # learning R
     ############
     learnr,   # interactive tutorials in RStudio Tutorial pane
     swirl,    # interactive tutorials in R console
        
     # project and file management
     #############################
     here,     # file paths relative to R project root folder
     rio,      # import/export of many types of data
     openxlsx, # import/export of multi-sheet Excel workbooks 
     
     # package install and management
     ################################
     pacman,   # package install/load
     renv,     # managing versions of packages when working in collaborative groups
     remotes,  # install from github
     
     # General data management
     #########################
     tidyverse,    # includes many packages for tidy data wrangling and presentation
          #dplyr,      # data management
          #tidyr,      # data management
          #ggplot2,    # data visualization
          #stringr,    # work with strings and characters
          #forcats,    # work with factors 
          #lubridate,  # work with dates
          #purrr       # iteration and working with lists
     linelist,     # cleaning linelists
     naniar,       # assessing missing data
     
     # statistics  
     ############
     janitor,      # tables and data cleaning
     gtsummary,    # making descriptive and statistical tables
     rstatix,      # quickly run statistical tests and summaries
     broom,        # tidy up results from regressions
     lmtest,       # likelihood-ratio tests
     easystats,
          # parameters, # alternative to tidy up results from regressions
          # see,        # alternative to visualise forest plots 
     
     # epidemic modeling
     ###################
     epicontacts,  # Analysing transmission networks
     EpiNow2,      # Rt estimation
     EpiEstim,     # Rt estimation
     projections,  # Incidence projections
     incidence2,   # Make epicurves and handle incidence data
     i2extras,     # Extra functions for the incidence2 package
     epitrix,      # Useful epi functions
     distcrete,    # Discrete delay distributions
     
     
     # plots - general
     #################
     #ggplot2,         # included in tidyverse
     cowplot,          # combining plots  
     # patchwork,      # combining plots (alternative)     
     RColorBrewer,     # color scales
     ggnewscale,       # to add additional layers of color schemes

     
     # plots - specific types
     ########################
     DiagrammeR,       # diagrams using DOT language
     incidence2,       # epidemic curves
     gghighlight,      # highlight a subset
     ggrepel,          # smart labels
     plotly,           # interactive graphics
     gganimate,        # animated graphics 

     
     # gis
     ######
     sf,               # to manage spatial data using a Simple Feature format
     tmap,             # to produce simple maps, works for both interactive and static maps
     OpenStreetMap,    # to add OSM basemap in ggplot map
     spdep,            # spatial statistics 
     
     # routine reports
     #################
     rmarkdown,        # produce PDFs, Word Documents, Powerpoints, and HTML files
     reportfactory,    # auto-organization of R Markdown outputs
     officer,          # powerpoints
     
     # dashboards
     ############
     flexdashboard,    # convert an R Markdown script into a dashboard
     shiny,            # interactive web apps
     
     # tables for presentation
     #########################
     knitr,            # R Markdown report generation and html tables
     flextable,        # HTML tables
     #DT,              # HTML tables (alternative)
     #gt,              # HTML tables (alternative)
     #huxtable,        # HTML tables (alternative) 
     
     # phylogenetics
     ###############
     ggtree,           # visualization and annotation of trees
     ape,              # analysis of phylogenetics and evolution
     treeio            # to visualize phylogenetic files
 
)

```

## Packages từ Github  


Dưới đây là các lệnh giúp cài đặt trực tiếp packages từ kho lưu trữ trên Github.  

* Phiên bản phát triển của **epicontacts** có khả năng tạo cây lây nhiễm với trục x tạm thời  
* Package **epirhandbook** chứa tất cả các dữ liệu minh họa cho sổ tay này và có thể được sử dụng để tải xuống phiên bản ngoại tuyến của sổ tay.  


```{r, eval=F}
# Packages to download from Github (not available on CRAN)
##########################################################

# Development version of epicontacts (for transmission chains with a time x-axis)
pacman::p_install_gh("reconhub/epicontacts@timeline")

# The package for this handbook, which includes all the example data  
pacman::p_install_gh("appliedepi/epirhandbook")



```

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/packages_suggested.Rmd-->


# Dự án R {#r-projects}  


Một dự án R cho phép công việc của bạn được đóng gói trong một thư mục khép kín. Trong dự án, tất cả các tập lệnh, tệp dữ liệu, biểu đồ/kết quả đầu ra và lịch sử có liên quan được lưu trữ trong các thư mục con và quan trọng là - *thư mục làm việc* là thư mục gốc của dự án.  


## Gợi ý sử dụng  

Một cách phổ biến, hiệu quả và ít rắc rối để sử dụng R là sự kết hợp của 3 thành tố này. Mỗi dự án công việc cụ thể sẽ được lưu trữ trong một dự án R. Từng thành tố được mô tả như dưới đây.  

1) Một **Dự án R**  
     - Một môi trường làm việc khép kín với các thư mục bao gồm dữ liệu, tập lệnh, các kết quả đầu ra, v.v.  
2) Package **here** dành cho các đường dẫn tương đối  
     - Đường dẫn tệp được ghi một cách tương đối dẫn đến thư mục gốc của dự án R - xem chương [Nhập xuất dữ liệu] để biết thêm chi tiết  
3) Package **rio** để nhập/xuất  
     - `import()` và `export()` giúp giải quyết tất cả các tệp với phần mở rộng khác nhau (ví dụ: .csv, .xlsx, .png)  
     
     


<!-- ======================================================= -->
## Tạo một dự án R {}

Để tạo một dự án R, hãy chọn “New Project” từ menu File.

* Nếu bạn muốn tạo một thư mục mới cho dự án, hãy chọn "New directory" và cho biết nơi bạn muốn nó được tạo.  
* Nếu bạn muốn tạo dự án trong một thư mục có sẵn, hãy chọn “Existing directory” và trỏ tới đường dẫn thư mục đó.  
* Nếu bạn muốn tạo một bản sao từ kho lưu trữ Github, hãy chọn tùy chọn thứ ba “Version Control” và sau đó chọn “Git”. Xem chương [Version control với Git và Github] để biết thêm chi tiết.  


```{r out.width = "75%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "create_project.png"))
```


Dự án R bạn tạo ra sẽ có dạng một thư mục chứa tệp *.Rproj*. Tệp này có thể đóng vai trò là một lối tắt mà bạn sẽ mở dự án của mình. Bạn cũng có thể mở một dự án bằng cách chọn “Open Project” từ menu File. Ngoài ra, ở phía trên bên phải trên của RStudio, bạn sẽ thấy biểu tượng dự án R và menu thả xuống gồm các dự án R có sẵn. 

Để thoát khỏi một dự án R, hãy mở một dự án mới hoặc đóng dự án (File - Close Project).  


### Di chuyển giữa các dự án {.unnumbered}

Để di chuyển giữa các dự án, hãy bấm vào biểu tượng dự án R và menu thả xuống ở phía trên cùng bên phải của RStudio. Bạn sẽ thấy các tùy chọn Close Project, Open Project và danh sách các dự án gần đây.  

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "Rproject_dropdown.png"))
```


### Thiết lập {.unnumbered}  

Thông thường, mỗi khi bạn khởi động RStudio nên là một “clean slate - khởi đầu mới” - nghĩa là với không gian làm việc hiện tại **không** được giữ nguyên so với phiên làm việc trước đó. Điều này có nghĩa là các đối tượng và kết quả của bạn sẽ không tồn tại giữa các phiên làm việc (bạn phải tạo lại chúng bằng cách chạy lại scripts của mình). Điều này là tốt, vì nó sẽ buộc bạn phải viết các đoạn code tốt hơn và tránh được lỗi về lâu dài.  

Để thiết lập RStudio có một “khởi đầu mới” mỗi khi khởi động:  

* Chọn “Project Options” từ menu Tools.  
* Trong tab “General”, thiết lập RStudio to **không** khôi phục .RData vào môi trường làm việc của bạn mỗi khi khởi động, và cũng **không** lưu môi trường làm việc vào tệp .RData khi kết thúc.  



### Tổ chức {.unnumbered}  

Thông thường sẽ có các thư mục con trong dự án của bạn. Hãy cân nhắc đặt tên các thư mục như “data”, “scripts”, “figures”, “presentations”. Bạn có thể thêm các thư mục theo cách thông thường mà bạn sẽ thêm một thư mục mới cho máy tính của mình. Ngoài ra, hãy xem chương [Tương tác với thư mục làm việc] để tìm hiểu cách tạo thư mục mới bằng lệnh R.  


### Kiểm soát phiên bản {.unnumbered}  

Hãy cân nhắc sử dụng một hệ thống kiểm soát phiên bản. Nó có thể là một cái gì đó đơn giản như có ngày tháng trên tên của các scripts (ví dụ: “transmission_analysis_2020-10-03.R”) và một thư mục “lưu trữ”. Bạn cũng có thể thêm các đoạn văn bản tiêu đề nhận xét ở đầu mỗi scripts bao gồm các thông tin như mô tả, thẻ, tác giả và nhật ký thay đổi.  

Một phương pháp phức tạp hơn đó là việc sử dụng Github hoặc một nền tảng tương tự để kiểm soát phiên bản. Xem chương [Version control với Git và Github].  

Một mẹo là bạn có thể tìm kiếm trong toàn bộ dự án hoặc thư mục bằng cách sử dụng công cụ “Find in Files” (Edit menu)). Công cụ này có thể tìm kiếm và thậm chí thay thế các chuỗi trên nhiều tệp.  






## Các ví dụ  

Dưới đây là một vài ví dụ về cách nhập/xuất/lưu trữ sử dụng lệnh `here()` from within an R projct. bên trong một dự án R. Đọc thêm về package **here** trong chương [Nhập xuất dữ liệu].  


*Nhập `linelist_raw.xlsx` từ thư mục “data” trong dự án R của bạn*  

```{r eval=F}
linelist <- import(here("data", "linelist_raw.xlsx"))
```

*Xuất đối tượng `linelist` thành tệp "my_linelist.rds" vào thư mục “clean” nằm trong thư mục “data” trong dự án R của bạn.*   

```{r, eval=F}
export(linelist, here("data","clean", "my_linelist.rds"))
```

*Lưu biểu đồ được in gần đây nhất thành tệp "epicurve_2021-02-15.png" nằm trong thư mục “epicurves” của thư mục “outputs” trong dự án R của bạn.*  

```{r, eval=F}
ggsave(here("outputs", "epicurves", "epicurve_2021-02-15.png"))
```




<!-- ======================================================= -->
## Nguồn {}

Trang web của RStudio về việc [sử dụng các dự án R](https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects)




```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/r_projects.Rmd-->

# Nhập xuất dữ liệu {#importing}

```{r, out.width=c('100%'), echo=F, message=F}
knitr::include_graphics(here::here("images", "Import_Export_1500x500.png"))
```

Trong chương này, chúng tôi mô tả các cách để định vị, nhập và xuất tệp:

-   Sử dụng package **rio** để `import()` và `export()` linh hoạt nhiều loại tệp\

-   Sử dụng package **here** để định vị tệp liên quan đến dự án R gốc - để ngăn ngừa sự phức tạp do nhiều đường dẫn tệp chỉ dành riêng cho một máy tính\

-   Các trường hợp nhập cụ thể:

    -   Excel sheet cụ thể\
    -   Tiêu đề sắp xếp lộn xộn và bỏ qua một số hàng\
    -   Từ Google sheet\
    -   Từ dữ liệu được đăng lên các trang web\
    -   Với APIs\
    -   Nhập tệp *gần đây nhất*\

-   Nhập dữ liệu thủ công\

-   Các loại tệp R cụ thể ví dụ như RDS và RData\

-   Xuất/lưu tệp và biểu đồ

<!-- ======================================================= -->

## Tổng quan

Khi bạn nhập một "dataset" vào R, bạn thường tạo ra một đối tượng *data frame* mới trong môi trường R của mình và định nghĩa nó là tệp được nhập (ví dụ: Excel, CSV, TSV, RDS), từ trong các thư mục của bạn tại một đường dẫn/địa chỉ tệp nhất định.

Bạn có thể nhập/xuất nhiều loại tệp, bao gồm cả những tệp được tạo bởi các chương trình thống kê khác (SAS, STATA, SPSS). Bạn cũng có thể kết nối với các cơ sở dữ liệu liên quan.

R thậm chí còn có các định dạng dữ liệu riêng:

-   Một tệp RDS (.rds) lưu trữ một đối tượng R đơn lẻ chẳng hạn như một data frame. Chúng hữu ích trong việc lưu trữ dữ liệu đã được làm sạch, vì chúng giữ lại kiểu dữ liệu cho các cột R. Đọc thêm trong [mục này](#import_rds).\
-   Một tệp RData (.Rdata) có thể được sử dụng để lưu trữ nhiều đối tượng hoặc thậm chí là một không gian làm việc trong R hoàn chỉnh. Đọc thêm trong [mục này](#import_rdata).

<!-- ======================================================= -->

## Package **rio**

Package R chúng tôi gợi ý là: **rio**. Tên "rio" là chữ viết tắt của "R I/O" (dữ liệu đầu vào (input)/kết quả đầu ra (output)).

Hàm `import()` và `export()` có thể xử lý nhiều loại tệp khác nhau (ví dụ: .xlsx, .csv, .rds, .tsv). Khi bạn cung cấp đường dẫn tệp đến một trong các hàm này (bao gồm cả đuôi file mở rộng như ".csv"), **rio** sẽ đọc phần mở rộng và sử dụng đúng công cụ để nhập hoặc xuất tệp.

Giải pháp thay thế cho việc sử dụng **rio** là sử dụng các hàm từ nhiều package khác, mỗi gói cụ thể cho một loại tệp. Ví dụ như, `read.csv()` (**base** R), `read.xlsx()` (package **openxlsx**) và `write_csv()` (package **readr**), v.v... Những lựa chọn thay thế này có thể khó nhớ, trong khi sử dụng `import()` và `export()` từ **rio** rất dễ dàng.

Các hàm `import()` và `export()` của **rio** sử dụng package và lệnh phù hợp cho một tệp nhất định, dựa trên phần mở rộng của tệp đó. Xem phần cuối của chương này để thấy bảng đầy đủ về các package/hàm **rio** có thể xử lý được. Hàm này cũng có thể được sử dụng để nhập các tệp STATA, SAS và SPSS trong hàng tá các loại tệp khác.

Nhập/xuất shapefiles đòi hỏi các package khác, được mô tả cụ thể trong chương [GIS cơ bản].

## Package **here** {#here}

Package **here** và hàm `here()` của nó giúp R dễ dàng biết nơi tìm và lưu tệp của bạn - về bản chất, nó xây dựng đường dẫn tệp.

Được sử dụng cùng với dự án R, **here** cho phép bạn mô tả vị trí các tệp trong dự án R của bạn trong *thư mục gốc (root directory*) của dự án R (thư mục cấp cao nhất). Điều này hữu ích khi dự án R có thể được chia sẻ hoặc truy cập bởi nhiều người dùng/máy tính. Package này ngăn ngừa sự phức tạp do các đường dẫn tệp là duy nhất trên các máy tính khác nhau (ví dụ: `"C:/Users/Laura/Documents..."`) bằng cách "khởi động (starting)" đường dẫn tệp ở thư mục chung cho tất cả người dùng (dự án R gốc).

Đây là cách `here()` làm việc trong một dự án R:

-   Khi package **here** được tải lần đầu tiên trong dự án R, nó đặt một tệp nhỏ có tên là ".here" trong thư mục gốc dự án R của bạn như là một "điểm chuẩn" hoặc "mỏ neo"\
-   Trong script của bạn, để tham chiếu một tệp trong các thư mục con của dự án R, bạn sử dụng hàm `here()` để tạo đường dẫn tệp *liên quan đến thư mục gốc (anchor)*
-   Để tạo đường dẫn tệp, viết tên các thư mục bên ngoài thư mục gốc, trong dấu ngoặc kép, được phân tách bằng dấu phẩy, cuối cùng kết thúc bằng tên và phần mở rộng của tệp như được trình bày dưới đây\
-   Các đường dẫn tệp `here()` có thể được sử dụng cả để nhập và xuất dữ liệu

Ví dụ, dưới đây, một đường dẫn tệp được tạo bởi `here()` đang cung cấp cho hàm `import()`

```{r, eval=F}
linelist <- import(here("data", "linelists", "ebola_linelist.xlsx"))
```

Lệnh `here("data", "linelists", "ebola_linelist.xlsx")` đang cung cấp đường dẫn tệp đầy đủ mà *duy nhất cho máy tính của người dùng:*

    "C:/Users/Laura/Documents/my_R_project/data/linelists/ebola_linelist.xlsx"

Ưu điểm là lệnh `here()` được R sử dụng có thể chạy thành công trên bất kỳ máy tính nào truy cập dự án R.

[***MẸO:*** Nếu bạn không chắc gốc ".here" được đặt ở đâu, hãy chạy lệnh `here()` với dấu ngoặc đơn trống.]{style="color: darkgreen;"}

Đọc thêm về package **here** [tại đường dẫn này](https://here.r-lib.org/).

<!-- ======================================================= -->

## Đường dẫn tệp

Khi nhập hoặc xuất dữ liệu, bạn phải cung cấp một đường dẫn tệp. Bạn có thể thực hiện thao tác này bằng một trong ba cách sau:

1)  *Khuyên dùng:* cung cấp một đường dẫn tệp "tương đối" bằng package **here**\
2)  Cung cấp đường dẫn tệp "đầy đủ" / "tuyệt đối"\
3)  Chọn tệp theo cách thủ công

### Đường dẫn tệp "tương đối" {.unnumbered}

Trong R, đường dẫn tệp "tương đối" bao gồm đường dẫn tệp mà *liên quan* đến phần gốc của dự án R. Chúng cho phép nhiều đường dẫn tệp đơn giản hơn có thể làm việc trên nhiều máy tính khác nhau (ví dụ: nếu dự án R nằm trên bộ nhớ dùng chung hoặc được gửi qua thư điện tử). Như đã được [mô tả ở trên](#here), đường dẫn tệp tương đối được tạo ra dễ dàng bằng cách sử dụng package **here**.

Dưới đây là một ví dụ về đường dẫn tệp tương đối được tạo bằng package `here()`. Chúng tôi giả sử công việc nằm trong một dự án R có chứa một thư mục con "data" và bên trong thư mục con "linelists", trong đó có tệp .xlsx được quan tâm.

```{r, eval=F}
linelist <- import(here("data", "linelists", "ebola_linelist.xlsx"))
```

### Đường dẫn tệp "tuyệt đối" {.unnumbered}

Đường dẫn tệp tuyệt đối hay "đầy đủ" có thể được cung cấp cho các hàm như `import()` nhưng chúng "dễ vỡ" vì chúng là duy nhất cho cho các máy tính khác nhau của người dùng và do đó *không được khuyến khích* sử dụng.

Dưới đây là một ví dụ về đường dẫn tệp tuyệt đối, trong máy tính của Laura có một thư mục "analysis", một thư mục con "data" và bên trong là thư mục con "linelists", trong đó có tệp .xlsx được quan tâm.

```{r, eval=F}
linelist <- import("C:/Users/Laura/Documents/analysis/data/linelists/ebola_linelist.xlsx")
```

Một vài điều cần lưu ý về đường dẫn tệp tuyệt đối:

-   **Tránh sử dụng đường dẫn tệp tuyệt đối** vì chúng sẽ bị vỡ nếu script được chạy trên một máy tính khác
-   Sử dụng dấu gạch chéo *lên* (`/`), như trong ví dụ trên (lưu ý: đây *KHÔNG* phải là mặc định đối với đường dẫn tệp trong Windows)\
-   Đường dẫn tệp bắt đầu với hai dấu gạch chéo (ví dụ: "//...") sẽ có khả năng **không được R nhận ra** và tạo ra lỗi. Cân nhắc chuyển công việc của bạn sang ổ đĩa "có tên" hoặc "có chữ" bắt đầu bằng một chữ cái (ví dụ: "J:" hoặc "C:"). Xem trang về [Tương tác với thư mục làm việc] để biết thêm chi tiết về vấn đề này.

Một tình huống mà đường dẫn tệp tuyệt đối có thể phù hợp là khi bạn muốn nhập một tệp từ bộ nhớ dùng chung có cùng đường dẫn tệp đầy đủ cho tất cả người dùng.

[***MẸO:*** Để nhanh chóng chuyển đổi tất cả `\` thành `/`, đánh dấu code được quan tâm, sử dụng Ctrl + f (trong Windows), kiểm tra hộp tùy chọn cho "In selection", sau đó sử dụng chức năng thay thế (replace) để chuyển đổi chúng.]{style="color: darkgreen;"}

<!-- ======================================================= -->

### Chọn tệp theo cách thủ công {.unnumbered}

Bạn có thể nhập dữ liệu theo cách thủ công thông qua một trong các phương pháp sau:

1)  Từ cửa sổ Environment trong RStudio, nhấp vào "Import Dataset" và chọn loại dữ liệu
2)  Nhấp vào File / Import Dataset / (chọn loại dữ liệu)\
3)  Để lựa chọn thủ công bằng hard-code, hãy sử dụng lệnh *base R* `file.choose()` (để trống dấu ngoặc đơn) để kích hoạt sự xuất hiện của **cửa sổ pop-up** cho phép người dùng chọn tệp theo cách thủ công từ máy tính của họ. Ví dụ:

```{r import_choose, eval=F}
# Manual selection of a file. When this command is run, a POP-UP window will appear. 
# The file path selected will be supplied to the import() command.

my_data <- import(file.choose())
```

[***MẸO:*** **Cửa sổ pop-up** có thể xuất hiện SAU cửa sổ RStudio của bạn.]{style="color: darkgreen;"}

## Nhập dữ liệu

Sử dụng lệnh `import()` để nhập một bộ dữ liệu khá đơn giản. Chỉ cần cung cấp đường dẫn đến tệp (bao gồm tên và phần mở rộng của tệp) trong dấu ngoặc kép. Nếu sử dụng `here()` để xây dựng đường dẫn tệp, hãy làm theo hướng dẫn ở trên. Dưới đây là một vài ví dụ:

Nhập tệp csv nằm trong "thư mục làm việc (working directory)" của bạn hoặc trong thư mục gốc của dự án R:

```{r, eval=F}
linelist <- import("linelist_cleaned.csv")
```

Nhập sheet đầu tiên Excel workbook nằm trong thư mục con "data" và "linelists" của dự án R (đường dẫn tệp được tạo bằng `here()`):

```{r, eval=F}
linelist <- import(here("data", "linelists", "linelist_cleaned.xlsx"))
```

Nhập một data frame (một tệp .rds ) sử dụng đường dẫn tệp tuyệt đối:

```{r, eval=F}
linelist <- import("C:/Users/Laura/Documents/tuberculosis/data/linelists/linelist_cleaned.rds")
```

### Trang tính Excel cụ thể {.unnumbered}

Theo mặc định, nếu bạn cung cấp một Excel workbook (.xlsx) để `import()`, trang tính đầu tiên của workbook sẽ được nhập. Nếu bạn muốn nhập một **trang tính** cụ thể, hãy nhập tên trang tính vào đối số `which =`. Ví dụ:

```{r eval=F}
my_data <- import("my_excel_file.xlsx", which = "Sheetname")
```

Nếu sử dụng hàm `here()` để cung cấp một đường dẫn tương đối đến `import()`, bạn vẫn có thể chỉ ra một trang tính cụ thể bằng cách thêm đối số `which =` sau dấu đóng ngoặc của hàm `here()`.

```{r import_sheet_here, eval=F}
# Demonstration: importing a specific Excel sheet when using relative pathways with the 'here' package
linelist_raw <- import(here("data", "linelist.xlsx"), which = "Sheet1")`  
```

Để *xuất* một data frame từ R sang một trang tính Excel sheet và phần còn lại của Excel workbook không thay đổi, bạn sẽ phải nhập, chỉnh sửa và xuất với một package thay thế phục vụ cho mục đích này, chẳng hạn như **openxlsx**. Xem thêm thông tin trong chương về [Tương tác với thư mục làm việc] hoặc [tại trang github này](https://ycphs.github.io/openxlsx/).

Nếu Excel workbook của bạn là .xlsb (định dạng nhị phân của Excel workbook) bạn có thể không nhập được bằng **rio**. Cân nhắc lưu lại nó dưới dạng .xlsx hoặc sử dụng một package như **readxlsb**, là package được xây dựng cho [mục đích này](https://cran.r-project.org/web/packages/readxlsb/vignettes/read-xlsb-workbook.html).

<!-- ======================================================= -->

### Giá trị missing {#import_missing .unnumbered}

Bạn có thể muốn xác định (các) giá trị nào trong bộ dữ liệu của mình nên được coi là missing. Như đã giải thích trong chương về [Dữ liệu missing], giá trị trong R cho dữ liệu missing là `NA`, nhưng có thể bộ dữ liệu bạn muốn nhập sử dụng giá trị 99, "Missing" hoặc chỉ là khoảng trống ký tự "".

Sử dụng đối số `na =` để `import()` và cung cấp (các) giá trị trong dấu ngoặc kép (ngay cả khi chúng là các số). Bạn có thể chỉ định nhiều giá trị bằng cách gộp chúng trong một vectơ, sử dụng `c()` như được thể hiện dưới đây.

Tại đây, giá trị "99" trong bộ dữ liệu đã nhập được coi là missing và được chuyển đổi thành `NA` trong R.

```{r, eval=F}
linelist <- import(here("data", "my_linelist.xlsx"), na = "99")
```

Tại đây, bất kỳ giá trị "Missing", "" (ô trống) hoặc "" (khoảng trắng) nào trong bộ dữ liệu đã nhập đều được chuyển đổi thành `NA` trong R.

```{r, eval=F}
linelist <- import(here("data", "my_linelist.csv"), na = c("Missing", "", " "))
```

<!-- ======================================================= -->

### Bỏ qua một số hàng {.unnumbered}

Đôi khi, bạn có thể không muốn nhập một hàng dữ liệu. Bạn có thể thực hiện thao tác này với đối số `skip =` nếu sử dụng `import()` từ **rio** trên tệp .xlsx hoặc .csv. Cung cấp số hàng bạn muốn bỏ qua.

```{r, eval=F}
linelist_raw <- import("linelist_raw.xlsx", skip = 1)  # does not import header row
```

Không may `skip =` chỉ chấp nhận một giá trị số nguyên, *không* chấp nhận một khoảng (ví dụ: "2:10" sẽ không hoạt động). Để bỏ qua việc nhập các hàng cụ thể không liên tiếp từ trên cùng, hãy cân nhắc nhập nhiều lần và sử dụng `bind_rows()` từ **dplyr**. Hãy xem ví dụ dưới đây về việc chỉ bỏ qua hàng thứ 2.

### Quản lý hàng tiêu đề thứ hai {.unnumbered}

Đôi khi, dữ liệu của bạn có thể có hàng *thứ hai*, với chức năng như là "từ điển dữ liệu" như hình dưới đây. Trường hợp này có thể xảy ra vấn đề vì nó có thể dẫn đến việc tất cả các cột được nhập vào dưới dạng nhóm "ký tự (character)".

```{r, echo=F}
# HIDDEN FROM READER
####################
# Create second header row of "data dictionary" and insert into row 2. Save as new dataframe.
linelist_2headers <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds")) %>%         
        mutate(across(everything(), as.character)) %>% 
        add_row(.before = 1,
                #row_num = "000",
                case_id = "case identification number assigned by MOH",
                generation = "transmission chain generation number",
                date_infection = "estimated date of infection, mm/dd/yyyy",
                date_onset = "date of symptom onset, YYYY-MM-DD",
                date_hospitalisation = "date of initial hospitalization, mm/dd/yyyy",
                date_outcome = "date of outcome status determination",
                outcome = "either 'Death' or 'Recovered' or 'Unknown'",
                gender = "either 'm' or 'f' or 'unknown'",
                hospital = "Name of hospital of first admission",
                lon = "longitude of residence, approx",
                lat = "latitude of residence, approx",
                infector = "case_id of infector",
                source = "context of known transmission event",
                age = "age number",
                age_unit = "age unit, either 'years' or 'months' or 'days'",
                fever = "presence of fever on admission, either 'yes' or 'no'",
                chills = "presence of chills on admission, either 'yes' or 'no'",
                cough = "presence of cough on admission, either 'yes' or 'no'",
                aches = "presence of aches on admission, either 'yes' or 'no'",
                vomit = "presence of vomiting on admission, either 'yes' or 'no'",
                time_admission = "time of hospital admission HH:MM")
```

Dưới đây là một ví dụ về loại bộ dữ liệu này (với hàng đầu tiên là từ điển dữ liệu).

```{r message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist_2headers, 5), rownames = FALSE, filter="top", options = list(pageLength = 4, scrollX=T), class = 'white-space: nowrap' )
```

### Xóa hàng tiêu đề thứ hai {.unnumbered}

Để bỏ hàng tiêu đề thứ hai, bạn có thể sẽ cần nhập dữ liệu hai lần.

1)  Nhập dữ liệu vào để lấy tên các cột một cách chính xác\
2)  Nhập lại dữ liệu, bỏ qua *hai* hàng đầu tiên (hàng tiêu đề và hàng thứ hai)\
3)  Liên kết dataframe đã xóa bỏ 2 hàng đầu tiên với tên cột ở bước 1

Đối số chính xác được sử dụng để liên kết các tên cột tùy thuộc vào loại tệp dữ liệu (.csv, .tsv, .xlsx, v.v.). Điều này là do **rio** sử dụng các hàm khác nhau cho các loại tệp khác nhau (xem bảng ở trên).

**Đối với tệp Excel:** (`col_names =`)

```{r, eval=F}
# import first time; store the column names
linelist_raw_names <- import("linelist_raw.xlsx") %>% names()  # save true column names

# import second time; skip row 2, and assign column names to argument col_names =
linelist_raw <- import("linelist_raw.xlsx",
                       skip = 2,
                       col_names = linelist_raw_names
                       ) 
```

**Đối với tệp CSV:** (`col.names =`)

```{r, eval=F}
# import first time; sotre column names
linelist_raw_names <- import("linelist_raw.csv") %>% names() # save true column names

# note argument for csv files is 'col.names = '
linelist_raw <- import("linelist_raw.csv",
                       skip = 2,
                       col.names = linelist_raw_names
                       ) 
```

**Tùy chọn sao lưu** - thay đổi tên cột dưới dạng một lệnh riêng biệt

```{r, eval=F}
# assign/overwrite headers using the base 'colnames()' function
colnames(linelist_raw) <- linelist_raw_names
```

#### Tạo từ điển dữ liệu {.unnumbered}

Thông tin thêm! Nếu bạn có hàng thứ hai là từ điển dữ liệu, bạn có thể dễ dàng tạo từ điển dữ liệu thích hợp từ nó. Mẹo này được điều chỉnh từ [bài đăng](https://alison.rbind.io/post/2018-02-23-read-multiple-header-rows/) này.

```{r}
dict <- linelist_2headers %>%             # begin: linelist with dictionary as first row
  head(1) %>%                             # keep only column names and first dictionary row                
  pivot_longer(cols = everything(),       # pivot all columns to long format
               names_to = "Column",       # assign new column names
               values_to = "Description")
```

```{r message=FALSE, echo=F}
DT::datatable(dict, rownames = FALSE, filter="top", options = list(pageLength = 4, scrollX=T), class = 'white-space: nowrap' )
```

#### Kết hợp hai hàng tiêu đề {.unnumbered}

Trong một số trường hợp khi bộ dữ liệu thô của bạn có *hai* hàng tiêu đề (hoặc cụ thể hơn, hàng dữ liệu thứ 2 là tiêu đề phụ), bạn có thể muốn "kết hợp" chúng hoặc thêm các giá trị trong hàng tiêu đề thứ hai vào hàng tiêu đề đầu tiên.

Lệnh dưới đây sẽ xác định tên cột của data frame là sự kết hợp (dán với nhau) của các tiêu đề (đúng) đầu tiên với giá trị ngay bên dưới (trong hàng đầu tiên).

```{r, eval=F}
names(my_data) <- paste(names(my_data), my_data[1, ], sep = "_")
```

<!-- ======================================================= -->

### Trang tính Google {.unnumbered}

Bạn có thể nhập dữ liệu từ một trang tính Google trực tuyến với package **googlesheet4** và bằng cách xác thực quyền truy cập của bạn vào trang tính.

```{r, eval=F}
pacman::p_load("googlesheets4")
```

Dưới đây là một trang tính Google minh họa được nhập và lưu. Lệnh này có thể yêu cầu xác thực tài khoản Google của bạn. Làm theo lời nhắc và cửa sổ bật lên trong trình duyệt Internet của bạn để cấp cho các package Tidyverse API quyền chỉnh sửa, tạo và xóa trang tính của bạn trong Google Drive.

Trang tính dưới đây "có thể được xem bởi bất kỳ ai có liên kết" và bạn có thể thử nhập trang tính đó.

```{r, eval=F}
Gsheets_demo <- read_sheet("https://docs.google.com/spreadsheets/d/1scgtzkVLLHAe5a6_eFQEwkZcc14yFUx1KgOMZ4AKUfY/edit#gid=0")
```

Trang tính cũng có thể được nhập chỉ bằng ID của sheet, một phần ngắn hơn của URL:

```{r, eval=F}
Gsheets_demo <- read_sheet("1scgtzkVLLHAe5a6_eFQEwkZcc14yFUx1KgOMZ4AKUfY")
```

Một package khác, **googledrive** cung cấp các hàm hữu ích để viết, chỉnh sửa và xóa các trang tính Google. Ví dụ: các hàm được sử dụng `gs4_create()` và `sheet_write()` đều được tìm thấy trong package này.

Dưới đây là một số hướng dẫn trực tuyến hữu ích khác:\
[hướng dẫn nhập Google sheet cơ bản](https://arbor-analytics.com/post/getting-your-data-into-r-from-google-sheets/)\
[hướng dẫn chi tiết hơn](https://googlesheets4.tidyverse.org/articles/googlesheets4.html)\
[tương tác giữa googlesheets4 và tidyverse](https://googlesheets4.tidyverse.org/articles/articles/drive-and-sheets.html)

## Nhập, xuất, tách, kết hợp - nhiều tệp

Xem chương về [Lặp, vòng lặp và danh sách] để biết ví dụ về cách nhập và kết hợp nhiều tệp hoặc nhiều Excel workbook. Chương này cũng có các ví dụ về cách chia một data frame thành các phần và xuất từng phần riêng biệt hoặc dưới dạng các sheet được đặt tên trong một Excel workbook.

<!-- ======================================================= -->

## Nhập từ Github {#import_github}

Nhập dữ liệu trực tiếp từ Github vào R có thể rất dễ dàng hoặc có thể yêu cầu một vài bước - tùy thuộc vào loại tệp. Dưới đây là một số cách tiếp cận:

### Tệp CSV {.unnumbered}

Có thể dễ dàng nhập tệp .csv trực tiếp từ Github vào R bằng lệnh R.

1)  Đi tới repo Github, tìm tệp quan tâm và nhấp vào tệp đó\
2)  Nhấp vào nút "Raw" (sau đó bạn sẽ thấy dữ liệu csv "thô", như được hiển thị bên dưới)\
3)  Sao chép URL (địa chỉ web)\
4)  Đặt URL trong dấu ngoặc kép trong lệnh R `import()`

```{r, out.width=c('100%', '100%'), fig.align = "left", echo=F}
knitr::include_graphics(here::here("images", "download_csv_raw.png"))
```

### Tệp XLSX {.unnumbered}

Bạn có thể không xem được dữ liệu "Thô" cho một số tệp (ví dụ: .xlsx, .rds, .nwk, .shp)

1)  Đi tới repo Github, tìm tệp quan tâm và nhấp vào tệp đó\
2)  Nhấp vào nút "Download", như được hiển thị bên dưới\
3)  Lưu tệp trên máy tính của bạn và nhập tệp đó vào R

```{r , out.width=c('100%', '100%'), fig.align = "left", echo=F}
knitr::include_graphics(here::here("images", "download_xlsx.png"))
```

### Shapefiles {.unnumbered}

Các Shapefiles có nhiều tệp thành phần phụ, mỗi tệp có một phần mở rộng khác nhau. Một tệp sẽ có phần mở rộng ".shp", nhưng những tệp khác có thể là ".dbf", ".prj", v.v. Để tải xuống shapefiles từ Github, bạn sẽ cần tải xuống từng tệp thành phần phụ riêng lẻ và lưu chúng trong cùng một thư mục trên máy tính của bạn. Trong Github, nhấp vào từng tệp riêng lẻ và tải chúng xuống bằng cách nhấp vào nút "Download".

Một khi được lưu vào máy tính, bạn có thể nhập định dạng tệp như được trình bày trong chương [GIS cơ bản] bằng cách sử dụng hàm `st_read()` từ package **sf**. Bạn chỉ cần cung cấp đường dẫn tệp và tên của tệp ".shp" - miễn là các tệp liên quan khác nằm trong cùng một thư mục trên máy tính của bạn.

Dưới đây, bạn có thể thấy shapefiles tên "sl_adm3" bao gồm nhiều tệp như thế nào - mỗi tệp phải được tải xuống từ Github.

```{r , out.width=c('100%', '100%'), fig.align = "left", echo=F}
knitr::include_graphics(here::here("images", "download_shp.png"))
```

<!-- ======================================================= -->

## Nhập dữ liệu thủ công

### Nhập theo hàng {.unnumbered}

Sử dụng hàm `tribble` của package **tibble** từ tidyverse ([tài liệu tham khảo trực tuyến](https://tibble.tidyverse.org/reference/tribble.html)).

Lưu ý cách tiêu đề cột bắt đầu bằng dấu ngã (`~`). Cũng lưu ý rằng mỗi cột chỉ được chứa một nhóm dữ liệu (ký tự, số, v.v.). Bạn có thể sử dụng các tab, khoảng cách và hàng mới để làm cho việc nhập dữ liệu trực quan và dễ đọc hơn. Khoảng trắng không quan trọng giữa các giá trị, nhưng mỗi hàng được biểu thị bằng một dòng code mới. Ví dụ:

```{r import_manual_row}
# create the dataset manually by row
manual_entry_rows <- tibble::tribble(
  ~colA, ~colB,
  "a",   1,
  "b",   2,
  "c",   3
  )
```

Và giờ chúng ta hiển thị bộ dữ liệu mới:

```{r, echo=F}
# display the new dataset
DT::datatable(manual_entry_rows)
```

### Nhập theo cột {.unnumbered}

Vì data frame bao gồm các vectơ (cột dọc), cách tiếp cận **cơ bản** để tạo data frame thủ công trong R muốn bạn xác định từng cột và sau đó liên kết chúng lại với nhau. Điều này có thể phản trực quan trong dịch tễ học, vì chúng ta thường nghĩ về dữ liệu của mình theo hàng (như trên).

```{r import_manual_col}
# define each vector (vertical column) separately, each with its own name
PatientID <- c(235, 452, 778, 111)
Treatment <- c("Yes", "No", "Yes", "Yes")
Death     <- c(1, 0, 1, 0)
```

[***CHÚ Ý:*** Tất cả các vectơ phải có cùng độ dài (cùng số giá trị).]{style="color: orange;"}

Các vectơ sau đó có thể được liên kết với nhau bằng cách sử dụng lệnh `data.frame()`:

```{r}
# combine the columns into a data frame, by referencing the vector names
manual_entry_cols <- data.frame(PatientID, Treatment, Death)
```

Và giờ chúng ta hiển thị bộ dữ liệu mới:

```{r, echo=F}
# display the new dataset
DT::datatable(manual_entry_cols)
```

### Dán từ bộ nhớ tạm {.unnumbered}

Nếu bạn sao chép dữ liệu từ nơi khác và có nó trong bộ nhớ tạm, bạn có thể thử một trong hai cách dưới đây:

Từ package **clipr**, bạn có thể sử dụng hàm `read_clip_tbl()` để nhập dưới dạng data frame hoặc chỉ cần hàm `read_clip()` để nhập dưới dạng một vectơ ký tự. Trong cả hai trường hợp, hãy để trống dấu ngoặc đơn.

```{r, eval=F}
linelist <- clipr::read_clip_tbl()  # imports current clipboard as data frame
linelist <- clipr::read_clip()      # imports as character vector
```

Bạn cũng có thể dễ dàng xuất sang clipboard của hệ thống bằng **clipr**. Xem mục bên dưới về Xuất dữ liệu.

Ngoài ra, bạn có thể sử dụng lệnh `read.table()` từ **base** R với `file = "clipboard")` để nhập dưới dạng data frame:

```{r, eval=F}
df_from_clipboard <- read.table(
  file = "clipboard",  # specify this as "clipboard"
  sep = "t",           # separator could be tab, or commas, etc.
  header=TRUE)         # if there is a header row
```

## Nhập tệp gần đây nhất

Thường thì bạn có thể nhận được các bản cập nhật hàng ngày cho bộ dữ liệu của mình. Trong trường hợp này, bạn sẽ muốn viết code mà nhập tệp gần đây nhất. Dưới đây, chúng tôi trình bày hai cách để tiếp cận điều này:

-   Chọn tệp dựa trên ngày trong tên tệp\
-   Chọn tệp dựa trên siêu dữ liệu (metadata) của tệp (lần sửa đổi cuối cùng)

### Ngày trong tên tệp {.unnumbered}

Cách tiếp cận này dựa trên ba cơ sở:

1)  Bạn tin tưởng ngày tháng trong tên tệp\
2)  Ngày tháng ở dạng số và *thường* xuất hiện ở cùng một định dạng (ví dụ: năm rồi tháng rồi ngày)\
3)  Không có số nào khác trong tên tệp

Chúng tôi sẽ giải thích từng bước và sau đó cho bạn thấy cách chúng được kết hợp ở phần cuối.

Đầu tiên, sử dụng `dir()` từ **base** R để chỉ trích xuất tên tệp cho mỗi tệp trong thư mục quan tâm. Xem chương về [Tương tác với thư mục làm việc] để biết thêm chi tiết về `dir()`. Trong ví dụ này, thư mục quan tâm là thư mục "linelists" trong thư mục "example" chứa trong thư mục "data" của dự án R.

```{r}
linelist_filenames <- dir(here("data", "example", "linelists")) # get file names from folder
linelist_filenames                                              # print
```

Một khi bạn có vectơ chứa các tên này, bạn có thể trích xuất ngày với chúng bằng cách áp dụng hàm `str_extract()` từ **stringr** với việc sử dụng biểu thức chính quy sau đây. Nó giúp trích xuất bất kỳ số nào trong tên tệp (bao gồm bất kỳ ký tự nào khác ở giữa như dấu gạch ngang hoặc dấu gạch chéo). Bạn có thể đọc thêm về **stringr** trong chương [Ký tự và chuỗi].

```{r}
linelist_dates_raw <- stringr::str_extract(linelist_filenames, "[0-9].*[0-9]") # extract numbers and any characters in between
linelist_dates_raw  # print
```

Giả sử ngày thường được viết theo cùng một định dạng ngày (ví dụ: Năm rồi Tháng rồi Ngày) và năm có 4 chữ số, bạn có thể sử dụng các hàm chuyển đổi linh hoạt của **lubridate** (`ymd()`, `dmy()`, or `mdy()`) để chuyển đổi chúng thành ngày. Đối với các hàm này, dấu gạch ngang, dấu cách hoặc dấu gạch chéo không quan trọng, quan trọng chỉ là thứ tự của các số. Đọc thêm trong chương [Làm việc với ngày tháng].

```{r}
linelist_dates_clean <- lubridate::ymd(linelist_dates_raw)
linelist_dates_clean
```

Sau đó, hàm **base** R `which.max()` có thể được sử dụng để trả về vị trí chỉ mục (ví dụ: 1, 2, 3,...) của giá trị ngày lớn nhất. Tệp mới nhất được xác định chính xác là tệp thứ 6 - "case_linelist_2020-10-08.xlsx".

```{r}
index_latest_file <- which.max(linelist_dates_clean)
index_latest_file
```

Nếu chúng ta tổng hợp tất cả các lệnh này, code hoàn chỉnh có thể trông giống như bên dưới. Lưu ý rằng dấu `.` ở dòng cuối cùng thay thế cho thành phần được piping vào hàm đó. Tại thời điểm đó, giá trị chỉ đơn giản là số 6. Giá trị này được đặt trong dấu ngoặc kép để trích xuất phần tử thứ 6 của vectơ tên tệp được tạo bởi `dir()`.

```{r}
# load packages
pacman::p_load(
  tidyverse,         # data management
  stringr,           # work with strings/characters
  lubridate,         # work with dates
  rio,               # import / export
  here,              # relative file paths
  fs)                # directory interactions

# extract the file name of latest file
latest_file <- dir(here("data", "example", "linelists")) %>%  # file names from "linelists" sub-folder          
  str_extract("[0-9].*[0-9]") %>%                  # pull out dates (numbers)
  ymd() %>%                                        # convert numbers to dates (assuming year-month-day format)
  which.max() %>%                                  # get index of max date (latest file)
  dir(here("data", "example", "linelists"))[[.]]              # return the filename of latest linelist

latest_file  # print name of latest file
```

Bây giờ bạn có thể sử dụng tên này để kết thúc đường dẫn tệp tương đối, với `here()`:

```{r, eval=F}
here("data", "example", "linelists", latest_file) 
```

Và bây giờ bạn có thể nhập tệp mới nhất:

```{r, eval=F}
# import
import(here("data", "example", "linelists", latest_file)) # import 
```

### Sử dụng thông tin tệp {.unnumbered}

Nếu tệp của bạn không có ngày trong tên của chúng (hoặc bạn không tin tưởng vào những ngày đó), bạn có thể thử trích xuất ngày sửa đổi cuối cùng từ siêu dữ liệu tệp. Sử dụng các hàm từ package **fs** để kiểm tra thông tin siêu dữ liệu cho từng tệp, bao gồm thời gian sửa đổi cuối cùng và đường dẫn tệp.

Dưới đây, chúng tôi cung cấp thư mục quan tâm tới `dir_info()` của **fs**. Trong trường hợp này, thư mục quan tâm nằm trong dự án R trong thư mục "data", thư mục con "example" và thư mục con thư mục này "linelists". Kết quả là một data frame với một dòng cho mỗi tệp và các cột cho `modification_time`, `path`, v.v. Bạn có thể xem ví dụ trực quan về điều này trong chương về [Tương tác với thư mục làm việc].

Chúng ta có thể sắp xếp data frame này của các tệp theo cột với `modification_time`, và sau đó chỉ giữ lại hàng trên cùng/mới nhất (tệp) với `head()`của **base** R. Sau đó, chúng ta có thể trích xuất đường dẫn tệp của tệp mới nhất này chỉ với hàm `pull()` của **dplyr** trên `path` cột. Cuối cùng, chúng ta có thể chuyển đường dẫn tệp này đến `import()`. Tệp đã nhập được lưu dưới dạng `latest_file`.

```{r, eval=F}
latest_file <- dir_info(here("data", "example", "linelists")) %>%  # collect file info on all files in directory
  arrange(desc(modification_time)) %>%      # sort by modification time
  head(1) %>%                               # keep only the top (latest) file
  pull(path) %>%                            # extract only the file path
  import()                                  # import the file

```

<!-- ======================================================= -->

## API {#import_api}

Một "Giao diện lập trình tự động (Automated Programming Interface)" (API) có thể được sử dụng để yêu cầu trực tiếp dữ liệu từ một trang web. API là một tập hợp các quy tắc cho phép một ứng dụng phần mềm tương tác với một ứng dụng phần mềm khác. Khách hàng (bạn) gửi một "yêu cầu (request)" và nhận được một "phản hồi (response)" có chứa nội dung. Các package R **httr** và **jsonlite** có thể hỗ trợ quá trình này.

Mỗi trang web hỗ trợ API sẽ có tài liệu và chi tiết cụ thể riêng để làm quen. Một số trang web công khai API và cho phép có thể được truy cập bởi bất kỳ ai. Những nền tảng khác, chẳng hạn như nền tảng có ID người dùng và thông tin đăng nhập, yêu cầu xác thực để truy cập dữ liệu của họ.

Không cần phải nói, để nhập dữ liệu qua API thì cần phải có kết nối internet. Chúng tôi sẽ đưa ra các ví dụ ngắn gọn về việc sử dụng API để nhập dữ liệu và liên kết bạn với các tài nguyên khác.

*Lưu ý: Hãy nhớ lại rằng dữ liệu có thể được đăng trên một trang web không có API, điều này có thể dễ dàng truy xuất hơn. Ví dụ: một tệp CSV đã đăng có thể được truy cập chỉ bằng cách cung cấp URL của trang web để `import()` như được mô tả trong mục [nhập từ Github](#import_github).*

### Yêu cầu HTTP (HTTP request) {.unnumbered}

Trao đổi API thường được thực hiện thông qua một HTTP request. HTTP là Giao thức truyền siêu văn bản (Hypertext Transfer Protocol) và là định dạng cơ bản của giao thức yêu cầu (request)/phản hồi (response) giữa máy khách và máy chủ. Đầu vào và đầu ra chính xác có thể khác nhau tùy thuộc vào loại API nhưng quy trình là giống nhau - "Request" (thường là HTTP request) từ người dùng, thường chứa một truy vấn, theo sau là "Response", chứa thông tin trạng thái về request và có thể là nội dung được yêu cầu.

Dưới đây là một số thành phần của một *HTTP request*:

-   URL của điểm cuối API\
-   "Method (Phương thức)" (hoặc "Verb (Động từ)")\
-   Các tiêu đề\
-   Phần thân

HTTP request "method" là hành động bạn muốn thực hiện. Hai phương thức HTTP phổ biến nhất là `GET` và `POST` nhưng những phương thức khác có thể bao gồm `PUT`, `DELETE`, `PATCH`, v.v. Khi nhập dữ liệu vào R, rất có thể bạn sẽ sử dụng `GET`.

Sau request của bạn, máy tính của bạn sẽ nhận được "phản hồi" ở định dạng tương tự như những gì bạn đã gửi, bao gồm URL, trạng thái HTTP (Trạng thái 200 là thứ bạn muốn!), loại tệp, kích thước và nội dung mong muốn. Sau đó, bạn sẽ cần phân tích cú pháp phản hồi này và biến nó thành một data frame khả thi trong môi trường R của bạn.

### Package {.unnumbered}

Package **httr** hoạt động tốt để xử lý các yêu cầu HTTP trong R. Nó đòi hỏi ít kiến thức về API Web và có thể được sử dụng bởi những người ít quen thuộc với thuật ngữ phát triển phần mềm. Ngoài ra, nếu phản hồi HTTP là .json, bạn có thể sử dụng **jsonlite** để phân tích cú pháp phản hồi.

```{r, eval=F}
# load packages
pacman::p_load(httr, jsonlite, tidyverse)
```

### Dữ liệu công khai {.unnumbered}

Dưới đây là một ví dụ về một HTTP request, được mượn từ một hướng dẫn từ [Phòng thí nghiệm Dữ liệu Trafford](https://www.trafforddatalab.io/open_data_companion/#A_quick_introduction_to_APIs). Trang web này chứa một số tài nguyên khác để tìm hiểu và các bài tập về API.

Tình huống: Chúng ta muốn nhập một danh sách các cửa hàng thức ăn nhanh ở thành phố Trafford, Vương quốc Anh. Dữ liệu có thể được truy cập từ API của Cơ quan Tiêu chuẩn Thực phẩm, cơ quan cung cấp dữ liệu xếp hạng vệ sinh thực phẩm cho Vương quốc Anh.

Dưới đây là các thông số cho yêu cầu của chúng tôi:

-   Phương thức HTTP: GET\
-   URL của điểm cuối API: <http://api.ratings.food.gov.uk/Establishments>\
-   Các thông số đã chọn: tên, địa chỉ, kinh độ, vĩ độ, businessTypeId, ratingKey, localAuthorityId\
-   Các tiêu đề: "x-api-version", 2\
-   (Các) Định dạng dữ liệu: JSON, XML\
-   Tài liệu: <http://api.ratings.food.gov.uk/help>

R code sẽ như sau:

```{r, eval=F, warning=F, message=F}
# prepare the request
path <- "http://api.ratings.food.gov.uk/Establishments"
request <- GET(url = path,
             query = list(
               localAuthorityId = 188,
               BusinessTypeId = 7844,
               pageNumber = 1,
               pageSize = 5000),
             add_headers("x-api-version" = "2"))

# check for any server error ("200" is good!)
request$status_code

# submit the request, parse the response, and convert to a data frame
response <- content(request, as = "text", encoding = "UTF-8") %>%
  fromJSON(flatten = TRUE) %>%
  pluck("establishments") %>%
  as_tibble()
```

Bây giờ bạn có thể làm sạch và sử dụng data frame có tên `response`, với mỗi hàng là một cơ sở thức ăn nhanh.

### Yêu cầu xác thực {.unnumbered}

Một số API yêu cầu xác thực - để bạn chứng minh mình là ai và có thể truy cập vào dữ liệu bị hạn chế. Để nhập những dữ liệu này, trước tiên bạn có thể cần sử dụng phương thức POST để cung cấp tên người dùng, mật khẩu hoặc code. Điều này sẽ trả về một mã thông báo truy cập, có thể được sử dụng cho các yêu cầu phương thức GET tiếp theo để truy xuất dữ liệu mong muốn.

Dưới đây là một ví dụ về truy vấn dữ liệu từ *Go.Data*, một công cụ điều tra ổ dịch. *Go.Data* sử dụng một API cho tất cả các tương tác giữa giao diện người dùng web và các ứng dụng điện thoại thông minh được sử dụng để thu thập dữ liệu. *Go.Data* được sử dụng trên khắp thế giới. Bởi vì dữ liệu các vụ dịch là nhạy cảm và bạn nên là người duy nhất có thể truy cập vào dữ liệu vụ dịch *của mình*, nên việc xác thực là bắt buộc.

Dưới đây là một số code R mẫu sử dụng **httr** và **jsonlite** để kết nối với API *Go.Data* để nhập dữ liệu liên hệ truy vết từ vụ dịch của bạn.

```{r, eval=F}
# set credentials for authorization
url <- "https://godatasampleURL.int/"           # valid Go.Data instance url
username <- "username"                          # valid Go.Data username 
password <- "password"                          # valid Go,Data password 
outbreak_id <- "xxxxxx-xxxx-xxxx-xxxx-xxxxxxx"  # valid Go.Data outbreak ID

# get access token
url_request <- paste0(url,"api/oauth/token?access_token=123") # define base URL request

# prepare request
response <- POST(
  url = url_request,  
  body = list(
    username = username,    # use saved username/password from above to authorize                               
    password = password),                                       
    encode = "json")

# execute request and parse response
content <-
  content(response, as = "text") %>%
  fromJSON(flatten = TRUE) %>%          # flatten nested JSON
  glimpse()

# Save access token from response
access_token <- content$access_token    # save access token to allow subsequent API calls below

# import outbreak contacts
# Use the access token 
response_contacts <- GET(
  paste0(url,"api/outbreaks/",outbreak_id,"/contacts"),          # GET request
  add_headers(
    Authorization = paste("Bearer", access_token, sep = " ")))

json_contacts <- content(response_contacts, as = "text")         # convert to text JSON

contacts <- as_tibble(fromJSON(json_contacts, flatten = TRUE))   # flatten JSON to tibble
```

[***CẨN TRỌNG:*** Nếu bạn đang nhập một lượng lớn dữ liệu từ một API yêu cầu xác thực, nó có thể hết thời gian chờ. Để tránh điều này, hãy truy xuất lại access_token trước mỗi yêu cầu API GET và thử sử dụng các bộ lọc hoặc giới hạn trong truy vấn.]{style="color: orange;"}

[***MẸO:*** Lệnh `fromJSON()` từ package **jsonlite** không hoàn toàn không - lồng ghép vào lần đầu tiên nó được chạy, vì vậy bạn vẫn có thể có danh sách các hàng trong phần kết quả của mình. Bạn sẽ cần phải bỏ lồng ghép thêm cho một số biến nhất định; tùy thuộc vào cách .json của bạn được lồng ghép vào nhau. Để xem thêm thông tin về điều này, hãy xem tài liệu về package **jsonlite**, chẳng hạn như [`flatten()` function](https://rdrr.io/cran/jsonlite/man/flatten.html).]{style="color: darkgreen;"}

Để biết thêm chi tiết, hãy xem tài liệu trên [LoopBack Explorer](https://loopback.io/doc/en/lb4/index.html), chương [Truy vết tiếp xúc] hoặc các mẹo API trên [Go.Data Github repository](https://worldhealthorganization.github.io/godata/api-docs)

Bạn có thể đọc thêm về *httr* trong package [here](https://httr.r-lib.org/articles/quickstart.html)

Phần này cũng đã được trình bày trong [hướng dẫn này](https://www.dataquest.io/blog/r-api-tutorial/)và [hướng dẫn này](https://medium.com/@traffordDataLab/querying-apis-in-r-39029b73d5f1) hướng dẫn này.

<!-- ======================================================= -->

## Xuất dữ liệu

### Với package **rio** {.unnumbered}

Với **rio**, bạn có thể sử dụng lệnh `export()` theo cách tương tự với `import()`. Đầu tiên, cung cấp tên của đối tượng R bạn muốn lưu (ví dụ: `linelist`), sau đó trong dấu ngoặc kép đặt đường dẫn tệp nơi bạn muốn lưu tệp, bao gồm tên tệp mong muốn và phần mở rộng tệp. Ví dụ:

Thao tác này lưu data frame `linelist` dưới dạng một Excel workbook vào thư mục làm việc/thư mục gốc của dự án R:

```{r, eval=F}
export(linelist, "my_linelist.xlsx") # will save to working directory
```

Bạn có thể lưu cùng một data frame dưới dạng tệp csv bằng cách thay đổi phần mở rộng. Ví dụ, chúng tôi cũng lưu nó vào một đường dẫn tệp được tạo bằng `here()`:

```{r, eval=F}
export(linelist, here("data", "clean", "my_linelist.csv"))
```

### Tới bộ nhớ tạm {.unnumbered}

Để xuất khung dữ liệu sang "bộ nhớ tạm" của máy tính (để sau đó dán vào một phần mềm khác như Excel, Google Spreadsheets, v.v.), bạn có thể sử dụng `write_clip()` từ package **clipr**.

```{r, eval=F}
# export the linelist data frame to your system's clipboard
clipr::write_clip(linelist)
```

## Tệp RDS {#import_rds}

Giống như .csv, .xlsx, v.v., bạn cũng có thể xuất/lưu các R data frame dưới dạng tệp .rds. Đây là định dạng tệp dành riêng cho R và rất hữu ích nếu bạn biết mình sẽ làm việc lại với dữ liệu đã xuất trong R.

Các nhóm của cột được lưu trữ, vì vậy bạn không cần phải làm sạch lại khi chúng được nhập (với Excel hoặc thậm chí là tệp CSV, điều này có thể khiến bạn đau đầu!). Nó cũng là một tệp nhỏ hơn, hữu ích cho việc xuất và nhập nếu bộ dữ liệu của bạn lớn.

Ví dụ: nếu bạn làm việc trong nhóm Dịch tễ học và cần gửi tệp cho nhóm GIS để lập bản đồ và họ cũng sử dụng R, chỉ cần gửi tệp .rds cho họ! Sau đó, tất cả các nhóm cột được giữ lại và có ít việc phải xử lý hơn.

```{r, eval=F}
export(linelist, here("data", "clean", "my_linelist.rds"))
```

<!-- ======================================================= -->

## Tệp và danh sách Rdata {#import_rdata}

Tệp `.Rdata` có thể lưu trữ nhiều đối tượng R - ví dụ: nhiều data frame, kết quả mô hình, danh sách, v.v. Điều này có thể rất hữu ích để hợp nhất hoặc chia sẻ nhiều dữ liệu của bạn cho một dự án nhất định.

Trong ví dụ dưới đây, nhiều đối tượng R được lưu trữ trong tệp "my_objects.Rdata" đã xuất:

```{r, eval=F}
rio::export(my_list, my_dataframe, my_vector, "my_objects.Rdata")
```

Lưu ý: nếu bạn đang thử *nhập* một danh sách, hãy sử dụng `import_list()` từ **rio** để nhập nó với cấu trúc và nội dung gốc hoàn chỉnh.

```{r, eval=F}
rio::import_list("my_list.Rdata")
```

<!-- ======================================================= -->

## Lưu biểu đồ

Hướng dẫn về cách lưu các biểu đồ, chẳng hạn như các biểu đồ được tạo bởi `ggplot()`, được thảo luận sâu trong chương [ggplot cơ bản].

Tóm lại, chạy lệnh `ggsave("my_plot_filepath_and_name.png")` sau khi in biểu đồ của bạn. Bạn có thể cung cấp một đối tượng biểu đồ đã lưu cho đối số `plot =` hoặc chỉ cần xác định đường dẫn tệp đích (với phần mở rộng tệp) để lưu biểu đồ được hiển thị gần đây nhất. Bạn cũng có thể kiểm soát `width =`, `height =`, `units =` và `dpi =`.

Cách để lưu đồ thị mạng lưới (network graph), chẳng hạn như cây lây nhiễm, được đề cập trong chương [Chuỗi lây nhiễm].

<!-- ======================================================= -->

## Tài nguyên học liệu

[R Data Import/Export Manual](https://cran.r-project.org/doc/manuals/r-release/R-data.html)\
[R 4 Data Science chapter on data import](https://r4ds.had.co.nz/data-import.html#data-import)\
[ggsave() documentation](https://ggplot2.tidyverse.org/reference/ggsave.html)

Dưới đây là một bảng, lấy từ **rio** [vignette](https://cran.r-project.org/web/packages/rio/vignettes/rio.html) trực tuyến. Đối với mỗi loại dữ liệu, nó hiển thị: phần mở rộng tệp dự kiến, package **rio** sử dụng để nhập hoặc xuất dữ liệu và trả lời chức năng này có được bao gồm trong phiên bản **rio** được cài đặt mặc định hay không.

+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
| Định dạng                                 | Phần mở rộng điển hình  | Package nhập         | Package xuất | Được cài đặt theo mặc định |
+===========================================+=========================+======================+==============+============================+
| Dữ liệu được phân tách bằng-dấu phẩy      | .csv                    | data.table `fread()` | data.table   | Có                         |
+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
| Dữ liệu được phân tách bằng-dấu gạch      | .psv                    | data.table `fread()` | data.table   | Có                         |
+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
| Dữ liệu được phân tách bằng-tab           | .tsv                    | data.table `fread()` | data.table   | Có                         |
+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
| SAS                                       | .sas7bdat               | haven                | haven        | Có                         |
+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
| SPSS                                      | .sav                    | haven                | haven        | Có                         |
+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
| Stata                                     | .dta                    | haven                | haven        | Có                         |
+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
| SAS                                       | XPORT                   | .xpt                 | haven        | haven                      |
+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
| SPSS Portable                             | .por                    | haven                |              | Có                         |
+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
| Excel                                     | .xls                    | readxl               |              | Có                         |
+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
| Excel                                     | .xlsx                   | readxl               | openxlsx     | Có                         |
+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
| Cú pháp R                                 | .R                      | base                 | base         | Có                         |
+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
| Đối tượng R được lưu                      | .RData, .rda            | base                 | base         | Có                         |
+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
| Đối tượng R được nối tiếp                 | .rds                    | base                 | base         | Có                         |
+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
| Epiinfo                                   | .rec                    | foreign              |              | Có                         |
+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
| Minitab                                   | .mtp                    | foreign              |              | Có                         |
+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
| Systat                                    | .syd                    | foreign              |              | Có                         |
+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
| "XBASE"                                   | database files          | .dbf                 | foreign      | foreign                    |
+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
| Định dạng tệp Weka Attribute-Relation     | .arff                   | foreign              | foreign      | Có                         |
+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
| Định dạng trao đổi dữ liệu                | .dif                    | utils                |              | Có                         |
+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
| Dữ liệu Fortran                           | no recognized extension | utils                |              | Có                         |
+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
| Định dạng dữ liệu Fixed-width             | .fwf                    | utils                | utils        | Có                         |
+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
| Dữ liệu gzip được phân tách bằng-dấu phẩy | .csv.gz                 | utils                | utils        | Có                         |
+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
| CSVY (Tiêu đề siêu dữ liệu CSV + YAML)    | .csvy                   | csvy                 | csvy         | Không                      |
+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
| EViews                                    | .wf1                    | hexView              |              | Không                      |
+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
| Định dạng trao đổi Feather giữa R/Python  | .feather                | feather              | feather      | Không                      |
+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
| Fast Storage                              | .fst                    | fst                  | fst          | Không                      |
+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
| JSON                                      | .json                   | jsonlite             | jsonlite     | Không                      |
+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
| Matlab                                    | .mat                    | rmatio               | rmatio       | Không                      |
+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
| OpenDocument Spreadsheet                  | .ods                    | readODS              | readODS      | Không                      |
+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
| Bảng HTML                                 | .html                   | xml2                 | xml2         | Không                      |
+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
| Tài liệu XML cạn                          | .xml                    | xml2                 | xml2         | Không                      |
+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
| YAML                                      | .yml                    | yaml                 | yaml         | Không                      |
+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
| Bộ nhớ tạm mặc định là tsv                |                         | clipr                | clipr        | Không                      |
+-------------------------------------------+-------------------------+----------------------+--------------+----------------------------+
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/importing.Rmd-->

# (PART) Quản lý dữ liệu {.unnumbered}
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/cat_data_management.Rmd-->

# Làm sạch số liệu và các hàm quan trọng {#cleaning}


```{r, out.height = "10%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "cleaning.png"))
```


This page demonstrates common steps used in the process of "cleaning" a dataset, and also explains the use of many essential R data management functions.  

To demonstrate data cleaning, this page begins by importing a raw case linelist dataset, and proceeds step-by-step through the cleaning process. In the R code, this manifests as a "pipe" chain, which references the "pipe" operator ` %>%` that passes a dataset from one operation to the next.  


### Core functions {.unnumbered}  

This handbook emphasizes use of the functions from the [**tidyverse**](https://www.tidyverse.org/) family of R packages. The essential R functions demonstrated in this page are listed below.  

Many of these functions belong to the [**dplyr**](https://dplyr.tidyverse.org/) R package, which provides "verb" functions to solve data manipulation challenges (the name is a reference to a "data frame-[plier](https://www.thefreedictionary.com/plier#:~:text=also%20ply%C2%B7er%20(pl%C4%AB%E2%80%B2,holding%2C%20bending%2C%20or%20cutting.)"). **dplyr** is part of the **tidyverse** family of R packages (which also includes **ggplot2**, **tidyr**, **stringr**, **tibble**, **purrr**, **magrittr**, and **forcats** among others).  


Function       | Utility                               | Package
---------------|---------------------------------------|------------------------------
` %>% `|"pipe" (pass) data from one function to the next|**magrittr** 
`mutate()`|create, transform, and re-define columns|**dplyr**  
`select()`|keep, remove, select, or re-name columns|**dplyr**
`rename()`|rename columns|**dplyr** 
`clean_names()`|standardize the syntax of column names|**janitor**
`as.character()`, `as.numeric()`, `as.Date()`, etc.|convert the class of a column|**base** R
`across()`|transform multiple columns at one time|**dplyr** 
**tidyselect** functions|use logic to select columns|**tidyselect**   
`filter()`|keep certain rows|**dplyr** 
`distinct()`|de-duplicate rows|**dplyr** 
`rowwise()`|operations by/within each row|**dplyr**  
`add_row()`|add rows manually|**tibble** 
`arrange()`|sort rows|**dplyr**
`recode()`|re-code values in a column|**dplyr** 
`case_when()`|re-code values in a column using more complex logical criteria|**dplyr** 
`replace_na()`, `na_if()`, `coalesce()`|special functions for re-coding|**tidyr**  
`age_categories()` and `cut()`|create categorical groups from a numeric column|**epikit** and **base** R
`clean_variable_spelling()`|re-code/clean values using a data dictionary|**linelist**
`which()`|apply logical criteria; return indices|**base** R

If you want to see how these functions compare to Stata or SAS commands, see the page on [Transition to R].  

You may encounter an alternative data management framework from the **data.table** R package with operators like `:=` and frequent use of brackets `[ ]`. This approach and syntax is briefly explained in the [Data Table] page.  

### Nomenclature {.unnumbered}  

In this handbook, we generally reference "columns" and "rows" instead of "variables" and "observations". As explained in this primer on ["tidy data"](https://tidyr.tidyverse.org/articles/tidy-data.html), most epidemiological statistical datasets consist structurally of rows, columns, and values.  

*Variables* contain the values that measure the same underlying attribute (like age group, outcome, or date of onset). *Observations* contain all values measured on the same unit (e.g. a person, site, or lab sample). So these aspects can be more difficult to tangibly define.  

In "tidy" datasets, each column is a variable, each row is an observation, and each cell is a single value. However some datasets you encounter will not fit this mold - a "wide" format dataset may have a variable split across several columns (see an example in the [Pivoting data] page). Likewise, observations could be split across several rows.  

Most of this handbook is about managing and transforming data, so referring to the concrete data structures of rows and columns is more relevant than the more abstract observations and variables. Exceptions occur primarily in pages on data analysis, where you will see more references to variables and observations.  



<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Cleaning pipeline

**This page proceeds through typical cleaning steps, adding them sequentially to a cleaning pipe chain.**

In epidemiological analysis and data processing, cleaning steps are often performed sequentially, linked together. In R, this often manifests as a cleaning "pipeline", where *the raw dataset is passed or "piped" from one cleaning step to another*.  

Such chains utilize **dplyr** "verb" functions and the **magrittr** pipe operator `%>%`. This pipe begins with the "raw" data ("linelist_raw.xlsx") and ends with a "clean" R data frame (`linelist`) that can be used, saved, exported, etc.  

In a cleaning pipeline the order of the steps is important. Cleaning steps might include:  

* Importing of data  
* Column names cleaned or changed  
* De-duplication  
* Column creation and transformation (e.g. re-coding or standardising values)  
* Rows filtered or added  



<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Load packages  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r, message = F}
pacman::p_load(
  rio,        # importing data  
  here,       # relative file pathways  
  janitor,    # data cleaning and tables
  lubridate,  # working with dates
  epikit,     # age_categories() function
  tidyverse   # data management and visualization
)
```




<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Import data  

### Import {.unnumbered}  

Here we import the "raw" case linelist Excel file using the `import()` function from the package **rio**. The **rio** package flexibly handles many types of files (e.g. .xlsx, .csv, .tsv, .rds. See the page on [Import and export] for more information and tips on unusual situations (e.g. skipping rows, setting missing values, importing Google sheets, etc).  

If you want to follow along, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_raw.xlsx' class='download-button'>click to download the "raw" linelist</a> (as .xlsx file).  

If your dataset is large and takes a long time to import, it can be useful to have the import command be separate from the pipe chain and the "raw" saved as a distinct file. This also allows easy comparison between the original and cleaned versions.  

Below we import the raw Excel file and save it as the data frame `linelist_raw`. We assume the file is located in your working directory or R project root, and so no sub-folders are specified in the file path.  

```{r, echo=F, message=F}
# HIDDEN FROM READER
# actually load the data using here()
linelist_raw <- rio::import(here::here("data", "case_linelists", "linelist_raw.xlsx"))
```

```{r, eval=F}
linelist_raw <- import("linelist_raw.xlsx")
```

You can view the first 50 rows of the the data frame below. Note: the **base** R function `head(n)` allow you to view just the first `n` rows in the R console.  

```{r message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist_raw,50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```
### Review {.unnumbered}  

You can use the function `skim()` from the package **skimr** to get an overview of the entire dataframe (see page on [Descriptive tables] for more info). Columns are summarised by class/type such as character, numeric. Note: "POSIXct" is a type of raw date class (see [Working with dates].  


```{r, eval=F}
skimr::skim(linelist_raw)
```

```{r, echo=F}
skimr::skim_without_charts(linelist_raw)
```




 





<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Column names {} 

In R, column *names* are the "header" or "top" value of a column. They are used to refer to columns in the code, and serve as a default label in figures.  

Other statistical software such as SAS and STATA use *"labels"* that co-exist as longer printed versions of the shorter column names. While R does offer the possibility of adding column labels to the data, this is not emphasized in most practice. To make column names "printer-friendly" for figures, one typically adjusts their display within the plotting commands that create the outputs (e.g. axis or legend titles of a plot, or column headers in a printed table - see the [scales section of the ggplot tips page](#ggplot_tips_scales) and [Tables for presentation] pages). If you want to assign column labels in the data, read more online [here](https://cran.r-project.org/web/packages/expss/vignettes/labels-support.html) and [here](https://cran.r-project.org/web/packages/labelled/vignettes/intro_labelled.html).  

As R column names are used very often, so they must have "clean" syntax. We suggest the following:  

* Short names
* No spaces (replace with underscores _ ) 
* No unusual characters (&, #, <, >, ...)  
* Similar style nomenclature (e.g. all date columns named like **date_**onset, **date_**report, **date_**death...)  

The columns names of `linelist_raw` are printed below using `names()` from **base** R. We can see that initially:  

* Some names contain spaces (e.g. `infection date`)  
* Different naming patterns are used for dates (`date onset` vs. `infection date`)  
* There must have been a *merged header* across the two last columns in the .xlsx. We know this because the name of two merged columns ("merged_header") was assigned by R to the first column, and the second column was assigned a placeholder  name "...28" (as it was then empty and is the 28th column).  

```{r}
names(linelist_raw)
```

<span style="color: black;">**_NOTE:_** To reference a column name that includes spaces, surround the name with back-ticks, for example: linelist$`` ` '\x60infection date\x60'` ``. note that on your keyboard, the back-tick (`) is different from the single quotation mark (').</span>


### Labels {.unnumbered}  

Some other statistical software such as SAS have variable *labels*


### Automatic cleaning {.unnumbered}  

The function `clean_names()` from the package **janitor** standardizes column names and makes them unique by doing the following:  

* Converts all names to consist of only underscores, numbers, and letters  
* Accented characters are transliterated to ASCII (e.g. german o with umlaut becomes "o", spanish "enye" becomes "n")  
* Capitalization preference for the new column names can be specified using the `case = ` argument ("snake" is default, alternatives include "sentence", "title", "small_camel"...)  
* You can specify specific name replacements by providing a vector to the `replace = ` argument (e.g. `replace = c(onset = "date_of_onset")`)  
* Here is an online [vignette](https://cran.r-project.org/web/packages/janitor/vignettes/janitor.html#cleaning)  

Below, the cleaning pipeline begins by using `clean_names()` on the raw linelist.  

```{r clean_names}
# pipe the raw dataset through the function clean_names(), assign result as "linelist"  
linelist <- linelist_raw %>% 
  janitor::clean_names()

# see the new column names
names(linelist)
```

<span style="color: black;">**_NOTE:_** The last column name "...28" was changed to "x28".</span>


### Manual name cleaning {.unnumbered}  

Re-naming columns manually is often necessary, even after the standardization step above. Below, re-naming is performed using the `rename()` function from the **dplyr** package, as part of a pipe chain. `rename()` uses the style `NEW = OLD` - the new column name is given before the old column name.  

Below, a re-naming command is added to the cleaning pipeline. Spaces have been added strategically to align code for easier reading.  

```{r}
# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)
##################################################################################
linelist <- linelist_raw %>%
    
    # standardize column name syntax
    janitor::clean_names() %>% 
    
    # manually re-name columns
           # NEW name             # OLD name
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome)
```


Now you can see that the columns names have been changed:  

```{r message=FALSE, echo=F}
names(linelist)
```


#### Rename by column position {.unnumbered} 

You can also rename by column position, instead of column name, for example:  

```{r, eval=F}
rename(newNameForFirstColumn  = 1,
       newNameForSecondColumn = 2)
```



#### Rename via `select()` and `summarise()` {.unnumbered}  

As a shortcut, you can also rename columns within the **dplyr** `select()` and `summarise()` functions. `select()` is used to keep only certain columns (and is covered later in this page). `summarise()` is covered in the [Grouping data] and [Descriptive tables] pages. These functions also uses the format `new_name = old_name`. Here is an example:  

```{r, eval=F}
linelist_raw %>% 
  select(# NEW name             # OLD name
         date_infection       = `infection date`,    # rename and KEEP ONLY these columns
         date_hospitalisation = `hosp date`)
```





### Other challenges {.unnumbered}  


#### Empty Excel column names {.unnumbered} 

R cannot have dataset columns that do not have column names (headers). So, if you import an Excel dataset with data but no column headers, R will fill-in the headers with names like "...1" or "...2". The number represents the column number (e.g. if the 4th column in the dataset has no header, then R will name it "...4").  

You can clean these names manually by referencing their position number (see example above), or their assigned name (`linelist_raw$...1`).  



#### Merged Excel column names and cells {.unnumbered}  

Merged cells in an Excel file are a common occurrence when receiving data. As explained in [Transition to R], merged cells can be nice for human reading of data, but are not "tidy data" and cause many problems for machine reading of data. R cannot accommodate merged cells.  

Remind people doing data entry that **human-readable data is not the same as machine-readable data**. Strive to train users about the principles of [**tidy data**](https://r4ds.had.co.nz/tidy-data.html). If at all possible, try to change procedures so that data arrive in a tidy format without merged cells.  

* Each variable must have its own column.  
* Each observation must have its own row.  
* Each value must have its own cell.  

When using **rio**'s `import()` function, the value in a merged cell will be assigned to the first cell and subsequent cells will be empty.  

One solution to deal with merged cells is to import the data with the function `readWorkbook()` from the package **openxlsx**. Set the argument `fillMergedCells = TRUE`. This gives the value in a merged cell to all cells within the merge range.

```{r, eval=F}
linelist_raw <- openxlsx::readWorkbook("linelist_raw.xlsx", fillMergedCells = TRUE)
```

<span style="color: red;">**_DANGER:_** If column names are merged with `readWorkbook()`, you will end up with duplicate column names, which you will need to fix manually - R does not work well with duplicate column names! You can re-name them by referencing their position (e.g. column 5), as explained in the section on manual column name cleaning.</span>






<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Select or re-order columns {} 

Use `select()` from **dplyr** to select the columns you want to retain, and to specify their order in the data frame. 

<span style="color: orange;">**_CAUTION:_** In the examples below, the `linelist` data frame is modified with `select()` and displayed, but not saved. This is for demonstration purposes. The modified column names are printed by piping the data frame to `names()`.</span>

**Here are ALL the column names in the linelist at this point in the cleaning pipe chain:**

```{r}
names(linelist)
```

### Keep columns {.unnumbered}  

**Select only the columns you want to remain**  

Put their names in the `select()` command, with no quotation marks. They will appear in the data frame in the order you provide. Note that if you include a column that does not exist, R will return an error (see use of `any_of()` below if you want no error in this situation).  

```{r}
# linelist dataset is piped through select() command, and names() prints just the column names
linelist %>% 
  select(case_id, date_onset, date_hospitalisation, fever) %>% 
  names()  # display the column names
```




### "tidyselect" helper functions {#clean_tidyselect .unnumbered}  

These helper functions exist to make it easy to specify columns to keep, discard, or transform. They are from the package **tidyselect**, which is included in **tidyverse** and underlies how columns are selected in **dplyr** functions.  

For example, if you want to re-order the columns, `everything()` is a useful function to signify "all other columns not yet mentioned". The command below moves columns `date_onset` and `date_hospitalisation` to the beginning (left) of the dataset, but keeps all the other columns afterward. Note that `everything()` is written with empty parentheses:  

```{r}
# move date_onset and date_hospitalisation to beginning
linelist %>% 
  select(date_onset, date_hospitalisation, everything()) %>% 
  names()
```

Here are other "tidyselect" helper functions that also work *within* **dplyr** functions like `select()`, `across()`, and `summarise()`:  

* `everything()`  - all other columns not mentioned  
* `last_col()`    - the last column  
* `where()`       - applies a function to all columns and selects those which are TRUE  
* `contains()`    - columns containing a character string  
  * example: `select(contains("time"))`  
* `starts_with()` - matches to a specified prefix  
  * example: `select(starts_with("date_"))`  
* `ends_with()`   - matches to a specified suffix  
  * example: `select(ends_with("_post"))`  
* `matches()`     - to apply a regular expression (regex)  
  * example: `select(matches("[pt]al"))`  
* `num_range()`   - a numerical range like x01, x02, x03  
* `any_of()`      - matches IF column exists but returns no error if it is not found  
  * example: `select(any_of(date_onset, date_death, cardiac_arrest))`  

In addition, use normal operators such as `c()` to list several columns, `:` for consecutive columns, `!` for opposite, `&` for AND, and `|` for OR.  


Use `where()` to specify logical criteria for columns. If providing a function inside `where()`, do not include the function's empty parentheses. The command below selects columns that are class Numeric.

```{r}
# select columns that are class Numeric
linelist %>% 
  select(where(is.numeric)) %>% 
  names()
```

Use `contains()` to select only columns in which the column name contains a specified character string. `ends_with()` and `starts_with()` provide more nuance.  

```{r}
# select columns containing certain characters
linelist %>% 
  select(contains("date")) %>% 
  names()
```

The function `matches()` works similarly to `contains()` but can be provided a regular expression (see page on [Characters and strings]), such as multiple strings separated by OR bars within the parentheses:  

```{r}
# searched for multiple character matches
linelist %>% 
  select(matches("onset|hosp|fev")) %>%   # note the OR symbol "|"
  names()
```

<span style="color: orange;">**_CAUTION:_** If a column name that you specifically provide does not exist in the data, it can return an error and stop your code. Consider using `any_of()` to cite columns that may or may not exist, especially useful in negative (remove) selections.</span>

Only one of these columns exists, but no error is produced and the code continues without stopping your cleaning chain.  

```{r}
linelist %>% 
  select(any_of(c("date_onset", "village_origin", "village_detection", "village_residence", "village_travel"))) %>% 
  names()
```



### Remove columns {.unnumbered} 

**Indicate which columns to remove** by placing a minus symbol "-" in front of the column name (e.g. `select(-outcome)`), or a vector of column names (as below). All other columns will be retained. 

```{r}
linelist %>% 
  select(-c(date_onset, fever:vomit)) %>% # remove date_onset and all columns from fever to vomit
  names()
```

You can also remove a column using **base** R syntax, by defining it as `NULL`. For example:  

```{r, eval=F}
linelist$date_onset <- NULL   # deletes column with base R syntax 
```



### Standalone {.unnumbered}

`select()` can also be used as an independent command (not in a pipe chain). In this case, the first argument is the original dataframe to be operated upon.  

```{r}
# Create a new linelist with id and age-related columns
linelist_age <- select(linelist, case_id, contains("age"))

# display the column names
names(linelist_age)
```



#### Add to the pipe chain {.unnumbered}  

In the `linelist_raw`, there are a few columns we do not need: `row_num`, `merged_header`, and `x28`. We remove them with a `select()` command in the cleaning pipe chain:  

```{r}
# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)
##################################################################################

# begin cleaning pipe chain
###########################
linelist <- linelist_raw %>%
    
    # standardize column name syntax
    janitor::clean_names() %>% 
    
    # manually re-name columns
           # NEW name             # OLD name
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome) %>% 
    
    # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED
    #####################################################

    # remove column
    select(-c(row_num, merged_header, x28))
```




<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Deduplication


See the handbook page on [De-duplication] for extensive options on how to de-duplicate data. Only a very simple row de-duplication example is presented here.  

The package **dplyr** offers the `distinct()` function. This function examines every row and reduce the data frame to only the unique rows. That is, it removes rows that are 100% duplicates.  

When evaluating duplicate rows, it takes into account a range of columns - by default it considers all columns. As shown in the de-duplication page, you can adjust this column range so that the uniqueness of rows is only evaluated in regards to certain columns.  

In this simple example, we just add the empty command `distinct()` to the pipe chain. This ensures there are no rows that are 100% duplicates of other rows (evaluated across all columns).  

We begin with ` nrow(linelist)` rows in `linelist`. 

```{r}
linelist <- linelist %>% 
  distinct()
```

After de-duplication there are ` nrow(linelist)` rows. Any removed rows would have been 100% duplicates of other rows.  

Below, the `distinct()` command is added to the cleaning pipe chain:

```{r}
# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)
##################################################################################

# begin cleaning pipe chain
###########################
linelist <- linelist_raw %>%
    
    # standardize column name syntax
    janitor::clean_names() %>% 
    
    # manually re-name columns
           # NEW name             # OLD name
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome) %>% 
    
    # remove column
    select(-c(row_num, merged_header, x28)) %>% 
  
    # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED
    #####################################################
    
    # de-duplicate
    distinct()
```





<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Column creation and transformation { }


**We recommend using the dplyr function `mutate()` to add a new column, or to modify an existing one.**  

Below is an example of creating a new column with `mutate()`. The syntax is: `mutate(new_column_name = value or transformation)`  

In Stata, this is similar to the command `generate`, but R's `mutate()` can also be used to modify an existing column.  


### New columns {.unnumbered}

The most basic `mutate()` command to create a new column might look like this. It creates a new column `new_col` where the value in every row is 10.  

```{r, eval=F}
linelist <- linelist %>% 
  mutate(new_col = 10)
```

You can also reference values in other columns, to perform calculations. Below, a new column `bmi` is created to hold the Body Mass Index (BMI) for each case - as calculated using the formula BMI = kg/m^2, using column `ht_cm` and column `wt_kg`.  

```{r}
linelist <- linelist %>% 
  mutate(bmi = wt_kg / (ht_cm/100)^2)
```

If creating multiple new columns, separate each with a comma and new line. Below are examples of new columns, including ones that consist of values from other columns combined using `str_glue()` from the **stringr** package (see page on [Characters and strings].  

```{r}
new_col_demo <- linelist %>%                       
  mutate(
    new_var_dup    = case_id,             # new column = duplicate/copy another existing column
    new_var_static = 7,                   # new column = all values the same
    new_var_static = new_var_static + 5,  # you can overwrite a column, and it can be a calculation using other variables
    new_var_paste  = stringr::str_glue("{hospital} on ({date_hospitalisation})") # new column = pasting together values from other columns
    ) %>% 
  select(case_id, hospital, date_hospitalisation, contains("new"))        # show only new columns, for demonstration purposes
```


Review the new columns. For demonstration purposes, only the new columns and the columns used to create them are shown:  


```{r message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(new_col_demo,50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

<span style="color: darkgreen;">**_TIP:_** A variation on `mutate()` is the function `transmute()`. This function adds a new column just like `mutate()`, but also drops/removes all other columns that you do not mention within its parentheses.</span>


```{r, eval=F}
# HIDDEN FROM READER
# removes new demo columns created above
# linelist <- linelist %>% 
#   select(-contains("new_var"))
```



### Convert column class {.unnumbered}
  
Columns containing values that are dates, numbers, or logical values (TRUE/FALSE) will only behave as expected if they are correctly classified. There is a difference between "2" of class character and 2 of class numeric!  

There are ways to set column class during the import commands, but this is often cumbersome. See the [R Basics] section on object classes to learn more about converting the class of objects and columns.  

First, let's run some checks on important columns to see if they are the correct class. We also saw this in the beginning when we ran `skim()`.  

Currently, the class of the `age` column is character. To perform quantitative analyses, we need these numbers to be recognized as numeric! 

```{r}
class(linelist$age)
```

The class of the `date_onset` column is also character! To perform analyses, these dates must be recognized as dates! 
 
```{r}
class(linelist$date_onset)
```


To resolve this, use the ability of `mutate()` to re-define a column with a transformation. We define the column as itself, but converted to a different class. Here is a basic example, converting or ensuring that the column `age` is class Numeric:  

```{r, eval=F}
linelist <- linelist %>% 
  mutate(age = as.numeric(age))
```

In a similar way, you can use `as.character()` and `as.logical()`. To convert to class Factor, you can use `factor()` from **base** R or `as_factor()` from **forcats**. Read more about this in the [Factors] page.  

You must be careful when converting to class Date. Several methods are explained on the page [Working with dates]. Typically, the raw date values must all be in the same format for conversion to work correctly (e.g "MM/DD/YYYY", or "DD MM YYYY"). After converting to class Date, check your data to confirm that each value was converted correctly.  




### Grouped data {.unnumbered}  

If your data frame is already *grouped* (see page on [Grouping data]), `mutate()` may behave differently than if the data frame is not grouped. Any summarizing functions, like `mean()`, `median()`, `max()`, etc. will calculate by group, not by all the rows.     

```{r, eval=F}
# age normalized to mean of ALL rows
linelist %>% 
  mutate(age_norm = age / mean(age, na.rm=T))

# age normalized to mean of hospital group
linelist %>% 
  group_by(hospital) %>% 
  mutate(age_norm = age / mean(age, na.rm=T))
```

Read more about using `mutate ()` on grouped dataframes in this [tidyverse mutate documentation](https://dplyr.tidyverse.org/reference/mutate.html).  



### Transform multiple columns {#clean_across .unnumbered}


Often to write concise code you want to apply the same transformation to multiple columns at once. A transformation can be applied to multiple columns at once using the `across()` function from the package **dplyr** (also contained within **tidyverse** package). `across()` can be used with any **dplyr** function, but is commonly used within `select()`, `mutate()`, `filter()`, or `summarise()`. See how it is applied to `summarise()` in the page on [Descriptive tables].  

Specify the columns to the argument `.cols = ` and the function(s) to apply to `.fns = `. Any additional arguments to provide to the `.fns` function can be included after a comma, still within `across()`.   

#### `across()` column selection {.unnumbered}  

Specify the columns to the argument `.cols = `. You can name them individually, or use "tidyselect" helper functions. Specify the function to `.fns = `. Note that using the function mode demonstrated below, the function is written *without* its parentheses ( ).  

Here the transformation `as.character()` is applied to specific columns named within `across()`. 

```{r, eval=F}
linelist <- linelist %>% 
  mutate(across(.cols = c(temp, ht_cm, wt_kg), .fns = as.character))
```

The "tidyselect" helper functions are available to assist you in specifying columns. They are detailed above in the section on Selecting and re-ordering columns, and they include: `everything()`, `last_col()`, `where()`, `starts_with()`, `ends_with()`, `contains()`, `matches()`, `num_range()` and `any_of()`.  

Here is an example of how one would change **all columns** to character class:  

```{r, eval=F}
#to change all columns to character class
linelist <- linelist %>% 
  mutate(across(.cols = everything(), .fns = as.character))
```

Convert to character all columns where the name contains the string "date" (note the placement of commas and parentheses):  

```{r, eval=F}
#to change all columns to character class
linelist <- linelist %>% 
  mutate(across(.cols = contains("date"), .fns = as.character))
```

Below, an example of mutating the columns that are currently class POSIXct (a raw datetime class that shows timestamps) - in other words, where the function `is.POSIXct()` evaluates to `TRUE`. Then we want to apply the function `as.Date()` to these columns to convert them to a normal class Date.  

```{r, eval=F}
linelist <- linelist %>% 
  mutate(across(.cols = where(is.POSIXct), .fns = as.Date))
```

* Note that within `across()` we also use the function `where()` as `is.POSIXct` is evaluating to either TRUE or FALSE.  
* Note that `is.POSIXct()` is from the package **lubridate**. Other similar "is" functions like `is.character()`, `is.numeric()`, and `is.logical()` are from **base R**  

#### `across()` functions {.unnumbered}

You can read the documentation with `?across` for details on how to provide functions to `across()`. A few summary points: there are several ways to specify the function(s) to perform on a column and you can even define your own functions:  

* You can provide the function name alone (e.g. `mean` or `as.character`)  
* You can provide the function in **purrr**-style (e.g. `~ mean(.x, na.rm = TRUE)`) (see [this page][Iteration, loops, and lists])  
* You can specify multiple functions by providing a list (e.g. `list(mean = mean, n_miss = ~ sum(is.na(.x))`).  
  * If you provide multiple functions, multiple transformed columns will be returned per input column, with unique names in the format `col_fn`. You can adjust how the new columns are named with the `.names =` argument using **glue** syntax (see page on [Characters and strings]) where `{.col}` and `{.fn}` are shorthand for the input column and function.  
  
  
Here are a few online resources on using `across()`: [creator Hadley Wickham's thoughts/rationale](https://www.tidyverse.org/blog/2020/04/dplyr-1-0-0-colwise/)




### `coalesce()` {.unnumbered}  

This **dplyr** function finds the first non-missing value at each position. It "fills-in" missing values with the first available value in an order you specify.

Here is an example *outside the context of a data frame*: Let us say you have two vectors, one containing the patient's village of detection and another containing the patient's village of residence. You can use coalesce to pick the first non-missing value for each index:  

```{r}
village_detection <- c("a", "b", NA,  NA)
village_residence <- c("a", "c", "a", "d")

village <- coalesce(village_detection, village_residence)
village    # print
```

This works the same if you provide data frame columns: for each row, the function will assign the new column value with the first non-missing value in the columns you provided (in order provided).

```{r, eval=F}
linelist <- linelist %>% 
  mutate(village = coalesce(village_detection, village_residence))
```

This is an example of a "row-wise" operation. For more complicated row-wise calculations, see the section below on Row-wise calculations.  



### Cumulative math {.unnumbered}

If you want a column to reflect the cumulative sum/mean/min/max etc as assessed down the rows of a dataframe to that point, use the following functions:  

`cumsum()` returns the cumulative sum, as shown below:  

```{r}
sum(c(2,4,15,10))     # returns only one number
cumsum(c(2,4,15,10))  # returns the cumulative sum at each step
```

This can be used in a dataframe when making a new column. For example, to calculate the cumulative number of cases per day in an outbreak, consider code like this:  

```{r, warning=F, message=F}
cumulative_case_counts <- linelist %>%  # begin with case linelist
  count(date_onset) %>%                 # count of rows per day, as column 'n'   
  mutate(cumulative_cases = cumsum(n))  # new column, of the cumulative sum at each row
```

Below are the first 10 rows:  

```{r}
head(cumulative_case_counts, 10)
```

See the page on [Epidemic curves] for how to plot cumulative incidence with the epicurve.  

See also:  
`cumsum()`, `cummean()`, `cummin()`, `cummax()`, `cumany()`, `cumall()`  





### Using **base** R {.unnumbered}  

To define a new column (or re-define a column) using **base** R, write the name of data frame, connected with `$`, to the *new* column (or the column to be modified). Use the assignment operator `<-` to define the new value(s). Remember that when using **base** R you must specify the data frame name before the column name every time (e.g. `dataframe$column`). Here is an example of creating the `bmi` column using **base** R:  

```{r, eval=F}
linelist$bmi = linelist$wt_kg / (linelist$ht_cm / 100) ^ 2)
```




### Add to pipe chain {.unnumbered}  

**Below, a new column is added to the pipe chain and some classes are converted.**  

```{r }
# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)
##################################################################################

# begin cleaning pipe chain
###########################
linelist <- linelist_raw %>%
    
    # standardize column name syntax
    janitor::clean_names() %>% 
    
    # manually re-name columns
           # NEW name             # OLD name
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome) %>% 
    
    # remove column
    select(-c(row_num, merged_header, x28)) %>% 
  
    # de-duplicate
    distinct() %>% 
  
    # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED
    ###################################################
    # add new column
    mutate(bmi = wt_kg / (ht_cm/100)^2) %>% 
  
    # convert class of columns
    mutate(across(contains("date"), as.Date), 
           generation = as.numeric(generation),
           age        = as.numeric(age)) 
```





## Re-code values

Here are a few scenarios where you need to re-code (change) values:  

* to edit one specific value (e.g. one date with an incorrect year or format)  
* to reconcile values not spelled the same
* to create a new column of categorical values  
* to create a new column of numeric categories (e.g. age categories)  



### Specific values {.unnumbered}  

To change values manually you can use the `recode()` function within the `mutate()` function. 

Imagine there is a nonsensical date in the data (e.g. "2014-14-15"): you could fix the date manually in the raw source data, or, you could write the change into the cleaning pipeline via `mutate()` and `recode()`. The latter is more transparent and reproducible to anyone else seeking to understand or repeat your analysis.  

```{r, eval=F}
# fix incorrect values                   # old value       # new value
linelist <- linelist %>% 
  mutate(date_onset = recode(date_onset, "2014-14-15" = "2014-04-15"))
```

The `mutate()` line above can be read as: "mutate the column `date_onset` to equal the column `date_onset` re-coded so that OLD VALUE is changed to NEW VALUE". Note that this pattern (OLD = NEW) for `recode()` is the opposite of most R patterns (new = old). The R development community is working on revising this.  

**Here is another example re-coding multiple values within one column.** 

In `linelist` the values in the column "hospital" must be cleaned. There are several different spellings and many missing values.

```{r}
table(linelist$hospital, useNA = "always")  # print table of all unique values, including missing  
```

The `recode()` command below re-defines the column "hospital" as the current column "hospital", but with the specified recode changes. Don't forget commas after each!  

```{r}
linelist <- linelist %>% 
  mutate(hospital = recode(hospital,
                     # for reference: OLD = NEW
                      "Mitylira Hopital"  = "Military Hospital",
                      "Mitylira Hospital" = "Military Hospital",
                      "Military Hopital"  = "Military Hospital",
                      "Port Hopital"      = "Port Hospital",
                      "Central Hopital"   = "Central Hospital",
                      "other"             = "Other",
                      "St. Marks Maternity Hopital (SMMH)" = "St. Mark's Maternity Hospital (SMMH)"
                      ))
```


Now we see the spellings in the `hospital` column have been corrected and consolidated:  

```{r}
table(linelist$hospital, useNA = "always")
```

<span style="color: darkgreen;">**_TIP:_** The number of spaces before and after an equals sign does not matter. Make your code easier to read by aligning the = for all or most rows. Also, consider adding a hashed comment row to clarify for future readers which side is OLD and which side is NEW. </span>  

<span style="color: darkgreen;">**_TIP:_** Sometimes a *blank* character value exists in a dataset (not recognized as R's value for missing - `NA`). You can reference this value with two quotation marks with no space inbetween ("").</span>  




### By logic {.unnumbered}

Below we demonstrate how to re-code values in a column using logic and conditions:  

* Using `replace()`, `ifelse()` and `if_else()` for simple logic
* Using `case_when()` for more complex logic  



### Simple logic {.unnumbered}  


#### `replace()` {.unnumbered}  

To re-code with simple logical criteria, you can use `replace()` within `mutate()`. `replace()` is a function from **base** R. Use a logic condition to specify the rows to change . The general syntax is:  

`mutate(col_to_change = replace(col_to_change, criteria for rows, new value))`.  

One common situation to use `replace()` is **changing just one value in one row, using an unique row identifier**. Below, the gender is changed to "Female" in the row where the column `case_id` is "2195".  

```{r, eval=F}
# Example: change gender of one specific observation to "Female" 
linelist <- linelist %>% 
  mutate(gender = replace(gender, case_id == "2195", "Female"))
```

The equivalent command using **base** R syntax and indexing brackets `[ ]` is below. It reads as "Change the value of the dataframe `linelist`'s column `gender` (for the rows where `linelist`'s column `case_id` has the value  '2195') to 'Female' ".   

```{r, eval=F}
linelist$gender[linelist$case_id == "2195"] <- "Female"
```




#### `ifelse()` and `if_else()` {.unnumbered}  

Another tool for simple logic is `ifelse()` and its partner `if_else()`. However, in most cases for re-coding it is more clear to use `case_when()` (detailed below). These "if else" commands are simplified versions of an `if` and `else` programming statement. The general syntax is:  
`ifelse(condition, value to return if condition evaluates to TRUE, value to return if condition evaluates to FALSE)` 

Below, the column `source_known` is defined. Its value in a given row is set to "known" if the row's value in column `source` is *not* missing. If the value in `source` *is* missing, then the value in `source_known` is set to "unknown".  

```{r, eval=F}
linelist <- linelist %>% 
  mutate(source_known = ifelse(!is.na(source), "known", "unknown"))
```

`if_else()` is a special version from **dplyr** that handles dates. Note that if the 'true' value is a date, the 'false' value must also qualify a date, hence using the special value `NA_real_` instead of just `NA`.

```{r, eval=F}
# Create a date of death column, which is NA if patient has not died.
linelist <- linelist %>% 
  mutate(date_death = if_else(outcome == "Death", date_outcome, NA_real_))
```

**Avoid stringing together many ifelse commands... use `case_when()` instead!** `case_when()` is much easier to read and you'll make fewer errors.  

```{r, fig.align = "center", out.width = "100%", echo=F}
knitr::include_graphics(here::here("images", "ifelse bad.png"))
```

Outside of the context of a data frame, if you want to have an object used in your code switch its value, consider using `switch()` from **base** R.  




### Complex logic {#clean_case_when .unnumbered}  

Use **dplyr**'s `case_when()` if you are re-coding into many new groups, or if you need to use complex logic statements to re-code values. This function evaluates every row in the data frame, assess whether the rows meets specified criteria, and assigns the correct new value.  

`case_when()` commands consist of statements that have a Right-Hand Side (RHS) and a Left-Hand Side (LHS) separated by a "tilde" `~`. The logic criteria are in the left side and the pursuant values are in the right side of each statement. Statements are separated by commas.  

For example, here we utilize the columns `age` and `age_unit` to create a column `age_years`:  


```{r}
linelist <- linelist %>% 
  mutate(age_years = case_when(
            age_unit == "years"  ~ age,       # if age is given in years
            age_unit == "months" ~ age/12,    # if age is given in months
            is.na(age_unit)      ~ age,       # if age unit is missing, assume years
            TRUE                 ~ NA_real_)) # any other circumstance, assign missing
```


As each row in the data is evaluated, the criteria are applied/evaluated in the order the `case_when()` statements are written - from top-to-bottom. If the top criteria evaluates to `TRUE` for a given row, the RHS value is assigned, and the remaining criteria are not even tested for that row. Thus, it is best to write the most specific criteria first, and the most general last.  

Along those lines, in your final statement, place `TRUE` on the left-side, which will capture any row that did not meet any of the previous criteria. The right-side of this statement could be assigned a value like "check me!" or missing.  


<span style="color: red;">**_DANGER:_** **Vvalues on the right-side must all be the same class** - either numeric, character, date, logical, etc. To assign missing (`NA`), you may need to use special variations of `NA` such as `NA_character_`, `NA_real_` (for numeric or POSIX), and `as.Date(NA)`. Read more in [Working with dates].</span>  




### Missing values {.unnumbered} 

Below are special functions for handling missing values in the context of data cleaning.  

See the page on [Missing data] for more detailed tips on identifying and handling missing values. For example, the `is.na()` function which logically tests for missingness.  


**`replace_na()`**  

To change missing values (`NA`) to a specific value, such as "Missing", use the **dplyr** function `replace_na()` within `mutate()`. Note that this is used in the same manner as `recode` above - the name of the variable must be repeated within `replace_na()`.  

```{r}
linelist <- linelist %>% 
  mutate(hospital = replace_na(hospital, "Missing"))
```


**fct_explicit_na()**  

This is a function from the **forcats** package. The **forcats** package handles columns of class Factor. Factors are R's way to handle *ordered* values such as `c("First", "Second", "Third")` or to set the order that values (e.g. hospitals) appear in tables and plots. See the page on [Factors].  

If your data are class Factor and you try to convert `NA` to "Missing" by using `replace_na()`, you will get this error: `invalid factor level, NA generated`. You have tried to add "Missing" as a value, when it was not defined as a possible level of the factor, and it was rejected.  

The easiest way to solve this is to use the **forcats** function `fct_explicit_na()` which converts a column to class factor, and converts `NA` values to the character "(Missing)".  

```{r, eval=F}
linelist %>% 
  mutate(hospital = fct_explicit_na(hospital))
```

A slower alternative would be to add the factor level using `fct_expand()` and then convert the missing values.  

**`na_if()`**  

To convert a *specific value to* `NA`, use **dplyr**'s `na_if()`. The command below performs the opposite operation of `replace_na()`. In the example below, any values of "Missing" in the column `hospital` are converted to `NA`.  

```{r}
linelist <- linelist %>% 
  mutate(hospital = na_if(hospital, "Missing"))
```

Note: `na_if()` **cannot be used for logic criteria** (e.g. "all values > 99") - use `replace()` or `case_when()` for this:  

```{r, eval=F}
# Convert temperatures above 40 to NA 
linelist <- linelist %>% 
  mutate(temp = replace(temp, temp > 40, NA))

# Convert onset dates earlier than 1 Jan 2000 to missing
linelist <- linelist %>% 
  mutate(date_onset = replace(date_onset, date_onset > as.Date("2000-01-01"), NA))
```




### Cleaning dictionary {.unnumbered}

Use the R package **linelist** and it's function `clean_variable_spelling()` to clean a data frame with a *cleaning dictionary*. **linelist** is a package developed by [RECON](https://github.com/reconhub/linelist) - the R Epidemics Consortium.  

1) Create a cleaning dictionary with 3 columns:  
    * A "from" column (the incorrect value)  
    * A "to" column (the correct value)  
    * A column specifying the column for the changes to be applied (or ".global" to apply to all columns)  

Note: .global dictionary entries will be overridden by column-specific dictionary entries.  

```{r, fig.align = "center", out.width = "100%", echo=F}
knitr::include_graphics(here::here("images", "cleaning_dict.png"))
```


2) Import the dictionary file into R. This example can be downloaded via instructions on the [Download handbook and data] page.  

```{r, echo=F}
cleaning_dict <- rio::import(here("data", "case_linelists", "cleaning_dict.csv"))
```

```{r, eval=F}
cleaning_dict <- import("cleaning_dict.csv")
```

3) Pass the raw linelist to `clean_variable_spelling()`, specifying to `wordlists = ` the cleaning dictionary data frame. The `spelling_vars = ` argument can be used to specify which column in the dictionary refers to the columns (3rd by default), or can be set to `NULL` to have the dictionary apply to all character and factor columns. Note this function can take a long time to run.  

```{r}
linelist <- linelist %>% 
  linelist::clean_variable_spelling(
    wordlists = cleaning_dict,
    spelling_vars = "col",        # dict column containing column names, defaults to 3rd column in dict
  )
```

Now scroll to the right to see how values have changed - particularly `gender` (lowercase to uppercase), and all the symptoms columns have been transformed from yes/no to 1/0.  

```{r message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist,50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


Note that your column names in the cleaning dictionary must correspond to the names *at this point* in your cleaning script. See this [online reference for the linelist package](https://www.repidemicsconsortium.org/linelist/reference/clean_data.html) for more details.





#### Add to pipe chain {.unnumbered}  

**Below, some new columns and column transformations are added to the pipe chain.**  

```{r}
# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)
##################################################################################

# begin cleaning pipe chain
###########################
linelist <- linelist_raw %>%
    
    # standardize column name syntax
    janitor::clean_names() %>% 
    
    # manually re-name columns
           # NEW name             # OLD name
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome) %>% 
    
    # remove column
    select(-c(row_num, merged_header, x28)) %>% 
  
    # de-duplicate
    distinct() %>% 
  
    # add column
    mutate(bmi = wt_kg / (ht_cm/100)^2) %>%     

    # convert class of columns
    mutate(across(contains("date"), as.Date), 
           generation = as.numeric(generation),
           age        = as.numeric(age)) %>% 
    
    # add column: delay to hospitalisation
    mutate(days_onset_hosp = as.numeric(date_hospitalisation - date_onset)) %>% 
    
   # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED
   ###################################################

    # clean values of hospital column
    mutate(hospital = recode(hospital,
                      # OLD = NEW
                      "Mitylira Hopital"  = "Military Hospital",
                      "Mitylira Hospital" = "Military Hospital",
                      "Military Hopital"  = "Military Hospital",
                      "Port Hopital"      = "Port Hospital",
                      "Central Hopital"   = "Central Hospital",
                      "other"             = "Other",
                      "St. Marks Maternity Hopital (SMMH)" = "St. Mark's Maternity Hospital (SMMH)"
                      )) %>% 
    
    mutate(hospital = replace_na(hospital, "Missing")) %>% 

    # create age_years column (from age and age_unit)
    mutate(age_years = case_when(
          age_unit == "years" ~ age,
          age_unit == "months" ~ age/12,
          is.na(age_unit) ~ age,
          TRUE ~ NA_real_))
```






<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Numeric categories {#num_cats}


Here we describe some special approaches for creating categories from numerical columns. Common examples include age categories, groups of lab values, etc. Here we will discuss:  

* `age_categories()`, from the **epikit** package  
* `cut()`, from **base** R  
* `case_when()`  
* quantile breaks with `quantile()` and `ntile()` 


### Review distribution {.unnumbered}

For this example we will create an `age_cat` column using the `age_years` column.  

```{r}
#check the class of the linelist variable age
class(linelist$age_years)
```

First, examine the distribution of your data, to make appropriate cut-points. See the page on [ggplot basics].  

```{r, out.height='50%'}
# examine the distribution
hist(linelist$age_years)
```

```{r}
summary(linelist$age_years, na.rm=T)
```

<span style="color: orange;">**_CAUTION:_** Sometimes, numeric variables will import as class "character". This occurs if there are non-numeric characters in some of the values, for example an entry of "2 months" for age, or (depending on your R locale settings) if a comma is used in the decimals place (e.g. "4,5" to mean four and one half years)..</span>


<!-- ======================================================= -->
### `age_categories()` {.unnumbered}

With the **epikit** package, you can use the `age_categories()` function to easily categorize and label numeric columns (note: this function can be applied to non-age numeric variables too). As a bonum, the output column is automatically an ordered factor.  

Here are the required inputs:  

* A numeric vector (column)  
* The `breakers = ` argument - provide a numeric vector of break points for the new groups  

First, the simplest example:  

```{r}
# Simple example
################
pacman::p_load(epikit)                    # load package

linelist <- linelist %>% 
  mutate(
    age_cat = age_categories(             # create new column
      age_years,                            # numeric column to make groups from
      breakers = c(0, 5, 10, 15, 20,        # break points
                   30, 40, 50, 60, 70)))

# show table
table(linelist$age_cat, useNA = "always")
```

The break values you specify are by default the lower bounds - that is, they are included in the "higher" group / the groups are "open" on the lower/left side. As shown below, you can add 1 to each break value to achieve groups that are open at the top/right.
 
```{r}
# Include upper ends for the same categories
############################################
linelist <- linelist %>% 
  mutate(
    age_cat = age_categories(
      age_years, 
      breakers = c(0, 6, 11, 16, 21, 31, 41, 51, 61, 71)))

# show table
table(linelist$age_cat, useNA = "always")
```


You can adjust how the labels are displayed with `separator = `. The default is "-"  

You can adjust how the top numbers are handled, with the `ceiling = ` arguemnt. To set an upper cut-off set `ceiling = TRUE`. In this use, the highest break value provided is a "ceiling" and a category "XX+" is not created. Any values above highest break value (or to `upper = `, if defined) are categorized as `NA`. Below is an example with `ceiling = TRUE`, so that there is no category of XX+ and values above 70 (the highest break value) are assigned as NA.  

```{r}
# With ceiling set to TRUE
##########################
linelist <- linelist %>% 
  mutate(
    age_cat = age_categories(
      age_years, 
      breakers = c(0, 5, 10, 15, 20, 30, 40, 50, 60, 70),
      ceiling = TRUE)) # 70 is ceiling, all above become NA

# show table
table(linelist$age_cat, useNA = "always")
```

Alternatively, instead of `breakers = `, you can provide all of `lower = `, `upper = `, and `by = `:  

* `lower = ` The lowest number you want considered - default is 0  
* `upper = ` The highest number you want considered  
* `by = `    The number of years between groups  

```{r}
linelist <- linelist %>% 
  mutate(
    age_cat = age_categories(
      age_years, 
      lower = 0,
      upper = 100,
      by = 10))

# show table
table(linelist$age_cat, useNA = "always")
```


See the function's Help page for more details (enter `?age_categories` in the R console). 


<!-- ======================================================= -->
### `cut()` {.unnumbered}

`cut()` is a **base** R alternative to `age_categories()`, but I think you will see why `age_categories()` was developed to simplify this process. Some notable differences from `age_categories()` are:  

* You do not need to install/load another package  
* You can specify whether groups are open/closed on the right/left  
* You must provide accurate labels yourself  
* If you want 0 included in the lowest group you must specify this  

The basic syntax within `cut()` is to first provide the numeric column to be cut (`age_years`), and then the *breaks* argument, which is a numeric vector `c()` of break points. Using `cut()`, the resulting column is an ordered factor.  

By default, the categorization occurs so that the right/upper side is "open" and inclusive (and the left/lower side is "closed" or exclusive). This is the opposite behavior from the `age_categories()` function. The default labels use the notation "(A, B]", which means A is not included but B is. **Reverse this behavior by providing the `right = TRUE` argument**.   

Thus, by default, "0" values are excluded from the lowest group, and categorized as `NA`! "0" values could be infants coded as age 0 so be careful! To change this, add the argument `include.lowest = TRUE` so that any "0" values will be included in the lowest group. The automatically-generated label for the lowest category will then be "[A],B]". Note that if you include the `include.lowest = TRUE` argument **and** `right = TRUE`, the extreme inclusion will now apply to the *highest* break point value and category, not the lowest.  

You can provide a vector of customized labels using the `labels = ` argument. As these are manually written, be very careful to ensure they are accurate! Check your work using cross-tabulation, as described below. 

An example of `cut()` applied to `age_years` to make the new variable `age_cat` is below:  

```{r}
# Create new variable, by cutting the numeric age variable
# lower break is excluded but upper break is included in each category
linelist <- linelist %>% 
  mutate(
    age_cat = cut(
      age_years,
      breaks = c(0, 5, 10, 15, 20,
                 30, 50, 70, 100),
      include.lowest = TRUE         # include 0 in lowest group
      ))

# tabulate the number of observations per group
table(linelist$age_cat, useNA = "always")
```


**Check your work!!!** Verify that each age value was assigned to the correct category by cross-tabulating the numeric and category columns. Examine assignment of boundary values (e.g. 15, if neighboring categories are 10-15 and 16-20).  

```{r}
# Cross tabulation of the numeric and category columns. 
table("Numeric Values" = linelist$age_years,   # names specified in table for clarity.
      "Categories"     = linelist$age_cat,
      useNA = "always")                        # don't forget to examine NA values
```





**Re-labeling `NA` values**

You may want to assign `NA` values a label such as "Missing". Because the new column is class Factor (restricted values), you cannot simply mutate it with `replace_na()`, as this value will be rejected. Instead, use `fct_explicit_na()` from **forcats** as explained in the [Factors] page.   

```{r}
linelist <- linelist %>% 
  
  # cut() creates age_cat, automatically of class Factor      
  mutate(age_cat = cut(
    age_years,
    breaks = c(0, 5, 10, 15, 20, 30, 50, 70, 100),          
    right = FALSE,
    include.lowest = TRUE,        
    labels = c("0-4", "5-9", "10-14", "15-19", "20-29", "30-49", "50-69", "70-100")),
         
    # make missing values explicit
    age_cat = fct_explicit_na(
      age_cat,
      na_level = "Missing age")  # you can specify the label
  )    

# table to view counts
table(linelist$age_cat, useNA = "always")
```

**Quickly make breaks and labels**  

For a fast way to make breaks and label vectors, use something like below. See the [R basics] page for references on `seq()` and `rep()`.  

```{r, eval=F}
# Make break points from 0 to 90 by 5
age_seq = seq(from = 0, to = 90, by = 5)
age_seq

# Make labels for the above categories, assuming default cut() settings
age_labels = paste0(age_seq + 1, "-", age_seq + 5)
age_labels

# check that both vectors are the same length
length(age_seq) == length(age_labels)
```


Read more about `cut()` in its Help page by entering `?cut` in the R console.  




### Quantile breaks {.unnumbered}  

In common understanding, "quantiles" or "percentiles" typically refer to a value below which a proportion of values fall. For example, the 95th percentile of ages in `linelist` would be the age below which 95% of the age fall.  

However in common speech, "quartiles" and "deciles" can also refer to the *groups of data* as equally divided into 4, or 10 groups (note there will be one more break point than group).    

To get quantile break points, you can use `quantile()` from the **stats** package from **base** R. You provide a numeric vector (e.g. a column in a dataset) and vector of numeric probability values ranging from 0 to 1.0. The break points are returned as a numeric vector. Explore the details of the statistical methodologies by entering `?quantile`.  

* If your input numeric vector has any missing values it is best to set `na.rm = TRUE`  
* Set `names = FALSE` to get an un-named numeric vector  

```{r}
quantile(linelist$age_years,               # specify numeric vector to work on
  probs = c(0, .25, .50, .75, .90, .95),   # specify the percentiles you want
  na.rm = TRUE)                            # ignore missing values 
```

You can use the results of `quantile()` as break points in `age_categories()` or `cut()`. Below we create a new column `deciles` using `cut()` where the breaks are defined using `quantiles()` on `age_years`. Below, we display the results using `tabyl()` from **janitor** so you can see the percentages (see the [Descriptive tables] page). Note how they are not exactly 10% in each group.  

```{r}
linelist %>%                                # begin with linelist
  mutate(deciles = cut(age_years,           # create new column decile as cut() on column age_years
    breaks = quantile(                      # define cut breaks using quantile()
      age_years,                               # operate on age_years
      probs = seq(0, 1, by = 0.1),             # 0.0 to 1.0 by 0.1
      na.rm = TRUE),                           # ignore missing values
    include.lowest = TRUE)) %>%             # for cut() include age 0
  janitor::tabyl(deciles)                   # pipe to table to display
```

### Evenly-sized groups {.unnumbered}  

Another tool to make numeric groups is the the **dplyr** function `ntile()`, which attempts to break your data into n *evenly-sized groups* - *but be aware that unlike with `quantile()` the same value could appear in more than one group.* Provide the numeric vector and then the number of groups. The values in the new column created is just group "numbers" (e.g. 1 to 10), not the range of values themselves as when using `cut()`.  

```{r}
# make groups with ntile()
ntile_data <- linelist %>% 
  mutate(even_groups = ntile(age_years, 10))

# make table of counts and proportions by group
ntile_table <- ntile_data %>% 
  janitor::tabyl(even_groups)
  
# attach min/max values to demonstrate ranges
ntile_ranges <- ntile_data %>% 
  group_by(even_groups) %>% 
  summarise(
    min = min(age_years, na.rm=T),
    max = max(age_years, na.rm=T)
  )

# combine and print - note that values are present in multiple groups
left_join(ntile_table, ntile_ranges, by = "even_groups")
```


<!-- ======================================================= -->
### `case_when()` { .unnumbered}

It is possible to use the **dplyr** function `case_when()` to create categories from a numeric column, but it is easier to use `age_categories()` from **epikit** or `cut()` because these will create an ordered factor automatically. 

If using `case_when()`, please review the proper use as described earlier in the Re-code values section of this page. Also be aware that all right-hand side values must be of the same class. Thus, if you want `NA` on the right-side you should either write "Missing" or use the special `NA` value `NA_character_`.  


### Add to pipe chain {.unnumbered}  

Below, code to create two categorical age columns is added to the cleaning pipe chain:  

```{r}
# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)
##################################################################################

# begin cleaning pipe chain
###########################
linelist <- linelist_raw %>%
    
    # standardize column name syntax
    janitor::clean_names() %>% 
    
    # manually re-name columns
           # NEW name             # OLD name
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome) %>% 
    
    # remove column
    select(-c(row_num, merged_header, x28)) %>% 
  
    # de-duplicate
    distinct() %>% 

    # add column
    mutate(bmi = wt_kg / (ht_cm/100)^2) %>%     

    # convert class of columns
    mutate(across(contains("date"), as.Date), 
           generation = as.numeric(generation),
           age        = as.numeric(age)) %>% 
    
    # add column: delay to hospitalisation
    mutate(days_onset_hosp = as.numeric(date_hospitalisation - date_onset)) %>% 
    
    # clean values of hospital column
    mutate(hospital = recode(hospital,
                      # OLD = NEW
                      "Mitylira Hopital"  = "Military Hospital",
                      "Mitylira Hospital" = "Military Hospital",
                      "Military Hopital"  = "Military Hospital",
                      "Port Hopital"      = "Port Hospital",
                      "Central Hopital"   = "Central Hospital",
                      "other"             = "Other",
                      "St. Marks Maternity Hopital (SMMH)" = "St. Mark's Maternity Hospital (SMMH)"
                      )) %>% 
    
    mutate(hospital = replace_na(hospital, "Missing")) %>% 

    # create age_years column (from age and age_unit)
    mutate(age_years = case_when(
          age_unit == "years" ~ age,
          age_unit == "months" ~ age/12,
          is.na(age_unit) ~ age,
          TRUE ~ NA_real_)) %>% 
  
    # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED
    ###################################################   
    mutate(
          # age categories: custom
          age_cat = epikit::age_categories(age_years, breakers = c(0, 5, 10, 15, 20, 30, 50, 70)),
        
          # age categories: 0 to 85 by 5s
          age_cat5 = epikit::age_categories(age_years, breakers = seq(0, 85, 5)))
```








<!-- ======================================================= -->
## Add rows  

### One-by-one {.unnumbered}  

Adding rows one-by-one manually is tedious but can be done with `add_row()` from **dplyr**. Remember that each column must contain values of only one class (either character, numeric, logical, etc.). So adding a row requires nuance to maintain this. 

```{r, eval=F}
linelist <- linelist %>% 
  add_row(row_num = 666,
          case_id = "abc",
          generation = 4,
          `infection date` = as.Date("2020-10-10"),
          .before = 2)
```

Use `.before` and `.after.` to specify the placement of the row you want to add. `.before = 3` will put the new row before the current 3rd row. The default behavior is to add the row to the end. Columns not specified will be left empty (`NA`).  

The new *row number* may look strange ("...23") but the row numbers in the pre-existing rows *have* changed. So if using the command twice, examine/test the insertion carefully.

If a class you provide is off you will see an error like this:  

```
Error: Can't combine ..1$infection date <date> and ..2$infection date <character>.
```

(when inserting a row with a date value, remember to wrap the date in the function `as.Date()` like `as.Date("2020-10-10")`).


### Bind rows {.unnumbered}  

To combine datasets together by binding the rows of one dataframe to the bottom of another data frame, you can use `bind_rows()` from **dplyr**. This is explained in more detail in the page [Joining data].  




<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Filter rows {  }


A typical cleaning step after you have cleaned the columns and re-coded values is to *filter* the data frame for specific rows using the **dplyr** verb `filter()`.  

Within `filter()`, specify the logic that must be `TRUE` for a row in the dataset to be kept. Below we show how to filter rows based on simple and complex logical conditions.  



<!-- ======================================================= -->
### Simple filter {.unnumbered} 

This simple example re-defines the dataframe `linelist` as itself, having filtered the rows to meet a logical condition. **Only the rows where the logical statement within the parentheses evaluates to `TRUE` are kept.**  

In this example, the logical statement is `gender == "f"`, which is asking whether the value in the column `gender` is equal to "f" (case sensitive).   

Before the filter is applied, the number of rows in `linelist` is ` nrow(linelist)`.

```{r, eval=F}
linelist <- linelist %>% 
  filter(gender == "f")   # keep only rows where gender is equal to "f"
```

After the filter is applied, the number of rows in `linelist` is ` linelist %>% filter(gender == "f") %>% nrow()`.


### Filter out missing values {.unnumbered}  

It is fairly common to want to filter out rows that have missing values. Resist the urge to write `filter(!is.na(column) & !is.na(column))` and instead use the **tidyr** function that is custom-built for this purpose: `drop_na()`. If run with empty parentheses, it removes rows with *any* missing values. Alternatively, you can provide names of specific columns to be evaluated for missingness, or use the "tidyselect" helper functions described [above](#clean_tidyselect).  

```{r, eval=F}
linelist %>% 
  drop_na(case_id, age_years)  # drop rows with missing values for case_id or age_years
```

See the page on [Missing data] for many techniques to analyse and manage missingness in your data. 




### Filter by row number {.unnumbered}  

In a data frame or tibble, each row will usually have a "row number" that (when seen in R Viewer) appears to the left of the first column. It is not itself a true column in the data, but it can be used in a `filter()` statement.  

To filter based on "row number", you can use the **dplyr** function `row_number()` with open parentheses as part of a logical filtering statement. Often you will use the `%in%` operator and a range of numbers as part of that logical statement, as shown below. To see the *first* N rows, you can also use the special **dplyr** function `head()`.   

```{r, eval=F}
# View first 100 rows
linelist %>% head(100)     # or use tail() to see the n last rows

# Show row 5 only
linelist %>% filter(row_number() == 5)

# View rows 2 through 20, and three specific columns
linelist %>% filter(row_number() %in% 2:20) %>% select(date_onset, outcome, age)
```

You can also convert the row numbers to a true column by piping your data frame to the **tibble** function `rownames_to_column()` (do not put anything in the parentheses).  


<!-- ======================================================= -->
### Complex filter {.unnumbered} 

More complex logical statements can be constructed using parentheses `( )`, OR `|`, negate `!`, `%in%`, and AND `&` operators. An example is below:  


Note: You can use the `!` operator in front of a logical criteria to negate it. For example, `!is.na(column)` evaluates to true if the column value is *not* missing. Likewise `!column %in% c("a", "b", "c")` evaluates to true if the column value is *not* in the vector.  


#### Examine the data  {.unnumbered}  

Below is a simple one-line command to create a histogram of onset dates. See that a second smaller outbreak from 2012-2013 is also included in this raw dataset. **For our analyses, we want to remove entries from this earlier outbreak.**  

```{r, out.width = "50%"}
hist(linelist$date_onset, breaks = 50)
```


#### How filters handle missing numeric and date values {.unnumbered}  

Can we just filter by `date_onset` to rows after June 2013? **Caution! Applying the code `filter(date_onset > as.Date("2013-06-01")))` would remove any rows in the later epidemic with a missing date of onset!**  

<span style="color: red;">**_DANGER:_** Filtering to greater than (>) or less than (<) a date or number can remove any rows with missing values (`NA`)! This is because `NA` is treated as infinitely large and small.</span>

*(See the page on [Working with dates] for more information on working with dates and the package **lubridate**)*

#### Design the filter {.unnumbered}  

Examine a cross-tabulation to make sure we exclude only the correct rows:  


```{r}
table(Hospital  = linelist$hospital,                     # hospital name
      YearOnset = lubridate::year(linelist$date_onset),  # year of date_onset
      useNA     = "always")                              # show missing values
```

What other criteria can we filter on to remove the first outbreak (in 2012 & 2013) from the dataset? We see that:  

* The first epidemic  in 2012 & 2013 occurred at Hospital A, Hospital B, and that there were also 10 cases at Port Hospital.  
* Hospitals A & B did *not* have cases in the second epidemic, but Port Hospital did.  

We want to exclude:  

* The ` nrow(linelist %>% filter(hospital %in% c("Hospital A", "Hospital B") | date_onset < as.Date("2013-06-01")))` rows with onset in 2012 and 2013 at either hospital A, B, or Port:  
  * Exclude ` nrow(linelist %>% filter(date_onset < as.Date("2013-06-01")))` rows with onset in 2012 and 2013
  * Exclude ` nrow(linelist %>% filter(hospital %in% c('Hospital A', 'Hospital B') & is.na(date_onset)))` rows from Hospitals A & B with missing onset dates  
  * Do **not** exclude ` nrow(linelist %>% filter(!hospital %in% c('Hospital A', 'Hospital B') & is.na(date_onset)))` other rows with missing onset dates.  

We start with a linelist of ` `nrow(linelist)`. Here is our filter statement:  

```{r}
linelist <- linelist %>% 
  # keep rows where onset is after 1 June 2013 OR where onset is missing and it was a hospital OTHER than Hospital A or B
  filter(date_onset > as.Date("2013-06-01") | (is.na(date_onset) & !hospital %in% c("Hospital A", "Hospital B")))

nrow(linelist)
```

When we re-make the cross-tabulation, we see that Hospitals A & B are removed completely, and the 10 Port Hospital cases from 2012 & 2013 are removed, and all other values are the same - just as we wanted.  
 
```{r}
table(Hospital  = linelist$hospital,                     # hospital name
      YearOnset = lubridate::year(linelist$date_onset),  # year of date_onset
      useNA     = "always")                              # show missing values
```

Multiple statements can be included within one filter command (separated by commas), or you can always pipe to a separate filter() command for clarity.  


*Note: some readers may notice that it would be easier to just filter by `date_hospitalisation` because it is 100% complete with no missing values. This is true. But `date_onset` is used for purposes of demonstrating a complex filter.* 




### Standalone {.unnumbered}  

Filtering can also be done as a stand-alone command (not part of a pipe chain). Like other **dplyr** verbs, in this case the first argument must be the dataset itself.  

```{r, eval=F}
# dataframe <- filter(dataframe, condition(s) for rows to keep)

linelist <- filter(linelist, !is.na(case_id))
```

You can also use **base** R to subset using square brackets which reflect the [rows, columns] that you want to retain.  

```{r, eval=F}
# dataframe <- dataframe[row conditions, column conditions] (blank means keep all)

linelist <- linelist[!is.na(case_id), ]
```





### Quickly review records {.unnumbered} 

Often you want to quickly review a few records, for only a few columns. The **base** R function `View()` will print a data frame for viewing in your RStudio. 

View the linelist in RStudio:  

```{r, eval=F}
View(linelist)
```

Here are two examples of viewing specific cells (specific rows, and specific columns):  


**With dplyr functions `filter()` and `select()`:**  

Within `View()`, pipe the dataset to `filter()` to keep certain rows, and then to `select()` to keep certain columns. For example, to review onset and hospitalization dates of 3 specific cases:   

```{r, eval=F}
View(linelist %>%
       filter(case_id %in% c("11f8ea", "76b97a", "47a5f5")) %>%
       select(date_onset, date_hospitalisation))
```


You can achieve the same with **base** R syntax, using brackets `[ ]` to subset you want to see. 

```{r, eval=F}
View(linelist[linelist$case_id %in% c("11f8ea", "76b97a", "47a5f5"), c("date_onset", "date_hospitalisation")])
```





#### Add to pipe chain {.unnumbered}  


```{r}
# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)
##################################################################################

# begin cleaning pipe chain
###########################
linelist <- linelist_raw %>%
    
    # standardize column name syntax
    janitor::clean_names() %>% 
    
    # manually re-name columns
           # NEW name             # OLD name
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome) %>% 
    
    # remove column
    select(-c(row_num, merged_header, x28)) %>% 
  
    # de-duplicate
    distinct() %>% 

    # add column
    mutate(bmi = wt_kg / (ht_cm/100)^2) %>%     

    # convert class of columns
    mutate(across(contains("date"), as.Date), 
           generation = as.numeric(generation),
           age        = as.numeric(age)) %>% 
    
    # add column: delay to hospitalisation
    mutate(days_onset_hosp = as.numeric(date_hospitalisation - date_onset)) %>% 
    
    # clean values of hospital column
    mutate(hospital = recode(hospital,
                      # OLD = NEW
                      "Mitylira Hopital"  = "Military Hospital",
                      "Mitylira Hospital" = "Military Hospital",
                      "Military Hopital"  = "Military Hospital",
                      "Port Hopital"      = "Port Hospital",
                      "Central Hopital"   = "Central Hospital",
                      "other"             = "Other",
                      "St. Marks Maternity Hopital (SMMH)" = "St. Mark's Maternity Hospital (SMMH)"
                      )) %>% 
    
    mutate(hospital = replace_na(hospital, "Missing")) %>% 

    # create age_years column (from age and age_unit)
    mutate(age_years = case_when(
          age_unit == "years" ~ age,
          age_unit == "months" ~ age/12,
          is.na(age_unit) ~ age,
          TRUE ~ NA_real_)) %>% 
  
    mutate(
          # age categories: custom
          age_cat = epikit::age_categories(age_years, breakers = c(0, 5, 10, 15, 20, 30, 50, 70)),
        
          # age categories: 0 to 85 by 5s
          age_cat5 = epikit::age_categories(age_years, breakers = seq(0, 85, 5))) %>% 
    
    # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED
    ###################################################
    filter(
          # keep only rows where case_id is not missing
          !is.na(case_id),  
          
          # also filter to keep only the second outbreak
          date_onset > as.Date("2013-06-01") | (is.na(date_onset) & !hospital %in% c("Hospital A", "Hospital B")))
```







<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Row-wise calculations  

If you want to perform a calculation within a row, you can use `rowwise()` from **dplyr**. See this online vignette on [row-wise calculations](https://cran.r-project.org/web/packages/dplyr/vignettes/rowwise.html).  
For example, this code applies `rowwise()` and then creates a new column that sums the number of the specified symptom columns that have value "yes", for each row in the linelist. The columns are specified within `sum()` by name within a vector `c()`. `rowwise()` is essentially a special kind of `group_by()`, so it is best to use `ungroup()` when you are done (page on [Grouping data]).  

```{r,}
linelist %>%
  rowwise() %>%
  mutate(num_symptoms = sum(c(fever, chills, cough, aches, vomit) == "yes")) %>% 
  ungroup() %>% 
  select(fever, chills, cough, aches, vomit, num_symptoms) # for display
```

  
As you specify the column to evaluate, you may want to use the  "tidyselect" helper functions described in the `select()` section of this page. You just have to make one adjustment (because you are not using them within a **dplyr** function like `select()` or `summarise()`).  

Put the column-specification criteria within the **dplyr** function `c_across()`. This is because `c_across` ([documentation](https://dplyr.tidyverse.org/reference/c_across.html)) is designed to work with `rowwise()` specifically. For example, the following code:  

* Applies `rowwise()` so the following operation (`sum()`) is applied within each row (not summing entire columns)  
* Creates new column `num_NA_dates`, defined for each row as the number of columns (with name containing "date") for which `is.na()` evaluated to TRUE (they are missing data).  
* `ungroup()` to remove the effects of `rowwise()` for subsequent steps  

```{r,}
linelist %>%
  rowwise() %>%
  mutate(num_NA_dates = sum(is.na(c_across(contains("date"))))) %>% 
  ungroup() %>% 
  select(num_NA_dates, contains("date")) # for display
```

You could also provide other functions, such as `max()` to get the latest or most recent date for each row:  

```{r}
linelist %>%
  rowwise() %>%
  mutate(latest_date = max(c_across(contains("date")), na.rm=T)) %>% 
  ungroup() %>% 
  select(latest_date, contains("date"))  # for display
```


## Arrange and sort  

Use the **dplyr** function `arrange()` to sort or order the rows by column values.  

Simple list the columns in the order they should be sorted on. Specify `.by_group = TRUE` if you want the sorting to to first occur by any *groupings* applied to the data (see page on [Grouping data]).  

By default, column will be sorted in "ascending" order (which applies to numeric and also to character columns). You can sort a variable in "descending" order by wrapping it with `desc()`.  

Sorting data with `arrange()` is particularly useful when making [Tables for presentation], using `slice()` to take the "top" rows per group, or setting factor level order by order of appearance.  

For example, to sort the our linelist rows by `hospital`, then by `date_onset` in descending order, we would use:  

```{r, eval=F}
linelist %>% 
   arrange(hospital, desc(date_onset))
```


```{r, echo=F}
# HIDDEN
#
# convert one remaining old outbreak row to missing for ease
linelist <- linelist %>% 
  mutate(
    date_hospitalisation = case_when(
      date_hospitalisation < as.Date("2013-01-01") ~ as.Date(NA),
      TRUE                                         ~ date_hospitalisation),
    date_outcome = case_when(
      date_outcome < as.Date("2013-01-01") ~ as.Date(NA),
      TRUE                                 ~ date_outcome)
    )

#min(linelist$date_hospitalisation, na.rm=T)
#min(linelist$date_outcome, na.rm=T)
```



```{r echo=F}
# REARRANGE COLUMNS FOR EXPORT
linelist <- linelist %>% 
  select(case_id:gender, age, age_unit, age_years, age_cat, age_cat5, everything())
```

```{r echo=F}
# EXPORT CLEANED LINELIST FILE TO "DATA" FOLDER
rio::export(linelist, here::here("data", "case_linelists", "linelist_cleaned.xlsx"))
rio::export(linelist, here::here("data", "case_linelists", "linelist_cleaned.rds"))
```
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/cleaning.Rmd-->


# Làm việc với ngày tháng {#dates}


```{r, out.width=c('50%'), fig.align='center', echo=F, message=F}
knitr::include_graphics(here::here("images", "Dates_500x500.png"))
```


Working with dates in R requires more attention than working with other object classes. Below, we offer some tools and example to make this process less painful. Luckily, dates can be wrangled easily with practice, and with a set of helpful packages such as **lubridate**.  

Upon import of raw data, R often interprets dates as character objects - this means they cannot be used for general date operations such as making time series and calculating time intervals. To make matters more difficult, there are many ways a date can be formatted and you must help R know which part of a date represents what (month, day, hour, etc.). 

Dates in R are their own class of object - the `Date` class. It should be noted that there is also a class that stores objects with date *and* time. Date time objects are formally referred to as `POSIXt`, `POSIXct`, and/or `POSIXlt` classes (the difference isn't important). These objects are informally referred to as *datetime* classes.

* It is important to make R recognize when a column contains dates.  
* Dates are an object class and can be tricky to work with.  
* Here we present several ways to convert date columns to Date class.  


<!-- ======================================================= -->
## Preparation

### Load packages {.unnumbered}  

This code chunk shows the loading of packages required for this page. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r dates_packages, warning=F, message=F}
# Checks if package is installed, installs if necessary, and loads package for current session

pacman::p_load(
  lubridate,  # general package for handling and converting dates  
  linelist,   # has function to "guess" messy dates
  aweek,      # another option for converting dates to weeks, and weeks to dates
  zoo,        # additional date/time functions
  tidyverse,  # data management and visualization  
  rio)        # data import/export
```

### Import data {.unnumbered}  

We import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow along step-by-step, see instruction in the [Download handbook and data] page. We assume the file is in the working directory so no sub-folders are specified in this file path.  

```{r,  echo=F}
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

```

```{r, eval=F}
linelist <- import("linelist_cleaned.xlsx")

```



<!-- ======================================================= -->
## Current date  

You can get the current "system" date or system datetime of your computer by doing the following with **base** R.  

```{r}
# get the system date - this is a DATE class
Sys.Date()

# get the system time - this is a DATETIME class
Sys.time()
```


With the **lubridate** package these can also be returned with `today()` and `now()`, respectively. `date()` returns the current date and time with weekday and month names.  
  
  

<!-- ======================================================= -->
## Convert to Date  

After importing a dataset into R, date column values may look like "1989/12/30", "05/06/2014", or "13 Jan 2020". In these cases, R is likely still treating these values as Character values. R must be *told* that these values are dates... and what the format of the date is (which part is Day, which is Month, which is Year, etc).  

Once told, R converts these values to class Date. In the background, R will store the dates as numbers (the number of days from its "origin" date 1 Jan 1970). You will not interface with the date number often, but this allows for R to treat dates as continuous variables and to allow special operations such as calculating the distance between dates.  

By default, values of class Date in R are displayed as YYYY-MM-DD. Later in this section we will discuss how to change the display of date values.  

Below we present two approaches to converting a column from character values to class Date.  


<span style="color: darkgreen;">**_TIP:_** You can check the current class of a column with **base** R function `class()`, like `class(linelist$date_onset)`.</span>  

  

### **base** R {.unnumbered}  

`as.Date()` is the standard, **base** R function to convert an object or column to class Date (note capitalization of "D").  

Use of `as.Date()` requires that:  

* You *specify the **existing** format of the raw character date* or the origin date if supplying dates as numbers (see section on Excel dates)  
* If used on a character column, all date values must have the same exact format (if this is not the case, try `guess_dates()` from the **linelist** package)  

**First**, check the class of your column with `class()` from **base** R. If you are unsure or confused about the class of your data (e.g. you see "POSIXct", etc.) it can be easiest to first convert the column to class Character with `as.character()`, and then convert it to class Date.  

**Second**, within the `as.Date()` function, use the `format =` argument to tell R the *current* format of the character date components - which characters refer to the month, the day, and the year, and how they are separated. If your values are already in one of R's standard date formats ("YYYY-MM-DD" or "YYYY/MM/DD") the `format =` argument is not necessary.  

To `format = `, provide a character string (in quotes) that represents the *current* date format using the special "strptime" abbreviations below. For example, if your character dates are currently in the format "DD/MM/YYYY", like "24/04/1968", then you would use `format = "%d/%m/%Y"` to convert the values into dates. **Putting the format in quotation marks is necessary. And don't forget any slashes or dashes!**  

```{r eval=F}
# Convert to class date
linelist <- linelist %>% 
  mutate(date_onset = as.Date(date_of_onset, format = "%d/%m/%Y"))
```

Most of the strptime abbreviations are listed below. You can see the complete list by running `?strptime`.  

%d = Day number of month (5, 17, 28, etc.)  
%j = Day number of the year (Julian day 001-366)  
%a = Abbreviated weekday (Mon, Tue, Wed, etc.)  
%A = Full weekday (Monday, Tuesday, etc.)
%w = Weekday number (0-6, Sunday is 0)  
%u = Weekday number (1-7, Monday is 1)  
%W = Week number (00-53, Monday is week start)  
%U = Week number (01-53, Sunday is week start)  
%m = Month number (e.g. 01, 02, 03, 04)  
%b = Abbreviated month (Jan, Feb, etc.)  
%B = Full month (January, February, etc.)  
%y = 2-digit year  (e.g. 89)  
%Y = 4-digit year  (e.g. 1989)  
%h = hours (24-hr clock)  
%m = minutes  
%s = seconds
%z = offset from GMT  
%Z = Time zone (character)  

<span style="color: darkgreen;">**_TIP:_** The `format =` argument of `as.Date()` is *not* telling R the format you want the dates to be, but rather how to identify the date parts as they are *before* you run the command.</span>  

<span style="color: darkgreen;">**_TIP:_** Be sure that in the `format =` argument you use the *date-part separator* (e.g. /, -, or space) that is present in your dates.</span>  

Once the values are in class Date, R will by default display them in the standard format, which is YYYY-MM-DD.



### **lubridate** {.unnumbered}  

Converting character objects to dates can be made easier by using the **lubridate** package. This is a **tidyverse** package designed to make working with dates and times more simple and consistent than in **base** R. For these reasons, **lubridate** is often considered the gold-standard package for dates and time, and is recommended whenever working with them.

The **lubridate** package provides several different helper functions designed to convert character objects to dates in an intuitive, and more lenient way than specifying the format in `as.Date()`. These functions are specific to the rough date format, but allow for a variety of separators, and synonyms for dates (e.g. 01 vs Jan vs January) - they are named after abbreviations of date formats. 


```{r, }
# install/load lubridate 
pacman::p_load(lubridate)
```

The `ymd()` function flexibly converts date values supplied as **year, then month, then day**.  

```{r}
# read date in year-month-day format
ymd("2020-10-11")
ymd("20201011")
```

The `mdy()` function flexibly converts date values supplied as **month, then day, then year**.  

```{r}
# read date in month-day-year format
mdy("10/11/2020")
mdy("Oct 11 20")
```

The `dmy()` function flexibly converts date values supplied as **day, then month, then year**.  

```{r}
# read date in day-month-year format
dmy("11 10 2020")
dmy("11 October 2020")
```

<!-- The `as.character()` and `as.Date()` commands can optionally be combined as:   -->

<!-- ```{r eval=F} -->
<!-- linelist_cleaned$date_of_onset <- as.Date(as.character(linelist_cleaned$date_of_onset), format = "%d/%m/%Y") -->
<!-- ``` -->

If using piping, the conversion of a character column to dates with **lubridate** might look like this:  

```{r, eval=F}
linelist <- linelist %>%
  mutate(date_onset = lubridate::dmy(date_onset))
```

Once complete, you can run `class()` to verify the class of the column  

```{r, eval=F}
# Check the class of the column
class(linelist$date_onset)  
```


Once the values are in class Date, R will by default display them in the standard format, which is YYYY-MM-DD.  

Note that the above functions work best with 4-digit years. 2-digit years can produce unexpected results, as lubridate attempts to guess the century.  

To convert a 2-digit year into a 4-digit year (all in the same century) you can convert to class character and then combine the existing digits with a pre-fix using `str_glue()` from the **stringr** package (see [Characters and strings]). Then convert to date.  

```{r}
two_digit_years <- c("15", "15", "16", "17")
str_glue("20{two_digit_years}")
```



### Combine columns {.unnumbered}  

You can use the **lubridate** functions `make_date()` and `make_datetime()` to combine multiple numeric columns into one date column. For example if you have numeric columns `onset_day`, `onset_month`, and `onset_year` in the data frame `linelist`:  

```{r, eval=F}
linelist <- linelist %>% 
  mutate(onset_date = make_date(year = onset_year, month = onset_month, day = onset_day))
```




<!-- ======================================================= -->
## Excel dates

In the background, most software store dates as numbers. R stores dates from an origin of 1st January, 1970. Thus, if you run `as.numeric(as.Date("1970-01-01))` you will get `0`. 

Microsoft Excel stores dates with an origin of either December 30, 1899 (Windows) or January 1, 1904 (Mac), depending on your operating system. See this [Microsoft guidance](https://docs.microsoft.com/en-us/office/troubleshoot/excel/1900-and-1904-date-system) for more information.  

Excel dates often import into R as these numeric values instead of as characters. If the dataset you imported from Excel shows dates as numbers or characters like "41369"... use `as.Date()` (or **lubridate**'s `as_date()` function) to convert, but **instead of supplying a "format" as above, supply the Excel origin date** to the argument `origin = `.  

This will not work if the Excel date is stored in R as a character type, so be sure to ensure the number is class Numeric!

<span style="color: black;">**_NOTE:_** You should provide the origin date in R's default date format ("YYYY-MM-DD").</span>

```{r, eval = FALSE}
# An example of providing the Excel 'origin date' when converting Excel number dates
data_cleaned <- data %>% 
  mutate(date_onset = as.numeric(date_onset)) %>%   # ensure class is numeric
  mutate(date_onset = as.Date(date_onset, origin = "1899-12-30")) # convert to date using Excel origin
```



<!-- ======================================================= -->
## Messy dates  

The function `guess_dates()` from the **linelist** package attempts to read a "messy" date column containing dates in many different formats and convert the dates to a standard format. You can [read more online about `guess_dates()`](https://www.repidemicsconsortium.org/linelist/reference/guess_dates.html). If `guess_dates()` is not yet available on CRAN for R 4.0.2, try install via `pacman::p_load_gh("reconhub/linelist")`.

For example `guess_dates` would see a vector of the following character dates "03 Jan 2018", "07/03/1982", and "08/20/85" and convert them to class Date as: `2018-01-03`, `1982-03-07`, and `1985-08-20`.  

```{r, }
linelist::guess_dates(c("03 Jan 2018",
                        "07/03/1982",
                        "08/20/85"))
```

Some optional arguments for `guess_dates()` that you might include are:  

* `error_tolerance` - The proportion of entries which cannot be identified as dates to be tolerated (defaults to 0.1 or 10%)
* `last_date` - the last valid date (defaults to current date)  
* `first_date` - the first valid date. Defaults to fifty years before the last_date.


```{r eval = FALSE}
# An example using guess_dates on the column dater_onset
linelist <- linelist %>%                 # the dataset is called linelist
  mutate(
    date_onset = linelist::guess_dates(  # the guess_dates() from package "linelist"
      date_onset,
      error_tolerance = 0.1,
      first_date = "2016-01-01"
    )
```




<!-- ======================================================= -->
## Working with date-time class  

As previously mentioned, R also supports a `datetime` class - a column that contains date **and** time information. As with the `Date` class, these often need to be converted from `character` objects to `datetime` objects. 

### Convert dates with times {.unnumbered}  

A standard `datetime` object is formatted with the date first, which is followed by a time component - for example  _01 Jan 2020, 16:30_. As with dates, there are many ways this can be formatted, and there are numerous levels of precision (hours, minutes, seconds) that can be supplied.  

Luckily, **lubridate** helper functions also exist to help convert these strings to `datetime` objects. These functions are extensions of the date helper functions, with `_h` (only hours supplied), `_hm` (hours and minutes supplied), or `_hms` (hours, minutes, and seconds supplied) appended to the end (e.g. `dmy_hms()`). These can be used as shown:

Convert datetime with only hours to datetime object  

```{r}
ymd_h("2020-01-01 16hrs")
ymd_h("2020-01-01 4PM")
```

Convert datetime with hours and minutes to datetime object  

```{r}
dmy_hm("01 January 2020 16:20")
```

Convert datetime with hours, minutes, and seconds to datetime object  

```{r}
mdy_hms("01 January 2020, 16:20:40")
```

You can supply time zone but it is ignored. See section later in this page on time zones.  

```{r}
mdy_hms("01 January 2020, 16:20:40 PST")

```

When working with a data frame, time and date columns can be combined to create a datetime column using `str_glue()` from **stringr** package and an appropriate **lubridate** function. See the page on [Characters and strings] for details on **stringr**.  

In this example, the `linelist` data frame has a column in format "hours:minutes". To convert this to a datetime we follow a few steps:  

1) Create a "clean" time of admission column with missing values filled-in with the column median. We do this because **lubridate** won't operate on missing values. Combine it with the column `date_hospitalisation`, and then use the function `ymd_hm()` to convert.  

```{r, eval = FALSE}
# packages
pacman::p_load(tidyverse, lubridate, stringr)

# time_admission is a column in hours:minutes
linelist <- linelist %>%
  
  # when time of admission is not given, assign the median admission time
  mutate(
    time_admission_clean = ifelse(
      is.na(time_admission),         # if time is missing
      median(time_admission),        # assign the median
      time_admission                 # if not missing keep as is
  ) %>%
  
    # use str_glue() to combine date and time columns to create one character column
    # and then use ymd_hm() to convert it to datetime
  mutate(
    date_time_of_admission = str_glue("{date_hospitalisation} {time_admission_clean}") %>% 
      ymd_hm()
  )

```

### Convert times alone {.unnumbered}  

If your data contain only a character time (hours and minutes), you can convert and manipulate them as times using `strptime()` from **base** R. For example, to get the difference between two of these times:  

```{r}
# raw character times
time1 <- "13:45" 
time2 <- "15:20"

# Times converted to a datetime class
time1_clean <- strptime(time1, format = "%H:%M")
time2_clean <- strptime(time2, format = "%H:%M")

# Difference is of class "difftime" by default, here converted to numeric hours 
as.numeric(time2_clean - time1_clean)   # difference in hours

```

Note however that without a date value provided, it assumes the date is today. To combine a string date and a string time together see how to use **stringr** in the section just above. Read more about `strptime()` [here](https://rdrr.io/r/base/strptime.html).  

To convert single-digit numbers to double-digits (e.g. to "pad" hours or minutes with leading zeros to achieve 2 digits), see this ["Pad length" section of the Characters and strings page](#str_pad).  


### Extract time {.unnumbered}  

You can extract elements of a time with `hour()`, `minute()`, or `second()` from **lubridate**.  

Here is an example of extracting the hour, and then classifing by part of the day. We begin with the column `time_admission`, which is class Character in format "HH:MM". First, the `strptime()` is used as described above to convert the characters to datetime class. Then, the hour is extracted with `hour()`, returning a number from 0-24. Finally, a column `time_period` is created using logic with `case_when()` to classify rows into Morning/Afternoon/Evening/Night based on their hour of admission.  

```{r}
linelist <- linelist %>%
  mutate(hour_admit = hour(strptime(time_admission, format = "%H:%M"))) %>%
  mutate(time_period = case_when(
    hour_admit > 06 & hour_admit < 12 ~ "Morning",
    hour_admit >= 12 & hour_admit < 17 ~ "Afternoon",
    hour_admit >= 17 & hour_admit < 21 ~ "Evening",
    hour_admit >=21 | hour_admit <= 6 ~ "Night"))
```

To learn more about `case_when()` see the page on [Cleaning data and core functions].  

<!-- ======================================================= -->
## Working with dates   

`lubridate` can also be used for a variety of other functions, such as **extracting aspects of a date/datetime**, **performing date arithmetic**, or **calculating date intervals**

Here we define a date to use for the examples:  

```{r, }
# create object of class Date
example_date <- ymd("2020-03-01")
```

### Extract date components {.unnumbered}  

You can extract common aspects such as month, day, weekday:  

```{r}
month(example_date)  # month number
day(example_date)    # day (number) of the month
wday(example_date)   # day number of the week (1-7)
```

You can also extract time components from a `datetime` object or column. This can be useful if you want to view the distribution of admission times.  

```{r, eval=F}
example_datetime <- ymd_hm("2020-03-01 14:45")

hour(example_datetime)     # extract hour
minute(example_datetime)   # extract minute
second(example_datetime)   # extract second
```

There are several options to retrieve weeks. See the section on Epidemiological weeks below.  

Note that if you are seeking to *display* a date a certain way (e.g. "Jan 2020" or "Thursday 20 March" or "Week 20, 1977") you can do this more flexibly as described in the section on Date display.  


### Date math {.unnumbered}  

You can add certain numbers of days or weeks using their respective function from **lubridate**.  

```{r}
# add 3 days to this date
example_date + days(3)
  
# add 7 weeks and subtract two days from this date
example_date + weeks(7) - days(2)
```

### Date intervals {.unnumbered}  

The difference between dates can be calculated by:  

1. Ensure both dates are of class date  
2. Use subtraction to return the "difftime" difference between the two dates  
3. If necessary, convert the result to numeric class to perform subsequent mathematical calculations  

Below the interval between two dates is calculated and displayed. You can find intervals by using the subtraction "minus" symbol on values that are class Date. Note, however that the class of the returned value is "difftime" as displayed below, and must be converted to numeric. 

```{r}
# find the interval between this date and Feb 20 2020 
output <- example_date - ymd("2020-02-20")
output    # print
class(output)
```

To do subsequent operations on a "difftime", convert it to numeric with `as.numeric()`. 

This can all be brought together to work with data - for example:

```{r, eval = F}
pacman::p_load(lubridate, tidyverse)   # load packages

linelist <- linelist %>%
  
  # convert date of onset from character to date objects by specifying dmy format
  mutate(date_onset = dmy(date_onset),
         date_hospitalisation = dmy(date_hospitalisation)) %>%
  
  # filter out all cases without onset in march
  filter(month(date_onset) == 3) %>%
    
  # find the difference in days between onset and hospitalisation
  mutate(days_onset_to_hosp = date_hospitalisation - date_of_onset)
```



In a data frame context, if either of the above dates is missing, the operation will fail for that row. This will result in an `NA` instead of a numeric value. When using this column for calculations, be sure to set the `na.rm = ` argument to `TRUE`. For example:

```{r, eval = FALSE}
# calculate the median number of days to hospitalisation for all cases where data are available
median(linelist_delay$days_onset_to_hosp, na.rm = T)
```


<!-- ======================================================= -->
## Date display  

Once dates are the correct class, you often want them to display differently, for example to display as "Monday 05 January" instead of "2018-01-05". You may also want to adjust the display in order to then group rows by the date elements displayed - for example to group by month-year.  

### `format()` {.unnumbered}  

Adjust date display with the **base** R function `format()`. This function accepts a character string (in quotes) specifying the *desired* output format in the "%" strptime abbreviations (the same syntax as used in `as.Date()`). Below are most of the common abbreviations.  

Note: using `format()` will convert the values to class Character, so this is generally used towards the end of an analysis or for display purposes only! You can see the complete list by running `?strptime`.  

%d = Day number of month (5, 17, 28, etc.)  
%j = Day number of the year (Julian day 001-366)  
%a = Abbreviated weekday (Mon, Tue, Wed, etc.)  
%A = Full weekday (Monday, Tuesday, etc.)  
%w = Weekday number (0-6, Sunday is 0)  
%u = Weekday number (1-7, Monday is 1)  
%W = Week number (00-53, Monday is week start)  
%U = Week number (01-53, Sunday is week start)  
%m = Month number (e.g. 01, 02, 03, 04)  
%b = Abbreviated month (Jan, Feb, etc.)  
%B = Full month (January, February, etc.)  
%y = 2-digit year  (e.g. 89)  
%Y = 4-digit year  (e.g. 1989)  
%h = hours (24-hr clock)  
%m = minutes  
%s = seconds  
%z = offset from GMT  
%Z = Time zone (character)

An example of formatting today's date:  

```{r}
# today's date, with formatting
format(Sys.Date(), format = "%d %B %Y")

# easy way to get full date and time (default formatting)
date()

# formatted combined date, time, and time zone using str_glue() function
str_glue("{format(Sys.Date(), format = '%A, %B %d %Y, %z  %Z, ')}{format(Sys.time(), format = '%H:%M:%S')}")

# Using format to display weeks
format(Sys.Date(), "%Y Week %W")
```

Note that if using `str_glue()`, be aware of that within the expected double quotes " you should only use single quotes (as above).  


### Month-Year {.unnumbered}  

To convert a Date column to Month-year format, we suggest you use the function `as.yearmon()` from the **zoo** package. This converts the date to class "yearmon" and retains the proper ordering. In contrast, using `format(column, "%Y %B")` will convert to class Character and will order the values alphabetically (incorrectly). 

Below, a new column `yearmonth` is created from the column `date_onset`, using the `as.yearmon()` function. The default (correct) ordering of the resulting values are shown in the table.  

```{r}
# create new column 
test_zoo <- linelist %>% 
     mutate(yearmonth = zoo::as.yearmon(date_onset))

# print table
table(test_zoo$yearmon)
```

In contrast, you can see how only using `format()` does achieve the desired display format, but not the correct ordering.  

```{r}
# create new column
test_format <- linelist %>% 
     mutate(yearmonth = format(date_onset, "%b %Y"))

# print table
table(test_format$yearmon)
```

Note: if you are working within a `ggplot()` and want to adjust how dates are *displayed* only, it may be sufficient to provide a strptime format to the `date_labels = ` argument in `scale_x_date()` - you can use `"%b %Y"` or `"%Y %b"`. See the [ggplot tips] page.  


**zoo** also offers the function `as.yearqtr()`, and you can use `scale_x_yearmon()` when using `ggplot()`.  



<!-- ======================================================= -->
## Epidemiological weeks {#dates_epi_wks}

### **lubridate** {.unnumbered}  

See the page on [Grouping data] for more extensive examples of grouping data by date. Below we briefly describe grouping data by weeks.  

We generally recommend using the `floor_date()` function from **lubridate**, with the argument `unit = "week"`. This rounds the date down to the "start" of the week, as defined by the argument `week_start = `. The default week start is 1 (for Mondays) but you can specify any day of the week as the start (e.g. 7 for Sundays). `floor_date()` is versitile and can be used to round down to other time units by setting `unit = ` to "second", "minute", "hour", "day", "month", or "year".  

The returned value is the start date of the week, in Date class. Date class is useful when plotting the data, as it will be easily recognized and ordered correctly by `ggplot()`.

If you are only interested in adjusting dates to *display* by week in a plot, see the section in this page on Date display. For example when plotting an epicurve you can format the date display by providing the desired strptime "%" nomenclature. For example, use "%Y-%W" or "%Y-%U" to return the year and week number (given Monday or Sunday week start, respectively).  

### Weekly counts {.unnumbered}  

See the page on [Grouping data] for a thorough explanation of grouping data with `count()`, `group_by()`, and `summarise()`. A brief example is below.  

1) Create a new 'week' column with `mutate()`, using `floor_date()` with `unit = "week"`  
2) Get counts of rows (cases) per week with `count()`; filter out any cases with missing date  
3) Finish with `complete()` from **tidyr** to ensure that *all* weeks appear in the data - even those with no rows/cases. By default the count values for any "new" rows are NA, but you can make them 0 with the `fill = ` argument, which expects a named list (below, `n` is the name of the counts column).  

```{r}
# Make aggregated dataset of weekly case counts
weekly_counts <- linelist %>% 
  drop_na(date_onset) %>%             # remove cases missing onset date
  mutate(weekly_cases = floor_date(   # make new column, week of onset
    date_onset,
    unit = "week")) %>%            
  count(weekly_cases) %>%           # group data by week and count rows per group (creates column 'n')
  tidyr::complete(                  # ensure all weeks are present, even those with no cases reported
    weekly_cases = seq.Date(          # re-define the "weekly_cases" column as a complete sequence,
      from = min(weekly_cases),       # from the minimum date
      to = max(weekly_cases),         # to the maxiumum date
      by = "week"),                   # by weeks
    fill = list(n = 0))             # fill-in NAs in the n counts column with 0
```

Here are the first rows of the resulting data frame:  

```{r message=FALSE, echo=F}
DT::datatable(head(weekly_counts, 20), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


### Epiweek alternatives {.unnumbered}  

Note that **lubridate** also has functions `week()`, `epiweek()`, and `isoweek()`, each of which has slightly different start dates and other nuances. Generally speaking though, `floor_date()` should be all that you need. Read the details for these functions by entering `?week` into the console or reading the documentation [here](https://www.rdocumentation.org/packages/lubridate/versions/1.7.4/topics/week). 


You might consider using the package **aweek** to set epidemiological weeks. You can read more about it [on the RECON website](https://www.repidemicsconsortium.org/aweek/). It has the functions `date2week()` and `week2date()` in which you can set the week start day with `week_start = "Monday"`. This package is easiest if you want "week"-style outputs (e.g. "2020-W12"). Another advantage of **aweek** is that when `date2week()` is applied to a date column, the returned column (week format) is automatically of class Factor and includes levels for all weeks in the time span (this avoids the extra step of `complete()` described above). However, **aweek** does not have the functionality to round dates to other time units such as months, years, etc.  


Another alternative for time series which also works well to show a a "week" format ("2020 W12") is `yearweek()` from the package **tsibble**, as demonstrated in the page on [Time series and outbreak detection].  


<!-- ======================================================= -->
## Converting dates/time zones

When data is present in different time time zones, it can often be important to standardise this data in a unified time zone. This can present a further challenge, as the time zone component of data must be coded manually in most cases.

In R, each *datetime* object has a timezone component. By default, all datetime objects will carry the local time zone for the computer being used - this is generally specific to a *location* rather than a named timezone, as time zones will often change in locations due to daylight savings time. It is not possible to accurately compensate for time zones without a time component of a date, as the event a date column represents cannot be attributed to a specific time, and therefore time shifts measured in hours cannot be reasonably accounted for.

To deal with time zones, there are a number of helper functions in lubridate that can be used to change the time zone of a datetime object from the local time zone to a different time zone. Time zones are set by attributing a valid tz database time zone to the datetime object. A list of these can be found here - if the location you are using data from is not on this list, nearby large cities in the time zone are available and serve the same purpose. 

https://en.wikipedia.org/wiki/List_of_tz_database_time_zones


```{r}
# assign the current time to a column
time_now <- Sys.time()
time_now

# use with_tz() to assign a new timezone to the column, while CHANGING the clock time
time_london_real <- with_tz(time_now, "Europe/London")

# use force_tz() to assign a new timezone to the column, while KEEPING the clock time
time_london_local <- force_tz(time_now, "Europe/London")


# note that as long as the computer that was used to run this code is NOT set to London time,
# there will be a difference in the times 
# (the number of hours difference from the computers time zone to london)
time_london_real - time_london_local

```

This may seem largely abstract, and is often not needed if the user isn't working across time zones.  





<!-- ======================================================= -->
## Lagging and leading calculations  

`lead()` and `lag()` are functions from the **dplyr** package which help find previous (lagged) or subsequent (leading) values in a vector - typically a numeric or date vector. This is useful when doing calculations of change/difference between time units.  


```{r, echo=F}
counts <- import(here("data", "example", "district_weekly_count_data.xlsx")) %>% 
  filter(District == "Nibari") %>% 
  mutate(Date = as.Date(Date),
         week_start = lubridate::floor_date(Date, "week")) %>%
  group_by(week_start) %>% 
  summarize(cases_wk = sum(Cases, na.rm=T)) %>% 
  complete(week_start = seq.Date(min(week_start), max(week_start), by = "week"), fill = list(cases_wk = 0))
```

Let's say you want to calculate the difference in cases between a current week and the previous one. The data are initially provided in weekly counts as shown below.  

```{r message=FALSE, echo=F}
DT::datatable(counts, rownames = FALSE,  options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

**When using `lag()` or `lead()` the order of rows in the dataframe is very important! - pay attention to whether your dates/numbers are ascending or descending**  

First, create a new column containing the value of the previous (lagged) week.  

* Control the number of units back/forward with `n = ` (must be a non-negative integer)  
* Use `default = ` to define the value placed in non-existing rows (e.g. the first row for which there is no lagged value). By default this is `NA`.  
* Use `order_by = TRUE` if your the rows are not ordered by your reference column  


```{r}
counts <- counts %>% 
  mutate(cases_prev_wk = lag(cases_wk, n = 1))
```

```{r message=FALSE, echo=F}
DT::datatable(counts, rownames = FALSE,  options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Next, create a new column which is the difference between the two cases columns:  

```{r}
counts <- counts %>% 
  mutate(cases_prev_wk = lag(cases_wk, n = 1),
         case_diff = cases_wk - cases_prev_wk)
```

```{r message=FALSE, echo=F}
DT::datatable(counts, rownames = FALSE,  options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


You can read more about `lead()` and `lag()` in the documentation [here](https://dplyr.tidyverse.org/reference/lead-lag.html) or by entering `?lag` in your console.  


<!-- ======================================================= -->
## Resources  

**lubridate** [tidyverse page](https://lubridate.tidyverse.org/)  
**lubridate** RStudio [cheatsheet](https://rawgit.com/rstudio/cheatsheets/master/lubridate.pdf)  
R for Data Science page on [dates and times](https://r4ds.had.co.nz/dates-and-times.html)  
[Online tutorial](https://www.statmethods.net/input/dates.html)
[Date formats](https://www.r-bloggers.com/2013/08/date-formats-in-r/)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/dates.Rmd-->


# Ký tự và chuỗi {#characters-strings}  

```{r, out.width=c('100%'), echo=F, message=F}
knitr::include_graphics(here::here("images", "Characters_Strings_1500x500.png"))
```



This page demonstrates use of the **stringr** package to evaluate and handle character values ("strings").  

1. Combine, order, split, arrange - `str_c()`, `str_glue()`, `str_order()`, `str_split()`  
2. Clean and standardise  
    * Adjust length - `str_pad()`, `str_trunc()`, `str_wrap()`  
    * Change case - `str_to_upper()`, `str_to_title()`, `str_to_lower()`, `str_to_sentence()`  
3. Evaluate and extract by position - `str_length()`, `str_sub()`, `word()`  
4. Patterns  
    * Detect and locate - `str_detect()`, `str_subset()`, `str_match()`, `str_extract()`  
    * Modify and replace - `str_sub()`, `str_replace_all()`  
7. Regular expressions ("regex")


For ease of display most examples are shown acting on a short defined character vector, however they can easily be adapted to a column within a data frame.  

This [stringr vignette](
https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html) provided much of the inspiration for this page.  



<!-- ======================================================= -->
## Preparation { }

### Load packages {.unnumbered}  

Install or load the **stringr** and other **tidyverse** packages.  

```{r}
# install/load packages
pacman::p_load(
  stringr,    # many functions for handling strings
  tidyverse,  # for optional data manipulation
  tools)      # alternative for converting to title case

```


### Import data  {.unnumbered}  


In this page we will occassionally reference the cleaned `linelist` of cases from a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Import data with the `import()` function from the **rio** package (it handles many file types like .xlsx, .csv, .rds - see the [Import and export] page for details).  

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import case linelist 
linelist <- import("linelist_cleaned.rds")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```




<!-- ======================================================= -->
## Unite, split, and arrange { }


This section covers:  

* Using `str_c()`, `str_glue()`, and `unite()` to combine strings  
* Using `str_order()` to arrange strings  
* Using `str_split()` and `separate()` to split strings  



<!-- ======================================================= -->
### Combine strings {.unnumbered}

To combine or concatenate multiple strings into one string, we suggest using `str_c` from **stringr**. If you have distinct character values to combine, simply provide them as unique arguments, separated by commas.     

```{r}
str_c("String1", "String2", "String3")
```

The argument `sep = ` inserts a character value between each of the arguments you provided (e.g. inserting a comma, space, or newline `"\n"`)  

```{r}
str_c("String1", "String2", "String3", sep = ", ")
```

The argument `collapse = ` is relevant if you are inputting multiple *vectors* as arguments to `str_c()`. It is used to separate the elements of what would be an *output* vector, such that the output vector only has one long character element.   

The example below shows the combination of two vectors into one (first names and last names). Another similar example might be jurisdictions and their case counts. In this example:  

* The `sep = ` value appears between each first and last name  
* The `collapse = ` value appears between each person  


```{r}
first_names <- c("abdul", "fahruk", "janice") 
last_names  <- c("hussein", "akinleye", "okeke")

# sep displays between the respective input strings, while collapse displays between the elements produced
str_c(first_names, last_names, sep = " ", collapse = ";  ")
```

Note: Depending on your desired display context, when printing such a combined string with newlines, you may need to wrap the whole phrase in `cat()` for the newlines to print properly:  

```{r}
# For newlines to print correctly, the phrase may need to be wrapped in cat()
cat(str_c(first_names, last_names, sep = " ", collapse = ";\n"))
```



<!-- ======================================================= -->
### Dynamic strings {.unnumbered}

Use `str_glue()` to insert dynamic R code into a string. This is a very useful function for creating dynamic plot captions, as demonstrated below.  

* All content goes between double quotation marks `str_glue("")`  
* Any dynamic code or references to pre-defined values are placed within curly brackets `{}` within the double quotation marks. There can be many curly brackets in the same `str_glue()` command.  
* To display character quotes '', use *single* quotes within the surrounding double quotes (e.g. when providing date format - see example below)  
* Tip: You can use `\n` to force a new line  
* Tip: You use `format()` to adjust date display, and use `Sys.Date()` to display the current date  

A simple example, of a dynamic plot caption:  

```{r}
str_glue("Data include {nrow(linelist)} cases and are current to {format(Sys.Date(), '%d %b %Y')}.")
```

An alternative format is to use placeholders within the brackets and define the code in separate arguments at the end of the `str_glue()` function, as below. This can improve code readability if the text is long.

```{r}
str_glue("Linelist as of {current_date}.\nLast case hospitalized on {last_hospital}.\n{n_missing_onset} cases are missing date of onset and not shown",
         current_date = format(Sys.Date(), '%d %b %Y'),
         last_hospital = format(as.Date(max(linelist$date_hospitalisation, na.rm=T)), '%d %b %Y'),
         n_missing_onset = nrow(linelist %>% filter(is.na(date_onset)))
         )

```


**Pulling from a data frame**  

Sometimes, it is useful to pull data from a data frame and have it pasted together in sequence. Below is an example data frame. We will use it to to make a summary statement about the jurisdictions and the new and total case counts.  

```{r}
# make case data frame
case_table <- data.frame(
  zone        = c("Zone 1", "Zone 2", "Zone 3", "Zone 4", "Zone 5"),
  new_cases   = c(3, 0, 7, 0, 15),
  total_cases = c(40, 4, 25, 10, 103)
  )
```

```{r, echo=F}
DT::datatable(case_table, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Use `str_glue_data()`, which is specially made for taking data from data frame rows:  

```{r}
case_table %>% 
  str_glue_data("{zone}: {new_cases} ({total_cases} total cases)")
```


**Combine strings across rows**  

If you are trying to "roll-up" values in a data frame column, e.g. combine values from multiple rows into just one row by pasting them together with a separator, see the section of the [De-duplication] page on ["rolling-up" values](#str_rollup).  

**Data frame to one line**  

You can make the statement appear in one line using `str_c()` (specifying the data frame and column names), and providing `sep = ` and `collapse = ` arguments.  

```{r}
str_c(case_table$zone, case_table$new_cases, sep = " = ", collapse = ";  ")
```

You could add the pre-fix text "New Cases:" to the beginning of the statement by wrapping with a separate `str_c()` (if "New Cases:" was within the original `str_c()` it would appear multiple times).  

```{r}
str_c("New Cases: ", str_c(case_table$zone, case_table$new_cases, sep = " = ", collapse = ";  "))
```





### Unite columns  {#str_unite .unnumbered}

Within a data frame, bringing together character values from multiple columns can be achieved with `unite()` from **tidyr**. This is the opposite of `separate()`.  

Provide the name of the new united column. Then provide the names of the columns you wish to unite.  

* By default, the separator used in the united column is underscore `_`, but this can be changed with the `sep = ` argument.  
* `remove = ` removes the input columns from the data frame (TRUE by default)  
* `na.rm = ` removes missing values while uniting (FALSE by default)  

Below, we define a mini-data frame to demonstrate with:  

```{r, message = F, warning=F}
df <- data.frame(
  case_ID = c(1:6),
  symptoms  = c("jaundice, fever, chills",     # patient 1
                "chills, aches, pains",        # patient 2 
                "fever",                       # patient 3
                "vomiting, diarrhoea",         # patient 4
                "bleeding from gums, fever",   # patient 5
                "rapid pulse, headache"),      # patient 6
  outcome = c("Recover", "Death", "Death", "Recover", "Recover", "Recover"))
```


```{r}
df_split <- separate(df, symptoms, into = c("sym_1", "sym_2", "sym_3"), extra = "merge")
```

Here is the example data frame:  

```{r, echo=F}
DT::datatable(df_split, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Below, we unite the three symptom columns:  

```{r}
df_split %>% 
  unite(
    col = "all_symptoms",         # name of the new united column
    c("sym_1", "sym_2", "sym_3"), # columns to unite
    sep = ", ",                   # separator to use in united column
    remove = TRUE,                # if TRUE, removes input cols from the data frame
    na.rm = TRUE                  # if TRUE, missing values are removed before uniting
  )
```







<!-- ======================================================= -->
### Split {.unnumbered}  

To split a string based on a pattern, use `str_split()`. It evaluates the string(s) and returns a `list` of character vectors consisting of the newly-split values.

The simple example below evaluates one string and splits it into three. By default it returns an object of class `list` with one element (a character vector) for each string initially provided. If `simplify = TRUE` it returns a character matrix.  

In this example, one string is provided, and the function returns a list with one element - a character vector with three values.  

```{r}
str_split(string = "jaundice, fever, chills",
          pattern = ",")
```

If the output is saved, you can then access the nth split value with bracket syntax. To access a specific value you can use syntax like this: `the_returned_object[[1]][2]`, which would access the second value from the first evaluated string ("fever"). See the [R basics] page for more detail on accessing elements.    

```{r}
pt1_symptoms <- str_split("jaundice, fever, chills", ",")

pt1_symptoms[[1]][2]  # extracts 2nd value from 1st (and only) element of the list
```

If multiple strings are provided by `str_split()`, there will be more than one element in the returned list.  

```{r}
symptoms <- c("jaundice, fever, chills",     # patient 1
              "chills, aches, pains",        # patient 2 
              "fever",                       # patient 3
              "vomiting, diarrhoea",         # patient 4
              "bleeding from gums, fever",   # patient 5
              "rapid pulse, headache")       # patient 6

str_split(symptoms, ",")                     # split each patient's symptoms
```


To return a "character matrix" instead, which may be useful if creating data frame columns, set the argument `simplify = TRUE` as shown below:  

```{r}
str_split(symptoms, ",", simplify = TRUE)
```

You can also adjust the number of splits to create with the `n = ` argument. For example, this restricts the number of splits to 2. Any further commas remain within the second values. 

```{r}
str_split(symptoms, ",", simplify = TRUE, n = 2)
```

*Note - the same outputs can be achieved with `str_split_fixed()`, in which you do not give the `simplify` argument, but must instead designate the number of columns (`n`).* 

```{r, eval=F}
str_split_fixed(symptoms, ",", n = 2)
```




### Split columns {.unnumbered}  

If you are trying to split data frame column, it is best to use the `separate()` function from **dplyr**. It is used to split one character column into other columns.  

Let's say we have a simple data frame `df` (defined and united in the [unite section](#str_unite)) containing a `case_ID` column, one character column with many symptoms, and one outcome column. Our goal is to separate the `symptoms` column into many columns - each one containing one symptom.  


```{r, echo=F}
DT::datatable(df, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Assuming the data are piped into `separate()`, first provide the column to be separated. Then provide `into = ` as a vector `c( )` containing the *new* columns names, as shown below.  

* `sep = ` the separator, can be a character, or a number (interpreted as the character position to split at) 
* `remove = ` FALSE by default, removes the input column  
* `convert = ` FALSE by default, will cause string "NA"s to become `NA`  
* `extra = ` this controls what happens if there are more values created by the separation than new columns named.  
     * `extra = "warn"` means you will see a warning but it will drop excess values (**the default**)  
     * `extra = "drop"` means the excess values will be dropped with no warning  
     * **`extra = "merge"` will only split to the number of new columns listed in `into` - *this setting will preserve all your data***  


An example with `extra = "merge"` is below - no data is lost. Two new columns are defined but any third symptoms are left in the second new column:  

```{r}
# third symptoms combined into second new column
df %>% 
  separate(symptoms, into = c("sym_1", "sym_2"), sep=",", extra = "merge")
```

When the default `extra = "drop"` is used below, a warning is given but the third symptoms are lost:  

```{r}
# third symptoms are lost
df %>% 
  separate(symptoms, into = c("sym_1", "sym_2"), sep=",")
```


<span style="color: orange;">**_CAUTION:_** If you do not provide enough `into` values for the new columns, your data may be truncated.</span>  






<!-- ======================================================= -->
### Arrange alphabetically {.unnumbered} 

Several strings can be sorted by alphabetical order. `str_order()` returns the order, while `str_sort()` returns the strings in that order.  

```{r}
# strings
health_zones <- c("Alba", "Takota", "Delta")

# return the alphabetical order
str_order(health_zones)

# return the strings in alphabetical order
str_sort(health_zones)
```

To use a different alphabet, add the argument `locale = `. See the full list of locales by entering `stringi::stri_locale_list()` in the R console.  





<!-- ======================================================= -->
### base R functions {.unnumbered}

It is common to see **base** R functions `paste()` and `paste0()`, which concatenate vectors after converting all parts to character. They act similarly to `str_c()` but the syntax is arguably more complicated - in the parentheses each part is separated by a comma. The parts are either character text (in quotes) or pre-defined code objects (no quotes). For example:

```{r}
n_beds <- 10
n_masks <- 20

paste0("Regional hospital needs ", n_beds, " beds and ", n_masks, " masks.")
```

`sep = ` and `collapse = ` arguments can be specified. `paste()` is simply `paste0()` with a default `sep = " "` (one space).  






## Clean and standardise  


<!-- ======================================================= -->
### Change case {.unnumbered}

Often one must alter the case/capitalization of a string value, for example names of jursidictions. Use `str_to_upper()`, `str_to_lower()`, and `str_to_title()`, from **stringr**, as shown below:  

```{r}
str_to_upper("California")

str_to_lower("California")
```

Using *base** R, the above can also be achieved with `toupper()`, `tolower()`.  


**Title case**  

Transforming the string so each word is capitalized can be achieved with `str_to_title()`:  

```{r}
str_to_title("go to the US state of california ")
```

Use `toTitleCase()` from the **tools** package to achieve more nuanced capitalization (words like "to", "the", and "of" are not capitalized).  

```{r}
tools::toTitleCase("This is the US state of california")
```

You can also use `str_to_sentence()`, which capitalizes only the first letter of the string.

```{r}
str_to_sentence("the patient must be transported")
```



### Pad length  {#str_pad .unnumbered}

Use `str_pad()` to add characters to a string, to a minimum length. By default spaces are added, but you can also pad with other characters using the `pad = ` argument.  


```{r}
# ICD codes of differing length
ICD_codes <- c("R10.13",
               "R10.819",
               "R17")

# ICD codes padded to 7 characters on the right side
str_pad(ICD_codes, 7, "right")

# Pad with periods instead of spaces
str_pad(ICD_codes, 7, "right", pad = ".")
```

For example, to pad numbers with leading zeros (such as for hours or minutes), you can pad the number to minimum length of 2 with `pad = "0"`.

```{r}
# Add leading zeros to two digits (e.g. for times minutes/hours)
str_pad("4", 2, pad = "0") 

# example using a numeric column named "hours"
# hours <- str_pad(hours, 2, pad = "0")
```


### Truncate {.unnumbered} 

`str_trunc()` sets a maximum length for each string. If a string exceeds this length, it is truncated (shortened) and an ellipsis (...) is included to indicate that the string was previously longer. Note that the ellipsis *is* counted in the length. The ellipsis characters can be changed with the argument `ellipsis = `.  The optional `side = ` argument specifies which where the ellipsis will appear within the truncated string ("left", "right", or "center").  

```{r}
original <- "Symptom onset on 4/3/2020 with vomiting"
str_trunc(original, 10, "center")
```


### Standardize length {.unnumbered}

Use `str_trunc()` to set a maximum length, and then use `str_pad()` to expand the very short strings to that truncated length. In the example below, 6 is set as the maximum length (one value is truncated), and then one very short value is padded to achieve length of 6.    

```{r}
# ICD codes of differing length
ICD_codes   <- c("R10.13",
                 "R10.819",
                 "R17")

# truncate to maximum length of 6
ICD_codes_2 <- str_trunc(ICD_codes, 6)
ICD_codes_2

# expand to minimum length of 6
ICD_codes_3 <- str_pad(ICD_codes_2, 6, "right")
ICD_codes_3
```


### Remove leading/trailing whitespace {.unnumbered}  

Use `str_trim()` to remove spaces, newlines (`\n`) or tabs (`\t`) on sides of a string input. Add `"right"` `"left"`, or `"both"` to the command to specify which side to trim (e.g. `str_trim(x, "right")`. 

```{r}
# ID numbers with excess spaces on right
IDs <- c("provA_1852  ", # two excess spaces
         "provA_2345",   # zero excess spaces
         "provA_9460 ")  # one excess space

# IDs trimmed to remove excess spaces on right side only
str_trim(IDs)
```


### Remove repeated whitespace within {.unnumbered}  

Use `str_squish()` to remove repeated spaces that appear *inside* a string. For example, to convert double spaces into single spaces. It also removes spaces, newlines, or tabs on the outside of the string like `str_trim()`.  


```{r}
# original contains excess spaces within string
str_squish("  Pt requires   IV saline\n") 
```

Enter `?str_trim`, `?str_pad` in your R console to see further details.  


### Wrap into paragraphs {.unnumbered}  

Use `str_wrap()` to wrap a long unstructured text into a structured paragraph with fixed line length. Provide the ideal character length for each line, and it applies an algorithm to insert newlines (`\n`) within the paragraph, as seen in the example below.   

```{r}
pt_course <- "Symptom onset 1/4/2020 vomiting chills fever. Pt saw traditional healer in home village on 2/4/2020. On 5/4/2020 pt symptoms worsened and was admitted to Lumta clinic. Sample was taken and pt was transported to regional hospital on 6/4/2020. Pt died at regional hospital on 7/4/2020."

str_wrap(pt_course, 40)
```

The **base** function `cat()` can be wrapped around the above command in order to print the output, displaying the new lines added.  

```{r}
cat(str_wrap(pt_course, 40))
```












<!-- ======================================================= -->
## Handle by position { }


### Extract by character position {.unnumbered}  

Use `str_sub()` to return only a part of a string. The function takes three main arguments:  

1) the character vector(s)  
2) start position  
3) end position  

A few notes on position numbers:  

* If a position number is positive, the position is counted starting from the left end of the string.  
* If a position number is negative, it is counted starting from the right end of the string.  
* Position numbers are inclusive.  
* Positions extending beyond the string will be truncated (removed).  

Below are some examples applied to the string "pneumonia":  

```{r}
# start and end third from left (3rd letter from left)
str_sub("pneumonia", 3, 3)

# 0 is not present
str_sub("pneumonia", 0, 0)

# 6th from left, to the 1st from right
str_sub("pneumonia", 6, -1)

# 5th from right, to the 2nd from right
str_sub("pneumonia", -5, -2)

# 4th from left to a position outside the string
str_sub("pneumonia", 4, 15)
```



### Extract by word position {.unnumbered} 

To extract the nth 'word', use `word()`, also from **stringr**. Provide the string(s), then the first word position to extract, and the last word position to extract.  

By default, the separator between 'words' is assumed to be a space, unless otherwise indicated with `sep = ` (e.g. `sep = "_"` when words are separated by underscores.  


```{r}
# strings to evaluate
chief_complaints <- c("I just got out of the hospital 2 days ago, but still can barely breathe.",
                      "My stomach hurts",
                      "Severe ear pain")

# extract 1st to 3rd words of each string
word(chief_complaints, start = 1, end = 3, sep = " ")
```


### Replace by character position {.unnumbered} 

`str_sub()` paired with the assignment operator (`<-`) can be used to modify a part of a string: 

```{r}
word <- "pneumonia"

# convert the third and fourth characters to X 
str_sub(word, 3, 4) <- "XX"

# print
word
```

An example applied to multiple strings (e.g. a column). Note the expansion in length of "HIV".  

```{r}
words <- c("pneumonia", "tubercolosis", "HIV")

# convert the third and fourth characters to X 
str_sub(words, 3, 4) <- "XX"

words
```



### Evaluate length  {.unnumbered}


```{r}
str_length("abc")
```

Alternatively, use `nchar()` from **base** R







<!-- ======================================================= -->
## Patterns { }

Many **stringr** functions work to detect, locate, extract, match, replace, and split based on a specified *pattern*.  



<!-- ======================================================= -->
### Detect a pattern {.unnumbered}

Use `str_detect()` as below to detect presence/absence of a pattern within a string. First provide the string or vector to search in (`string = `), and then the pattern to look for (`pattern = `). Note that by default the search *is case sensitive*!

```{r}
str_detect(string = "primary school teacher", pattern = "teach")
```

The argument `negate = ` can be included and set to `TRUE` if you want to know if the pattern is NOT present.  
 
```{r}
str_detect(string = "primary school teacher", pattern = "teach", negate = TRUE)
```

To ignore case/capitalization, wrap the pattern within `regex()`, and *within* `regex()` add the argument `ignore_case = TRUE` (or `T` as shorthand).  

```{r}
str_detect(string = "Teacher", pattern = regex("teach", ignore_case = T))
```

When `str_detect()` is applied to a character vector or a data frame column, it will return TRUE or FALSE for each of the values. 

```{r}
# a vector/column of occupations 
occupations <- c("field laborer",
                 "university professor",
                 "primary school teacher & tutor",
                 "tutor",
                 "nurse at regional hospital",
                 "lineworker at Amberdeen Fish Factory",
                 "physican",
                 "cardiologist",
                 "office worker",
                 "food service")

# Detect presence of pattern "teach" in each string - output is vector of TRUE/FALSE
str_detect(occupations, "teach")
```

If you need to count the `TRUE`s, simply `sum()` the output. This counts the number `TRUE`.  

```{r}
sum(str_detect(occupations, "teach"))
```

To search inclusive of multiple terms, include them separated by OR bars (`|`) within the `pattern = ` argument, as shown below:  

```{r}
sum(str_detect(string = occupations, pattern = "teach|professor|tutor"))
```

If you need to build a long list of search terms, you can combine them using `str_c()` and `sep = |`, then define this is a character object, and then reference the vector later more succinctly. The example below includes possible occupation search terms for front-line medical providers.     

```{r}
# search terms
occupation_med_frontline <- str_c("medical", "medicine", "hcw", "healthcare", "home care", "home health",
                                "surgeon", "doctor", "doc", "physician", "surgery", "peds", "pediatrician",
                               "intensivist", "cardiologist", "coroner", "nurse", "nursing", "rn", "lpn",
                               "cna", "pa", "physician assistant", "mental health",
                               "emergency department technician", "resp therapist", "respiratory",
                                "phlebotomist", "pharmacy", "pharmacist", "hospital", "snf", "rehabilitation",
                               "rehab", "activity", "elderly", "subacute", "sub acute",
                                "clinic", "post acute", "therapist", "extended care",
                                "dental", "dential", "dentist", sep = "|")

occupation_med_frontline
```

This command returns the number of occupations which contain any one of the search terms for front-line medical providers (`occupation_med_frontline`):  

```{r}
sum(str_detect(string = occupations, pattern = occupation_med_frontline))
```



**Base R string search functions**  

The **base** function `grepl()` works similarly to `str_detect()`, in that it searches for matches to a pattern and returns a logical vector. The basic syntax is `grepl(pattern, strings_to_search, ignore.case = FALSE, ...)`. One advantage is that the `ignore.case` argument is easier to write (there is no need to involve the `regex()` function).  

Likewise, the **base** functions `sub()` and `gsub()` act similarly to `str_replace()`. Their basic syntax is: `gsub(pattern, replacement, strings_to_search, ignore.case = FALSE)`. `sub()` will replace the first instance of the pattern, whereas `gsub()` will replace all instances of the pattern.  


#### Convert commas to periods {.unnumbered}  

Here is an example of using `gsub()` to convert commas to periods in a vector of numbers. This could be useful if your data come from parts of the world other than the United States or Great Britain.  

The inner `gsub()` which acts first on `lengths` is converting any periods to no space "". The period character  "." has to be "escaped" with two slashes to actually signify a period, because "." in regex means "any character". Then, the result (with only commas) is passed to the outer `gsub()` in which commas are replaced by periods.   

```{r, eval=F}
lengths <- c("2.454,56", "1,2", "6.096,5")

as.numeric(gsub(pattern = ",",                # find commas     
                replacement = ".",            # replace with periods
                x = gsub("\\.", "", lengths)  # vector with other periods removed (periods escaped)
                )
           )                                  # convert outcome to numeric
```





### Replace all {.unnumbered}  

Use `str_replace_all()` as a "find and replace" tool. First, provide the strings to be evaluated to `string = `, then the pattern to be replaced to `pattern = `, and then the replacement value to `replacement = `. The example below replaces all instances of "dead" with "deceased". Note, this IS case sensitive.  

```{r}
outcome <- c("Karl: dead",
            "Samantha: dead",
            "Marco: not dead")

str_replace_all(string = outcome, pattern = "dead", replacement = "deceased")
```

Notes:  

* To replace a pattern with `NA`, use `str_replace_na()`.  
* The function `str_replace()` replaces only the first instance of the pattern within each evaluated string.  





<!-- ======================================================= -->
### Detect within logic {.unnumbered}


**Within `case_when()`**  

`str_detect()` is often used within `case_when()` (from **dplyr**). Let's say `occupations` is a column in the linelist. The `mutate()` below creates a new column called `is_educator` by using conditional logic via `case_when()`. See the page on data cleaning to learn more about `case_when()`.  


```{r, eval=F}
df <- df %>% 
  mutate(is_educator = case_when(
    # term search within occupation, not case sensitive
    str_detect(occupations,
               regex("teach|prof|tutor|university",
                     ignore_case = TRUE))              ~ "Educator",
    # all others
    TRUE                                               ~ "Not an educator"))
```

As a reminder, it may be important to add exclusion criteria to the conditional logic (`negate = F`):  

```{r, eval=F}
df <- df %>% 
  # value in new column is_educator is based on conditional logic
  mutate(is_educator = case_when(
    
    # occupation column must meet 2 criteria to be assigned "Educator":
    # it must have a search term AND NOT any exclusion term
    
    # Must have a search term
    str_detect(occupations,
               regex("teach|prof|tutor|university", ignore_case = T)) &              
    
    # AND must NOT have an exclusion term
    str_detect(occupations,
               regex("admin", ignore_case = T),
               negate = TRUE                        ~ "Educator"
    
    # All rows not meeting above criteria
    TRUE                                            ~ "Not an educator"))
```





<!-- ======================================================= -->
### Locate pattern position {.unnumbered}  

To locate the *first* position of a pattern, use `str_locate()`. It outputs a start and end position.   

```{r}
str_locate("I wish", "sh")
```

Like other `str` functions, there is an "_all" version (`str_locate_all()`) which will return the positions of *all* instances of the pattern within each string. This outputs as a `list`.  

```{r}
phrases <- c("I wish", "I hope", "he hopes", "He hopes")

str_locate(phrases, "h" )     # position of *first* instance of the pattern
str_locate_all(phrases, "h" ) # position of *every* instance of the pattern
```





<!-- ======================================================= -->
### Extract a match {.unnumbered}  

`str_extract_all()` returns the matching patterns themselves, which is most useful when you have offered several patterns via "OR" conditions. For example, looking in the string vector of occupations (see previous tab) for *either* "teach", "prof", or "tutor".

`str_extract_all()` returns a `list` which contains *all matches* for each evaluated string. See below how occupation 3 has two pattern matches within it.  

```{r}
str_extract_all(occupations, "teach|prof|tutor")
```


`str_extract()` extracts *only the first match* in each evaluated string, producing a character vector with one element for each evaluated string. It returns `NA` where there was no match. The `NA`s can be removed by wrapping the returned vector with `na.exclude()`. Note how the second of occupation 3's matches is not shown.  

```{r}
str_extract(occupations, "teach|prof|tutor")
```

<!-- ======================================================= -->
### Subset and count {.unnumbered}  

Aligned functions include `str_subset()` and `str_count()`.  

`str_subset()` returns the actual values which contained the pattern: 

```{r}
str_subset(occupations, "teach|prof|tutor")
```

`str_count()` returns a vector of numbers: the **number of times** a search term appears in each evaluated value.  

```{r}
str_count(occupations, regex("teach|prof|tutor", ignore_case = TRUE))
```












<!-- ======================================================= -->
### Regex groups {.unnumbered}

UNDER CONSTRUCTION






<!-- ======================================================= -->
## Special characters  

**Backslash `\` as escape**  

The backslash `\` is used to "escape" the meaning of the next character. This way, a backslash can be used to have a quote mark display *within* other quote marks (`\"`) - the middle quote mark will not "break" the surrounding quote marks.  

Note - thus, if you want to *display* a backslash, you must escape it's meaning with *another* backslash. So you must write two backslashes `\\` to display one.  

**Special characters**  

Special character | Represents  
----------------- | --------------------------------------------------------------    
`"\\"` | backslash  
`"\n"` | a new line (newline)   
`"\""` | double-quote *within* double quotes  
`'\''` | single-quote *within* single quotes  
`"\`"` | grave accent  
`"\r"` | carriage return  
`"\t"` | tab  
`"\v"` | vertical tab 
`"\b"` | backspace  


Run `?"'"` in the R Console to display a complete list of these special characters (it will appear in the RStudio Help pane). 



<!-- ======================================================= -->
## Regular expressions (regex) 


<!-- ======================================================= -->
## Regex and special characters { } 

Regular expressions, or "regex", is a concise language for describing patterns in strings. If you are not familiar with it, a regular expression can look like an alien language. Here we try to de-mystify this language a little bit.  

*Much of this section is adapted from [this tutorial](https://towardsdatascience.com/a-gentle-introduction-to-regular-expressions-with-r-df5e897ca432) and [this cheatsheet](https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_strings.pdf)*. We selectively adapt here knowing that this handbook might be viewed by people without internet access to view the other tutorials.    


A regular expression is often applied to extract specific patterns from "unstructured" text - for example medical notes, chief complaints, patient history, or other free text columns in a data frame  

There are four basic tools one can use to create a basic regular expression:  

1) Character sets  
2) Meta characters  
3) Quantifiers  
4) Groups  


**Character sets**  

Character sets, are a way of expressing listing options for a character match, within brackets. So any a match will be triggered if any of the characters within the brackets are found in the string. For example, to look for vowels one could use this character set: "[aeiou]". Some other common character sets are:  

Character set | Matches for  
----------------- | --------------------------------------------------------------    
`"[A-Z]"` | any single capital letter  
`"[a-z]"` | any single lowercase letter  
`"[0-9]"` | any digit  
`[:alnum:]` | any alphanumeric character  
`[:digit:]` | any numeric digit  
`[:alpha:]` | any letter (upper or lowercase)  
`[:upper:]` | any uppercase letter  
`[:lower:]` | any lowercase letter  


Character sets can be combined within one bracket (no spaces!), such as `"[A-Za-z]"` (any upper or lowercase letter), or another example `"[t-z0-5]"` (lowercase t through z OR number 0 through 5).  



**Meta characters**  

Meta characters are shorthand for character sets. Some of the important ones are listed below:  

Meta character | Represents  
----------------- | --------------------------------------------------------------    
`"\\s"` | a single space  
`"\\w"` | any single alphanumeric character (A-Z, a-z, or 0-9)  
`"\\d"` | any single numeric digit (0-9)  


**Quantifiers**  

Typically you do not want to search for a match on only one character. Quantifiers allow you to designate the length of letters/numbers to allow for the match.  

Quantifiers are numbers written within curly brackets `{ }` *after* the character they are quantifying, for example,  

* `"A{2}"` will return instances of **two** capital A letters.  
* `"A{2,4}"` will return instances of **between two and four** capital A letters *(do not put spaces!)*.  
* `"A{2,}"` will return instances of **two or more** capital A letters.  
* `"A+"` will return instances of **one or more** capital A letters (group extended until a different character is encountered).  
* Precede with an `*` asterisk to return **zero or more** matches (useful if you are not sure the pattern is present)  


Using the `+` plus symbol as a quantifier, the match will occur until a different character is encountered. For example, this expression will return all *words* (alpha characters: `"[A-Za-z]+"`  


```{r}
# test string for quantifiers
test <- "A-AA-AAA-AAAA"
```

When a quantifier of {2} is used, only pairs of consecutive A's are returned. Two pairs are identified within `AAAA`.  

```{r}
str_extract_all(test, "A{2}")
```

When a quantifier of {2,4} is used, groups of consecutive A's that are two to four in length are returned.  

```{r}
str_extract_all(test, "A{2,4}")
```

With the quantifier `+`, groups of **one or more** are returned:  

```{r}
str_extract_all(test, "A+")
```

**Relative position**  

These express requirements for what precedes or follows a pattern. For example, to extract sentences, "two numbers that are followed by a period" (`""`).  (?<=\\.)\\s(?=[A-Z]) 

```{r}
str_extract_all(test, "")
```

Position statement | Matches to  
----------------- | --------------------------------------------------------------    
`"(?<=b)a"` | "a" that **is preceded** by a "b"  
`"(?<!b)a"` | "a" that **is NOT preceded** by a "b"  
`"a(?=b)"` | "a" that **is followed** by a "b"  
`"a(?!b)"` | "a" that **is NOT followed** by a "b"  





**Groups**  

Capturing groups in your regular expression is a way to have a more organized output upon extraction.  




**Regex examples**  

Below is a free text for the examples. We will try to extract useful information from it using a regular expression search term.  

```{r}
pt_note <- "Patient arrived at Broward Hospital emergency ward at 18:00 on 6/12/2005. Patient presented with radiating abdominal pain from LR quadrant. Patient skin was pale, cool, and clammy. Patient temperature was 99.8 degrees farinheit. Patient pulse rate was 100 bpm and thready. Respiratory rate was 29 per minute."
```

This expression matches to all words (any character until hitting non-character such as a space):  

```{r}
str_extract_all(pt_note, "[A-Za-z]+")
```

The expression `"[0-9]{1,2}"` matches to consecutive numbers that are 1 or 2 digits in length. It could also be written `"\\d{1,2}"`, or `"[:digit:]{1,2}"`.  

```{r}
str_extract_all(pt_note, "[0-9]{1,2}")
```

<!-- This expression will extract all sentences (assuming first letter is capitalized, and the sentence ends with a period). The pattern reads in English as: "A capital letter followed by some lowercase letters, a space, some letters, a space,     -->

<!-- ```{r} -->
<!-- str_extract_all(pt_note, "[A-Z][a-z]+\\s\\w+\\s\\d{1,2}\\s\\w+\\s*\\w*") -->
<!-- ``` -->


You can view a useful list of regex expressions and tips on page 2 of [this cheatsheet](https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_strings.pdf)  

Also see this [tutorial](https://towardsdatascience.com/a-gentle-introduction-to-regular-expressions-with-r-df5e897ca432).  




<!-- ======================================================= -->
## Resources { }

A reference sheet for **stringr** functions can be found [here](https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_strings.pdf)


A vignette on **stringr** can be found [here](
https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html)



```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/characters_strings.Rmd-->

# Factors {#factors}

```{r, out.width=c('100%'), echo=F, message=F}
knitr::include_graphics(here::here("images", "Factors_1500x500.png"))
```

Trong R, *factors* là một kiểu dữ liệu cho phép sắp xếp các danh mục với một tập hợp các giá trị có thể chấp nhận.

Thông thường, bạn sẽ chuyển đổi một cột từ dạng ký tự hoặc dạng số thành dạng factor khi bạn muốn sắp xếp một thứ đặc biệt cho các giá trị ("*levels*") để chúng không hiển thị mặc định theo thứ tự bảng chữ cái trong các biểu đồ và bảng. Một cách sử dụng phổ biến khác của factor là chuẩn hóa các chú thích của biểu đồ để chúng không thay đổi nếu một giá trị tạm thời không có trong dữ liệu.

Chương này giới thiệu cách sử dụng các hàm từ package **forcats** (tên viết tắt của "**For** **categorical variables**") và một số hàm **base** R. Chúng tôi cũng đề cập đến việc sử dụng **lubridate** và **aweek** cho các trường hợp factor đặc biệt liên quan đến tuần dịch tễ học.

Bạn có thể tìm thấy danh sách đầy đủ các hàm của package **forcats** trực tuyến tại đường [link này](https://forcats.tidyverse.org/reference/index.html). Sau đây, chúng tôi sẽ chỉ trình bày một số hàm phổ biến nhất.

<!-- ======================================================= -->

## Chuẩn bị

### Gọi packages {.unnumbered}

Đoạn code dưới đây hiển thị cách gọi các package cần thiết cho việc phân tích. Trong sách này, chúng tôi nhấn mạnh đến việc sử dụng hàm `p_load()` từ package **pacman**, giúp cài đặt package nếu nó chưa được cài *và* gọi nó ra cho phiên làm việc. Bạn cũng có thể gọi các package đã được cài đặt bằng hàm `library()` từ **base** R. Xem chương [R cơ bản] để biết thêm thông tin về các package trong R.

```{r}
pacman::p_load(
  rio,           # import/export
  here,          # filepaths
  lubridate,     # working with dates
  forcats,       # factors
  aweek,         # create epiweeks with automatic factor levels
  janitor,       # tables
  tidyverse      # data mgmt and viz
  )
```

### Nhập dữ liệu {.unnumbered}

Chúng ta sẽ nhập bộ dữ liệu về các trường hợp từ một vụ dịch Ebola mô phỏng. Để tiện muốn theo dõi, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>bấm để tải bộ dữ liệu linelist "đã được làm sạch"</a> (as .rds file). Nhập dữ liệu bằng hàm `import()` từ package **rio** (hàm có thể áp dụng với nhiều loại dữ liệu như .xlsx, .rds, .csv - Xem chương [Nhập xuất dữ liệu] để biết thêm chi tiết).

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import your dataset
linelist <- import("linelist_cleaned.rds")
```

### Thêm biến danh mục mới {#fct_newcat .unnumbered}

Trong chương này, chúng tôi sẽ minh họa một trường hợp thường gặp, đó là tạo ra một biến danh mục mới.

Lưu ý rằng khi bạn chuyển đổi một cột dạng số thành dạng factor, bạn sẽ không thể thực hiện các tính toán thống kê đối với dữ liệu dạng số trên cột đó nữa.

#### Tạo biến {.unnumbered}

Chúng ta sẽ sử dụng một biến có sẵn, tên là `days_onset_hosp` (số ngày, tính từ khi bắt đầu có triệu chứng cho đến khi nhập viện) và tạo một biến mới có tên `delay_cat` bằng cách phân loại các giá trị trong mỗi hàng của biến có sẵn đó thành một số nhóm khác nhau. Chúng ta sẽ thực hiện việc này bằng hàm `case_when()` trong package **dplyr**, hàm này sẽ giúp áp dụng tuần tự các tiêu chí logic (phía bên phải) cho mỗi giá trị của biễn có sẵn và trả về giá trị bên trái tương ứng ở biến mới `delay_cat`. Đọc thêm về `case_when()` tại chương [Làm sạch số liệu và các hàm quan trọng].

```{r}
linelist <- linelist %>% 
  mutate(delay_cat = case_when(
    # criteria                                   # new value if TRUE
    days_onset_hosp < 2                        ~ "<2 days",
    days_onset_hosp >= 2 & days_onset_hosp < 5 ~ "2-5 days",
    days_onset_hosp >= 5                       ~ ">5 days",
    is.na(days_onset_hosp)                     ~ NA_character_,
    TRUE                                       ~ "Check me"))  
```

#### Thứ tự mặc định của các giá trị {.unnumbered}

Khi sử dụng hàm `case_when()`, biến mới `delay_cat` tạo ra sẽ là một biến danh mục với kiêu dữ liệu là ký tự - *chưa* phải là một factor. Do đó, trong bảng tần suất dưới đây, chúng ta thấy rằng các giá trị xuất hiện theo thứ tự mặc định của bảng chữ cái, điều này không có nhiều ý nghĩa trực quan:

```{r}
table(linelist$delay_cat, useNA = "always")
```

Tương tự như vậy, nếu chúng ta tạo biểu đồ cột, các giá trị cũng xuất hiện theo thứ tự này trên trục x (xem chương [ggplot cơ bản] để hiểu thêm về package **ggplot2** - package giúp trực quan hóa dữ liệu phổ biến nhất trong R).

```{r, warning=F, message=F}
ggplot(data = linelist)+
  geom_bar(mapping = aes(x = delay_cat))
```

## Chuyển đổi sang factor

Để chuyển đổi một biến dạng ký tự hoặc dạng số sang dạng *factor*, bạn có thể sử dụng bất kỳ hàm nào trong package **forcats** (nhiều hàm được nêu chi tiết tại [mục dưới đây](#fct_adjust)). Các biến sẽ chuyển đổi sang dạng factor và sau đó cũng thực hiện hoặc sắp xếp theo một thứ tự nhất định của các levels - ví dụ: hàm `fct_relevel()` cho phép bạn chỉ định thứ tự levels theo cách thủ công. Hàm `as_factor()` chỉ đơn giản là chuyển đổi biến sang dạng factor mà không có thêm bất kỳ chức năng nào khác.

Hàm `factor()` trong **base** R chuyển đổi một biến thành factor và cho phép bạn tự sắp xếp thứ tự của các nhóm giá trị, dưới dạng một vectơ ký tự của đối số `levels =`.

Dưới đây, chúng tôi sử dụng hàm `mutate()` và hàm `fct_relevel()` để chuyển đối biến có sẵn `delay_cat` từ dạng ký tự sang dạng factor. Biến `delay_cat` đã được tạo ở phần [Chuẩn bị](#fct_newcat) bên trên.

```{r}
linelist <- linelist %>%
  mutate(delay_cat = fct_relevel(delay_cat))
```

*Các "giá trị" duy nhất trong biến số được gọi là các "thứ bậc" của biến factor.* Các thứ bậc này được sắp xếp *theo một trật tự nhất định* và có thể được in ra bằng hàm `levels()` từ **base** R, hoặc bạn có thể xem nó bằng một bảng đếm thông qua hàm `table()`từ **base** R, hoặc hàm `tabyl()` từ package **janitor**. Trật tự này sẽ được hiển thị theo thứ tự của bảng chữ cái. Lưu ý rằng `NA` không được xem là một thứ bậc trong factor.

```{r}
levels(linelist$delay_cat)
```

Hàm `fct_relevel()` có thêm chức năng cho phép bạn có thể tự sắp xếp trật tự của các thứ bậc trong factor. Đơn giản, bạn chỉ cần viết các thứ bậc theo thứ tự bạn muốn, để chúng trong dấu ngoặc kép, được phân tách bằng dấu phẩy, như được hiển thị bên dưới. Lưu ý rằng chính tả phải khớp chính xác với tên các thứ bậc. Nếu bạn muốn tạo các thứ bậc không tồn tại trong dữ liệu, hãy sử dụng hàm [`fct_expand()`](#fct_add) nhé.

```{r}
linelist <- linelist %>%
  mutate(delay_cat = fct_relevel(delay_cat, "<2 days", "2-5 days", ">5 days"))
```

Bây giờ chúng ta có thể thấy rằng các thứ bậc đã được sắp xếp theo một thứ tự hợp lý.

```{r}
levels(linelist$delay_cat)
```

Bây giờ trật tự các cột trong biểu đồ cũng trực quan hơn.

```{r, warning=F, message=F}
ggplot(data = linelist)+
  geom_bar(mapping = aes(x = delay_cat))
```

## Thêm hoặc xóa thứ bậc

### Thêm thứ bậc {#fct_add .unnumbered}

Nếu bạn cần thêm thứ bâc trong factor, bạn có thể sử dụng hàm `fct_expand()`. Bạn chỉ cần viết tên biến và theo sau là tên các thứ bậc mới (phân tách bằng dấu phẩy). Bằng cách lập bảng, chúng ta có thể thấy các thứ bậc mới xuất hiện và chưa nhận giá trị nào. Bạn có thể sử dụng hàm `table()` trong **base** R, hoặc hàm `tabyl()` trong package **janitor**:

```{r}
linelist %>% 
  mutate(delay_cat = fct_expand(delay_cat, "Not admitted to hospital", "Transfer to other jurisdiction")) %>% 
  tabyl(delay_cat)   # print table
```

Lưu ý: Package **forcats** có thể dễ dàng thêm các giá trị missing (`NA`) như là một thứ bậc. Bạn có thể xem thêm tại mục [Giá trị Missing](#fct_missing) dưới đây.

### Xóa thứ bậc {.unnumbered}

Nếu bạn sử dụng hàm `fct_drop()`, các thứ bậc "không được sử dụng" và không có quan sát nào sẽ bị loại bỏ khỏi factors. Thứ bậc mà chúng ta đã thêm ở trên ("Not admitted to a hospital") có tổn tại nhưng không có biến số nào. Vì vậy, chúng sẽ bị loại bỏ khỏi biến factor của chúng ta bằng cách sử dụng hàm `fct_drop()` như sau:

```{r}
linelist %>% 
  mutate(delay_cat = fct_drop(delay_cat)) %>% 
  tabyl(delay_cat)
```

## Thay đổi trật tự của các thứ bậc {#fct_adjust}

Package **forcats** cung cấp các hàm hữu ích để dễ dàng thay đổi trật tự của các thứ bậc trong một biến kiểu factor (sau khi một biến số được định nghĩa là một factor):

Các hàm trong package này có thể được áp dụng cho biến dạng factor trong hai trường hợp dưới đây:

1)  Đối với cột trong một data frame, thông thường, việc thay đổi sẽ được giữ nguyên cho các lần sử dụng dữ liệu tiếp theo
2)  *Trong một biểu đồ*, sự thay đổi trật tự chỉ được áp dụng cho biểu đồ đó

### Thay đổi thủ công {.unnumbered}

Hàm này được sử dụng để thay đổi trật tự của các thứ bậc trong một biến dạng factor theo cách thủ công. Nếu hàm này được sử dụng trên một biến dạng khác, không phải factor, hàm sẽ giúp chuyển biến đó sang dạng factor trước.

Trong dấu ngoặc đơn trước tiên điền tên của biến factor, sau đó điền:

-   Tất cả các thứ bậc trong biến factor mà bạn mong muốn thay đổi trật tự (dưới dạng vector ký tự `c()`), hoặc
-   Chỉ một giá trị thứ bậc với vị trí tương ứng mong muốn, sử dụng đối số `after =`

Dưới đây là một ví dụ về chuyển biến `delay_cat` thành dạng factor (mặc dù biến này đã ở dạng Factor rồi) và sắp xếp lại các thứ bậc theo thứ tự mong muốn.

```{r}
# re-define level order
linelist <- linelist %>% 
  mutate(delay_cat = fct_relevel(delay_cat, c("<2 days", "2-5 days", ">5 days")))
```

Nếu bạn chỉ muốn chỉ định vị trí cho một thứ bậc, bạn có thể dùng hàm `fct_relevel()` và sử dụng đối số `after =` để chỉ định một giá trị thứ bậc với vị trí tương ứng mong muốn. Ví dụ: lệnh dưới đây chuyển thứ bậc "\<2 days" sang vị trí thứ hai:

```{r, eval=F}
# re-define level order
linelist %>% 
  mutate(delay_cat = fct_relevel(delay_cat, "<2 days", after = 1)) %>% 
  tabyl(delay_cat)
```

### Đối với biểu đồ {.unnumbered}

Các lệnh trong package **forcats** có thể được sử dụng để thay đổi trật tự của biến trong data frame hoặc trong biểu đồ. Bằng cách sử dụng các lệnh để "gói" tên biến vào *trong* các lệnh vẽ biểu đồ của package `ggplot()`, bạn có thể dảo ngược/thay đổi một trật tự có sẵn của biến. Sự thay đổi này chỉ áp dụng trong biểu đồ đang vẽ.

Dưới đây, hai biểu đồ đều được vẽ bởi hàm `ggplot()` (xem thêm tại chương [ggplot cơ bản]). Trong biểu đồ đầu tiên, biến `delay_cat` được vẽ trên trục x của biểu đồ với thứ tự các thứ bậc là mặc định trong dữ liệu `linelist`. Trong biểu đồ thứ hai, biến được đặt trong bởi hàm `fct_relevel()` và trật tự của các thứ bậc đã được sắp xếp lại.

```{r, echo =F}
linelist <- linelist %>% 
  mutate(delay_cat = fct_relevel(delay_cat, c("2-5 days", "<2 days", ">5 days")))

```

```{r, warning=F, message=F, out.width = c('50%', '50%'), fig.show='hold'}
# Alpha-numeric default order - no adjustment within ggplot
ggplot(data = linelist)+
    geom_bar(mapping = aes(x = delay_cat))

# Factor level order adjusted within ggplot
ggplot(data = linelist)+
  geom_bar(mapping = aes(x = fct_relevel(delay_cat, c("<2 days", "2-5 days", ">5 days"))))
```

Lưu ý rằng, ở biểu đồ thứ hai, tiêu đề mặc định của trục x được hiện khá phức tạp - bạn có thể sử tiêu đề này bằng đối số `labs()` trong **ggplot2**.

### Đảo ngược thứ tự {.unnumbered}

Bạn sẽ thường xuyên cần đảo ngược trật tự của các thứ bậc trong một biến. Đơn giản, bạn chỉ cần thêm tên biến vào bên trong hàm `fct_rev()`.

Lưu ý rằng, nếu bạn *chỉ* muốn đảo ngược thứ tự trong một biểu đồ chứ không phải thứ tự của biến đó, bạn có thể thực hiện điều đó với hàm `guides()` (Xem thêm tại chương [Các tips với ggplot]).

### Theo tần suất {.unnumbered}

Để sắp xếp trật tự các thứ bậc theo tần suất mà nó xuất hiện trong dữ liệu, hãy sử dụng hàm `fct_infreq()`. Tất cả các giá trị mising (`NA`) sẽ tự động được đưa xuống cuối, trừ khi chúng được chuyển sang một thứ bậc khác (xem thêm ở [mục này](#fct_missing)). Bạn có thể đảo ngược trật tự bằng cách thêm hàm `fct_rev()` vào câu lệnh.

Hàm này có thể được sử dụng trong `ggplot()`, như hình bên dưới.

```{r, out.width = c('50%', '50%', '50%'), fig.show='hold', warning=F, message=F}
# ordered by frequency
ggplot(data = linelist, aes(x = fct_infreq(delay_cat)))+
  geom_bar()+
  labs(x = "Delay onset to admission (days)",
       title = "Ordered by frequency")

# reversed frequency
ggplot(data = linelist, aes(x = fct_rev(fct_infreq(delay_cat))))+
  geom_bar()+
  labs(x = "Delay onset to admission (days)",
       title = "Reverse of order by frequency")
```

### Theo sự xuất hiện {.unnumbered}

Sử dụng hàm `fct_inorder()` để thiết lập thứ bậc tương tự với thứ tự xuất hiện của các giá trị trong dữ liệu, bắt đầu từ hàng đầu tiên. Điều này có thể hữu ích nếu trước đó bạn đã cẩn thận sắp xếp dữ liệu trong data frame bằng hàm `arrange()`, sau đó sử dụng điều này để đặt trật tự các thứ bậc của biến facror.

### Theo thống kê tóm tắt của một cột khác {.unnumbered}

Bạn có thể sử dụng hàm `fct_reorder()` để sắp xếp các thứ bậc của một biến *theo thống kê tóm tắt của một biến khác*. Về mặt trực quan, điều này có cho kết quả là các biểu đồ như ý bạn, có các cột/điểm lên hoặc xuống theo một chiều trong toàn bộ biểu đồ.

Trong các ví dụ bên dưới, trục x là `delay_cat`, và trục y là `ct_blood` (giá trị ngưỡng chu kỳ). Biểu đồ Box plots hiển thị phân bố của giá trị CT theo nhóm `delay_cat`. Chúng ta cần sắp xếp các box theo thứ tự tăng dần của giá trị trung vị CT của nhóm.

Trong ví dụ đầu tiên bên dưới, các thứ bậc được sắp xếp một cách mặc định. Bạn có thể thấy các chiều cao của box bị lộn xộn và không theo bất kỳ thứ tự cụ thể nào. Trong ví dụ thứ hai, cột `delay_cat` (được sắp xếp theo trục x) đã được viết lệnh với hàm `fct_reorder()`, cột `ct_blood` được đưa ra làm đối số thứ hai và "trung vị" được đưa ra làm đối số thứ ba (bạn cũng có thể sử dụng "max", "mean", "min", v.v.). Do đó, thứ tự các thứ bậc của biến `delay_cat` bây giờ sẽ phản ánh các giá trị trung vị CT tăng dần theo nhóm `delay_cat`. Điều này được trình bày trong biểu đồ thứ hai - các box đã được sắp xếp lại theo chiều tăng dần. Lưu ý biến missing `NA` sẽ luôn xuất hiện ở cuối, trừ khi được chuyển đổi thành một thứ bậc khác.

```{r, fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# boxplots ordered by original factor levels
ggplot(data = linelist)+
  geom_boxplot(
    aes(x = delay_cat,
        y = ct_blood, 
        fill = delay_cat))+
  labs(x = "Delay onset to admission (days)",
       title = "Ordered by original alpha-numeric levels")+
  theme_classic()+
  theme(legend.position = "none")


# boxplots ordered by median CT value
ggplot(data = linelist)+
  geom_boxplot(
    aes(x = fct_reorder(delay_cat, ct_blood, "median"),
        y = ct_blood,
        fill = delay_cat))+
  labs(x = "Delay onset to admission (days)",
       title = "Ordered by median CT value in group")+
  theme_classic()+
  theme(legend.position = "none")
```

Lưu ý trong ví dụ bên trên không có bước nào được yêu cầu cần thực hiện trước khi gọi hàm `ggplot()` - việc nhóm và tính toán đều được thực hiện bên trong hàm ggplot.

### Theo giá trị cuối {.unnumbered}

Sử dụng hàm `fct_reorder2()` cho biểu đồ đường theo nhóm. Hàm sẽ sắp xếp thứ tự xuất hiện các nhóm (bao gồm cả phần *chú giải*) dọc theo biểu đồ. Nói về mặt kỹ thuật, nó "sắp xếp theo các giá trị y tương ứng với các giá trị x lớn nhất."

Ví dụ, nếu bạn có các dòng hiển thị số lượng trường hợp theo bệnh viện và thời gian, bạn có thể áp dụng hàm `fct_reorder2()` cho đối số `color =` trong `aes()`, sao cho thứ tự của các bệnh viện xuất hiện trong phần chú giải tương đương với thứ tự xuất hiện của các đường trong biểu đồ. Đọc thêm trong [tài liệu trực tuyến sau đây](https://forcats.tidyverse.org/reference/fct_reorder.html).

```{r, warning=F, message=F}
epidemic_data <- linelist %>%         # begin with the linelist   
    filter(date_onset < as.Date("2014-09-21")) %>%    # cut-off date, for visual clarity
    count(                                            # get case counts per week and by hospital
      epiweek = lubridate::floor_date(date_onset, "week"),  
      hospital                                            
    ) 
  
ggplot(data = epidemic_data)+                       # start plot
  geom_line(                                        # make lines
    aes(
      x = epiweek,                                  # x-axis epiweek
      y = n,                                        # height is number of cases per week
      color = fct_reorder2(hospital, epiweek, n)))+ # data grouped and colored by hospital, with factor order by height at end of plot
  labs(title = "Factor levels (and legend display) by line height at end of plot",
       color = "Hospital")                          # change legend title
```

## Giá trị Missing {#fct_missing}

Nếu có giá trị missing `NA` trong biến factor của bạn, bạn có thể dễ dàng chuyển đổi chúng thành một thứ bậc được đặt tên với hàm `fct_explicit_na()`. Giá trị missing `NA` được chuyển đổi thành "(Missing)" mặc định sẽ được xếp cuối cùng. Bạn có thể điều chỉnh tên thứ bậc bằng đối số `na_level =`.

Ví dụ dưới đây được thực hiện trên biến `delay_cat` và một bảng được in bằng`tabyl()`với các giá trị missing `NA` được chuyển thành "Missing delay".

```{r}
linelist %>% 
  mutate(delay_cat = fct_explicit_na(delay_cat, na_level = "Missing delay")) %>% 
  tabyl(delay_cat)
```

## Kết hợp các thứ bậc trong biến factor

### Kết hợp thủ công {.unnumbered}

Bạn có thể điều chỉnh cách hiển thị của các thứ bậc theo cách thủ công với hàm `fct_recode()`. Điều này giống như hàm `recode()` trong package **dplyr** (xem thêm tại chương [Làm sạch số liệu và các hàm quan trọng]), nhưng nó cho phép tạo các thứ bậc mới trong factor. Nếu bạn đơn giản chỉ sử dụng hàm `recode()` trên một factor, các giá trị được mã hóa mới sẽ bị từ chối trừ khi chúng đã được đặt ở thứ bậc cho phép.

Công cụ này cũng có thể được sử dụng để "kết hợp" các thứ bậc trong factor, bằng cách gán cho nhiều thứ bậc cùng một giá trị được mã hóa lại. Bạn cần cẩn thận để không bị mất thông tin! Cân nhắc thực hiện các bước kết hợp này trong một biến mới (không ghi đè lên biến hiện tại).

Hàm `fct_recode()` có cú pháp khác với hàm `recode()`. Hàm `recode()` sử dụng câu lệnh `OLD = NEW`, trong khi hàm `fct_recode()` sửu dụng câu lệnh `NEW = OLD`.

Những thứ bậc sẵn có của biến `delay_cat` như sau:

```{r, echo=F}
linelist <- linelist %>% 
  mutate(delay_cat = fct_relevel(delay_cat, "<2 days", after = 0))
```

```{r}
levels(linelist$delay_cat)
```

Để tạo một thứ bậc mới, bạn sử dụng câu lệnh sau `fct_recode(column, "new" = "old", "new" = "old", "new" = "old")` và in ra như sau:

```{r}
linelist %>% 
  mutate(delay_cat = fct_recode(
    delay_cat,
    "Less than 2 days" = "<2 days",
    "2 to 5 days"      = "2-5 days",
    "More than 5 days" = ">5 days")) %>% 
  tabyl(delay_cat)
```

Ở đây các thứ bậc cũ được kết hợp theo cách thủ công với `fct_recode()`. Lưu ý rằng không có lỗi phát sinh khi tạo thứ bậc mới "Less tham 5 days".

```{r, warning=F, message=F}
linelist %>% 
  mutate(delay_cat = fct_recode(
    delay_cat,
    "Less than 5 days" = "<2 days",
    "Less than 5 days" = "2-5 days",
    "More than 5 days" = ">5 days")) %>% 
  tabyl(delay_cat)
```

### Rút gọn thành "Other" {.unnumbered}

Bạn có thể sử dụng hàm `fct_other()` để gán các thứ bậc của factor theo cách thủ công cho thứ bậc "Other". Dưới đây, tất cả các thứ bậc trong biến `hospital`, ngoại trừ "Port Hospital" và "Central Hospital", được gộp chung thành "Other". Bạn có thể cung cấp một vectơ để giữ `keep =`, hoặc loại bỏ `drop =`. Bạn có thể thay đổi cách hiển thị của thứ bậc "Other" bằng hàm `other_level =`.

```{r}
linelist %>%    
  mutate(hospital = fct_other(                      # adjust levels
    hospital,
    keep = c("Port Hospital", "Central Hospital"),  # keep these separate
    other_level = "Other Hospital")) %>%            # All others as "Other Hospital"
  tabyl(hospital)                                   # print table

```

### Rút gọn theo tần suất {.unnumbered}

Bạn có thể tự động kết hợp các thứ bậc trong biến factor có tần suất ít nhất bằng cách sử dụng hàm `fct_lump()`.

Để "gộp" nhiều giá trị tần suất thấp lại thành một nhóm khác "Other", hãy thực hiện một trong các thao tác sau:

-   Đặt `n =` là số nhóm bạn muốn giữ. n thứ bậc có tần suất nhiều nhất sẽ được giữ nguyên và tất cả các cấp độ khác sẽ kết hợp thành nhóm "Other".
-   Đặt `prop =` là ngưỡng tỷ lệ cho các thứ bậc bạn muốn giữ ở trên. Tất cả các giá trị khác sẽ kết hợp thành nhóm "Other".

Bạn có thể thay đổi cách hiển thị của thứ bậc "Other" bằng hàm `other_level =`. Dưới đây, tất cả các giá trị ngoài hai bệnh viện phổ biến nhất đều được kết hợp thành nhóm "Other Hospital".

```{r, warning=F, message=F}
linelist %>%    
  mutate(hospital = fct_lump(                      # adjust levels
    hospital,
    n = 2,                                          # keep top 2 levels
    other_level = "Other Hospital")) %>%            # all others as "Other Hospital"
  tabyl(hospital)                                   # print table

```

## Hiển thị tất cả thứ bậc

Một lợi ích của việc sử dụng factors là chuẩn hóa sự xuất hiện của các chú thích trong biểu đồ và bảng, bất kể giá trị nào thực sự có trong tập dữ liệu.

Nếu bạn đang chuẩn bị nhiều bảng biểu (ví dụ: cho nhiều khu vực pháp lý), bạn sẽ muốn các chú giải và bảng xuất hiện giống hệt nhau ngay cả với các mức độ hoàn thành dữ liệu hoặc thành phần dữ liệu khác nhau.

### Trong biểu đồ {.unnumbered}

Trong hàm vẽ biểu đồ `ggplot ()`, chỉ cần thêm đối số `drop = FALSE` trong hàm liên quan `scale_xxxx()`. Tất cả các thứ bậc trong biến factor sẽ được hiển thị, bất kể chúng có trong dữ liệu hay không. Nếu các thứ bậc trong biến factor của bạn được hiển thị bằng cách sử dụng `fill =`, thì trong `scale_fill_discrete ()`, bạn cần thêm `drop = FALSE`, như được trình bày bên dưới. Nếu các thứ bậc trong biến factor của bạn được hiển thị với `x =` (đến trục x) `color =` hoặc `size =`, bạn sẽ cung cấp chúng tới `scale_color_discrete()` hoặc `scale_size_discrete()`.

Ví dụ này là một biểu đồ cột chồng của nhóm tuổi, theo bệnh viện. Việc thêm `scale_fill_discrete (drop = FALSE)` đảm bảo rằng tất cả các nhóm tuổi đều xuất hiện trong chú giải, ngay cả khi không có trong dữ liệu.

```{r}
ggplot(data = linelist)+
  geom_bar(mapping = aes(x = hospital, fill = age_cat)) +
  scale_fill_discrete(drop = FALSE)+                        # show all age groups in the legend, even those not present
  labs(
    title = "All age groups will appear in legend, even if not present in data")
```

### Trong bảng {.unnumbered}

Cả hàm `table()` trong **base** R và hàm `tabyl()` trong package **janitor** đều sẽ hiển thị tất cả các thứ bậc trong factor (ngay cả các thứ bậc không sử dụng).

Nếu bạn sử dụng hàm `count()` hoặc `summarise()` từ package **dplyr** để tạo bảng, hãy thêm đối số `.drop = FALSE` để hiển thị số lượng cho tất cả các thứ bậc trong factor ngay cả các thứ bậc không sử dụng.

Đọc thêm tại chương [Bảng mô tả],hoặc tại các link sau [scale_discrete documentation](https://ggplot2.tidyverse.org/reference/scale_discrete.html), hoặc [count() documentation](https://dplyr.tidyverse.org/reference/count.html). Bạn có thể tìm các ví dụ khác tại chương [Truy vết tiếp xúc].

## Tuần dịch tễ

Vui lòng xem phần thảo luận đầy đủ về cách tạo các tuần dịch tễ trong chương [Nhóm dữ liệu].\
Vui lòng xem chương [Làm việc với ngày tháng] để biết các mẹo về cách tạo và định dạng các tuần dịch tễ học.

### Tuần dịch tễ trong biểu đồ {.unnumbered}

Nếu mục tiêu của bạn là tạo các tuần dịch tễ học để hiển thị trong một biểu đồ, bạn có thể thực hiện việc này đơn giản với hàm `floor_date()` trong package **lubridate**, như được giải thích trong chương [Nhóm dữ liệu]. Các giá trị trả về sẽ thuộc loại Ngày tháng với định dạng YYYY-MM-DD. Nếu bạn sử dụng cột này trong một biểu đồ, ngày tháng sẽ tự nhiên được sắp xếp chính xác và bạn không cần phải lo lắng về thứ bậc hoặc chuyển đổi sang dạng factor. Xem biểu đồ histogram trong hàm `ggplot()` về các ngày khởi phát bên dưới.

Trong cách tiếp cận này, bạn có thể điều chỉnh *việc hiển thị* của ngày tháng trên trục với `scale_x_date()`. Xem thêm tại trang [Đường cong dịch bệnh] để biết thêm thông tin. Bạn có thể chỉ định định dạng hiển thị `date_labels =` cho đối số `scale_x_date()`. Sử dụng "% Y" để đại diện cho năm có 4 chữ số và "% W" hoặc "% U" để đại diện cho số tuần (các tuần thứ Hai hoặc Chủ Nhật tương ứng).

```{r, warning=F, message=F}
linelist %>% 
  mutate(epiweek_date = floor_date(date_onset, "week")) %>%  # create week column
  ggplot()+                                                  # begin ggplot
  geom_histogram(mapping = aes(x = epiweek_date))+           # histogram of date of onset
  scale_x_date(date_labels = "%Y-W%W")                       # adjust disply of dates to be YYYY-WWw
```

### Tuần dịch tễ trong dữ liệu {.unnumbered}

Tuy nhiên, nếu mục đích của bạn với biến factor *không phải* để lập biểu đồ, bạn có thể tiếp cận theo một trong hai cách:

1)  *Để kiểm soát tốt việc hiển thị*, hãy chuyển đổi cột **lubridate** tuần dịch tễ (YYYY-MM-DD) sang định dạng hiển thị mong muốn (YYYY-WWw) *trong chính data frame*, rồi chuyển đổi nó thành dạng factor.

Đầu tiên, sử dụng hàm `format()` từ **base** R để chuyển đổi hiển thị ngày từ hiển thị YYYY-MM-DD sang hiển thị YYYY-Www (xem thêm tại chương [Làm việc với ngày tháng]). Trong quá trình này, kiểu của biến số sẽ được chuyển đổi thành ký tự. Sau đó, chuyển đổi từ kiểu ký tự sang kiểu Factor với hàm `factor()`.

```{r}
linelist <- linelist %>% 
  mutate(epiweek_date = floor_date(date_onset, "week"),       # create epiweeks (YYYY-MM-DD)
         epiweek_formatted = format(epiweek_date, "%Y-W%W"),  # Convert to display (YYYY-WWw)
         epiweek_formatted = factor(epiweek_formatted))       # Convert to factor

# Display levels
levels(linelist$epiweek_formatted)
```

[**Nguy hiểm:** Nếu bạn đặt các tuần trước các năm ("Www-YYYY") ("%W-%Y"), thứ tự các thứ bậc sẽ sắp xếp mặc định theo bảng chữ cái và điều này là không chính xác (ví dụ: 01-2015 sẽ là trước 35-2014). Bạn có thể cần phải điều chỉnh thứ tự theo cách thủ công, đây sẽ là một quá trình dài khó khăn .]{style="color: red;"}

2)  *Để hiển thị nhanh theo mặc định*, sử dụng hàm `date2week()` trong package **aweek**. Bạn có thể đặt ngày theo hàm `week_start =`, và nếu bạn đặt đối số `factor = TRUE` thì cột đầu ra là một factor có thứ tự. Factor sẽ bao gồm các thứ bậc cho *tất cả* các tuần có thể có trong khoảng thời gian - ngay cả khi không có trường hợp nào xuất hiện trong tuần đó.

```{r, eval=F}
df <- linelist %>% 
  mutate(epiweek = date2week(date_onset, week_start = "Monday", factor = TRUE))

levels(df$epiweek)
```

Xem chương [Làm việc với ngày tháng] để biết thêm thông tin về package **aweek**. Nó cũng cung cấp thông tin về hàm đảo ngược `week2date()`.

<!-- ======================================================= -->

## Nguồn tham khảo

R trong Khoa học dữ liệu, chương [factors](https://r4ds.had.co.nz/factors.html)\
[aweek package vignette](https://cran.r-project.org/web/packages/aweek/vignettes/introduction.html)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/factors.Rmd-->


<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
# Xoay trục dữ liệu {#pivoting}

```{r, warning=F, message=F, out.height = c('50%'), fig.align="center", fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "pivoting", "Pivoting_500x500.png"))

#knitr::include_graphics(here::here("images", "pivoting", "pivot_longer_new.png"))
#knitr::include_graphics(here::here("images", "pivoting", "pivot_bar.png"))
#knitr::include_graphics(here::here("images", "pivoting", "pivot_wider_new.png"))
```



When managing data, *pivoting* can be understood to refer to one of two processes:  

1. The creation of *pivot tables*, which are tables of statistics that summarise the data of a more extensive table  
2. The conversion of a table from **long** to **wide** format, or vice versa. 

**In this page, we will focus on the latter definition.** The former is a crucial step in data analysis, and is covered elsewhere in the [Grouping data] and [Descriptive tables] pages. 

This page discusses the formats of data. It is useful to be aware of the idea of "tidy data", in which each variable has it's own column, each observation has it's own row, and each value has it's own cell. More about this topic can be found [at this online chapter in R for Data Science](https://r4ds.had.co.nz/tidy-data.html). 





## Preparation  

### Load packages {.unnumbered}  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  rio,          # File import
  here,         # File locator
  tidyverse)    # data management + ggplot2 graphics
```



### Import data {.unnumbered}


### Malaria count data {-}  

In this page, we will use a fictional dataset of daily malaria cases, by facility and age group. If you want to follow along, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/malaria_facility_count_data.rds' class='download-button'>click here to download (as .rds file)<span></a>. Import data with the `import()` function from the **rio** package (it handles many file types like .xlsx, .csv, .rds - see the [Import and export] page for details).  

```{r, echo=F}
count_data <- rio::import(here::here("data", "malaria_facility_count_data.rds")) %>% 
  as_tibble()
```

```{r, eval=F}
# Import data
count_data <- import("malaria_facility_count_data.rds")
```

The first 50 rows are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(count_data, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


### Linelist case data {-}  

In the later part of this page, we will also use the dataset of cases from a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Import your data with the `import()` function from the **rio** package (it accepts many file types like .xlsx, .rds, .csv - see the [Import and export] page for details).  

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```


```{r, eval=F}
# import your dataset
linelist <- import("linelist_cleaned.xlsx")
```







<!-- ======================================================= -->
## Wide-to-long {}

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "pivoting", "pivot_longer_new.png"))
```


<!-- ======================================================= -->
### "Wide" format {.unnumbered}

Data are often entered and stored in a "wide" format - where a subject's characteristics or responses are stored in a single row. While this may be useful for presentation, it is not ideal for some types of analysis.  

Let us take the `count_data` dataset imported in the Preparation section above as an example. You can see that each row represents a "facility-day". The actual case counts (the right-most columns) are stored in a "wide" format such that the information for every age group on a given facility-day is stored in a single row.  

```{r, echo=F}
DT::datatable(count_data, rownames = FALSE, options = list(pageLength = 5, scrollX=T) )
```

Each observation in this dataset refers to the malaria counts at one of 65 facilities on a given date, ranging from ` count_data$data_date %>% min()` to ` count_data$data_date %>% max()`. These facilities are located in one `Province` (North) and four `District`s (Spring, Bolo, Dingo, and Barnard). The dataset provides the overall counts of malaria, as well as age-specific counts in each of three age groups - <4 years, 5-14 years, and 15 years and older.

"Wide" data like this are not adhering to "tidy data" standards, because the column headers do not actually represent "variables" - they represent *values* of a hypothetical "age group" variable. 


This format can be useful for presenting the information in a table, or for entering data (e.g. in Excel) from case report forms. However, in the analysis stage, these data typically should be transformed to a "longer" format more aligned with "tidy data" standards. The plotting R package **ggplot2** in particular works best when data are in a "long" format.  


Visualising the *total* malaria counts over time poses no difficulty with the data in it's current format:

```{r, warning=F, message=F}
ggplot(count_data) +
  geom_col(aes(x = data_date, y = malaria_tot), width = 1)
```

However, what if we wanted to display the relative contributions of each age group to this total count? In this case, we need to ensure that the variable of interest (age group), appears in the dataset in a single column that can be passed to `{ggplot2}`'s "mapping aesthetics" `aes()` argument.




<!-- ======================================================= -->
### `pivot_longer()` {.unnumbered}

The **tidyr** function `pivot_longer()` makes data "longer". **tidyr** is part of the **tidyverse** of R packages.  

It accepts a range of columns to transform (specified to `cols = `). Therefore, it can operate on only a part of a dataset. This is useful for the malaria data, as we only want to pivot the case count columns.  

In this process, you will end up with two "new" columns - one with the categories (the former column names), and one with the corresponding values (e.g. case counts). You can accept the default names for these new columns, or you can specify your own to `names_to = ` and `values_to = ` respectively.  

Let's see `pivot_longer()` in action... 



### Standard pivoting {.unnumbered}  

We want to use **tidyr**'s `pivot_longer()` function to convert the "wide" data to a "long" format. Specifically, to convert the four numeric columns with data on malaria counts to two new columns: one which holds the *age groups* and one which holds the corresponding *values*.  

```{r, eval=F}
df_long <- count_data %>% 
  pivot_longer(
    cols = c(`malaria_rdt_0-4`, `malaria_rdt_5-14`, `malaria_rdt_15`, `malaria_tot`)
  )

df_long
```

Notice that the newly created data frame (`df_long`) has more rows (12,152 vs 3,038); it has become *longer*. In fact, it is precisely four times as long, because each row in the original dataset now represents four rows in df_long, one for each of the malaria count observations (<4y, 5-14y, 15y+, and total).

In addition to becoming longer, the new dataset has fewer columns (8 vs 10), as the data previously stored in four columns (those beginning with the prefix `malaria_`) is now stored in two. 

Since the names of these four columns all begin with the prefix `malaria_`, we could have made use of the handy "tidyselect" function `starts_with()` to achieve the same result (see the page [Cleaning data and core functions] for more of these helper functions).  

```{r}
# provide column with a tidyselect helper function
count_data %>% 
  pivot_longer(
    cols = starts_with("malaria_")
  )
```

or by position: 

```{r, eval=F}
# provide columns by position
count_data %>% 
  pivot_longer(
    cols = 6:9
  )
```

or by named range:

```{r, eval=F}
# provide range of consecutive columns
count_data %>% 
  pivot_longer(
    cols = `malaria_rdt_0-4`:malaria_tot
  )
```



These two new columns are given the default names of `name` and `value`, but we can override these defaults to provide more meaningful names, which can help remember what is stored within, using the `names_to` and `values_to` arguments. Let's use the names `age_group` and `counts`:

```{r}
df_long <- 
  count_data %>% 
  pivot_longer(
    cols = starts_with("malaria_"),
    names_to = "age_group",
    values_to = "counts"
  )

df_long
```

We can now pass this new dataset to `{ggplot2}`, and map the new column `count` to the y-axis and new column `age_group` to the `fill = ` argument (the column internal color). This will display the malaria counts in a stacked bar chart, by age group:

```{r, warning=F, message=F}
ggplot(data = df_long) +
  geom_col(
    mapping = aes(x = data_date, y = counts, fill = age_group),
    width = 1
  )
```

Examine this new plot, and compare it with the plot we created earlier - *what has gone wrong?*  

We have encountered a common problem when wrangling surveillance data - we have also included the total counts from the `malaria_tot` column, so the magnitude of each bar in the plot is twice as high as it should be. 

We can handle this in a number of ways. We could simply filter these totals from the dataset before we pass it to `ggplot()`:

```{r, warning=F, message=F}
df_long %>% 
  filter(age_group != "malaria_tot") %>% 
  ggplot() +
  geom_col(
    aes(x = data_date, y = counts, fill = age_group),
    width = 1
  )
```

Alternatively, we could have excluded this variable when we ran `pivot_longer()`, thereby maintaining it in the dataset as a separate variable. See how its values "expand" to fill the new rows. 

```{r, warning=F, message=F}
count_data %>% 
  pivot_longer(
    cols = `malaria_rdt_0-4`:malaria_rdt_15,   # does not include the totals column
    names_to = "age_group",
    values_to = "counts"
  )
```





### Pivoting data of multiple classes {.unnumbered}

The above example works well in situations in which all the columns you want to "pivot longer" are of the same class (character, numeric, logical...). 

However, there will be many cases when, as a field epidemiologist, you will be working with data that was prepared by non-specialists and which follow their own non-standard logic - as Hadley Wickham noted (referencing Tolstoy) in his [seminal article](https://vita.had.co.nz/papers/tidy-data.pdf) on **Tidy Data** principles: "Like families, tidy datasets are all alike but every messy dataset is messy in its own way."

One particularly common problem you will encounter will be the need to pivot columns that contain different classes of data. This pivot will result in storing these different data types in a single column, which is not a good situation. There are various approaches one can take to separate out the mess this creates, but there is an important step you can take using `pivot_longer()` to avoid creating such a situation yourself.

Take a situation in which there have been a series of observations at different time steps for each of three items A, B and C. Examples of such items could be individuals (e.g. contacts of an Ebola case being traced each day for 21 days) or remote village health posts being monitored once per year to ensure they are still functional. Let's use the contact tracing example. Imagine that the data are stored as follows:


```{r, message=FALSE, echo=F}

df <- 
  tibble::tribble(
     ~id,   ~obs1_date, ~obs1_status,   ~obs2_date, ~obs2_status,   ~obs3_date, ~obs3_status,
     "A", "2021-04-23",    "Healthy", "2021-04-24",    "Healthy", "2021-04-25",     "Unwell",
     "B", "2021-04-23",    "Healthy", "2021-04-24",    "Healthy", "2021-04-25",    "Healthy",
     "C", "2021-04-23",    "Missing", "2021-04-24",    "Healthy", "2021-04-25",    "Healthy"
     ) 

DT::datatable(df, rownames = FALSE)

```

As can be seen, the data are a bit complicated. Each row stores information about one item, but with the time series running further and further away to the right as time progresses. Moreover, the column classes alternate between date and character values.  

One particularly bad example of this encountered by this author involved cholera surveillance data, in which 8 new columns of observations were added *each day* over the course of __4 years__. Simply opening the Excel file in which these data were stored took >10 minuntes on my laptop!

In order to work with these data, we need to transform the data frame to long format, but keeping the separation between a `date` column and a `character` (status) column, for each observation for each item. If we don't, we might end up with a mixture of variable types in a single column (a very big "no-no" when it comes to data management and tidy data):

```{r}
df %>% 
  pivot_longer(
    cols = -id,
    names_to = c("observation")
  )

```

Above, our pivot has merged *dates* and *characters* into a single `value` column. R will react by converting the entire column to class character, and the utility of the dates is lost.  

To prevent this situation, we can take advantage of the syntax structure of the original column names. There is a common naming structure, with the observation number, an underscore, and then either "status" or "date". We can leverage this syntax to keep these two data types in separate columns after the pivot. 

We do this by:  

* Providing a character vector to the `names_to = ` argument, with the second item being (`".value"` ). This special term indicates that the pivoted columns will be split based on a character in their name...  
* You must also provide the "splitting" character to the `names_sep = ` argument. In this case, it is the underscore "_".  

Thus, the naming and split of new columns is based around the underscore in the existing variable names.  

```{r}

df_long <- 
  df %>% 
  pivot_longer(
    cols = -id,
    names_to = c("observation", ".value"),
    names_sep = "_"
  )

df_long

```

__Finishing touches__:

Note that the `date` column is currently in *character* class - we can easily convert this into it's proper date class using the `mutate()` and `as_date()` functions described in the [Working with dates] page.  

We may also want to convert the `observation` column to a `numeric` format by dropping the "obs" prefix and converting to numeric. We cando this with `str_remove_all()` from the **stringr** package (see the [Characters and strings] page).  

```{r}

df_long <- 
  df_long %>% 
  mutate(
    date = date %>% lubridate::as_date(),
    observation = 
      observation %>% 
      str_remove_all("obs") %>% 
      as.numeric()
  )

df_long

```

And now, we can start to work with the data in this format, e.g. by plotting a descriptive heat tile:  

```{r}
ggplot(data = df_long, mapping = aes(x = date, y = id, fill = status)) +
  geom_tile(colour = "black") +
  scale_fill_manual(
    values = 
      c("Healthy" = "lightgreen", 
        "Unwell" = "red", 
        "Missing" = "orange")
  )

```





<!-- ======================================================= -->
## Long-to-wide {}

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "pivoting", "pivot_wider_new.png"))
```


In some instances, we may wish to convert a dataset to a wider format. For this, we can use the `pivot_wider()` function.

A typical use-case is when we want to transform the results of an analysis into a format which is more digestible for the reader (such as a [Table for presentation][Tables for presentation]). Usually, this involves transforming a dataset in which information for one subject is are spread over multiple rows into a format in which that information is stored in a single row.

### Data {.unnumbered}

For this section of the page, we will use the case linelist (see the [Preparation](#pivot_prep) section), which contains one row per case.  

Here are the first 50 rows:  

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


Suppose that we want to know the counts of individuals in the different age groups, by gender:

```{r}
df_wide <- 
  linelist %>% 
  count(age_cat, gender)

df_wide
```

This gives us a long dataset that is great for producing visualisations in **ggplot2**, but not ideal for presentation in a table:

```{r}
ggplot(df_wide) +
  geom_col(aes(x = age_cat, y = n, fill = gender))
```

### Pivot wider {.unnumbered}  

Therefore, we can use `pivot_wider()` to transform the data into a better format for inclusion as tables in our reports.  

The argument `names_from` specifies the column *from* which to generate the new column *names*, while the argument `values_from` specifies the column *from* which to take the *values* to populate the cells. The argument `id_cols = ` is optional, but can be provided a vector of column names that should not be pivoted, and will thus identify each row.  

```{r}
table_wide <- 
  df_wide %>% 
  pivot_wider(
    id_cols = age_cat,
    names_from = gender,
    values_from = n
  )

table_wide
```

This table is much more reader-friendly, and therefore better for inclusion in our reports. You can convert into a pretty table with several packages including **flextable** and **knitr**. This process is elaborated in the page [Tables for presentation].  

```{r}
table_wide %>% 
  janitor::adorn_totals(c("row", "col")) %>% # adds row and column totals
  knitr::kable() %>% 
  kableExtra::row_spec(row = 10, bold = TRUE) %>% 
  kableExtra::column_spec(column = 5, bold = TRUE) 
```

---


<!-- ======================================================= -->
## Fill 

In some situations after a `pivot`, and more commonly after a `bind`, we are left with gaps in some cells that we would like to fill.  

<!-- ======================================================= -->
### Data {.unnumbered}

For example, take two datasets, each with observations for the measurement number, the name of the facility, and the case count at that time. However, the second dataset also has a variable `Year`. 

```{r}
df1 <- 
  tibble::tribble(
       ~Measurement, ~Facility, ~Cases,
                  1,  "Hosp 1",     66,
                  2,  "Hosp 1",     26,
                  3,  "Hosp 1",      8,
                  1,  "Hosp 2",     71,
                  2,  "Hosp 2",     62,
                  3,  "Hosp 2",     70,
                  1,  "Hosp 3",     47,
                  2,  "Hosp 3",     70,
                  3,  "Hosp 3",     38,
       )

df1 

df2 <- 
  tibble::tribble(
    ~Year, ~Measurement, ~Facility, ~Cases,
     2000,            1,  "Hosp 4",     82,
     2001,            2,  "Hosp 4",     87,
     2002,            3,  "Hosp 4",     46
  )

df2
```


When we perform a `bind_rows()` to join the two datasets together, the `Year` variable is filled with `NA` for those rows where there was no prior information (i.e. the first dataset):


```{r}
df_combined <- 
  bind_rows(df1, df2) %>% 
  arrange(Measurement, Facility)

df_combined

```

<!-- ======================================================= -->
### `fill()` {.unnumbered}

In this case, `Year` is a useful variable to include, particularly if we want to explore trends over time. Therefore, we use `fill()` to *fill* in those empty cells, by specifying the column to fill and the direction (in this case **up**):

```{r}
df_combined %>% 
  fill(Year, .direction = "up")
```

Alternatively, we can rearrange the data so that we would need to fill in a downward direction:

```{r}
df_combined <- 
  df_combined %>% 
  arrange(Measurement, desc(Facility))

df_combined

df_combined <- 
  df_combined %>% 
  fill(Year, .direction = "down")

df_combined
```

We now have a useful dataset for plotting:

```{r}
ggplot(df_combined) +
  aes(Year, Cases, fill = Facility) +
  geom_col()
```

But less useful for presenting in a table, so let's practice converting this long, untidy dataframe into a wider, tidy dataframe:

```{r}
df_combined %>% 
  pivot_wider(
    id_cols = c(Facility, Year, Cases),
    names_from = "Year",
    values_from = "Cases"
  ) %>% 
  arrange(Facility) %>% 
  janitor::adorn_totals(c("row", "col")) %>% 
  knitr::kable() %>% 
  kableExtra::row_spec(row = 5, bold = TRUE) %>% 
  kableExtra::column_spec(column = 5, bold = TRUE) 
```

N.B. In this case, we had to specify to only include the three variables `Facility`, `Year`, and `Cases` as the additional variable `Measurement` would interfere with the creation of the table:

```{r}
df_combined %>% 
  pivot_wider(
    names_from = "Year",
    values_from = "Cases"
  ) %>% 
  knitr::kable()
```

## Resources  

Here is a helpful [tutorial](https://datacarpentry.org/r-socialsci/03-dplyr-tidyr/index.html)

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/pivoting.Rmd-->


# Nhóm dữ liệu {#grouping}  


```{r, out.width=c('100%'), echo=F, message=F}
knitr::include_graphics(here::here("images", "Grouping_1500x500.png"))
```


This page covers how to group and aggregate data for descriptive analysis. It makes use of the **tidyverse** family of packages for common and easy-to-use functions. 


Grouping data is a core component of data management and analysis. Grouped data statistically summarised by group, and can be plotted by group. Functions from the **dplyr** package (part of the **tidyverse**) make grouping and subsequent operations quite easy.  

This page will address the following topics:  

* Group data with the `group_by()` function  
* Un-group data  
* `summarise()` grouped data with statistics  
* The difference between `count()` and `tally()`  
* `arrange()` applied to grouped data  
* `filter()` applied to grouped data  
* `mutate()` applied to grouped data  
* `select()` applied to grouped data  
* The **base** R `aggregate()` command as an alternative  




<!-- ======================================================= -->
## Preparation {  }
     
### Load packages {.unnumbered}  
     
This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  


```{r}
pacman::p_load(
  rio,       # to import data
  here,      # to locate files
  tidyverse, # to clean, handle, and plot the data (includes dplyr)
  janitor)   # adding total rows and columns
```




### Import data {.unnumbered}

We import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). The dataset is imported using the `import()` function from the **rio** package. See the page on [Import and export] for various ways to import data.

```{r, echo=F}
linelist <- rio::import(here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
linelist <- import("linelist_cleaned.rds")
```


The first 50 rows of `linelist`:  

```{r message=FALSE, echo=F}
DT::datatable(head(linelist,50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



<!-- ======================================================= -->
## Grouping {  }
     
The function `group_by()` from **dplyr** groups the rows by the unique values in the column specified to it. If multiple columns are specified, rows are grouped by the unique combinations of values across the columns. Each unique value (or combination of values) constitutes a group. Subsequent changes to the dataset or calculations can then be performed within the context of each group.  

For example, the command below takes the `linelist` and groups the rows by unique values in the column `outcome`, saving the output as a new data frame `ll_by_outcome`. The grouping column(s) are placed inside the parentheses of the function `group_by()`.  

```{r}
ll_by_outcome <- linelist %>% 
  group_by(outcome)
```

**Note that there is no perceptible change to the dataset** after running `group_by()`, *until* another **dplyr** verb such as `mutate()`, `summarise()`, or `arrange()` is applied on the "grouped" data frame.  

You can however "see" the groupings by printing the data frame. When you print a grouped data frame, you will see it has been transformed into a [`tibble` class object](https://tibble.tidyverse.org/) which, when printed, displays which groupings have been applied and how many groups there are - written just above the header row.  

```{r}
# print to see which groups are active
ll_by_outcome
```


### Unique groups {.unnumbered}  

**The groups created reflect each unique combination of values across the grouping columns.** 

To see the groups *and the number of rows in each group*, pass the grouped data to `tally()`. To see just the unique groups without counts you can pass to `group_keys()`.  

See below that there are **three** unique values in the grouping column `outcome`: "Death", "Recover", and `NA`. See that there were ` nrow(linelist %>% filter(outcome == "Death"))` deaths, ` nrow(linelist %>% filter(outcome == "Recover"))` recoveries, and ` nrow(linelist %>% filter(is.na(outcome)))` with no outcome recorded.

```{r}
linelist %>% 
  group_by(outcome) %>% 
  tally()
```


You can group by more than one column. Below, the data frame is grouped by `outcome` and `gender`, and then tallied. Note how each unique combination of `outcome` and `gender` is registered as its own group - including missing values for either column.   

```{r}
linelist %>% 
  group_by(outcome, gender) %>% 
  tally()
```

### New columns {.unnumbered} 

You can also create a new grouping column *within* the `group_by()` statement. This is equivalent to calling `mutate()` before the `group_by()`. For a quick tabulation this style can be handy, but for more clarity in your code consider creating this column in its own `mutate()` step and then piping to `group_by()`.

```{r}
# group dat based on a binary column created *within* the group_by() command
linelist %>% 
  group_by(
    age_class = ifelse(age >= 18, "adult", "child")) %>% 
  tally(sort = T)
```

### Add/drop grouping columns {.unnumbered}  

By default, if you run `group_by()` on data that are already grouped, the old groups will be removed and the new one(s) will apply. If you want to add new groups to the existing ones, include the argument `.add = TRUE`.  

````{r, eval=F}
# Grouped by outcome
by_outcome <- linelist %>% 
  group_by(outcome)

# Add grouping by gender in addition
by_outcome_gender <- by_outcome %>% 
  group_by(gender, .add = TRUE)
```


** Keep all groups**  

If you group on a column of class factor there may be levels of the factor that are not currently present in the data. If you group on this column, by default those non-present levels are dropped and not included as groups. To change this so that all levels appear as groups (even if not present in the data), set `.drop = FALSE` in your `group_by()` command.  


## Un-group  

Data that have been grouped will remain grouped until specifically ungrouped via `ungroup()`. If you forget to ungroup, it can lead to incorrect calculations! Below is an example of removing all groupings:  

```{r, eval=F}
linelist %>% 
  group_by(outcome, gender) %>% 
  tally() %>% 
  ungroup()
```

You can also remove grouping for only specific columns, by placing the column name inside `ungroup()`.  

```{r, eval=F}
linelist %>% 
  group_by(outcome, gender) %>% 
  tally() %>% 
  ungroup(gender) # remove the grouping by gender, leave grouping by outcome
```


<span style="color: black;">**_NOTE:_** The verb `count()` automatically ungroups the data after counting.</span>



## Summarise {#group_summarise} 

See the **dplyr** section of the [Descriptive tables] page for a detailed description of how to produce summary tables with `summarise()`. Here we briefly address how its behavior changes when applied to grouped data.  

The **dplyr** function `summarise()` (or `summarize()`) takes a data frame and converts it into a *new* summary data frame, with columns containing summary statistics that you define. On an ungrouped data frame, the summary statistics will be calculated from all rows. Applying `summarise()` to grouped data produces those summary statistics *for each group*.  

The syntax of `summarise()` is such that you provide the name(s) of the **new** summary column(s), an equals sign, and then a statistical function to apply to the data, as shown below. For example, `min()`, `max()`, `median()`, or `sd()`. Within the statistical function, list the column to be operated on and any relevant argument (e.g. `na.rm = TRUE`). You can use `sum()` to count the number of rows that meet a logical criteria (with double equals `==`).   

Below is an example of `summarise()` applied *without grouped data*. The statistics returned are produced from the entire dataset.     

```{r}
# summary statistics on ungrouped linelist
linelist %>% 
  summarise(
    n_cases  = n(),
    mean_age = mean(age_years, na.rm=T),
    max_age  = max(age_years, na.rm=T),
    min_age  = min(age_years, na.rm=T),
    n_males  = sum(gender == "m", na.rm=T))
```

In contrast, below is the same `summarise()` statement applied to grouped data. The statistics are calculated for each `outcome` group. Note how grouping columns will carry over into the new data frame.    

```{r}
# summary statistics on grouped linelist
linelist %>% 
  group_by(outcome) %>% 
  summarise(
    n_cases  = n(),
    mean_age = mean(age_years, na.rm=T),
    max_age  = max(age_years, na.rm=T),
    min_age  = min(age_years, na.rm=T),
    n_males    = sum(gender == "m", na.rm=T))
```

<span style="color: darkgreen;">**_TIP:_** The summarise function works with both UK and US spelling - `summarise()` and `summarize()` call the same function.</span>




## Counts and tallies  

`count()` and `tally()` provide similar functionality but are different. Read more about the distinction between `tally()` and `count()` [here](https://dplyr.tidyverse.org/reference/tally.html)    

### `tally()` {.unnumbered}  

`tally()` is shorthand for `summarise(n = n())`, and *does not* group data. Thus, to achieve grouped tallys it must follow a `group_by()` command. You can add `sort = TRUE` to see the largest groups first.    

```{r}
linelist %>% 
  tally()
```


```{r}
linelist %>% 
  group_by(outcome) %>% 
  tally(sort = TRUE)
```


### `count()`  {.unnumbered}  

In contrast, `count()` does the following:  

1) applies `group_by()` on the specified column(s)  
2) applies `summarise()` and returns column `n` with the number of rows per group  
3) applies `ungroup()`  

```{r}
linelist %>% 
  count(outcome)
```

Just like with `group_by()` you can create a new column within the `count()` command:  

```{r}
linelist %>% 
  count(age_class = ifelse(age >= 18, "adult", "child"), sort = T)
```


`count()` can be called multiple times, with the functionality "rolling up". For example, to summarise the number of hospitals present for each gender, run the following. Note, the name of the final column is changed from default "n" for clarity (with `name  = `).  

```{r}
linelist %>% 
  # produce counts by unique outcome-gender groups
  count(gender, hospital) %>% 
  # gather rows by gender (3) and count number of hospitals per gender (6)
  count(gender, name = "hospitals per gender" ) 
```


### Add counts {.unnumbered}  

In contrast to `count()` and `summarise()`, you can use `add_count()` to *add* a new column `n` with the counts of rows per group *while retaining all the other data frame columns*.   

This means that a group's count number, in the new column `n`, will be printed in each row of the group. For demonstration purposes, we add this column and then re-arrange the columns for easier viewing. See the section below on [filter on group size](#group_filter_grp_size) for another example.  


```{r}
linelist %>% 
  as_tibble() %>%                   # convert to tibble for nicer printing 
  add_count(hospital) %>%           # add column n with counts by hospital
  select(hospital, n, everything()) # re-arrange for demo purposes
```



### Add totals {.unnumbered} 

To easily add total *sum* rows or columns after using `tally()` or `count()`, see the **janitor** section of the [Descriptive tables](#tbl_janitor) page. This package offers functions like `adorn_totals()` and `adorn_percentages()` to add totals and convert to show percentages. Below is a brief example:  

```{r}
linelist %>%                                  # case linelist
  tabyl(age_cat, gender) %>%                  # cross-tabulate counts of two columns
  adorn_totals(where = "row") %>%             # add a total row
  adorn_percentages(denominator = "col") %>%  # convert to proportions with column denominator
  adorn_pct_formatting() %>%                  # convert proportions to percents
  adorn_ns(position = "front") %>%            # display as: "count (percent)"
  adorn_title(                                # adjust titles
    row_name = "Age Category",
    col_name = "Gender")
```


To add more complex totals rows that involve summary statistics other than *sums*, see [this section of the Descriptive Tables page](#tbl_dplyr_totals).  



## Grouping by date  

When grouping data by date, you must have (or create) a column for the date unit of interest - for example "day", "epiweek", "month", etc. You can make this column using `floor_date()` from **lubridate**, as explained in the [Epidemiological weeks section](#dates_epi_wks) of the [Working with dates] page. Once you have this column, you can use `count()` from **dplyr** to group the rows by those unique date values and achieve aggregate counts. 

One additional step common for date situations, is to "fill-in" any dates in the sequence that are not present in the data. Use `complete()` from **tidyr** so that the aggregated date series is *complete* including *all possible date units* within the range. Without this step, a week with no cases reported might not appear in your data!  

Within `complete()` you *re-define* your date column as a *sequence* of dates `seq.Date()` from the minimum to the maximum  - thus the dates are expanded. By default, the case count values in any new "expanded" rows will be `NA`. You can set them to 0 using the `fill = ` argument of `complete()`, which expects a named list (if your counts column is named `n`, provide `fill = list(n = 0)`. See `?complete` for details and the [Working with dates](#dates_epi_wks) page for an example.  



### Linelist cases into days  {.unnumbered}  

Here is an example of grouping cases into days *without* using `complete()`. Note the first rows skip over dates with no cases.  

```{r}
daily_counts <- linelist %>% 
  drop_na(date_onset) %>%        # remove that were missing date_onset
  count(date_onset)              # count number of rows per unique date
```

```{r message=FALSE, echo=F}
DT::datatable(daily_counts, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Below we add the `complete()` command to ensure every day in the range is represented.

```{r, eval=F}
daily_counts <- linelist %>% 
  drop_na(date_onset) %>%                 # remove case missing date_onset
  count(date_onset) %>%                   # count number of rows per unique date
  complete(                               # ensure all days appear even if no cases
    date_onset = seq.Date(                # re-define date colume as daily sequence of dates
      from = min(date_onset, na.rm=T), 
      to = max(date_onset, na.rm=T),
      by = "day"),
    fill = list(n = 0))                   # set new filled-in rows to display 0 in column n (not NA as default) 
```

```{r message=FALSE, echo=F}
DT::datatable(daily_counts, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

### Linelist cases into weeks {.unnumbered}  


The same principle can be applied for weeks. First create a new column that is the week of the case using `floor_date()` with `unit = "week"`. Then, use `count()` as above to achieve weekly case counts. Finish with `complete()` to ensure that all weeks are represented, even if they contain no cases.

```{r}
# Make dataset of weekly case counts
weekly_counts <- linelist %>% 
  drop_na(date_onset) %>%                 # remove cases missing date_onset
  mutate(week = lubridate::floor_date(date_onset, unit = "week")) %>%  # new column of week of onset
  count(week) %>%                         # group data by week and count rows per group
  complete(                               # ensure all days appear even if no cases
    week = seq.Date(                      # re-define date colume as daily sequence of dates
      from = min(week, na.rm=T), 
      to = max(week, na.rm=T),
      by = "week"),
    fill = list(n = 0))                   # set new filled-in rows to display 0 in column n (not NA as default) 
```

Here are the first 50 rows of the resulting data frame:  

```{r message=FALSE, echo=F}
DT::datatable(weekly_counts, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

### Linelist cases into months {.unnumbered}

To aggregate cases into months, again use `floor_date()` from the **lubridate** package, but with the argument `unit = "months"`. This rounds each date down to the 1st of its month. The output will be class Date. Note that in the `complete()` step we also use `by = "months"`.  


```{r}
# Make dataset of monthly case counts
monthly_counts <- linelist %>% 
  drop_na(date_onset) %>% 
  mutate(month = lubridate::floor_date(date_onset, unit = "months")) %>%  # new column, 1st of month of onset
  count(month) %>%                          # count cases by month
  complete(
    month = seq.Date(
      min(month, na.rm=T),     # include all months with no cases reported
      max(month, na.rm=T),
      by="month"),
    fill = list(n = 0))
```

```{r message=FALSE, echo=F}
DT::datatable(monthly_counts, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


### Daily counts into weeks {.unnumbered}

To aggregate daily counts into weekly counts, use `floor_date()` as above. However, use `group_by()` and `summarize()` instead of `count()` because you need to `sum()` daily case counts instead of just counting the number of rows per week.



#### Daily counts into months {.unnumbered}

To aggregate daily counts into months counts, use `floor_date()` with `unit = "month"` as above. However, use `group_by()` and `summarize()` instead of `count()` because you need to `sum()` daily case counts instead of just counting the number of rows per month.  




## Arranging grouped data

Using the **dplyr** verb `arrange()` to order the rows in a data frame behaves the same when the data are grouped, *unless* you set the argument `.by_group =TRUE`. In this case the rows are ordered first by the grouping columns and then by any other columns you specify to `arrange()`.   



## Filter on grouped data

### `filter()` {.unnumbered}

When applied in conjunction with functions that evaluate the data frame (like `max()`, `min()`, `mean()`), these functions will now be applied to the groups. For example, if you want to filter and keep rows where patients are above the median age, this will now apply per group - filtering to keep rows above the *group's* median age. 




### Slice rows per group {.unnumbered} 

The **dplyr** function `slice()`, which [filters rows based on their position](https://dplyr.tidyverse.org/reference/slice.html) in the data, can also be applied per group. Remember to account for sorting the data within each group to get the desired "slice".  

For example, to retrieve only the latest 5 admissions from each hospital:  

1) Group the linelist by column `hospital`  
2) Arrange the records from latest to earliest `date_hospitalisation` *within each hospital group*  
3) Slice to retrieve the first 5 rows from each hospital  

```{r,}
linelist %>%
  group_by(hospital) %>%
  arrange(hospital, date_hospitalisation) %>%
  slice_head(n = 5) %>% 
  arrange(hospital) %>%                            # for display
  select(case_id, hospital, date_hospitalisation)  # for display
```

`slice_head()` - selects n rows from the top  
`slice_tail()` - selects n rows from the end  
`slice_sample()` - randomly selects n rows  
`slice_min()` - selects n rows with highest values in `order_by = ` column, use `with_ties = TRUE` to keep ties  
`slice_max()` - selects n rows with lowest values in `order_by = ` column, use `with_ties = TRUE` to keep ties  

See the [De-duplication] page for more examples and detail on `slice()`.  




### Filter on group size {#group_filter_grp_size .unnumbered} 

The function `add_count()` adds a column `n` to the original data giving the number of rows in that row's group. 

Shown below, `add_count()` is applied to the column `hospital`, so the values in the new column `n` reflect the number of rows in that row's hospital group. Note how values in column `n` are repeated. In the example below, the column name `n` could be changed using `name = ` within `add_count()`. For demonstration purposes we re-arrange the columns with `select()`.  


```{r}
linelist %>% 
  as_tibble() %>% 
  add_count(hospital) %>%          # add "number of rows admitted to same hospital as this row" 
  select(hospital, n, everything())
```

It then becomes easy to filter for case rows who were hospitalized at a "small" hospital, say, a hospital that admitted fewer than 500 patients:  

```{r, eval=F}
linelist %>% 
  add_count(hospital) %>% 
  filter(n < 500)
```





## Mutate on grouped data  

To retain all columns and rows (not summarise) and *add a new column containing group statistics*, use `mutate()` after `group_by()` instead of `summarise()`. 

This is useful if you want group statistics in the original dataset *with all other columns present* - e.g. for calculations that compare one row to its group.  

For example, this code below calculates the difference between a row's delay-to-admission and the median delay for their hospital. The steps are:  

1) Group the data by hospital  
2) Use the column `days_onset_hosp` (delay to hospitalisation) to create a new column containing the mean delay at the hospital of *that row*  
3) Calculate the difference between the two columns  

We `select()` only certain columns to display, for demonstration purposes.  

```{r}
linelist %>% 
  # group data by hospital (no change to linelist yet)
  group_by(hospital) %>% 
  
  # new columns
  mutate(
    # mean days to admission per hospital (rounded to 1 decimal)
    group_delay_admit = round(mean(days_onset_hosp, na.rm=T), 1),
    
    # difference between row's delay and mean delay at their hospital (rounded to 1 decimal)
    diff_to_group     = round(days_onset_hosp - group_delay_admit, 1)) %>%
  
  # select certain rows only - for demonstration/viewing purposes
  select(case_id, hospital, days_onset_hosp, group_delay_admit, diff_to_group)
```



## Select on grouped data  

The verb `select()` works on grouped data, but the grouping columns are always included (even if not mentioned in `select()`). If you do not want these grouping columns, use `ungroup()` first.  










<!-- ======================================================= -->
## Resources {  }

Here are some useful resources for more information:  

You can perform any summary function on grouped data; see the [RStudio data transformation cheat sheet](https://github.com/rstudio/cheatsheets/blob/master/data-transformation.pdf)  

The Data Carpentry page on [**dplyr**](https://datacarpentry.org/R-genomics/04-dplyr.html)  
The **tidyverse** reference pages on [group_by()](https://dplyr.tidyverse.org/reference/group_by.html) and [grouping](https://dplyr.tidyverse.org/articles/grouping.html)  

This page on [Data manipulation](https://itsalocke.com/files/DataManipulationinR.pdf)  

[Summarize with conditions in dplyr](https://stackoverflow.com/questions/23528862/summarize-with-conditions-in-dplyr)  






```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/grouping.Rmd-->


# Nối dữ liệu {#joining-matching}  

```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "left-join.gif"))
```

*Above: an animated example of a left join ([image source](https://github.com/gadenbuie/tidyexplain/tree/master/images))*  


This page describes ways to "join", "match", "link" "bind", and otherwise combine data frames.  

It is uncommon that your epidemiological analysis or workflow does not involve multiple sources of data, and the linkage of multiple datasets. Perhaps you need to connect laboratory data to patient clinical outcomes, or Google mobility data to infectious disease trends, or even a dataset at one stage of analysis to a transformed version of itself.

In this page we demonstrate code to:  

* Conduct *joins* of two data frames such that rows are matched based on common values in identifier columns  
* Join two data frames based on *probabilistic* (likely) matches between values  
* Expand a data frame by directly *binding* or ("appending") rows or columns from another data frame  


<!-- ======================================================= -->
## Preparation { }

### Load packages {.unnumbered}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  rio,            # import and export
  here,           # locate files 
  tidyverse,      # data management and visualisation
  RecordLinkage,  # probabilistic matches
  fastLink        # probabilistic matches
)
```



### Import data {.unnumbered}

To begin, we import the cleaned linelist of cases from a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Import data with the `import()` function from the **rio** package (it handles many file types like .xlsx, .csv, .rds - see the [Import and export] page for details).  

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import case linelist 
linelist <- import("linelist_cleaned.rds")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```




<!-- ======================================================= -->
### Example datasets {.unnumbered}

In the joining section below, we will use the following datasets:  

1) A "miniature" version of the case `linelist`, containing only the columns `case_id`, `date_onset`, and `hospital`, and only the first 10 rows  
2) A separate data frame named `hosp_info`, which contains more details about each hospital  

In the section on probabilistic matching, we will use two different small datasets. The code to create those datasets is given in that section.  




#### "Miniature" case linelist {#joins_llmini .unnumbered}  

Below is the the miniature case linelist, which contains only 10 rows and only columns `case_id`, `date_onset`, and `hospital`.  

```{r}
linelist_mini <- linelist %>%                 # start with original linelist
  select(case_id, date_onset, hospital) %>%   # select columns
  head(10)                                    # only take the first 10 rows
```

```{r message=FALSE, echo=F}
DT::datatable(linelist_mini, rownames = FALSE, options = list(pageLength = nrow(10)))
```




#### Hospital information data frame {#joins_hosp_info .unnumbered}  

Below is the code to create a separate data frame with additional information about seven hospitals (the catchment population, and the level of care available). Note that the name "Military Hospital" belongs to two different hospitals - one a primary level serving 10000 residents and the other a secondary level serving 50280 residents.  

```{r}
# Make the hospital information data frame
hosp_info = data.frame(
  hosp_name     = c("central hospital", "military", "military", "port", "St. Mark's", "ignace", "sisters"),
  catchment_pop = c(1950280, 40500, 10000, 50280, 12000, 5000, 4200),
  level         = c("Tertiary", "Secondary", "Primary", "Secondary", "Secondary", "Primary", "Primary")
)
```

Here is this data frame:  

```{r message=FALSE, echo=F}
# display the hospital data as a table
DT::datatable(hosp_info, rownames = FALSE, options = list(pageLength = nrow(hosp_info)))
```





<!-- ======================================================= -->
### Pre-cleaning {.unnumbered}

Traditional joins (non-probabilistic) are case-sensitive and require exact character matches between values in the two data frames. To demonstrate some of the cleaning steps you might need to do before initiating a join, we will clean and align the `linelist_mini` and `hosp_info` datasets now.  

**Identify differences**  

We need the values of the `hosp_name` column in the `hosp_info` data frame to match the values of the `hospital` column in the `linelist_mini` data frame.  

Here are the values in the `linelist_mini` data frame, printed with the **base** R function `unique()`:  

```{r}
unique(linelist_mini$hospital)
```

and here are the values in the `hosp_info` data frame:  

```{r}
unique(hosp_info$hosp_name)
```

You can see that while some of the hospitals exist in both data frames, there are many differences in spelling.  



**Align values**  

We begin by cleaning the values in the `hosp_info` data frame. As explained in the [Cleaning data and core functions] page, we can re-code values with logical criteria using **dplyr**'s `case_when()` function. For the four hospitals that exist in both data frames we change the values to align with the values in `linelist_mini`. The other hospitals we leave the values as they are (`TRUE ~ hosp_name`).   

<span style="color: orange;">**_CAUTION:_** Typically when cleaning one should create a new column (e.g. `hosp_name_clean`), but for ease of demonstration we show modification of the old column</span>

```{r}
hosp_info <- hosp_info %>% 
  mutate(
    hosp_name = case_when(
      # criteria                         # new value
      hosp_name == "military"          ~ "Military Hospital",
      hosp_name == "port"              ~ "Port Hospital",
      hosp_name == "St. Mark's"        ~ "St. Mark's Maternity Hospital (SMMH)",
      hosp_name == "central hospital"  ~ "Central Hospital",
      TRUE                             ~ hosp_name
      )
    )
```

The hospital names that appear in both data frames are aligned. There are two hospitals in `hosp_info` that are not present in `linelist_mini` - we will deal with these later, in the join.  

```{r}
unique(hosp_info$hosp_name)
```

Prior to a join, it is often easiest to convert a column to all lowercase or all uppercase. If you need to convert all values in a column to UPPER or lower case, use `mutate()` and wrap the column with one of these functions from **stringr**, as shown in the page on [Characters and strings].  

`str_to_upper()`  
`str_to_upper()`  
`str_to_title()`  




<!-- ======================================================= -->
## **dplyr** joins { }

The **dplyr** package offers several different join functions. **dplyr** is included in the **tidyverse** package. These join functions are described below, with simple use cases.  

Many thanks to [https://github.com/gadenbuie](https://github.com/gadenbuie/tidyexplain/tree/master/images) for the informative gifs!  




<!-- ======================================================= -->
### General syntax {.unnumbered}

The join commands can be run as standalone commands to join two data frames into a new object, or they can be used within a pipe chain (`%>%`) to merge one data frame into another as it is being cleaned or otherwise modified.  

In the example below, the function `left_join()` is used as a standalone command to create the a new `joined_data` data frame. The inputs are data frames 1 and 2 (`df1` and `df2`). The first data frame listed is the baseline data frame, and the second one listed is joined *to* it.  

The third argument `by = ` is where you specify the columns in each data frame that will be used to aligns the rows in the two data frames. If the names of these columns are different, provide them within a `c()` vector as shown below, where the rows are matched on the basis of common values between the column `ID` in `df1` and the column `identifier` in `df2`.   

```{r, eval=F}
# Join based on common values between column "ID" (first data frame) and column "identifier" (second data frame)
joined_data <- left_join(df1, df2, by = c("ID" = "identifier"))
```

If the `by` columns in both data frames have the exact same name, you can just provide this one name, within quotes.  

```{r, eval=F}
# Joint based on common values in column "ID" in both data frames
joined_data <- left_join(df1, df2, by = "ID")
```

If you are joining the data frames based on common values across multiple fields, list these fields within the `c()` vector. This example joins rows if the values in three columns in each dataset align exactly.  

```{r, eval=F}
# join based on same first name, last name, and age
joined_data <- left_join(df1, df2, by = c("name" = "firstname", "surname" = "lastname", "Age" = "age"))
```


The join commands can also be run within a pipe chain. This will modify the data frame being piped. 

In the example below, `df1` is is passed through the pipes, `df2` is joined to it, and `df` is thus modified and re-defined.  

```{r eval=F}
df1 <- df1 %>%
  filter(date_onset < as.Date("2020-03-05")) %>% # miscellaneous cleaning 
  left_join(df2, by = c("ID" = "identifier"))    # join df2 to df1
```


<span style="color: orange;">**_CAUTION:_** Joins are case-specific! Therefore it is useful to convert all values to lowercase or uppercase prior to joining. See the page on characters/strings.</span>





<!-- ======================================================= -->
### Left and right joins {.unnumbered}  

**A left or right join is commonly used to add information to a data frame** - new information is added only to rows that already existed in the baseline data frame. These are common joins in epidemiological work as they are used to add information from one dataset into another. 

In using these joins, the written order of the data frames in the command is important*.  

* In a *left join*, the *first* data frame written is the baseline  
* In a *right join*, the *second* data frame written is the baseline  

**All rows of the baseline data frame are kept.** Information in the other (secondary) data frame is joined to the baseline data frame *only if there is a match via the identifier column(s)*. In addition:  

* Rows in the secondary data frame that do not match are dropped.  
* If there are many baseline rows that match to one row in the secondary data frame (many-to-one), the secondary information is added to *each matching baseline row*.  
* If a baseline row matches to multiple rows in the secondary data frame (one-to-many), all combinations are given, meaning *new rows may be added to your returned data frame!*  

Animated examples of left and right joins ([image source](https://github.com/gadenbuie/tidyexplain/tree/master/images))

```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "left-join.gif"))
knitr::include_graphics(here::here("images", "right-join.gif"))
```

**Example**  

Below is the output of a `left_join()` of `hosp_info` (secondary data frame, [view here](#joins_hosp_info))  *into* `linelist_mini` (baseline data frame, [view here](#joins_llmini)). The original `linelist_mini` has ` nrow(linelist_mini)` rows. The modified `linelist_mini` is displayed. Note the following:  

* Two new columns, `catchment_pop` and `level` have been added on the left side of `linelist_mini`  
* All original rows of the baseline data frame `linelist_mini` are kept  
* Any original rows of `linelist_mini` for "Military Hospital" are duplicated because it matched to *two* rows in the secondary data frame, so both combinations are returned  
* The join identifier column of the secondary dataset (`hosp_name`) has disappeared because it is redundant with the identifier column in the primary dataset (`hospital`)  
* When a baseline row did not match to any secondary row (e.g. when `hospital` is "Other" or "Missing"), `NA` (blank) fills in the columns from the secondary data frame  
* Rows in the secondary data frame with no match to the baseline data frame ("sisters" and "ignace" hospitals) were dropped  


```{r, eval=F}
linelist_mini %>% 
  left_join(hosp_info, by = c("hospital" = "hosp_name"))
```

```{r message=FALSE, echo=F}
linelist_mini %>% 
  left_join(hosp_info, by = c("hospital" = "hosp_name")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 11))
```





#### "Should I use a right join, or a left join?" {.unnumbered}  

To answer the above question, ask yourself "which data frame should retain all of its rows?" - use this one as the baseline. A *left join* keep all the rows in the first data frame written in the command, whereas a *right join* keeps all the rows in the second data frame.  

The two commands below achieve the same output - 10 rows of `hosp_info` joined *into* a `linelist_mini` baseline, but they use different joins. The result is that the column order will differ based on whether `hosp_info` arrives from the right (in the left join) or arrives from the left (in the right join). The order of the rows may also shift accordingly. But both of these consequences can be subsequently addressed, using `select()` to re-order columns or `arrange()` to sort rows.  

```{r, eval=F}
# The two commands below achieve the same data, but with differently ordered rows and columns
left_join(linelist_mini, hosp_info, by = c("hospital" = "hosp_name"))
right_join(hosp_info, linelist_mini, by = c("hosp_name" = "hospital"))
```

Here is the result of `hosp_info` into `linelist_mini` via a left join (new columns incoming from the right)

```{r message=FALSE, echo=F}
left_join(linelist_mini, hosp_info, by = c("hospital" = "hosp_name")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 11))
```

Here is the result of `hosp_info` into  `linelist_mini` via a right join (new columns incoming from the left)  

```{r message=FALSE, echo=F}
right_join(hosp_info, linelist_mini, by = c("hosp_name" = "hospital")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 11))
```

Also consider whether your use-case is within a pipe chain (`%>%`). If the dataset in the pipes is the baseline, you will likely use a left join to add data to it.


<!-- ======================================================= -->
### Full join {.unnumbered} 

**A full join is the most *inclusive* of the joins** - it returns all rows from both data frames.  

If there are any rows present in one and not the other (where no match was found), the data frame will include them and become longer. `NA` missing values are used to fill-in any gaps created. As you join, watch the number of columns and rows carefully to troubleshoot case-sensitivity and exact character matches. 

The "baseline" data frame is the one written first in the command. Adjustment of this will not impact which records are returned by the join, but it can impact the resulting column order, row order, and which identifier columns are retained.  

```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "full-join.gif"))
```

Animated example of a full join ([image source](https://github.com/gadenbuie/tidyexplain/tree/master/images))

**Example**  

Below is the output of a `full_join()` of `hosp_info` (originally ` nrow(hosp_info)`, [view here](#joins_hosp_info))  *into* `linelist_mini` (originally ` nrow(linelist_mini)`, [view here](#joins_llmini)). Note the following:  

* All baseline rows are kept (`linelist_mini`)  
* Rows in the secondary that do not match to the baseline are kept ("ignace" and "sisters"), with values in the corresponding baseline columns  `case_id` and `onset` filled in with missing values  
* Likewise, rows in the baseline data frame that do not match to the secondary ("Other" and "Missing") are kept, with secondary columns ` catchment_pop` and `level` filled-in with missing values  
* In the case of one-to-many or many-to-one matches (e.g. rows for "Military Hospital"), all possible combinations are returned (lengthening the final data frame)  
* Only the identifier column from the baseline is kept (`hospital`)  


```{r, eval=F}
linelist_mini %>% 
  full_join(hosp_info, by = c("hospital" = "hosp_name"))
```

```{r message=FALSE, echo=F}
linelist_mini %>% 
  full_join(hosp_info, by = c("hospital" = "hosp_name")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 15))
```





<!-- ======================================================= -->
### Inner join {.unnumbered} 

**An inner join is the most *restrictive* of the joins** - it returns only rows with matches across both data frames.  
This means that the number of rows in the baseline data frame may actually *reduce*. Adjustment of which data frame is the "baseline" (written first in the function) will not impact which rows are returned, but it will impact the column order, row order, and which identifier columns are retained.   


```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "inner-join.gif"))
```

Animated example of an inner join ([image source](https://github.com/gadenbuie/tidyexplain/tree/master/images))


**Example**  

Below is the output of an `inner_join()` of `linelist_mini` (baseline) with `hosp_info` (secondary). Note the following:  

* Baseline rows with no match to the secondary data are removed (rows where `hospital` is "Missing" or "Other")  
* Likewise, rows from the secondary data frame that had no match in the baseline are removed (rows where `hosp_name` is "sisters" or "ignace")  
* Only the identifier column from the baseline is kept (`hospital`)  


```{r, eval=F}
linelist_mini %>% 
  inner_join(hosp_info, by = c("hospital" = "hosp_name"))
```


```{r message=FALSE, echo=F}
linelist_mini %>% 
  inner_join(hosp_info, by = c("hospital" = "hosp_name")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 12))
```






<!-- ======================================================= -->
### Semi join {.unnumbered} 

A semi join is a "filtering join" which uses another dataset *not to add rows or columns, but to perform filtering*.  

A **semi-join keeps all observations in the baseline data frame that have a match in the secondary data frame** (but does not add new columns nor duplicate any rows for multiple matches). Read more about these "filtering" joins [here](https://towardsdatascience.com/level-up-with-semi-joins-in-r-a068426096e0).  

```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "semi-join.gif"))
```

Animated example of a semi join ([image source](https://github.com/gadenbuie/tidyexplain/tree/master/images))

As an example, the below code returns rows from the `hosp_info` data frame that have matches in `linelist_mini` based on hospital name.  

```{r}
hosp_info %>% 
  semi_join(linelist_mini, by = c("hosp_name" = "hospital"))
```



<!-- ======================================================= -->
### Anti join {.unnumbered} 

**The anti join is another "filtering join" that returns rows in the baseline data frame that *do not* have a match in the secondary data frame.**  

Read more about filtering joins [here](https://towardsdatascience.com/level-up-with-semi-joins-in-r-a068426096e0).  

Common scenarios for an anti-join include identifying records not present in another data frame, troubleshooting spelling in a join (reviewing records that *should have* matched), and examining records that were excluded after another join.  

**As with `right_join()` and `left_join()`, the *baseline* data frame (listed first) is important**. The returned rows are from the baseline data frame only. Notice in the gif below that row in the secondary data frame (purple row 4) is not returned even though it does not match with the baseline.  

```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "anti-join.gif"))
```

Animated example of an anti join ([image source](https://github.com/gadenbuie/tidyexplain/tree/master/images))


#### Simple `anti_join()` example {.unnumbered}  

For a simple example, let's find the `hosp_info` hospitals that do not have any cases present in `linelist_mini`. We list `hosp_info` first, as the baseline data frame. The hospitals which are not present in `linelist_mini` are returned.  

```{r, eval=F}
hosp_info %>% 
  anti_join(linelist_mini, by = c("hosp_name" = "hospital"))
```

```{r message=FALSE, echo=F}
hosp_info %>% 
  anti_join(linelist_mini, by = c("hosp_name" = "hospital")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 12))
```


#### Complex `anti_join()` example {.unnumbered}  

For another example, let us say we ran an `inner_join()` between `linelist_mini` and `hosp_info`. This returns only a subset of the original `linelist_mini` records, as some are not present in `hosp_info`.  

```{r, eval=F}
linelist_mini %>% 
  inner_join(hosp_info, by = c("hospital" = "hosp_name"))
```

```{r message=FALSE, echo=F}
linelist_mini %>% 
  inner_join(hosp_info, by = c("hospital" = "hosp_name")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 8))
```

To review the `linelist_mini` records that were excluded during the inner join, we can run an anti-join with the same settings (`linelist_mini` as the baseline).  

```{r, eval = F}
linelist_mini %>% 
  anti_join(hosp_info, by = c("hospital" = "hosp_name"))
```

```{r message=FALSE, echo=F}
linelist_mini %>% 
  anti_join(hosp_info, by = c("hospital" = "hosp_name")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 8))
```


To see the `hosp_info` records that were excluded in the inner join, we could also run an anti-join with `hosp_info` as the baseline data frame.  



<!-- ======================================================= -->
## Probabalistic matching { }

If you do not have a unique identifier common across datasets to join on, consider using a probabilistic matching algorithm. This would find matches between records based on similarity (e.g. Jaro–Winkler string distance, or numeric distance).  Below is a simple example using the package **fastLink** .  

**Load packages**  

```{r}
pacman::p_load(
  tidyverse,      # data manipulation and visualization
  fastLink        # record matching
  )
```


Here are two small example datasets that we will use to demonstrate the probabilistic matching (`cases` and `test_results`):  

Here is the code used to make the datasets:  


```{r}
# make datasets

cases <- tribble(
  ~gender, ~first,      ~middle,     ~last,        ~yr,   ~mon, ~day, ~district,
  "M",     "Amir",      NA,          "Khan",       1989,  11,   22,   "River",
  "M",     "Anthony",   "B.",        "Smith",      1970, 09, 19,      "River", 
  "F",     "Marialisa", "Contreras", "Rodrigues",  1972, 04, 15,      "River",
  "F",     "Elizabeth", "Casteel",   "Chase",      1954, 03, 03,      "City",
  "M",     "Jose",      "Sanchez",   "Lopez",      1996, 01, 06,      "City",
  "F",     "Cassidy",   "Jones",      "Davis",     1980, 07, 20,      "City",
  "M",     "Michael",   "Murphy",     "O'Calaghan",1969, 04, 12,      "Rural", 
  "M",     "Oliver",    "Laurent",    "De Bordow" , 1971, 02, 04,     "River",
  "F",      "Blessing",  NA,          "Adebayo",   1955,  02, 14,     "Rural"
)

results <- tribble(
  ~gender,  ~first,     ~middle,     ~last,          ~yr, ~mon, ~day, ~district, ~result,
  "M",      "Amir",     NA,          "Khan",         1989, 11,   22,  "River", "positive",
  "M",      "Tony",   "B",         "Smith",          1970, 09,   19,  "River", "positive",
  "F",      "Maria",    "Contreras", "Rodriguez",    1972, 04,   15,  "Cty",   "negative",
  "F",      "Betty",    "Castel",   "Chase",        1954,  03,   30,  "City",  "positive",
  "F",      "Andrea",   NA,          "Kumaraswamy",  2001, 01,   05,  "Rural", "positive",      
  "F",      "Caroline", NA,          "Wang",         1988, 12,   11,  "Rural", "negative",
  "F",      "Trang",    NA,          "Nguyen",       1981, 06,   10,  "Rural", "positive",
  "M",      "Olivier" , "Laurent",   "De Bordeaux",  NA,   NA,   NA,  "River", "positive",
  "M",      "Mike",     "Murphy",    "O'Callaghan",  1969, 04,   12,  "Rural", "negative",
  "F",      "Cassidy",  "Jones",     "Davis",        1980, 07,   02,  "City",  "positive",
  "M",      "Mohammad", NA,          "Ali",          1942, 01,   17,  "City",  "negative",
  NA,       "Jose",     "Sanchez",   "Lopez",        1995, 01,   06,  "City",  "negative",
  "M",      "Abubakar", NA,          "Abullahi",     1960, 01,   01,  "River", "positive",
  "F",      "Maria",    "Salinas",   "Contreras",    1955, 03,   03,  "River", "positive"
  )

```


**The `cases` dataset has 9 records** of patients who are awaiting test results.  

```{r message=FALSE, echo=F}
# display the hospital data as a table
DT::datatable(cases, rownames = FALSE, options = list(pageLength = nrow(cases), scrollX=T), class = 'white-space: nowrap')
```



**The `test_results` dataset** has 14 records and contains the column `result`, which we want to add to the records in `cases` based on probabilistic matching of records.  

```{r message=FALSE, echo=F}
# display the hospital data as a table
DT::datatable(results, rownames = FALSE, options = list(pageLength = nrow(results), scrollX=T), class = 'white-space: nowrap')
```

### Probabilistic matching {.unnumbered}  

The `fastLink()` function from the **fastLink** package can be used to apply a matching algorithm. Here is the basic information. You can read more detail by entering `?fastLink` in your console.  

* Define the two data frames for comparison to arguments `dfA = ` and `dfB = `  
* In `varnames = ` give all column names to be used for matching. They must all exist in both `dfA` and `dfB`.  
* In `stringdist.match = ` give columns from those in `varnames` to be evaluated on string "distance".  
* In `numeric.match = ` give columns from those in `varnames` to be evaluated on numeric distance.  
* Missing values are ignored  
* By default, each row in either data frame is matched to at most one row in the other data frame. If you want to see all the evaluated matches, set `dedupe.matches = FALSE`. The deduplication is done using Winkler's linear assignment solution.  

*Tip: split one date column into three separate numeric columns using `day()`, `month()`, and `year()` from **lubridate** package*  

The default threshold for matches is 0.94 (`threshold.match = `) but you can adjust it higher or lower. If you define the threshold, consider that higher thresholds could yield more false-negatives (rows that do not match which actually should match) and likewise a lower threshold could yield more false-positive matches.  

Below, the data are matched on string distance across the name and district columns, and on numeric distance for year, month, and day of birth. A match threshold of 95% probability is set.  


```{r, message=F, warning=F}
fl_output <- fastLink::fastLink(
  dfA = cases,
  dfB = results,
  varnames = c("gender", "first", "middle", "last", "yr", "mon", "day", "district"),
  stringdist.match = c("first", "middle", "last", "district"),
  numeric.match = c("yr", "mon", "day"),
  threshold.match = 0.95)
```

**Review matches**  

We defined the object returned from `fastLink()` as `fl_output`. It is of class `list`, and it actually contains several data frames within it, detailing the results of the matching. One of these data frames is `matches`, which contains the most likely matches across `cases` and `results`. You can access this "matches" data frame with `fl_output$matches`. Below, it is saved as `my_matches` for ease of accessing later.    

When `my_matches` is printed, you see two column vectors: the pairs of row numbers/indices (also called "rownames") in `cases` ("inds.a") and in `results` ("inds.b") representing the best matches. If a row number from a datafrane is missing, then no match was found in the other data frame at the specified match threshold.    

```{r}
# print matches
my_matches <- fl_output$matches
my_matches
```

Things to note:  

* Matches occurred despite slight differences in name spelling and dates of birth:  
  * "Tony B. Smith" matched to "Anthony B Smith"  
  * "Maria Rodriguez" matched to "Marialisa Rodrigues"  
  * "Betty Chase" matched to "Elizabeth Chase"  
  * "Olivier Laurent De Bordeaux" matched to "Oliver Laurent De Bordow" (missing date of birth ignored)  
* One row from `cases` (for "Blessing Adebayo", row 9) had no good match in `results`, so it is not present in `my_matches`.  




**Join based on the probabilistic matches**  

To use these matches to join `results` to `cases`, one strategy is:  

1) Use `left_join()` to join `my_matches` to `cases` (matching rownames in `cases` to "inds.a" in `my_matches`)  
2) Then use another `left_join()` to join `results` to `cases` (matching the newly-acquired "inds.b" in `cases` to rownames in `results`)  

Before the joins, we should clean the three data frames:  

* Both `dfA` and `dfB` should have their row numbers ("rowname") converted to a proper column.  
* Both the columns in `my_matches` are converted to class character, so they can be joined to the character rownames  

```{r}
# Clean data prior to joining
#############################

# convert cases rownames to a column 
cases_clean <- cases %>% rownames_to_column()

# convert test_results rownames to a column
results_clean <- results %>% rownames_to_column()  

# convert all columns in matches dataset to character, so they can be joined to the rownames
matches_clean <- my_matches %>%
  mutate(across(everything(), as.character))



# Join matches to dfA, then add dfB
###################################
# column "inds.b" is added to dfA
complete <- left_join(cases_clean, matches_clean, by = c("rowname" = "inds.a"))

# column(s) from dfB are added 
complete <- left_join(complete, results_clean, by = c("inds.b" = "rowname"))
```

As performed using the code above, the resulting data frame `complete` will contain *all* columns from both `cases` and `results`. Many will be appended with suffixes ".x" and ".y", because the column names would otherwise be duplicated.  

```{r message=FALSE, echo=F}
DT::datatable(complete, rownames = FALSE, options = list(pageLength = nrow(complete), scrollX=T), class = 'white-space: nowrap')
```

Alternatively, to achieve only the "original" 9 records in `cases` with the new column(s) from `results`, use `select()` on `results` before the joins, so that it contains only rownames and the columns that you want to add to `cases` (e.g. the column `result`).  

```{r}
cases_clean <- cases %>% rownames_to_column()

results_clean <- results %>%
  rownames_to_column() %>% 
  select(rowname, result)    # select only certain columns 

matches_clean <- my_matches %>%
  mutate(across(everything(), as.character))

# joins
complete <- left_join(cases_clean, matches_clean, by = c("rowname" = "inds.a"))
complete <- left_join(complete, results_clean, by = c("inds.b" = "rowname"))
```


```{r message=FALSE, echo=F}
DT::datatable(complete, rownames = FALSE, options = list(pageLength = nrow(complete), scrollX=T), class = 'white-space: nowrap')
```


If you want to subset either dataset to only the rows that matched, you can use the codes below:  

```{r}
cases_matched <- cases[my_matches$inds.a,]  # Rows in cases that matched to a row in results
results_matched <- results[my_matches$inds.b,]  # Rows in results that matched to a row in cases
```

Or, to see only the rows that did **not** match:  

```{r}
cases_not_matched <- cases[!rownames(cases) %in% my_matches$inds.a,]  # Rows in cases that did NOT match to a row in results
results_not_matched <- results[!rownames(results) %in% my_matches$inds.b,]  # Rows in results that did NOT match to a row in cases
```


### Probabilistic deduplication {.unnumbered}  

Probabilistic matching can be used to deduplicate a dataset as well. See the page on deduplication for other methods of deduplication.  

Here we began with the `cases` dataset, but are now calling it `cases_dup`, as it has 2 additional rows that could be duplicates of previous rows:
See "Tony" with "Anthony", and "Marialisa Rodrigues" with "Maria Rodriguez".  

```{r, echo=F}
## Add duplicates
#cases_dup <- rbind(cases, cases[sample(1:nrow(cases), 3, replace = FALSE),])

cases_dup <- tribble(
  ~gender, ~first,      ~middle,     ~last,        ~yr,   ~mon, ~day, ~district,
  "M",     "Amir",      NA,          "Khan",       1989,  11,   22,   "River",
  "M",     "Anthony",   "B.",        "Smith",      1970, 09, 19,      "River", 
  "F",     "Marialisa", "Contreras", "Rodrigues",  1972, 04, 15,      "River",
  "F",     "Elizabeth", "Casteel",   "Chase",      1954, 03, 03,      "City",
  "M",     "Jose",      "Sanchez",   "Lopez",      1996, 01, 06,      "City",
  "F",     "Cassidy",   "Jones",      "Davis",     1980, 07, 20,      "City",
  "M",     "Michael",   "Murphy",     "O'Calaghan",1969, 04, 12,      "Rural", 
  "M",     "Oliver",    "Laurent",    "De Bordow" , 1971, 02, 04,     "River",
  "F",      "Blessing",  NA,          "Adebayo",   1955,  02, 14,     "Rural",
  "M",     "Tony",   "B.",        "Smith",         1970, 09, 19,      "River", 
  "F",     "Maria",  "Contreras", "Rodriguez",     1972, 04, 15,      "River",
)

```

```{r message=FALSE, echo=F}
DT::datatable(cases_dup, rownames = FALSE, options = list(pageLength = nrow(cases_dup)))
```


Run `fastLink()` like before, but compare the `cases_dup` data frame to itself. When the two data frames provided are identical, the function assumes you want to de-duplicate. Note we do not specify `stringdist.match = ` or `numeric.match = ` as we did previously.  

```{r, message = F, warning = F}
## Run fastLink on the same dataset
dedupe_output <- fastLink(
  dfA = cases_dup,
  dfB = cases_dup,
  varnames = c("gender", "first", "middle", "last", "yr", "mon", "day", "district")
)
```

Now, you can review the potential duplicates with `getMatches()`. Provide the data frame as both `dfA = ` and `dfB = `, and provide the output of the `fastLink()` function as `fl.out = `.  `fl.out` must be of class `fastLink.dedupe`, or in other words, the result of `fastLink()`.  


```{r}
## Run getMatches()
cases_dedupe <- getMatches(
  dfA = cases_dup,
  dfB = cases_dup,
  fl.out = dedupe_output)
```

See the right-most column, which indicates the duplicate IDs - the final two rows are identified as being likely duplicates of rows 2 and 3.  

```{r message=FALSE, echo=F}
DT::datatable(cases_dedupe, rownames = FALSE, options = list(pageLength = nrow(cases_dedupe)))
```

To return the row numbers of rows which are likely duplicates, you can count the number of rows per unique value in the `dedupe.ids` column, and then filter to keep only those with more than one row. In this case this leaves rows 2 and 3.  

```{r}
cases_dedupe %>% 
  count(dedupe.ids) %>% 
  filter(n > 1)
```

To inspect the whole rows of the likely duplicates, put the row number in this command:  

```{r}
# displays row 2 and all likely duplicates of it
cases_dedupe[cases_dedupe$dedupe.ids == 2,]   
```



## Binding and aligning  

Another method of combining two data frames is "binding" them together. You can also think of this as "appending" or "adding" rows or columns.  

This section will also discuss how to "align" the order of rows of one data frame to the order in another data frame. This topic is discussed below in the section on Binding columns.  



### Bind rows {.unnumbered}

To bind rows of one data frame to the bottom of another data frame, use `bind_rows()` from **dplyr**. It is very inclusive, so any column present in either data frame will be included in the output. A few notes:  

* Unlike the **base** R version `row.bind()`, **dplyr**'s `bind_rows()` does not require that the order of columns be the same in both data frames. As long as the column names are spelled identically, it will align them correctly.  
* You can optionally specify the argument `.id = `. Provide a character column name. This will produce a new column that serves to identify which data frame each row originally came from.  
* You can use `bind_rows()` on a `list` of similarly-structured data frames to combine them into one data frame. See an example in the [Iteration, loops, and lists] page involving the import of multiple linelists with **purrr**.  

One common example of row binding is to bind a "total" row onto a descriptive table made with **dplyr**'s `summarise()` function. Below we create a table of case counts and median CT values by hospital with a total row.  

The function `summarise()` is used on data grouped by hospital to return a summary data frame by hospital. But the function `summarise()` does not automatically produce a "totals" row, so we create it by summarising the data *again*, but with the data not grouped by hospital. This produces a second data frame of just one row. We can then bind these data frames together to achieve the final table.  

See other worked examples like this in the [Descriptive tables] and [Tables for presentation] pages.  


```{r}
# Create core table
###################
hosp_summary <- linelist %>% 
  group_by(hospital) %>%                        # Group data by hospital
  summarise(                                    # Create new summary columns of indicators of interest
    cases = n(),                                  # Number of rows per hospital-outcome group     
    ct_value_med = median(ct_blood, na.rm=T))     # median CT value per group
```

Here is the `hosp_summary` data frame:  

```{r message=FALSE, echo=F}
DT::datatable(hosp_summary, rownames = FALSE, options = list(pageLength = nrow(10)))
```

Create a data frame with the "total" statistics (*not grouped by hospital*). This will return just one row.  

```{r}
# create totals
###############
totals <- linelist %>% 
  summarise(
    cases = n(),                               # Number of rows for whole dataset     
    ct_value_med = median(ct_blood, na.rm=T))  # Median CT for whole dataset
```

And below is that `totals` data frame. Note how there are only two columns. These columns are also in `hosp_summary`, but there is one column in `hosp_summary` that is not in `totals` (`hospital`).  

```{r message=FALSE, echo=F}
DT::datatable(totals, rownames = FALSE, options = list(pageLength = nrow(10)))
```

Now we can bind the rows together with `bind_rows()`.  

```{r}
# Bind data frames together
combined <- bind_rows(hosp_summary, totals)
```

Now we can view the result. See how in the final row, an empty `NA` value fills in for the column `hospital` that was not in `hosp_summary`. As explained in the [Tables for presentation] page, you could "fill-in" this cell with "Total" using `replace_na()`.  

```{r message=FALSE, echo=F}
DT::datatable(combined, rownames = FALSE, options = list(pageLength = nrow(10)))
```


### Bind columns {.unnumbered}

There is a similar **dplyr** function `bind_cols()` which you can use to combine two data frames sideways. Note that rows are matched to each other *by position* (not like a *join* above) - for example the 12th row in each data frame will be aligned.  

For an example, we bind several summary tables together. In order to do this, we also demonstrate how to re-arrange the order of rows in one data frame to match the order in another data frame, with `match()`.    

Here we define `case_info` as a summary data frame of linelist cases, by hospital, with the number of cases and the number of deaths.


```{r}
# Case information
case_info <- linelist %>% 
  group_by(hospital) %>% 
  summarise(
    cases = n(),
    deaths = sum(outcome == "Death", na.rm=T)
  )
```

```{r message=FALSE, echo=F}
DT::datatable(case_info, rownames = FALSE, options = list(pageLength = nrow(10)))
```

And let's say that here is a different data frame `contact_fu` containing information on the percent of exposed contacts investigated and "followed-up", again by hospital. 

```{r}
contact_fu <- data.frame(
  hospital = c("St. Mark's Maternity Hospital (SMMH)", "Military Hospital", "Missing", "Central Hospital", "Port Hospital", "Other"),
  investigated = c("80%", "82%", NA, "78%", "64%", "55%"),
  per_fu = c("60%", "25%", NA, "20%", "75%", "80%")
)
```

```{r message=FALSE, echo=F}
DT::datatable(contact_fu, rownames = FALSE, options = list(pageLength = nrow(10)))
```

Note that the hospitals are the same, but are in different orders in each data frame. The easiest solution would be to use a `left_join()` on the `hospital` column, but you could also use `bind_cols()` with one extra step.  

#### Use `match()` to align ordering {.unnumbered}  

Because the row orders are different, a simple `bind_cols()` command would result in a mis-match of data. To fix this we can use `match()` from **base** R to align the rows of a data frame in the same order as in another. We assume for this approach that there are no duplicate values in either data frame.  

When we use `match()`, the syntax is `match(TARGET ORDER VECTOR, DATA FRAME COLUMN TO CHANGE)`, where the first argument is the desired order (either a stand-alone vector, or in this case a column in a data frame), and the second argument is the data frame column in the data frame that will be re-ordered. The output of `match()` is a vector of numbers representing the correct position ordering. You can read more with `?match`.  

```{r}
match(case_info$hospital, contact_fu$hospital)
```

You can use this numeric vector to re-order the data frame - place it within subset brackets `[ ]` *before the comma*. Read more about **base** R bracket subset syntax in the [R basics] page. The command below creates a new data frame, defined as the old one in which the rows are ordered in the numeric vector above.  

```{r}
contact_fu_aligned <- contact_fu[match(case_info$hospital, contact_fu$hospital),]
```


```{r message=FALSE, echo=F}
DT::datatable(contact_fu_aligned, rownames = FALSE, options = list(pageLength = nrow(10)))
```

Now we can bind the data frame columns together, with the correct row order. Note that some columns are duplicated and will require cleaning with `rename()`. Read more aboout `bind_rows()` [here](https://dplyr.tidyverse.org/reference/bind.html).  

```{r}
bind_cols(case_info, contact_fu)
```

A **base** R alternative to `bind_cols` is `cbind()`, which performs the same operation.  




<!-- ======================================================= -->
## Resources { }

The [tidyverse page on joins](https://dplyr.tidyverse.org/reference/join.html)  

The [R for Data Science page on relational data](https://r4ds.had.co.nz/relational-data.html)  

Th [tidyverse page on dplyr](https://dplyr.tidyverse.org/reference/bind.html) on binding  

A vignette on [fastLink](https://github.com/kosukeimai/fastLink) at the package's Github page  

Publication describing methodology of [fastLink](https://imai.fas.harvard.edu/research/files/linkage.pdf)  

Publication describing [RecordLinkage package](https://journal.r-project.org/archive/2010/RJ-2010-017/RJ-2010-017.pdf)




```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/joining_matching.Rmd-->


# Loại bỏ trùng lặp {#deduplication}  

```{r, out.width=c("50%"), echo=F}
knitr::include_graphics(here::here("images", "deduplication.png"))
```

This page covers the following de-duplication techniques:  

1. Identifying and removing duplicate rows  
2. "Slicing" rows to keep only certain rows (e.g. min or max) from each group of rows  
3. "Rolling-up", or combining values from multiple rows into one row  


<!-- ======================================================= -->
## Preparation { }


### Load packages {.unnumbered}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  tidyverse,   # deduplication, grouping, and slicing functions
  janitor,     # function for reviewing duplicates
  stringr)      # for string searches, can be used in "rolling-up" values
```

### Import data {.unnumbered}

For demonstration, we will use an example dataset that is created with the R code below.  

The data are records of COVID-19 phone encounters, including encounters with contacts and with cases. The columns include `recordID` (computer-generated), `personID`, `name`, `date` of encounter, `time` of encounter, the `purpose` of the encounter (either to interview as a case or as a contact), and `symptoms_ever` (whether the person in that encounter reported *ever* having symptoms).  

Here is the code to create the `obs` dataset:  

```{r}
obs <- data.frame(
  recordID  = c(1,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18),
  personID  = c(1,1,2,2,3,2,4,5,6,7,2,1,3,3,4,5,5,7,8),
  name      = c("adam", "adam", "amrish", "amrish", "mariah", "amrish", "nikhil", "brian", "smita", "raquel", "amrish",
                "adam", "mariah", "mariah", "nikhil", "brian", "brian", "raquel", "natalie"),
  date      = c("1/1/2020", "1/1/2020", "2/1/2020", "2/1/2020", "5/1/2020", "5/1/2020", "5/1/2020", "5/1/2020", "5/1/2020","5/1/2020", "2/1/2020",
                "5/1/2020", "6/1/2020", "6/1/2020", "6/1/2020", "6/1/2020", "7/1/2020", "7/1/2020", "7/1/2020"),
  time      = c("09:00", "09:00", "14:20", "14:20", "12:00", "16:10", "13:01", "15:20", "14:20", "12:30", "10:24",
                "09:40", "07:25", "08:32", "15:36", "15:31", "07:59", "11:13", "17:12"),
  encounter = c(1,1,1,1,1,3,1,1,1,1,2,
                2,2,3,2,2,3,2,1),
  purpose   = c("contact", "contact", "contact", "contact", "case", "case", "contact", "contact", "contact", "contact", "contact",
                "case", "contact", "contact", "contact", "contact", "case", "contact", "case"),
  symptoms_ever = c(NA, NA, "No", "No", "No", "Yes", "Yes", "No", "Yes", NA, "Yes",
                    "No", "No", "No", "Yes", "Yes", "No","No", "No")) %>% 
  mutate(date = as.Date(date, format = "%d/%m/%Y"))
```


#### Here is the data frame {#dedup_data .unnumbered}  

Use the filter boxes along the top to review the encounters for each person.  

```{r message=FALSE, echo=F}
DT::datatable(obs, rownames = FALSE, filter = "top", options = list(pageLength = nrow(obs), scrollX=T), class = 'white-space: nowrap' )
```


A few things to note as you review the data:  

* The first two records are 100% complete duplicates including duplicate `recordID` (must be a computer glitch!)  
* The second two rows are duplicates, in all columns *except for `recordID`*  
* Several people had multiple phone encounters, at various dates and times, and as contacts and/or cases  
* At each encounter, the person was asked if they had **ever** had symptoms, and some of this information is missing.  


And here is a quick summary of the people and the purposes of their encounters, using `tabyl()` from **janitor**:  

```{r}
obs %>% 
  tabyl(name, purpose)
```
<!-- ======================================================= -->
## Deduplication { }


This section describes how to review and remove duplicate rows in a data frame. It also show how to handle duplicate elements in a vector.  


<!-- ======================================================= -->
### Examine duplicate rows {.unnumbered}  


To quickly review rows that have duplicates, you can use `get_dupes()` from the **janitor** package. *By default*, all columns are considered when duplicates are evaluated - rows returned by the function are 100% duplicates considering the values in *all* columns.  

In the `obs` data frame, the first two rows are *100% duplicates* - they have the same value in every column (including the `recordID` column, which is *supposed* to be unique - it must be some computer glitch). The returned data frame automatically includes a new column `dupe_count` on the right side, showing the number of rows with that combination of duplicate values. 

```{r, eval=F}
# 100% duplicates across all columns
obs %>% 
  janitor::get_dupes()
```

```{r message=FALSE, echo=F}
obs %>% 
  janitor::get_dupes() %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = nrow(obs), scrollX=T), class = 'white-space: nowrap' )
```

See the [original data](#dedup_data)  

However, if we choose to ignore `recordID`, the 3rd and 4th rows rows are also duplicates of each other. That is, they have the same values in all columns *except* for `recordID`. You can specify specific columns to be ignored in the function using a `-` minus symbol.  

```{r, eval=F}
# Duplicates when column recordID is not considered
obs %>% 
  janitor::get_dupes(-recordID)         # if multiple columns, wrap them in c()
```

```{r message=FALSE, echo=F}
obs %>% 
  janitor::get_dupes(-recordID) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = nrow(obs), scrollX=T), class = 'white-space: nowrap' )
```

You can also positively specify the columns to consider. Below, only rows that have the same values in the `name` and `purpose` columns are returned. Notice how "amrish" now has `dupe_count` equal to 3 to reflect his three "contact" encounters.  

*Scroll left for more rows**  

```{r, eval=F}
# duplicates based on name and purpose columns ONLY
obs %>% 
  janitor::get_dupes(name, purpose)
```

```{r message=FALSE, echo=F}
obs %>% 
  janitor::get_dupes(name, purpose) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 7, scrollX=T), class = 'white-space: nowrap' )
```

See the [original data](#dedup_data).  

See `?get_dupes` for more details, or see this [online reference](https://cran.r-project.org/web/packages/janitor/vignettes/janitor.html#explore-records-with-duplicated-values-for-specific-combinations-of-variables-with-get_dupes)  






<!-- ======================================================= -->
### Keep only unique rows  {.unnumbered}


To keep only unique rows of a data frame, use `distinct()` from **dplyr** (as demonstrated in the [Cleaning data and core functions] page). Rows that are duplicates are removed such that only the first of such rows is kept. By default, "first" means the highest `rownumber` (order of rows top-to-bottom). Only unique rows remain.  

In the example below, we run `distinct()` such that the column `recordID` is excluded from consideration - thus **two duplicate rows are removed**. The first row (for "adam") was 100% duplicated and has been removed. Also row 3 (for "amrish") was a duplicate in every column *except* `recordID` (which is not being considered) and so is also removed. The `obs` dataset n is now ` nrow(obs)-2`, not ` nrow(obs)` rows).  

*Scroll to the left to see the entire data frame*  


```{r, eval=F}
# added to a chain of pipes (e.g. data cleaning)
obs %>% 
  distinct(across(-recordID), # reduces data frame to only unique rows (keeps first one of any duplicates)
           .keep_all = TRUE) 

# if outside pipes, include the data as first argument 
# distinct(obs)
```

```{r message=FALSE, echo=F}
obs %>% 
  distinct(across(-recordID), # reduces data frame to only unique rows (keeps first one of any duplicates)
           .keep_all = TRUE) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 6, scrollX=T), class = 'white-space: nowrap' )
```

<span style="color: orange;">**_CAUTION:_** If using `distinct()` on grouped data, the function will apply to each group.</span>


**Deduplicate based on specific columns**  

You can also specify columns to be the basis for de-duplication. In this way, the de-duplication only applies to rows that are duplicates within the specified columns. Unless you set `.keep_all = TRUE`, all columns not mentioned will be dropped.  

In the example below, the de-duplication only applies to rows that have identical values for `name` and `purpose` columns. Thus, "brian" has only 2 rows instead of 3 - his *first* "contact" encounter and his only "case" encounter. To adjust so that brian's *latest* encounter of each purpose is kept, see the tab on Slicing within groups.  

*Scroll to the left to see the entire data frame*  

```{r, eval=F}
# added to a chain of pipes (e.g. data cleaning)
obs %>% 
  distinct(name, purpose, .keep_all = TRUE) %>%  # keep rows unique by name and purpose, retain all columns
  arrange(name)                                  # arrange for easier viewing
```

```{r message=FALSE, echo=F}
obs %>% 
  distinct(name, purpose, .keep_all = TRUE) %>%  # keep rows unique by name and purpose, retain all columns
  arrange(name) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 6, scrollX=T), class = 'white-space: nowrap' )
```

See the [original data](#dedup_data).  

<!-- ======================================================= -->
### Deduplicate elements in a vector {.unnumbered}  


The function `duplicated()` from **base** R will evaluate a vector (column) and return a logical vector of the same length (TRUE/FALSE). The first time a value appears, it will return FALSE (not a duplicate), and subsequent times that value appears it will return TRUE. Note how `NA` is treated the same as any other value.    

```{r}
x <- c(1, 1, 2, NA, NA, 4, 5, 4, 4, 1, 2)
duplicated(x)
```

To return only the duplicated elements, you can use brackets to subset the original vector: 

```{r}
x[duplicated(x)]
```

To return only the unique elements, use `unique()` from **base** R. To remove `NA`s from the output, nest `na.omit()` within `unique()`.  

```{r}
unique(x)           # alternatively, use x[!duplicated(x)]
unique(na.omit(x))  # remove NAs 
```


<!-- ======================================================= -->
### Using **base** R {.unnumbered}

**To return duplicate rows**  

In **base** R, you can also see which rows are 100% duplicates in a data frame `df` with the command `duplicated(df)` (returns a logical vector of the rows).  

Thus, you can also use the base subset `[ ]` on the data frame to see the *duplicated* rows with `df[duplicated(df),]` (don't forget the comma, meaning that you want to see all columns!). 

**To return unique rows**  

See the notes above. To see the *unique* rows you add the logical negator `!` in front of the `duplicated()` function:  
`df[!duplicated(df),]`  


**To return rows that are duplicates of only certain columns**  

Subset the `df` that is *within the `duplicated()` parentheses*, so this function will operate on only certain columns of the `df`.  

To specify the columns, provide column numbers or names after a comma (remember, all this is *within* the `duplicated()` function).  

Be sure to keep the comma `,` *outside* after the `duplicated()` function as well! 

For example, to evaluate only columns 2 through 5 for duplicates:  `df[!duplicated(df[, 2:5]),]`  
To evaluate only columns `name` and `purpose` for duplicates: `df[!duplicated(df[, c("name", "purpose)]),]`  





<!-- ======================================================= -->
## Slicing { }


To "slice" a data frame to apply a filter on the rows by row number/position. This becomes particularly useful if you have multiple rows per functional group (e.g. per "person") and you only want to keep one or some of them. 

The basic `slice()` function accepts numbers and returns rows in those positions. If the numbers provided are positive, only they are returned. If negative, those rows are *not* returned. Numbers must be either all positive or all negative.     

```{r}
obs %>% slice(4)  # return the 4th row
```

```{r}
obs %>% slice(c(2,4))  # return rows 2 and 4
#obs %>% slice(c(2:4))  # return rows 2 through 4
```


See the [original data](#dedup_data). 

There are several variations:  These should be provided with a column and a number of rows to return (to `n = `).  

* `slice_min()` and `slice_max()`  keep only the row(s) with the minimium or maximum value(s) of the specified column. This also works to return the "min" and "max" of ordered factors.    
* `slice_head()` and `slice_tail()` - keep only the *first* or *last* row(s).  
* `slice_sample()`  - keep only a random sample of the rows.  


```{r}
obs %>% slice_max(encounter, n = 1)  # return rows with the largest encounter number
```

Use arguments `n = ` or `prop = ` to specify the number or proportion of rows to keep. If not using the function in a pipe chain, provide the data argument first (e.g. `slice(data, n = 2)`). See `?slice` for more information. 

Other arguments:  

`.order_by = ` used in `slice_min()` and `slice_max()` this is a column to order by before slicing.  
`with_ties = ` TRUE by default, meaning ties are kept.  
`.preserve = ` FALSE by default. If TRUE then the grouping structure is re-calculated after slicing.  
`weight_by = ` Optional, numeric column to weight by (bigger number more likely to get sampled).  Also `replace = ` for whether sampling is done with/without replacement.  

<span style="color: darkgreen;">**_TIP:_** When using `slice_max()` and `slice_min()`, be sure to specify/write the `n = `  (e.g. `n = 2`, not just `2`). Otherwise you may get an error `Error: `...` is not empty.` </span>

<span style="color: black;">**_NOTE:_** You may encounter the function [`top_n()`](https://dplyr.tidyverse.org/reference/top_n.html), which has been superseded by the `slice` functions.</span>

 


<!-- ======================================================= -->
### Slice with groups  {.unnumbered}

The `slice_*()` functions can be very useful if applied to a grouped data frame because the slice operation is performed on each group separately. Use the **function** `group_by()` in conjunction with `slice()` to group the data to take a slice from each group.  

This is helpful for de-duplication if you have multiple rows per person but only want to keep one of them. You first use `group_by()` with key columns that are the same per person, and then use a slice function on a column that will differ among the grouped rows.  

In the example below, to keep only the *latest* encounter *per person*, we group the rows by `name` and then use `slice_max()` with `n = 1` on the `date` column. Be aware! To apply a function like `slice_max()` on dates, the date column must be class Date.   

By default, "ties" (e.g. same date in this scenario) are kept, and we would still get multiple rows for some people (e.g. adam). To avoid this we set `with_ties = FALSE`. We get back only one row per person.  

<span style="color: orange;">**_CAUTION:_** If using `arrange()`, specify `.by_group = TRUE` to have the data arranged within each group.</span>

<span style="color: red;">**_DANGER:_** If `with_ties = FALSE`, the first row of a tie is kept. This may be deceptive. See how for Mariah, she has two encounters on her latest date (6 Jan) and the first (earliest) one was kept. Likely, we want to keep her later encounter on that day. See how to "break" these ties in the next example. </span>  




```{r, eval=F}
obs %>% 
  group_by(name) %>%       # group the rows by 'name'
  slice_max(date,          # keep row per group with maximum date value 
            n = 1,         # keep only the single highest row 
            with_ties = F) # if there's a tie (of date), take the first row
```

```{r message=FALSE, echo=F}
obs %>% 
  group_by(name) %>%       # group the rows by 'name'
  slice_max(date,          # keep row per group with maximum date value 
            n = 1,         # keep only the single highest row 
            with_ties = F) %>%  # if there's a tie (of date), take the first row
  DT::datatable(rownames = FALSE, options = list(pageLength = 8, scrollX=T), class = 'white-space: nowrap' )
```

Above, for example we can see that only Amrish's row on 5 Jan was kept, and only Brian's row on 7 Jan was kept. See the [original data](#dedup_data).  


**Breaking "ties"**  

Multiple slice statements can be run to "break ties". In this case, if a person has multiple encounters on their latest *date*, the encounter with the latest *time* is kept (`lubridate::hm()` is used to convert the character times to a sortable time class).  
Note how now, the one row kept for "Mariah" on 6 Jan is encounter 3 from 08:32, not encounter 2 at 07:25.  

```{r, eval=F}
# Example of multiple slice statements to "break ties"
obs %>%
  group_by(name) %>%
  
  # FIRST - slice by latest date
  slice_max(date, n = 1, with_ties = TRUE) %>% 
  
  # SECOND - if there is a tie, select row with latest time; ties prohibited
  slice_max(lubridate::hm(time), n = 1, with_ties = FALSE)
```

```{r message=FALSE, echo=F}
# Example of multiple slice statements to "break ties"
obs %>%
  group_by(name) %>%
  
  # FIRST - slice by latest date
  slice_max(date, n = 1, with_ties = TRUE) %>% 
  
  # SECOND - if there is a tie, select row with latest time; ties prohibited
  slice_max(lubridate::hm(time), n = 1, with_ties = FALSE) %>% 
  
  DT::datatable(rownames = FALSE, options = list(pageLength = 8, scrollX=T), class = 'white-space: nowrap' )
```

*In the example above, it would also have been possible to slice by `encounter` number, but we showed the slice on `date` and `time` for example purposes.*  

<span style="color: darkgreen;">**_TIP:_** To use `slice_max()` or `slice_min()` on a "character" column, mutate it to an *ordered* factor class!</span>

See the [original data](#dedup_data).  


<!-- ======================================================= -->
### Keep all but mark them  {.unnumbered}

If you want to keep all records but mark only some for analysis, consider a two-step approach utilizing a unique recordID/encounter number:  

1) Reduce/slice the orginal data frame to only the rows for analysis. Save/retain this reduced data frame.  
2) In the original data frame, mark rows as appropriate with `case_when()`, based on whether their record unique identifier (recordID in this example) is present in the reduced data frame.  


```{r}
# 1. Define data frame of rows to keep for analysis
obs_keep <- obs %>%
  group_by(name) %>%
  slice_max(encounter, n = 1, with_ties = FALSE) # keep only latest encounter per person


# 2. Mark original data frame
obs_marked <- obs %>%

  # make new dup_record column
  mutate(dup_record = case_when(
    
    # if record is in obs_keep data frame
    recordID %in% obs_keep$recordID ~ "For analysis", 
    
    # all else marked as "Ignore" for analysis purposes
    TRUE                            ~ "Ignore"))

# print
obs_marked
```


```{r, echo=F}
DT::datatable(obs_marked, rownames = FALSE, options = list(pageLength = 8, scrollX=T), class = 'white-space: nowrap' )
```

See the [original data](#dedup_data).  

<!-- ======================================================= -->
### Calculate row completeness {.unnumbered} 

Create a column that contains a metric for the row's completeness (non-missingness). This could be helpful when deciding which rows to prioritize over others when de-duplicating/slicing.  

In this example, "key" columns over which you want to measure completeness are saved in a vector of column names.  

Then the new column `key_completeness` is created with `mutate()`. The new value in each row is defined as a calculated fraction: the number of non-missing values in that row among the key columns, divided by the number of key columns.  

This involves the function `rowSums()` from **base** R. Also used is `.`, which within piping refers to the data frame at that point in the pipe (in this case, it is being subset with brackets `[]`).  

*Scroll to the right to see more rows**  

```{r, eval=F}
# create a "key variable completeness" column
# this is a *proportion* of the columns designated as "key_cols" that have non-missing values

key_cols = c("personID", "name", "symptoms_ever")

obs %>% 
  mutate(key_completeness = rowSums(!is.na(.[,key_cols]))/length(key_cols)) 
```

```{r message=FALSE, echo=F}
key_cols = c("personID", "name", "symptoms_ever")

obs %>% 
  mutate(key_completeness = rowSums(!is.na(.[,key_cols]))/length(key_cols)) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

See the [original data](#dedup_data).  




<!-- ======================================================= -->
## Roll-up values {#str_rollup}


This section describes:  

1) How to "roll-up" values from multiple rows into just one row, with some variations  
2) Once you have "rolled-up" values, how to overwrite/prioritize the values in each cell  

This tab uses the example dataset from the Preparation tab.  



<!-- ======================================================= -->
### Roll-up values into one row {.unnumbered}  

The code example below uses `group_by()` and `summarise()` to group rows by person, and then paste together all unique values within the grouped rows. Thus, you get one summary row per person. A few notes:  

* A suffix is appended to all new columns ("_roll" in this example)  
* If you want to show only unique values per cell, then wrap the `na.omit()` with `unique()`  
* `na.omit()` removes `NA` values, but if this is not desired it can be removed `paste0(.x)`...  



```{r, eval=F}
# "Roll-up" values into one row per group (per "personID") 
cases_rolled <- obs %>% 
  
  # create groups by name
  group_by(personID) %>% 
  
  # order the rows within each group (e.g. by date)
  arrange(date, .by_group = TRUE) %>% 
  
  # For each column, paste together all values within the grouped rows, separated by ";"
  summarise(
    across(everything(),                           # apply to all columns
           ~paste0(na.omit(.x), collapse = "; "))) # function is defined which combines non-NA values
```

The result is one row per group (`ID`), with entries arranged by date and pasted together. *Scroll to the left to see more rows*    

```{r message=FALSE, echo=F}
# "Roll-up" values into one row per group (per "personID") 
obs %>% 
  
  # create groups by name
  group_by(personID) %>% 
  
  # order the rows within each group (e.g. by date)
  arrange(date, .by_group = TRUE) %>% 
  
  # For each column, paste together all values within the grouped rows, separated by ";"
  summarise(
    across(everything(),                                # apply to all columns
           ~paste0(na.omit(.x), collapse = "; "))) %>%  # function is defined which combines non-NA values

  DT::datatable(rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```

See the [original data](#dedup_data).  


**This variation shows unique values only:**  

```{r}
# Variation - show unique values only 
cases_rolled <- obs %>% 
  group_by(personID) %>% 
  arrange(date, .by_group = TRUE) %>% 
  summarise(
    across(everything(),                                   # apply to all columns
           ~paste0(unique(na.omit(.x)), collapse = "; "))) # function is defined which combines unique non-NA values
```

```{r message=FALSE, echo=F}
# Variation - show unique values only 
obs %>% 
  group_by(personID) %>% 
  arrange(date, .by_group = TRUE) %>% 
  summarise(
    across(everything(),                                   # apply to all columns
           ~paste0(unique(na.omit(.x)), collapse = "; "))) %>%  # function is defined which combines unique non-NA values

  DT::datatable(rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


**This variation appends a suffix to each column.**  
In this case "_roll" to signify that it has been rolled:  

```{r, eval=F}
# Variation - suffix added to column names 
cases_rolled <- obs %>% 
  group_by(personID) %>% 
  arrange(date, .by_group = TRUE) %>% 
  summarise(
    across(everything(),                
           list(roll = ~paste0(na.omit(.x), collapse = "; ")))) # _roll is appended to column names
```

```{r message=FALSE, echo=F}
# display the linelist data as a table
# Variation - suffix added to column names 
obs %>% 
  group_by(personID) %>% 
  arrange(date, .by_group = TRUE) %>% 
  summarise(
    across(everything(),                
           list(roll = ~paste0(na.omit(.x), collapse = "; ")))) %>%  # _roll is appended to column names
  DT::datatable(rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


<!-- ======================================================= -->
### Overwrite values/hierarchy {.unnumbered} 


If you then want to evaluate all of the rolled values, and keep only a specific value (e.g. "best" or "maximum" value), you can use `mutate()` across the desired columns, to implement `case_when()`, which uses `str_detect()` from the **stringr** package to sequentially look for string patterns and overwrite the cell content.  

```{r}
# CLEAN CASES
#############
cases_clean <- cases_rolled %>% 
    
    # clean Yes-No-Unknown vars: replace text with "highest" value present in the string
    mutate(across(c(contains("symptoms_ever")),                     # operates on specified columns (Y/N/U)
             list(mod = ~case_when(                                 # adds suffix "_mod" to new cols; implements case_when()
               
               str_detect(.x, "Yes")       ~ "Yes",                 # if "Yes" is detected, then cell value converts to yes
               str_detect(.x, "No")        ~ "No",                  # then, if "No" is detected, then cell value converts to no
               str_detect(.x, "Unknown")   ~ "Unknown",             # then, if "Unknown" is detected, then cell value converts to Unknown
               TRUE                        ~ as.character(.x)))),   # then, if anything else if it kept as is
      .keep = "unused")                                             # old columns removed, leaving only _mod columns
```


Now you can see in the column `symptoms_ever` that if the person EVER said "Yes" to symptoms, then only "Yes" is displayed.  

```{r message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(cases_clean, rownames = FALSE, options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap')
```


See the [original data](#dedup_data).  


## Probabilistic de-duplication  

Sometimes, you may want to identify "likely" duplicates based on similarity (e.g. string "distance") across several columns such as name, age, sex, date of birth, etc. You can apply a probabilistic matching algorithm to identify likely duplicates.  

See the page on [Joining data] for an explanation on this method. The section on Probabilistic Matching contains an example of applying these algorithms to compare a data frame to *itself*, thus performing probabilistic de-duplication.  



<!-- ======================================================= -->
## Resources { }

Much of the information in this page is adapted from these resources and vignettes online:  

[datanovia](https://www.datanovia.com/en/lessons/identify-and-remove-duplicate-data-in-r/)

[dplyr tidyverse reference](https://dplyr.tidyverse.org/reference/slice.html)  

[cran janitor vignette](https://cran.r-project.org/web/packages/janitor/vignettes/janitor.html#explore-records-with-duplicated-values-for-specific-combinations-of-variables-with-get_dupes)  

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/deduplication.Rmd-->


# Lặp, vòng lặp, và danh sách {#iteration}  

Epidemiologists often are faced with repeating analyses on subgroups such as countries, districts, or age groups. These are but a few of the many situations involving *iteration*. Coding your iterative operations using the approaches below will help you perform such repetitive tasks faster, reduce the chance of error, and reduce code length.  

This page will introduce two approaches to iterative operations - using *for loops* and using the package **purrr**.  

1) *for loops* iterate code across a series of inputs, but are less common in R than in other programming languages. Nevertheless, we introduce them here as a learning tool and reference  
2) The **purrr** package is the **tidyverse** approach to iterative operations - it works by "mapping" a function across many inputs (values, columns, datasets, etc.)  

Along the way, we'll show examples like:  

* Importing and exporting multiple files  
* Creating epicurves for multiple jurisdictions  
* Running T-tests for several columns in a data frame  

In the **purrr** [section](#iter_purrr) we will also provide several examples of creating and handling `lists`.  



## Chuẩn bị {  }
     
     
### Load packages {.unnumbered}  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
     rio,         # import/export
     here,        # file locator
     purrr,       # iteration
     tidyverse    # data management and visualization
)
```


### Import data {.unnumbered}  

We import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Import data with the `import()` function from the **rio** package (it handles many file types like .xlsx, .csv, .rds - see the [Import and export] page for details).  

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.rds")
```

The first 50 rows of the linelist are displayed below.


```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```



<!-- ======================================================= -->

## *for loops* {  }

### *for loops* in R {#iter_loops .unnumbered}  

*for loops* are not emphasized in R, but are common in other programming languages. As a beginner, they can be helpful to learn and practice with because they are easier to "explore", "de-bug", and otherwise grasp exactly what is happening for each iteration, especially when you are not yet comfortable writing your own functions.  

You may move quickly through *for loops* to iterating with mapped functions with **purrr** (see [section below](#iter_purrr)).  


### Core components {.unnumbered}   

A *for loop* has three core parts:  
     
1) The **sequence** of items to iterate through  
2) The **operations** to conduct per item in the sequence  
3) The **container** for the results (optional)  

The basic syntax is: `for (item in sequence) {do operations using item}`. Note the parentheses and the curly brackets. The results could be printed to console, or stored in a container R object.   

A simple *for loop* example is below.   

```{r}
for (num in c(1,2,3,4,5)) {  # the SEQUENCE is defined (numbers 1 to 5) and loop is opened with "{"
  print(num + 2)             # The OPERATIONS (add two to each sequence number and print)
}                            # The loop is closed with "}"                            
                             # There is no "container" in this example
```



### Sequence {.unnumbered}  

This is the "for" part of a *for loop* - the operations will run "for" each item in the sequence. The sequence can be a series of values (e.g. names of jurisdictions, diseases, column names, list elements, etc), or it can be a series of consecutive numbers (e.g. 1,2,3,4,5). Each approach has their own utilities, described below.  

The basic structure of a sequence statement is `item in vector`.  

* You can write any character or word in place of "item" (e.g. "i", "num", "hosp", "district", etc.). The value of this "item" changes with each iteration of the loop, proceeding through each value in the vector.  
* The *vector* could be of character values, column names, or perhaps a sequence of numbers - these are the values that will change with each iteration. You can use them within the *for loop* operations using the "item" term.  

**Example: sequence of character values**  

In this example, a loop is performed for each value in a pre-defined character vector of hospital names.  

```{r}
# make vector of the hospital names
hospital_names <- unique(linelist$hospital)
hospital_names # print
```

We have chosen the term `hosp` to represent values from the vector `hospital_names`. For the first iteration of the loop, the value of `hosp` will be ` hospital_names[[1]]`. For the second loop it will be ` hospital_names[[2]]`. And so on...  

```{r, eval=F}
# a 'for loop' with character sequence

for (hosp in hospital_names){       # sequence
  
       # OPERATIONS HERE
  }
```

**Example: sequence of column names**  
     
This is a variation on the character sequence above, in which the names of an existing R object are extracted and become the vector. For example, the column names of a data frame. Conveniently, in the operations code of the *for loop*, the column names can be used to *index* (subset) their original data frame  

Below, the sequence is the `names()` (column names) of the `linelist` data frame. Our "item" name is `col`, which will represent each column name as the loops proceeds.  

For purposes of example, we include operations code inside the *for loop*, which is run for every value in the sequence. In this code, the sequence values (column names) are used to *index* (subset) `linelist`, one-at-a-time. As taught in the [R basics] page, double branckets `[[ ]]` are used to subset. The resulting column is passed to `is.na()`, then to `sum()` to produce the number of values in the column that are missing. The result is printed to the console - one number for each column.  

A note on indexing with column names - whenever referencing the column itself *do not just write "col"!* `col` represents just the character column name! To refer to the entire column you must use the column name as an *index* on `linelist` via `linelist[[col]]`.  

```{r}
for (col in names(linelist)){        # loop runs for each column in linelist; column name represented by "col" 
  
  # Example operations code - print number of missing values in column
  print(sum(is.na(linelist[[col]])))  # linelist is indexed by current value of "col"
     
}
```



**Sequence of numbers**  
     
In this approach, the sequence is a series of consecutive numbers. Thus, the value of the "item" is not a character value (e.g. "Central Hospital" or "date_onset") but is a number. This is useful for looping through data frames, as you can use the "item" number inside the *for loop* to index the data frame by *row number*.  

For example, let's say that you want to loop through every row in your data frame and extract certain information. Your "items" would be numeric row numbers. Often, "items" in this case are written as `i`.  

The *for loop* process could be explained in words as "for every item in a sequence of numbers from 1 to the total number of rows in my data frame, do X". For the first iteration of the loop, the value of "item" `i` would be 1. For the second iteration, `i` would be 2, etc.  

Here is what the sequence looks like in code: `for (i in 1:nrow(linelist)) {OPERATIONS CODE}` where `i` represents the "item" and `1:nrow(linelist)` produces a sequence of consecutive numbers from 1 through the number of rows in `linelist`.  


```{r, eval=F}
for (i in 1:nrow(linelist)) {  # use on a data frame
  # OPERATIONS HERE
}  
```

If you want the sequence to be numbers, but you are starting from a vector (not a data frame), use the shortcut `seq_along()` to return a sequence of numbers for each element in the vector. For example, `for (i in seq_along(hospital_names) {OPERATIONS CODE}`.  

The below code actually returns numbers, which would become the value of `i` in their respective loop.     

```{r}
seq_along(hospital_names)  # use on a named vector
```

One advantage of using numbers in the sequence is that is easy to also use the `i` number to index a *container* that stores the loop outputs. There is an example of this in the Operations section below.  

### Operations  {.unnumbered}  

This is code within the curly brackets `{ }` of the *for loop*. You want this code to run for each "item" in the *sequence*. Therefore, be careful that every part of your code that changes by the "item" is correctly coded such that it actually changes! E.g. remember to use `[[ ]]` for indexing.  

In the example below, we iterate through each row in the `linelist`. The `gender` and `age` values of each row are pasted together and stored in the container character vector `cases_demographics`. Note how we also use indexing `[[i]]` to save the loop output to the correct position in the "container" vector.  

```{r}
# create container to store results - a character vector
cases_demographics <- vector(mode = "character", length = nrow(linelist))

# the for loop
for (i in 1:nrow(linelist)){
  
  # OPERATIONS
  # extract values from linelist for row i, using brackets for indexing
  row_gender  <- linelist$gender[[i]]
  row_age     <- linelist$age_years[[i]]    # don't forget to index!
     
  # combine gender-age and store in container vector at indexed location
  cases_demographics[[i]] <- str_c(row_gender, row_age, sep = ",") 

}  # end for loop


# display first 10 rows of container
head(cases_demographics, 10)
```


### Container {.unnumbered}

Sometimes the results of your *for loop* will be printed to the console or RStudio Plots pane. Other times, you will want to store the outputs in a "container" for later use. Such a container could be a vector, a data frame, or even a list.  

It is most efficient to create the container for the results *before* even beginning the *for loop*. In practice, this means creating an empty vector, data frame, or list. These can be created with the functions `vector()` for vectors or lists, or with `matrix()` and `data.frame()` for a data frame. 

**Empty vector**  

Use `vector()` and specify the `mode = ` based on the expected class of the objects you will insert - either "double" (to hold numbers), "character", or "logical". You should also set the `length = ` in advance. This should be the length of your *for loop* sequence.  

Say you want to store the median delay-to-admission for each hospital. You would use "double" and set the length to be the number of expected outputs (the number of unique hospitals in the data set).  

```{r}
delays <- vector(
  mode = "double",                            # we expect to store numbers
  length = length(unique(linelist$hospital))) # the number of unique hospitals in the dataset
```

**Empty data frame**  
     
You can make an empty data frame by specifying the number of rows and columns like this:  
     
```{r, eval=F}
delays <- data.frame(matrix(ncol = 2, nrow = 3))
```


**Empty list**  
     
You may want store some plots created by a *for loop* in a list. A list is like vector, but holds other R objects within it that can be of different classes. Items in a list could be a single number, a dataframe, a vector, and even another list.  

You actually initialize an empty list using the same `vector()` command as above, but with `mode = "list"`. Specify the length however you wish.  

```{r, eval=F}
plots <- vector(mode = "list", length = 16)
```




### Printing {.unnumbered}  

Note that to print from within a *for loop* you will likely need to explicitly wrap with the function `print()`.  

In this example below, the sequence is an explicit character vector, which is used to subset the linelist by hospital. The results are not stored in a container, but rather are printed to console with the `print()` function.    

```{r}
for (hosp in hospital_names){ 
     hospital_cases <- linelist %>% filter(hospital == hosp)
     print(nrow(hospital_cases))
}
```


### Testing your for loop {.unnumbered}

To test your loop, you can run a command to make a temporary assignment of the "item", such as `i <- 10` or `hosp <- "Central Hospital"`. Do this *outside the loop* and then run your operations code only (the code within the curly brackets) to see if the expected results are produced.  




### Looping plots {.unnumbered}

To put all three components together (container, sequence, and operations) let's try to plot an epicurve for each hospital (see page on [Epidemic curves]).  

We can make a nice epicurve of *all* the cases by gender using the **incidence2** package as below:  

```{r, warning=F, message=F}
# create 'incidence' object
outbreak <- incidence2::incidence(   
     x = linelist,                   # dataframe - complete linelist
     date_index = date_onset,        # date column
     interval = "week",              # aggregate counts weekly
     groups = gender,                # group values by gender
     na_as_group = TRUE)             # missing gender is own group

# plot epi curve
plot(outbreak,                       # name of incidence object
     fill = "gender",                # color bars by gender
     color = "black",                # outline color of bars
     title = "Outbreak of ALL cases" # title
     )
```

To produce a separate plot for each hospital's cases, we can put this epicurve code within a *for loop*. 

First, we save a named vector of the unique hospital names, `hospital_names`. The *for loop* will run once for each of these names: `for (hosp in hospital_names)`. Each iteration of the *for loop*, the current hospital name from the vector will be represented as `hosp` for use within the loop.  

Within the loop operations, you can write R code as normal, but use the "item" (`hosp` in this case) knowing that its value will be changing. Within this loop:  
     
* A `filter()` is applied to `linelist`, such that column `hospital` must equal the current value of `hosp`  
* The incidence object is created on the filtered linelist  
* The plot for the current hospital is created, with an auto-adjusting title that uses `hosp`  
* The plot for the current hospital is temporarily saved and then printed  
* The loop then moves onward to repeat with the next hospital in `hospital_names`  

```{r, out.width='50%', message = F}
# make vector of the hospital names
hospital_names <- unique(linelist$hospital)

# for each name ("hosp") in hospital_names, create and print the epi curve
for (hosp in hospital_names) {
     
     # create incidence object specific to the current hospital
     outbreak_hosp <- incidence2::incidence(
          x = linelist %>% filter(hospital == hosp),   # linelist is filtered to the current hospital
          date_index = date_onset,
          interval = "week", 
          groups = gender,
          na_as_group = TRUE
     )
     
     # Create and save the plot. Title automatically adjusts to the current hospital
     plot_hosp <- plot(
       outbreak_hosp,
       fill = "gender",
       color = "black",
       title = stringr::str_glue("Epidemic of cases admitted to {hosp}")
     )
     
     # print the plot for the current hospital
     print(plot_hosp)
     
} # end the for loop when it has been run for every hospital in hospital_names 
```



### Tracking progress of a loop {.unnumbered} 

A loop with many iterations can run for many minutes or even hours. Thus, it can be helpful to print the progress to the R console. The `if` statement below can be placed *within* the loop operations to print every 100th number. Just adjust it so that `i` is the "item" in your loop.  

```{r, eval=F}
# loop with code to print progress every 100 iterations
for (i in seq_len(nrow(linelist))){

  # print progress
  if(i %% 100==0){    # The %% operator is the remainder
    print(i)

}
```





<!-- ======================================================= -->
## **purrr** and lists {#iter_purrr}
     
Another approach to iterative operations is the **purrr** package - it is the **tidyverse** approach to iteration.  

If you are faced with performing the same task several times, it is probably worth creating a generalised solution that you can use across many inputs. For example, producing plots for multiple jurisdictions, or importing and combining many files.  

There are also a few other advantages to **purrr** - you can use it with pipes `%>%`, it handles errors better than normal *for loops*, and the syntax is quite clean and simple! If you are using a *for loop*, you can probably do it more clearly and succinctly with **purrr**! 
   
Keep in mind that **purrr** is a *functional programming tool*. That is, the operations that are to be iteratively applied are wrapped up into *functions*. See the [Writing functions] page to learn how to write your own functions.  

**purrr** is also almost entirely based around *lists* and *vectors* - so think about it as applying a function to each element of that list/vector!
     
### Load packages {.unnumbered}  
     
**purrr** is part of the **tidyverse**, so there is no need to install/load a separate package.  

```{r}
pacman::p_load(
     rio,            # import/export
     here,           # relative filepaths
     tidyverse,      # data mgmt and viz
     writexl,        # write Excel file with multiple sheets
     readxl          # import Excel with multiple sheets
)
```


### `map()` {.unnumbered}  

One core **purrr** function is `map()`, which "maps" (applies) a function to each input element of a list/vector you provide.  

The basic syntax is `map(.x = SEQUENCE, .f = FUNCTION, OTHER ARGUMENTS)`. In a bit more detail:  
     
* `.x = ` are the *inputs* upon which the `.f` function will be iteratively applied - e.g. a vector of jurisdiction names, columns in a data frame, or a list of data frames  
* `.f = ` is the *function* to apply to each element of the `.x` input - it could be a function like `print()` that already exists, or a custom function that you define. The function is often written after a tilde `~` (details below). 

A few more notes on syntax:  
     
* If the function needs no further arguments specified, it can be written with no parentheses and no tilde (e.g. `.f = mean`). To provide arguments that will be the same value for each iteration, provide them within `map()` but outside the `.f = ` argument, such as the `na.rm = T` in `map(.x = my_list, .f = mean, na.rm=T)`.  
* You can use `.x` (or simply `.`) *within* the `.f = ` function as a placeholder for the `.x` value of that iteration  
* Use tilde syntax (`~`) to have greater control over the function - write the function as normal with parentheses, such as: `map(.x = my_list, .f = ~mean(., na.rm = T))`. Use this syntax particularly if the value of an argument will change each iteration, or if it is the value `.x` itself (see examples below)  


**The output of using `map()` is a *list*** - a list is an object class like a vector but whose elements can be of different classes. So, a list produced by `map()` could contain many data frames, or many vectors, many single values, or even many lists! There are alternative versions of `map()` explained below that produce other types of outputs (e.g. `map_dfr()` to produce a data frame, `map_chr()` to produce character vectors, and `map_dbl()` to produce numeric vectors).  

#### Example - import and combine Excel sheets {#iter_combined .unnumbered}  

**Let's demonstrate with a common epidemiologist task:** - *You want to import an Excel workbook with case data, but the data are split across different named sheets in the workbook. How do you efficiently import and combine the sheets into one data frame?*  

Let's say we are sent the below Excel workbook. Each sheet contains cases from a given hospital.  

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "hospital_linelists_excel_sheets.png"))
```

Here is one approach that uses `map()`:  
     
1) `map()` the function `import()` so that it runs for each Excel sheet  
2) Combine the imported data frames into one using `bind_rows()`  
3) Along the way, preserve the original sheet name for each row, storing this information in a new column in the final data frame  

First, we need to extract the sheet names and save them. We provide the Excel workbook's file path to the function `excel_sheets()` from the package **readxl**, which extracts the sheet names. We store them in a character vector called `sheet_names`.  

```{r, echo=F}
sheet_names <- readxl::excel_sheets(here::here("data", "example", "hospital_linelists.xlsx"))

```

```{r, eval=F}
sheet_names <- readxl::excel_sheets("hospital_linelists.xlsx")
```

Here are the names:  

```{r}
sheet_names
```

Now that we have this vector of names, `map()` can provide them one-by-one to the function `import()`. In this example, the `sheet_names` are `.x` and `import()` is the function `.f`.  

Recall from the [Import and export] page that when used on Excel workbooks, `import()` can accept the argument `which = ` specifying the sheet to import. Within the `.f` function `import()`, we provide `which = .x`, whose value will change with each iteration through the vector `sheet_names` - first "Central Hospital", then "Military Hospital", etc.  

Of note - because we have used `map()`, the data in each Excel sheet will be saved as a separate data frame within a list. We want each of these list elements (data frames) to have a *name*, so before we pass `sheet_names` to `map()` we pass it through `set_names()` from **purrr**, which ensures that each list element gets the appropriate name.  

We save the output list as `combined`.  

```{r, echo=F}
combined <- sheet_names %>% 
  purrr::set_names() %>% 
  map(.f = ~import(here::here("data", "example", "hospital_linelists.xlsx"), which = .x))
```

```{r, eval=F}
combined <- sheet_names %>% 
  purrr::set_names() %>% 
  map(.f = ~import("hospital_linelists.xlsx", which = .x))
```

When we inspect output, we see that the data from each Excel sheet is saved in the list with a name. This is good, but we are not quite finished.  


```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "sheets_as_list.png"))
```

Lastly, we use the function `bind_rows()` (from **dplyr**) which accepts the list of similarly-structured data frames and combines them into one data frame. To create a new column from the list element *names*, we use the argument `.id = ` and provide it with the desired name for the new column.  

Below is the whole sequence of commands:  

```{r, echo=F}
sheet_names <- readxl::excel_sheets(here::here("data", "example", "hospital_linelists.xlsx"))

combined <- sheet_names %>% 
  purrr::set_names() %>% 
  map(.f = ~import(here::here("data", "example", "hospital_linelists.xlsx"), which = .x)) %>% 
  bind_rows(.id = "origin_sheet")
```


```{r, eval=F}
sheet_names <- readxl::excel_sheets("hospital_linelists.xlsx")  # extract sheet names
 
combined <- sheet_names %>%                                     # begin with sheet names
  purrr::set_names() %>%                                        # set their names
  map(.f = ~import("hospital_linelists.xlsx", which = .x)) %>%  # iterate, import, save in list
  bind_rows(.id = "origin_sheet") # combine list of data frames, preserving origin in new column  
```

And now we have one data frame with a column containing the sheet of origin!  

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "sheets_as_df.png"))
```

There are variations of `map()` that you should be aware of. For example, `map_dfr()` returns a data frame, not a list. Thus, we could have used it for the task above and not have had to bind rows. But then we would not have been able to capture which sheet (hospital) each case came from.  

Other variations include `map_chr()`, `map_dbl()`. These are very useful functions for two reasons. Firstly. they automatically convert the output of an iterative function into a vector (not a list). Secondly, they can explicitly control the class that the data comes back in - you ensure that your data comes back as a character vector with `map_chr()`, or numeric vector with `map_dbl()`. Lets return to these later in the section!

The functions `map_at()` and `map_if()` are also very useful for iteration - they allow you to specify which elements of a list you should iterate at! These work by simply applying a vector of indexes/names (in the case of `map_at()`) or a logical test (in the case of `map_if()`).  

Lets use an example where we didn't want to read the first sheet of hospital data. We use `map_at()` instead of `map()`, and specify the `.at = ` argument to `c(-1)` which means to *not* use the first element of `.x`. Alternatively, you can provide a vector of positive numbers, or names, to `.at = ` to specify which elements to use.  

```{r, echo=F}
sheet_names <- readxl::excel_sheets(here::here("data", "example", "hospital_linelists.xlsx"))

combined <- sheet_names %>% 
     purrr::set_names() %>% 
     # exclude the first sheet
     map_at(.f = ~import(here::here("data", "example", "hospital_linelists.xlsx"), which = .x),
            .at = c(-1))
```


```{r, eval=F}
sheet_names <- readxl::excel_sheets("hospital_linelists.xlsx")

combined <- sheet_names %>% 
     purrr::set_names() %>% 
     # exclude the first sheet
     map_at(.f = ~import( "hospital_linelists.xlsx", which = .x),
            .at = c(-1))
```

Note that the first sheet name will still appear as an element of the output list - but it is only a single character name (not a data frame). You would need to remove this element before binding rows. We will cover how to remove and modify list elements in a later section.  


### Split dataset and export {.unnumbered}  

Below, we give an example of how to split a dataset into parts and then use `map()` iteration to export each part as a separate Excel sheet, or as a separate CSV file.  

#### Split dataset {.unnumbered}  

Let's say we have the complete case `linelist` as a data frame, and we now want to create a separate linelist for each hospital and export each as a separate CSV file. Below, we do the following steps:  
     
Use `group_split()` (from **dplyr**) to split the `linelist` data frame by unique values in column `hospital`. The output is a list containing one data frame per hospital subset.  

```{r}
linelist_split <- linelist %>% 
     group_split(hospital)
```

We can run `View(linelist_split)` and see that this list contains 6 data frames ("tibbles"), each representing the cases from one hospital. 

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "purrr_linelist_split.png"))
```

However, note that the data frames in the list do not have names by default! We want each to have a name, and then to use that name when saving the CSV file.  

One approach to extracting the names is to use `pull()` (from **dplyr**) to extract the `hospital` column from each data frame in the list. Then, to be safe, we convert the values to character and then use `unique()` to get the name for that particular data frame. All of these steps are applied to each data frame via `map()`.  

```{r}
names(linelist_split) <- linelist_split %>%   # Assign to names of listed data frames 
     # Extract the names by doing the following to each data frame: 
     map(.f = ~pull(.x, hospital)) %>%        # Pull out hospital column
     map(.f = ~as.character(.x)) %>%          # Convert to character, just in case
     map(.f = ~unique(.x))                    # Take the unique hospital name
```

We can now see that each of the list elements has a name. These names can be accessed via `names(linelist_split)`.  

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "purrr_linelist_split_named.png"))
```

```{r}
names(linelist_split)
```


##### More than one `group_split()` column {.unnumbered}  

If you wanted to split the linelist by *more than one grouping column*, such as to produce subset linelist by intersection of hospital AND gender, you will need a different approach to naming the list elements. This involves collecting the unique "group keys" using `group_keys()` from **dplyr** - they are returned as a data frame. Then you can combine the group keys into values with `unite()` as shown below, and assign these conglomerate names to `linelist_split`.  

```{r}
# split linelist by unique hospital-gender combinations
linelist_split <- linelist %>% 
     group_split(hospital, gender)

# extract group_keys() as a dataframe
groupings <- linelist %>% 
     group_by(hospital, gender) %>%       
     group_keys()

groupings      # show unique groupings 
```

Now we combine the groupings together, separated by dashes, and assign them as the names of list elements in `linelist_split`. This takes some extra lines as we replace `NA` with "Missing", use `unite()` from **dplyr** to combine the column values together (separated by dashes), and then convert into an un-named vector so it can be used as names of `linelist_split`.  

```{r, eval=F}
# Combine into one name value 
names(linelist_split) <- groupings %>% 
     mutate(across(everything(), replace_na, "Missing")) %>%  # replace NA with "Missing" in all columns
     unite("combined", sep = "-") %>%                         # Unite all column values into one
     setNames(NULL) %>% 
     as_vector() %>% 
     as.list()
```



#### Export as Excel sheets {.unnumbered}  

To export the hospital linelists as *an Excel workbook with one linelist per sheet*, we can just provide the named list `linelist_split` to the `write_xlsx()` function from the **writexl** package. This has the ability to save one Excel workbook with multiple sheets. The list element names are automatically applied as the sheet names.  

```{r, eval=F}
linelist_split %>% 
     writexl::write_xlsx(path = here("data", "hospital_linelists.xlsx"))
```

You can now open the Excel file and see that each hospital has its own sheet.  

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "purrr_export_sheets.png"))
```

#### Export as CSV files {.unnumbered}  

It is a bit more complex command, but you can also export each hospital-specific linelist as a separate CSV file, with a file name specific to the hospital.  

Again we use `map()`: we take the vector of list element names (shown above) and use `map()` to iterate through them, applying `export()` (from the **rio** package, see [Import and export] page) on the data frame in the list `linelist_split` that has that name. We also use the name to create a unique file name. Here is how it works:  
     
* We begin with the vector of character names, passed to `map()` as `.x`  
* The `.f` function is `export()` , which requires a data frame and a file path to write to  
* The input `.x` (the hospital name) is used *within* `.f` to extract/index that specific element of `linelist_split` list. This results in only one data frame at a time being provided to `export()`.  
* For example, when `map()` iterates for "Military Hospital", then `linelist_split[[.x]]` is actually `linelist_split[["Military Hospital"]]`, thus returning the second element of `linelist_split` - which is all the cases from Military Hospital.  
* The file path provided to `export()` is dynamic via use of `str_glue()` (see [Characters and strings] page):  
     * `here()` is used to get the base of the file path and specify the "data" folder (note single quotes to not interrupt the `str_glue()` double quotes)  
* Then a slash `/`, and then again the `.x` which prints the current hospital name to make the file identifiable  
* Finally the extension ".csv" which `export()` uses to create a CSV file  

```{r, eval=F, message = F, warning=F}
names(linelist_split) %>%
     map(.f = ~export(linelist_split[[.x]], file = str_glue("{here('data')}/{.x}.csv")))
```
Now you can see that each file is saved in the "data" folder of the R Project "Epi_R_handbook"!  
     
```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "purrr_export_csv.png"))
```



### Custom functions {.unnumbered}  

You may want to create your own function to provide to `map()`.  

Let's say we want to create epidemic curves for each hospital's cases. To do this using **purrr**, our `.f` function can be `ggplot()` and extensions with `+` as usual. As the output of `map()` is always a list, the plots are stored in a list. Because they are plots, they can be extracted and plotted with the `ggarrange()` function from the **ggpubr** package ([documentation](https://rpkgs.datanovia.com/ggpubr/reference/ggarrange.html)).  


```{r, message = F, warning=F}

# load package for plotting elements from list
pacman::p_load(ggpubr)

# map across the vector of 6 hospital "names" (created earlier)
# use the ggplot function specified
# output is a list with 6 ggplots

hospital_names <- unique(linelist$hospital)

my_plots <- map(
  .x = hospital_names,
  .f = ~ggplot(data = linelist %>% filter(hospital == .x)) +
                geom_histogram(aes(x = date_onset)) +
                labs(title = .x)
)

# print the ggplots (they are stored in a list)
ggarrange(plotlist = my_plots, ncol = 2, nrow = 3)
```

If this `map()` code looks too messy, you can achieve the same result by saving your specific `ggplot()` command as a custom user-defined function, for example we can name it `make_epicurve())`. This function is then used within the `map()`. `.x` will be iteratively replaced by the hospital name, and used as `hosp_name` in the `make_epicurve()` function. See the page on [Writing functions].

```{r, eval=F}
# Create function
make_epicurve <- function(hosp_name){
  
  ggplot(data = linelist %>% filter(hospital == hosp_name)) +
    geom_histogram(aes(x = date_onset)) +
    theme_classic()+
    labs(title = hosp_name)
  
}
```

```{r, eval=F}
# mapping
my_plots <- map(hospital_names, ~make_epicurve(hosp_name = .x))

# print the ggplots (they are stored in a list)
ggarrange(plotlist = my_plots, ncol = 2, nrow = 3)
```




### Mapping a function across columns {.unnumbered}  

Another common use-case is to map a function across many columns. Below, we `map()` the function `t.test()` across numeric columns in the data frame `linelist`, comparing the numeric values by gender.  

Recall from the page on [Simple statistical tests] that `t.test()` can take inputs in a formula format, such as `t.test(numeric column ~ binary column)`. In this example, we do the following:    
     
* The numeric columns of interest are selected from `linelist` - these become the `.x` inputs to `map()`  
* The function `t.test()` is supplied as the `.f` function, which is applied to each numeric column  
* Within the parentheses of `t.test()`:  
  * the first `~` precedes the `.f` that `map()` will iterate over `.x`  
  * the `.x` represents the current column being supplied to the function `t.test()`  
  * the second `~` is part of the t-test equation described above  
  * the `t.test()` function expects a binary column on the right-hand side of the equation. We supply the vector `linelist$gender` independently and statically (note that it is not included in `select()`).  

`map()` returns a list, so the output is a list of t-test results - one list element for each numeric column analysed.  

```{r}
# Results are saved as a list
t.test_results <- linelist %>% 
  select(age, wt_kg, ht_cm, ct_blood, temp) %>%  # keep only some numeric columns to map across
  map(.f = ~t.test(.x ~ linelist$gender))        # t.test function, with equation NUMERIC ~ CATEGORICAL
```

Here is what the list `t.test_results` looks like when opened (Viewed) in RStudio. We have highlighted parts that are important for the examples in this page.  

* You can see at the top that the whole list is named `t.test_results` and has five elements. Those five elements are named `age`, `wt_km`, `ht_cm`, `ct_blood`, `temp` after each variable that was used in a t-test with `gender` from the `linelist`.  
* Each of those five elements are themselves lists, with elements within them such as `p.value` and `conf.int`. Some of these elements like `p.value` are single numbers, whereas some such as `estimate` consist of two or more elements (`mean in group f` and `mean in group m`).  

```{r, out.height="150%", echo=F}
knitr::include_graphics(here::here("images", "purrr_ttest.png"))
```


Note: Remember that if you want to apply a function to only certain columns in a data frame, you can also simply use `mutate()` and `across()`, as explained in the [Cleaning data and core functions] page. Below is an example of applying `as.character()` to only the "age" columns. Note the placement of the parentheses and commas.  

```{r, eval=F}
# convert columns with column name containing "age" to class Character
linelist <- linelist %>% 
  mutate(across(.cols = contains("age"), .fns = as.character))  
```


### Extract from lists {.unnumbered}  

As `map()` produces an output of class List, we will spend some time discussing how to extract data from lists using accompanying **purrr** functions. To demonstrate this, we will use the list `t.test_results` from the previous section. This is a list of 5 lists - each of the 5 lists contains the results of a t-test between a column from `linelist` data frame and its binary column `gender`. See the image in the section above for a visual of the list structure.  

#### Names of elements {.unnumbered}  

To extract the names of the elements themselves, simply use `names()` from **base** R. In this case, we use `names()` on `t.test_results` to return the names of each sub-list, which are the names of the 5 variables that had t-tests performed.  

```{r}
names(t.test_results)
```

#### Elements by name or position {.unnumbered}  

To extract list elements by name or by position you can use brackets `[[ ]]` as described in the [R basics] page. Below we use double brackets to index the list `t.tests_results` and display the first element which is the results of the t-test on `age`.  

```{r}
t.test_results[[1]] # first element by position
t.test_results[[1]]["p.value"] # return element named "p.value" from first element  
```

However, below we will demonstrate use of the simple and flexible **purrr** functions `map()` and `pluck()` to achieve the same outcomes.  

#### `pluck()` {.unnumbered}  

`pluck()` pulls out elements by name or by position. For example - to extract the t-test results for age, you can use `pluck()` like this:  

```{r}
t.test_results %>% 
  pluck("age")        # alternatively, use pluck(1)
```

Index deeper levels by specifying the further levels with commas. The below extracts the element named "p.value" from the list `age` within the list `t.test_results`. You can also use numbers instead of character names.    

```{r}
t.test_results %>% 
  pluck("age", "p.value")
```

You can extract such inner elements from *all* first-level elements by using `map()` to run the `pluck()` function across each first-level element. For example, the below code extracts the "p.value" elements from all lists within `t.test_results`. The list of t-test results is the `.x` iterated across, `pluck()` is the `.f` function being iterated, and the value "p-value" is provided to the function.     

```{r}
t.test_results %>%
  map(pluck, "p.value")   # return every p-value
```

As another alternative, `map()` offers a shorthand where you can write the element name in quotes, and it will pluck it out. If you use `map()` the output will be a list, whereas if you use `map_chr()` it will be a named character vector and if you use `map_dbl()` it will be a named numeric vector.  

```{r}
t.test_results %>% 
  map_dbl("p.value")   # return p-values as a named numeric vector
```

You can read more about `pluck()` in it's **purrr** [documentation](https://purrr.tidyverse.org/reference/pluck.html). It has a sibling function `chuck()` that will return an error instead of NULL if an element does not exist.  



### Convert list to data frame {.unnumbered}  

This is a complex topic - see the Resources section for more complete tutorials. Nevertheless, we will demonstrate converting the list of t-test results into a data frame. We will create a data frame with columns for the variable, its p-value, and the means from the two groups (male and female).  

Here are some of the new approaches and functions that will be used:  

* The function `tibble()` will be used to create a tibble (like a data frame)  
  * We surround the `tibble()` function with curly brackets `{ }` to prevent the entire `t.test_results` from being stored as the first tibble column  
* Within `tibble()`, each column is created explicitly, similar to the syntax of `mutate()`:  
  * The `.` represents `t.test_results`
  * To create a column with the t-test variable names (the names of each list element) we use `names()` as described above  
  * To create a column with the p-values we use `map_dbl()` as described above to pull the `p.value` elements and convert them to a numeric vector  

```{r}
t.test_results %>% {
  tibble(
    variables = names(.),
    p         = map_dbl(., "p.value"))
  }
```

But now let's add columns containing the means for each group (males and females).  

We would need to extract the element `estimate`, but this actually contains *two* elements within it (`mean in group f` and `mean in group m`). So, it cannot be simplified into a vector with `map_chr()` or `map_dbl()`. Instead, we use `map()`, which used within `tibble()` will create *a column of class list within the tibble*! Yes, this is possible!  

```{r}
t.test_results %>% 
  {tibble(
    variables = names(.),
    p = map_dbl(., "p.value"),
    means = map(., "estimate"))}
```

Once you have this list column, there are several **tidyr** functions (part of **tidyverse**) that help you "rectangle" or "un-nest" these "nested list" columns. Read more about them [here](), or by running `vignette("rectangle")`. In brief:  

* `unnest_wider()` - gives each element of a list-column its own column  
* `unnest_longer()` - gives each element of a list-column its own row
* `hoist()` - acts like `unnest_wider()` but you specify which elements to unnest  

Below, we pass the tibble to `unnest_wider()` specifying the tibble's `means` column (which is a nested list). The result is that `means` is replaced by two new columns, each reflecting the two elements that were previously in each `means` cell.  

```{r}
t.test_results %>% 
  {tibble(
    variables = names(.),
    p = map_dbl(., "p.value"),
    means = map(., "estimate")
    )} %>% 
  unnest_wider(means)
```



### Discard, keep, and compact lists {.unnumbered}  

Because working with **purrr** so often involves lists, we will briefly explore some **purrr** functions to modify lists. See the Resources section for more complete tutorials on **purrr** functions.    

* `list_modify()` has many uses, one of which can be to remove a list element  
* `keep()` retains the elements specified to `.p = `, or where a function supplied to `.p = ` evaluates to TRUE  
* `discard()` removes the elements specified to `.p`, or where a function supplied to `.p = ` evaluates to TRUE  
* `compact()` removes all empty elements  

Here are some examples using the `combined` list created in the section above on [using map() to import and combine multiple files](#iter_combined) (it contains 6 case linelist data frames):    

Elements can be removed by name with `list_modify()` and setting the name equal to `NULL`.  

```{r, eval=F}
combined %>% 
  list_modify("Central Hospital" = NULL)   # remove list element by name
```

You can also remove elements by criteria, by providing a "predicate" equation to `.p = ` (an equation that evaluates to either TRUE or FALSE). Place a tilde `~` before the function and use `.x` to represent the list element. Using `keep()` the list elements that evaluate to TRUE will be kept. Inversely, if using `discard()` the list elements that evaluate to TRUE will be removed.  

```{r, eval=F}
# keep only list elements with more than 500 rows
combined %>% 
  keep(.p = ~nrow(.x) > 500)  
```

In the below example, list elements are discarded if their class are not data frames.  

```{r, eval=F}
# Discard list elements that are not data frames
combined %>% 
  discard(.p = ~class(.x) != "data.frame")
```

Your predicate function can also reference elements/columns within each list item. For example, below, list elements where the mean of column `ct_blood` is over 25 are discarded.  

```{r, eval=F}
# keep only list elements where ct_blood column mean is over 25
combined %>% 
  discard(.p = ~mean(.x$ct_blood) > 25)  
```

This command would remove all empty list elements:  

```{r, eval=F}
# Remove all empty list elements
combined %>% 
  compact()
```



### `pmap()` {.unnumbered}

THIS SECTION IS UNDER CONSTRUCTION  



## Apply functions  

The "apply" family of functions is a **base** R alternative to **purrr** for iterative operations. You can read more about them [here](https://www.datacamp.com/community/tutorials/r-tutorial-apply-family).  






<!-- ======================================================= -->
## Resources { }

[for loops with Data Carpentry](https://datacarpentry.org/semester-biology/materials/for-loops-R/)  

The [R for Data Science page on iteration](https://r4ds.had.co.nz/iteration.html#iteration)  

[Vignette on write/read Excel files](https://martinctc.github.io/blog/vignette-write-and-read-multiple-excel-files-with-purrr/)  

A purrr [tutorial](https://jennybc.github.io/purrr-tutorial/index.html) by jennybc 

Another purrr [tutorial](http://www.rebeccabarter.com/blog/2019-08-19_purrr/) by Rebecca Barter  

A purrr [tutorial](http://zevross.com/blog/2019/06/11/the-power-of-three-purrr-poseful-iteration-in-r-with-map-pmap-and-imap/) on map, pmap, and imap  

[purrr cheatsheet](https://raw.githubusercontent.com/rstudio/cheatsheets/master/pngs/thumbnails/purrr-cheatsheet-thumbs.png)

[purrr tips and tricks](https://www.hvitfeldt.me/blog/purrr-tips-and-tricks/)

[keep and discard](https://hookedondata.org/going-off-the-map/#keep-and-discard)

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/iteration.Rmd-->

# (PART) Phân tích dữ liệu {.unnumbered}

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/cat_analysis.Rmd-->

# Bảng mô tả {#tables-descriptive}

```{r out.width = c('75%'), fig.align='center', fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "descriptive_tables.png"))
```

Chương này minh họa cách sử dụng các package **janitor**, **dplyr**, **gtsummary**, **rstatix**, và **base** R để tóm tắt dữ liệu và tạo bảng với thống kê mô tả.

*Chương này bao gồm cách để tạo* bảng cơ bản, trong khi đó chương [Trình bày bảng] bao gồm cách để định dạng đẹp và in chúng.\*

Mỗi package này đều có những ưu và nhược điểm trong từng khía cạnh như sự đơn giản, khả năng tiếp cận kết quả, chất lượng kết quả được hiển thị. Sử dụng chương này để quyết định cách tiếp cận nào phù hợp với trường hợp của bạn.

Bạn có một số lựa chọn khi tạo bảng tóm tắt và bảng chéo. Một số yếu tố cần xem xét bao gồm tính đơn giản của code, khả năng tùy chỉnh, đầu ra mong muốn (được in ra R console, dưới dạng dataframe hoặc dưới dạng hình ảnh "đẹp" .png/.jpeg /.html) và dễ xử lý hậu kỳ. Hãy xem xét các điểm dưới đây khi bạn chọn công cụ cho tình huống của mình.

-   Dùng `tabyl()` từ **janitor** để tạo và "làm đẹp" cho bảng và bảng chéo\
-   Dùng `get_summary_stats()` từ **rstatix** để dễ dàng tạo data frame các tóm tắt thống kê dạng số cho nhiều cột và / hoặc nhóm\
-   Dùng `summarise()` và `count()` từ **dplyr** dành choo các thống kê phức tạp hơn, đầu ra của tidy dataframe hoặc chuẩn bị dữ liệu cho `ggplot()`\
-   Dùng `tbl_summary()` từ **gtsummary** để tạo ra các bảng chi tiết sẵn sàng xuất bản\
-   Dùng `table()` từ **base** R nếu bạn không có khả năng truy cập vào các package trên

<!-- ======================================================= -->

## Chuẩn bị

### Gọi packages {.unnumbered}

Đoạn code này hiển thị việc gọi các packages cần thiết cho các phân tích. Trong sổ tay này, chúng tôi nhấn mạnh đến lệnh `p_load()` từ **pacman**, giúp cài đặt các package nếu cần *và* gọi chúng để sử dụng. Bạn cũng có thể gọi các package đã được cài đặt với `library()` từ **base** R. Xem chương [R cơ bản] để biết thêm thông tin về các package của R.

```{r, warning=F, message=F}
pacman::p_load(
  rio,          # File import
  here,         # File locator
  skimr,        # get overview of data
  tidyverse,    # data management + ggplot2 graphics 
  gtsummary,    # summary statistics and tests
  rstatix,      # summary statistics and statistical tests
  janitor,      # adding totals and percents to tables
  scales,       # easily convert proportions to percents  
  flextable     # converting tables to pretty images
  )
```

### Nhập dữ liệu {.unnumbered}

Chúng ta sẽ nhập bộ dữ liệu về các trường hợp từ một vụ dịch Ebola mô phỏng. Nếu bạn muốn theo dõi, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>bấm để tải xuống dữ liệu linelist "đã làm sạch"</a> (as .rds file). Nhập dữ liệu của bạn bằng hàm `import()` từ package **rio** (chấp nhận nhiều loại tệp như .xlsx, .rds, .csv - xem thêm chi tiết tại chương [Nhập xuất dữ liệu]).

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.rds")
```

50 hàng đầu tiên của linelist được hiển thị như dưới đây.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

<!-- ======================================================= -->

## Duyệt dữ liệu

### **skimr** package {.unnumbered}

Khi sử dụng package **skimr** package, bạn có thể có được cái nhìn tổng quan chi tiết và đẹp về mặt thẩm mỹ của từng biến trong tập dữ liệu của mình. Đọc thêm về **skimr** tại [trang github](https://github.com/ropensci/skimr) của nhà phát triển.

Dưới đây, hàm `skim()` được áp dụng cho toàn bộ data frame `linelist` giúp bạn có cái nhìn tổng quan về data frame và tóm tắt của tất cả các cột (theo lớp).

```{r eval=F}
## get information about each variable in a dataset 
skim(linelist)
```

```{r  echo=F}
# sparkline histograms not showing correctly, so avoiding them.
skim_without_charts(linelist)
```

Bạn cũng có thể sử dụng hàm `summary()` từ **base** R, để lấy thông tin về toàn bộ tập dữ liệu, nhưng kết quả đầu ra có thể khó đọc hơn so với sử dụng **skimr**. Do đó, kết quả không được hiển thị bên dưới để tiết kiệm không gian trang.

```{r, eval=F}
## get information about each column in a dataset 
summary(linelist)
```

### Thống kê tóm tắt {.unnumbered}

Bạn có thể sử dụng các hàm **base** R để trả về thống kê tóm tắt trên một cột dữ liệu dạng số. Bạn có thể trả về hầu hết các thống kê tóm tắt hữu ích cho một cột dạng số bằng cách sử dụng hàm `summary()`, như dưới đây. Lưu ý rằng tên data frame cũng phải được xác định như hình dưới đây.

```{r}
summary(linelist$age_years)
```

Bạn có thể truy cập và lưu một phần cụ thể của nó bằng dấu ngoặc vuông [ ]:

```{r}
summary(linelist$age_years)[[2]]            # return only the 2nd element
# equivalent, alternative to above by element name
# summary(linelist$age_years)[["1st Qu."]]  
```

Bạn có thể trả về các thống kê riêng lẻ với các hàm **base** R như `max()`, `min()`, `median()`, `mean()`, `quantile()`, `sd()`, và `range()`. Xem chương [R cơ bản] để có danh sách đầy đủ.

[***THẬN TRỌNG:*** Nếu dữ liệu của bạn chứa các giá trị missing, R muốn bạn biết điều này và do đó sẽ trả về `NA` trừ khi bạn chỉ định cho các hàm toán học ở trên mà bạn muốn R bỏ qua các giá trị bị thiếu, thông qua đối số `na.rm = TRUE`.]{style="color: orange;"}

Bạn có thể sử dụng hàm `get_summary_stats()` từ package **rstatix** để trả về thống kê tóm tắt *ở định dạng data frame*. Điều này có thể hữu ích cho việc thực hiện các hoạt động tiếp theo hoặc vẽ biểu đồ trên các con số. Xem chương [Các kiểm định thống kê cơ bản] để biết thêm chi tiết về package **rstatix** và các hàm của nó.

```{r}
linelist %>% 
  get_summary_stats(
    age, wt_kg, ht_cm, ct_blood, temp,  # columns to calculate for
    type = "common")                    # summary stats to return

```

## **janitor** package {#tbl_janitor}

Package **janitor** cung cấp hàm `tabyl()` giúp tạo ra các bảng đơn và bảng chéo, có thể được "tô điểm" hoặc sửa đổi bằng các hàm trợ giúp để hiển thị phần trăm, tỷ lệ, số đếm, v.v.

Sau đây, chúng ta sẽ pipe `linelist` data frame tới các hàm của **janitor** và in kết quả. Nếu muốn, bạn cũng có thể lưu các bảng kết quả bằng toán tử gán `<-`.

### tabyl đơn giản {.unnumbered}

Cách sử dụng mặc định của hàm `tabyl()` trên một cột cụ thể tạo ra các giá trị duy nhất, số lượng và "phần trăm" (tỷ lệ thực tế) theo cột. Tỷ lệ có thể có nhiều chữ số thập phân. Bạn có thể điều chỉnh số lượng số thập phân với hàm `adorn_rounding()` như được mô tả bên dưới.

```{r}
linelist %>% tabyl(age_cat)
```

Như bạn có thể thấy ở trên, các giá trị missing sẽ được hiển thị trong một hàng có nhãn `<NA>`. Bạn có thể ngăn điều này bằng cách thêm `show_na = FALSE`. Nếu không có giá trị missing, hàng này sẽ không xuất hiện. Nếu có giá trị missing, tất cả các tỷ lệ sẽ được trình bày dưới dạng thô (mẫu số bao gồm cả `NA`) và "hợp lý" (mẫu số không bao gồm `NA`).

Nếu giá trị cột là dạng Factor và chỉ một vài level nhất định có trong dữ liệu của bạn, thì tất cả các level sẽ vẫn xuất hiện trong bảng. Bạn có thể loại bỏ tính năng này bằng cách thêm `show_missing_levels = FALSE`. Đọc thêm trong chương [Factors].

### Bảng chéo {.unnumbered}

Bảng chéo được tạo bằng cách thêm một hoặc nhiều cột vào hàm `tabyl()`. Lưu ý rằng bây giờ chỉ có số lượng được hiện thị - tỷ lệ và phần trăm có thể được thêm vào bằng các bước bổ sung sẽ được trình bày bên dưới.

```{r}
linelist %>% tabyl(age_cat, gender)
```

### "Tô điểm" cho tabyl {#tbl_adorn .unnumbered}

Sử dụng các hàm "tô điểm" của **janitor** để thêm tổng hoặc chuyển đổi thành tỷ lệ, phần trăm hoặc điều chỉnh hiển thị. Thông thường, bạn sẽ pipe tabyl thông qua một số hàm này..

+--------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Hàm                      | Đầu ra                                                                                                                                                             |
+==========================+====================================================================================================================================================================+
| `adorn_totals()`         | Thêm tổng (`where =` "row", "col", or "both"). Đặt `name =` cho "Tổng".                                                                                            |
+--------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `adorn_percentages()`    | Chuyển đổi số lượng thành tỷ lệ, với `denominator =` "row", "col", hoặc "all"                                                                                      |
+--------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `adorn_pct_formatting()` | Chuyển đổi tỷ lệ thành tỷ lệ phần trăm. Chỉ rõ `digits =`. Loại bỏ ký hiệu "%" bằng `affix_sign = FALSE`.                                                          |
+--------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `adorn_rounding()`       | Làm tròn tỷ lệ bằng `digits =`. Để làm tròn tỷ lệ phần trăm, sử dụng hàm `adorn_pct_formatting()` với `digits =`.                                                  |
+--------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `adorn_ns()`             | Thêm số lượng vào bảng tỷ lệ hoặc phần trăm. Chỉ định `position =` "rear" để hiện thị số lượng trong ngoặc đơn, hoặc "front" để đặt phần trăm vào trong ngoặc đơn. |
+--------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `adorn_title()`          | Thêm tiều đề thông qua đối số `row_name =` và/hoặc `col_name =`                                                                                                    |
+--------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+

Hãy cẩn trọng về thứ tự bạn áp dụng các hàm trên. Dưới đây là một số ví dụ.

Bảng một chiều đơn giản với phần trăm thay vì tỷ lệ mặc định.

```{r}
linelist %>%               # case linelist
  tabyl(age_cat) %>%       # tabulate counts and proportions by age category
  adorn_pct_formatting()   # convert proportions to percents
```

Bảng chéo với tổng hàng và phần trăm hàng.

```{r}
linelist %>%                                  
  tabyl(age_cat, gender) %>%                  # counts by age and gender
  adorn_totals(where = "row") %>%             # add total row
  adorn_percentages(denominator = "row") %>%  # convert counts to proportions
  adorn_pct_formatting(digits = 1)            # convert proportions to percents
```

Bảng chéo được điều chỉnh để cả số lượng và phần trăm đều được hiển thị.

```{r}
linelist %>%                                  # case linelist
  tabyl(age_cat, gender) %>%                  # cross-tabulate counts
  adorn_totals(where = "row") %>%             # add a total row
  adorn_percentages(denominator = "col") %>%  # convert to proportions
  adorn_pct_formatting() %>%                  # convert to percents
  adorn_ns(position = "front") %>%            # display as: "count (percent)"
  adorn_title(                                # adjust titles
    row_name = "Age Category",
    col_name = "Gender")
```

### In với tabyl {.unnumbered}

Theo mặc định, lệnh tabyl sẽ in kết quả thô vào R console của bạn.

Ngoài ra, bạn có thể chuyển tabyl sang **flextable** hoặc package tương tự để in dưới dạng hình ảnh “đẹp” trong RStudio Viewer, có thể được xuất dưới dạng .png, .jpeg, .html, v.v. Điều này đã được thảo luận trong chương Trình bày bảng . Lưu ý rằng nếu in theo cách này và sử dụng `adorn_titles()`, bạn cần thêm vào `placement = "combined"`.

```{r}
linelist %>%
  tabyl(age_cat, gender) %>% 
  adorn_totals(where = "col") %>% 
  adorn_percentages(denominator = "col") %>% 
  adorn_pct_formatting() %>% 
  adorn_ns(position = "front") %>% 
  adorn_title(
    row_name = "Age Category",
    col_name = "Gender",
    placement = "combined") %>% # this is necessary to print as image
  flextable::flextable() %>%    # convert to pretty image
  flextable::autofit()          # format to one line per row 

```

### Sử dụng trên các bảng khác {.unnumbered}

Bạn có thể sử dụng các hàm`adorn_*()` của **janitor** lên các bảng khác, chẳng hạn các bảng được tạo bởi hàm `summarise()` và `count()` của **dplyr**, hoặc `table()` từ **base** R. Đơn giản chỉ cần pipe bảng đến hàm mong muốn của package **janitor**. Ví dụ:


```{r}
linelist %>% 
  count(hospital) %>%   # dplyr function
  adorn_totals()        # janitor function
```

### Lưu với tabyl {.unnumbered}

Nếu bạn muốn chuyển đổi bảng thành một hình ảnh “đẹp” với package **flextable**, bạn có thể lưu nó bằng các hàm như `save_as_html()`, `save_as_word()`, `save_as_ppt()`, và `save_as_image()` từ package **flextable** (sẽ được bàn luận kỹ hơn ở chương [Trình bày bảng]). Ví dụ dưới đây, bảng được lưu lại dưới dạng tệp Word, và có khả năng chỉnh sửa được.

```{r, eval=F}
linelist %>%
  tabyl(age_cat, gender) %>% 
  adorn_totals(where = "col") %>% 
  adorn_percentages(denominator = "col") %>% 
  adorn_pct_formatting() %>% 
  adorn_ns(position = "front") %>% 
  adorn_title(
    row_name = "Age Category",
    col_name = "Gender",
    placement = "combined") %>% 
  flextable::flextable() %>%                     # convert to image
  flextable::autofit() %>%                       # ensure only one line per row
  flextable::save_as_docx(path = "tabyl.docx")   # save as Word document to filepath
```

```{r out.width = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "tabyl_word.png"))
```

### Thống kê {#janitor_age_out_stats .unnumbered}

Bạn có thể áp dụng các kiểm định thống kê bằng tabyls, ví dụ như `chisq.test()` hoặc `fisher.test()` từ package **stats**, như được trình bày dưới đây. Chú ý là giá trị missing không được cho phép vì vậy chúng được loại bỏ khỏi tabyl bằng tùy chọn `show_na = FALSE`.

```{r, warning=F, message=F}
age_by_outcome <- linelist %>% 
  tabyl(age_cat, outcome, show_na = FALSE) 

chisq.test(age_by_outcome)
```

Xem chương [Các kiểm định thống kê cơ bản] để có thêm code và các mẹo liên quan đến thống kê.

### Các mẹo khác {.unnumbered}

-   Thêm đối số `na.rm = TRUE` để loại bỏ các giá trị missing.\
-   Nếu áp dụng bất kỳ hàm trợ giúp `adorn_*()` nào cho các bảng không được tạo bởi `tabyl()`, bạn có thể chỉ định (các) cột cụ thể để áp dụng chúng chẳng hạn như `adorn_percentage(,,,c(cases,deaths))` (chỉ định chúng cho đối số không tên thứ 4). Thay vào đó, hãy cân nhắc sử dụng hàm `summarise()`.\
-   Bạn có thể tìm đọc thêm ở [janitor page](https://cran.r-project.org/web/packages/janitor/vignettes/janitor.html) và [tabyl vignette](https://cran.r-project.org/web/packages/janitor/vignettes/tabyls.html).

## **dplyr** package

**dplyr** là một phần của package **tidyverse** và là một công cụ quản lý dữ liệu rất phổ biến. Tạo bảng với các hàm của **dplyr** như `summarise()` và `count()` là một cách tiếp cận hữu ích để tính toán các tóm tắt thống kê, tổng hợp *theo nhóm*, hoặc chuyển bảng tới `ggplot()`.

`summarise()` tạo một *data frame tổng hợp mới*. Nếu dữ liệu được *tách nhóm*, nó sẽ trả về data frame có một hàng với thống kê tóm tắt được chỉ định cho toàn bộ data frame. Nếu dữ liệu được *nhóm lại*, data frames sẽ có một hàng cho từng *nhóm* (xem chương [Nhóm dữ liệu]).

Bên trong dấu ngoặc đơn của hàm `summarise()`, bạn sẽ cung cấp tên của từng cột cần tổng hợp mới, theo sau là dấu bằng và một hàm thống kê để áp dụng.

[***MẸO:*** Hàm summarise hoạt động được với cả cách viết Anh-Anh và Anh-Mỹ (`summarise()` và `summarize()`).]{style="color: darkgreen;"}

### Lấy số lượng {.unnumbered}

Hàm đơn giản nhất để áp dụng cùng với hàm `summarise()` là `n()`. Để trống dấu ngoặc đơn để đếm số hàng.

```{r}
linelist %>%                 # begin with linelist
  summarise(n_rows = n())    # return new summary dataframe with column n_rows
```

Điều này sẽ thú vị hơn nếu chúng ta đã nhóm dữ liệu trước đó.

```{r}
linelist %>% 
  group_by(age_cat) %>%     # group data by unique values in column age_cat
  summarise(n_rows = n())   # return number of rows *per group*
```

Lệnh trên có thể được rút ngắn bằng cách sử dụng hàm`count()` thay thế. `count()` làm những việc sau:

1)  Nhóm dữ liệu theo các cột được cung cấp cho nó\
2)  Tổng hợp chúng với `n()` (tạo cột `n`)\
3)  Tách nhóm dữ liệu

```{r}
linelist %>% 
  count(age_cat)
```

Bạn có thể thay đổi tên của cột đếm từ mặc định là `n` thành một cái gì đó cụ thể chẳng hạn như `name =`.

Tạo bảng đếm cho hai hoặc nhiều cột sẽ vẫn trả về địng dạng "dọc", với số lượng ở cột `n`. Xem chương [Pivoting dữ liệu] để hiểu thêm về định dạng dữ liệu "dọc" và "ngang".

```{r}
linelist %>% 
  count(age_cat, outcome)
```

### Hiện tất cả các cấp độ {.unnumbered}

Nếu bạn tạo bảng cho một cột có kiểu dữ liệu là *factor*, bạn có thể chắc chắng rằng *tất cả* các cấp độ được trình bày (không chỉ các cấp có giá trị trong dữ liệu) bằng cách thêm `.drop = FALSE` vào lệnh `summarise()` hoặc `count()`.

Kỹ thuật này rất hữu ích để chuẩn hóa các bảng/biểu đồ của bạn. Ví dụ: nếu bạn đang tạo số liệu cho nhiều nhóm con, hoặc liên tục tạo số liệu cho các báo cáo thường quy. Trong các trường hợp này, sự hiện diện của các giá trị trong dữ liệu có thể dao động, nhưng bạn có thể xác định các mức không đổi.

Xem chương [Factors] để có nhiều thông tin hơn.

### Tỷ lệ {#tbl_dplyr_prop .unnumbered}

Tỷ lệ có thể được thêm vào bằng cách piping bảng tới hàm `mutate()` để tạo một cột mới. Định nghĩa cột mới là thương của số quan sát của từng yếu tố (mặc định là `n`) và tổng số quan sát `sum()` của cột (sẽ trả về giá trị là một tỷ lệ).

Lưu ý trong trường hợp này, `sum()` trong lệnh `mutate()` sẽ trả về giá trị của toàn bộ cột `n` để dùng làm mẫu số của tỷ lệ. Như đã được giải thích [trong chương Nhóm dữ liệu](#group_summarise), *nếu* `sum()` được sử dụng với dữ liệu *đã được nhóm* (vd: nếu hàm `mutate()` được theo ngay phía sai hàm `group_by()`), nó sẽ trả về kết quả tổng hợp *theo nhóm*. Như đã nếu ở trên, `count()` hoàn thành nhiệm vụ của mình bằng cách *tách nhóm*. Vì vậy, trong trường hợp này chúng ta sẽ lấy toàn bộ tỷ lệ của cột.

Để dễ dàng hiển thị phần trăm, bạn có thể đưa tỷ lệ vào trong hàm `percent()` từ package **scales** (lưu ý là điều nãy sẽ chuyển kết quả thành dạng ký tự (character)).

```{r}
age_summary <- linelist %>% 
  count(age_cat) %>%                     # group and count by gender (produces "n" column)
  mutate(                                # create percent of column - note the denominator
    percent = scales::percent(n / sum(n))) 

# print
age_summary
```

Dưới đây là phương pháp tính tỷ lệ *trong nhóm*. Nó dựa trên các cấp độ nhóm dữ liệu khác nhau được áp dụng và loại bỏ một cách có chọn lọc. Đầu tiên, dữ liệu được nhóm theo `outcome` thông qua hàm `group_by()`. Sau đó, hàm `count()` được áp dụng. Hàm này sẽ tiếp tục nhóm dữ liệu phân theo `age_cat` và trả vế số lượng theo từng tổ hợp `outcome`-`age-cat`. Quan trọng là - khi nó kết thúc quy trình của mình, hàm `count()` sẽ *tách nhóm* theo `age_cat`, nên nhóm dữ liệu duy nhất còn lại là nhóm ban đầu theo `outcome`. Do đó, bước cuối cùng để tính toán tỷ lệ (mẫu số là `sum(n)`) vẫn được nhóm theo `outcome`.

```{r}
age_by_outcome <- linelist %>%                  # begin with linelist
  group_by(outcome) %>%                         # group by outcome 
  count(age_cat) %>%                            # group and count by age_cat, and then remove age_cat grouping
  mutate(percent = scales::percent(n / sum(n))) # calculate percent - note the denominator is by outcome group
```

```{r, echo=F}
DT::datatable(age_by_outcome, rownames = FALSE, options = list(pageLength = 12, scrollX=T), class = 'white-space: nowrap' )
```

### Vẽ biểu đồ {.unnumbered}

Để hiển thị kết quả từ một bảng "dài" như trên thì vẽ biểu đồ bằng hàm `ggplot()` tương đối trực quan. Dữ liệu một cách tự nhiên có định dạng "dọc", nên tương thích với `ggplot()` một cách tự nhiên. Xem thêm các ví dụ ở chương [ggplot cơ bản] và [Các tips với ggplot].

```{r, warning=F, message=F}
linelist %>%                      # begin with linelist
  count(age_cat, outcome) %>%     # group and tabulate counts by two columns
  ggplot()+                       # pass new data frame to ggplot
    geom_col(                     # create bar plot
      mapping = aes(   
        x = outcome,              # map outcome to x-axis
        fill = age_cat,           # map age_cat to the fill
        y = n))                   # map the counts column `n` to the height
```

### Tổng hợp thống kê {.unnumbered}

Một điểm mạnh của **dplyr** và `summarise()` là khả năng trả về các bảng tổng hợp thống kê nâng cao hơn như `median()`, `mean()`, `max()`, `min()`, `sd()` (độ lệch chuẩn), và phân vị. Bạn cũng có thể sử dụng `sum()` để trả vể số lượng dòng thỏa mãn một điều kiện logic nào đó. Như trên, các kết quả đầu ra này có thể được tạo cho toàn bộ data frame hoặc theo nhóm.

Cú pháp là tương tự- bên trong dấu ngoặc hàm `summarise()` bạn cung cấp tên của từng cột tổng hợp được theo sau bởi dâu bằng và hàm thống kê được áp dụng. Trong hàm thống kê, cung cấp (các) cột sẽ được tính toán và bất kỳ các đối số có liên quan (vd: `na.rm = TRUE` cho tất cả các hàm toán học).

Bạn cũng có thể sử dụng hàm `sum()` để trả vể số lượng dòng thỏa mãn một điều kiện logic cụ thể. Biểu thức điều kiện sẽ được đếm nếu nó được đánh giá là `TRUE`. Ví dụ:

-   `sum(age_years < 18, na.rm=T)`\
-   `sum(gender == "male", na.rm=T)`\
-   `sum(response %in% c("Likely", "Very Likely"))`

Dưới đây, bộ dữ liệu `linelist` được tổng hợp để mô tả những ngày trì hoãn từ khi bắt đầu có triệu chứng đến khi nhập viện (cột `days_onset_hosp`), phân theo bệnh viện.

```{r}
summary_table <- linelist %>%                                        # begin with linelist, save out as new object
  group_by(hospital) %>%                                             # group all calculations by hospital
  summarise(                                                         # only the below summary columns will be returned
    cases       = n(),                                                # number of rows per group
    delay_max   = max(days_onset_hosp, na.rm = T),                    # max delay
    delay_mean  = round(mean(days_onset_hosp, na.rm=T), digits = 1),  # mean delay, rounded
    delay_sd    = round(sd(days_onset_hosp, na.rm = T), digits = 1),  # standard deviation of delays, rounded
    delay_3     = sum(days_onset_hosp >= 3, na.rm = T),               # number of rows with delay of 3 or more days
    pct_delay_3 = scales::percent(delay_3 / cases)                    # convert previously-defined delay column to percent 
  )

summary_table  # print
```

Một vài mẹp:

-   Sử dụng `sum()` với một biểu thức logic để "đếm" các dòng đáp ứng các tiêu chí nhất định (`==`)\

-   Lưu ý cách sử dụng của `na.rm = TRUE` bên trong biểu thức toán học như là `sum()`, nếu không `NA` sẽ được trả lại nếu dữ liệu có giá trị missing\

-   Sử dụng hàm `percent()` từ package **scales** để dễ dàng chuyển đổi tỷ lệ phần trăm

    -   Thiết lập `accuracy =` bằng 0.1 hoặc 0.01 để đảm bảo kết quả hiển thị 1 hoặc 2 chữ số thập phân sau dấ phẩy\

-   Sử dụng hàm `round()` từ **base** R để chỉ định số thập phân\

-   Để tính toán các thống kê này trên toàn bộ tập dữ liệu, sử dụng `summarise()` và không có `group_by()`\

-   Bạn có thể tạo các cột cho các mục đích tính toán sau này (ví dụ: mẫu số) mà thậm chí bạn bỏ ra khỏi data frame của mình với hàm `select()`.

### Thống kê có điều kiện {.unnumbered}

Bạn có thể sẽ muốn trả về các *thống kê có điều kiện* - vd: số hàng tối đa đáp ứng các tiêu chí nhất định. Điều này có thể thực hiện được bằng cáhc subsetting cột bằng dấu ngoặc vuông `[ ]`. Ví dụ dưới đây trả về nhiệt độ tối đa cho những bệnh nhân được phân loại là có hoặc không bị sốt. Tuy nhiên hãy lưu ý - có thể thích hợp hơn nếu thêm một cột khác vào hàm `group_by()` và `pivot_wider()` (như được minh họa [dưới đây](#tbls_pivot_wider)).

```{r}
linelist %>% 
  group_by(hospital) %>% 
  summarise(
    max_temp_fvr = max(temp[fever == "yes"], na.rm = T),
    max_temp_no = max(temp[fever == "no"], na.rm = T)
  )
```

### Gắn với nhau {.unnumbered}

Hàm `str_glue()` từ package **stringr** rất hữu ích để kết hợp các giá trị từ một số cột thành một cột mới. Trong trường hợp này nó được sử dụng *sau* hàm `summarise()`.

Trong chương [Ký tự và chuỗi], có nhiều lựa chọn khác nhau để kết hợp các cột được thảo luận, bao gồm cả `unite()`, và `paste0()`. Trong trường hợp sử dụng này, chúng tôi ủng hộ `str_glue()` bởi vì nó linh hoạt hơn `unite()` và có cú pháp đơn giẩn hơn `paste0()`.

Dưới đây, data frame `summary_table` (được tạo bên trên) được biến đổi để kết hợp cột `delay_mean` và `delay_sd`, định dạng dấu ngoặc đơn được thêm vào cột mới, và các cột cũ tương ứng của chúng bị xóa.

Sau đó, để làm cho bảng dễ nhìn hơn, tổng hàng được thêm vào bằng hàm `adorn_totals()` từ **janitor** (bỏ qua các cột không phải số). Cuối cùng, chúng tôi sử dụng hàm `select()` từ **dplyr** để sắp xếp và đặt tên lại cho các cột.

Bây giờ bạn có thể chuyển kết quả tới **flextable** và in chúng thành bảng trong Word, .png, .jpeg, .html, Powerpoint, RMarkdown, v.v.! (xem chương [Trình bày bảng]).

```{r}
summary_table %>% 
  mutate(delay = str_glue("{delay_mean} ({delay_sd})")) %>%  # combine and format other values
  select(-c(delay_mean, delay_sd)) %>%                       # remove two old columns   
  adorn_totals(where = "row") %>%                            # add total row
  select(                                                    # order and rename cols
    "Hospital Name"   = hospital,
    "Cases"           = cases,
    "Max delay"       = delay_max,
    "Mean (sd)"       = delay,
    "Delay 3+ days"   = delay_3,
    "% delay 3+ days" = pct_delay_3
    )
```

#### Bách phân vị {.unnumbered}

*Bách phân vị* và tứ phân vị trong **dplyr** xứng đáng được đề cập tới. Để trả về tứ phân vị, sử dụng `quantile()` với các giá trị mặc định hoặc chỉ rõ giá trị bạn muốn bằng đối số `probs =`.

```{r}
# get default percentile values of age (0%, 25%, 50%, 75%, 100%)
linelist %>% 
  summarise(age_percentiles = quantile(age_years, na.rm = TRUE))

# get manually-specified percentile values of age (5%, 50%, 75%, 98%)
linelist %>% 
  summarise(
    age_percentiles = quantile(
      age_years,
      probs = c(.05, 0.5, 0.75, 0.98), 
      na.rm=TRUE)
    )
```

Nếu bạn muốn trả về phân vị *theo nhóm*, bạn có thể gặp phải các kết quả đầu ra dài và ít hữu ích hơn nếu bạn chỉ cần thêm cột vào `group_by()`. Thay vào đó, hãy thử cách tiếp cận này - tạo một cột cho mỗi mức phân vị mong muốn.

```{r}
# get manually-specified percentile values of age (5%, 50%, 75%, 98%)
linelist %>% 
  group_by(hospital) %>% 
  summarise(
    p05 = quantile(age_years, probs = 0.05, na.rm=T),
    p50 = quantile(age_years, probs = 0.5, na.rm=T),
    p75 = quantile(age_years, probs = 0.75, na.rm=T),
    p98 = quantile(age_years, probs = 0.98, na.rm=T)
    )
```

Mặc dù **dplyr** `summarise()` chắc chắn cung cấp khả năng kiểm soát tốt hơn, bạn có thể thấy rằng tất cả các thống kê tổng hợp mà bạn cần có thể được tạo ra với hàm `get_summary_stat()` từ package **rstatix**. Nếu thực hiện trên dữ liệu đã được nhóm, nó sẽ trả về các phân vị 0%, 25%, 50%, 75%, và 100%. If applied to ungrouped data, you can specify the percentiles with `probs = c(.05, .5, .75, .98)`.

```{r}
linelist %>% 
  group_by(hospital) %>% 
  rstatix::get_summary_stats(age, type = "quantile")
```

```{r}
linelist %>% 
  rstatix::get_summary_stats(age, type = "quantile")
```

### Tóm tắt dữ liệu tổng hợp {.unnumbered}

*Nếu bạn bắt đầu với dữ liệu tổng hợp (aggregated data)*, sử dụng `n()` để trả về số lượng các *dòng*, không phải là tổng của các số lượng được đếm. Để lấy tổng, sử dụng `sum()` trên cột của dữ liệu đếm.

Ví dụ, giả sử bạn đang bắt đầu với data frame đếm số lượng như bên dưới, gọi là `linelist_agg` - nó hiển thị ở định dạng "dọc", các trường hợp được tính theo outcome và giới tính.

Sau đây chúng ta sẽ tạo data frame minh hoạt số trường hợp của `linelist` được đếm theo outcome và gender (các giá trị missing được loại bỏ để rõ ràng).

```{r}
linelist_agg <- linelist %>% 
  drop_na(gender, outcome) %>% 
  count(outcome, gender)

linelist_agg
```

Để tính tổng số lượng (trong cột `n`) theo nhóm bạn có thể sử dụng hàm `summarise()` nhưng đặt cột mới bằng `sum(n, na.rm=T)`. Để thêm phần tử điều kiện vào phép toán tổng, bạn có thể sử dụng cú pháp dấu ngoặc vuông tập hợp con [ ] trên cột đếm.

```{r}
linelist_agg %>% 
  group_by(outcome) %>% 
  summarise(
    total_cases  = sum(n, na.rm=T),
    male_cases   = sum(n[gender == "m"], na.rm=T),
    female_cases = sum(n[gender == "f"], na.rm=T))
```

### `across()` trên nhiều cột {.unnumbered}

Bạn có thể sử dụng `summarise()` trên nhiều cột bằng hàm `across()`. Điều này làm cho mọi thứ dễ dàng hơn khi bạn muốn tính toán các thống kê giống nhau cho nhiều cột. Đặt `across()` bên trong `summarise()` và chỉ rõ những điều sau:

-   `.cols =` tên cột viết dưới dạng vector `c()` hoặc sử dụng các hàm trợ giúp chọn cột "tidyselect" (được giải thích bên dưới)\
-   `.fns =` hàm thực hiện (không có dấu ngoặc) - bạn có thể đưa nhiều hàm vào thông qua `list()`

Ví dụ dưới đây, `mean()` được áp dụng cho các cột dữ liệu dạng số. Một vectơ tên của các cột được gán cho `.cols =` và hàm duy nhất `mean` được xác định (không có dấu ngoặc) cho `.fns =`. Bất kỳ đối số bổ sung nào cho hàm (vd: `na.rm=TRUE`) được cung cấp phía sau `.fns =`, ngăn cách bởi dấu phẩy.

Có thể khó để hiểu được thứ tự của dấu ngoặc đơn và dấu phẩy chính xác khi sử dụng `across()`. Hãy nhớ là bên trong hàm `across()` bạn phải bao gồm các cột, các hàm, và tất cả những đối số cần thiết cho các hàm.

```{r}
linelist %>% 
  group_by(outcome) %>% 
  summarise(across(.cols = c(age_years, temp, wt_kg, ht_cm),  # columns
                   .fns = mean,                               # function
                   na.rm=T))                                  # extra arguments
```

Nhiều hàm có thể được chạy cùng một lúc. Dưới đây hàm `mean` và `sd` được cung cấp cho `.fns =` bên trong một `list()`. Bạn có cơ hội cung cấp tên ký tự (vd: "mean" và "sd") để thêm vào tên các cột mới.

```{r}
linelist %>% 
  group_by(outcome) %>% 
  summarise(across(.cols = c(age_years, temp, wt_kg, ht_cm), # columns
                   .fns = list("mean" = mean, "sd" = sd),    # multiple functions 
                   na.rm=T))                                 # extra arguments
```

Dưới đây là danh sách các hàm trợ giúp "tidyselect" bạn có thể cung cấp cho `.cols =` để lựa chọn cột:

-   `everything()` - tất cả các cột khác không được đề cập\
-   `last_col()` - cột cuối cùng\
-   `where()` - áp dụng một hàm cho tất cả các cột và chọn những cột trả về giá trị TRUE\
-   `starts_with()` - khớp với một tiền tố được chỉ định. Ví dụ: `starts_with("date")`
-   `ends_with()` - khớp với một hậu tố được chỉ định. Ví dụ: `ends_with("_end")`\
-   `contains()` - cột chứa một chuỗi ký tự. Ví dụ: `contains("time")`
-   `matches()` - áp dụng một biểu thức chính quy (regex). Ví dụ: `contains("[pt]al")`\
-   `num_range()` - khoảng giá trị số
-   `any_of()` - khớp nếu cột được đặt tên. Hữu ích nếu tên có thể không tồn tại. Ví dụ: `any_of(date_onset, date_death, cardiac_arrest)`

Ví dụ, để trả về giá trị trung bình của tất cả các cột dạng số, sử dụng `where()` và thêm vào hàm `is.numeric()` (không có dấu ngoặc). Tất cả những thứ này vẫn được đặt trong hàm `across()`.

```{r}
linelist %>% 
  group_by(outcome) %>% 
  summarise(across(
    .cols = where(is.numeric),  # all numeric columns in the data frame
    .fns = mean,
    na.rm=T))
```

### Xoay trục ngang (Pivot wider) {#tbls_pivot_wider .unnumbered}

Nếu bạn thích bảng của mình ở định dạng "rộng", bạn có thể biến đổi nó sử dụng hàm **tidyr** `pivot_wider()`. Bạn có thể sẽ cần đặt lại tên cho các cột bằng `rename()`. Để tìm hiểu thêm, vui lòng xem chương [Pivoting dữ liệu].

Ví dụ sau đây bắt đầu bằng một bảng "dài" `age_by_outcome` từ mục [Tỷ lệ](#tbl_dplyr_prop). Để dễ hình dung, chúng ta tạo lại bảng và in ra:

```{r}
age_by_outcome <- linelist %>%                  # begin with linelist
  group_by(outcome) %>%                         # group by outcome 
  count(age_cat) %>%                            # group and count by age_cat, and then remove age_cat grouping
  mutate(percent = scales::percent(n / sum(n))) # calculate percent - note the denominator is by outcome group
```

```{r, echo=F}
DT::datatable(age_by_outcome, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Để xoay trục ngang, chúng ta tạo các cột mới từ các *giá trị* trong cột hiện có `age_cat` (bằng cách đặt `names_from = age_cat`). Chúng ta cũng chỉ định rằng các giá trị bảng mới sẽ đến từ cột hiện có `n`, với `values_from = n`. Các cột không được đề cập trong lệnh pivoting (`outcome`) sẽ không thay đổi ở phía ngoài cùng bên trái.

```{r}
age_by_outcome %>% 
  select(-percent) %>%   # keep only counts for simplicity
  pivot_wider(names_from = age_cat, values_from = n)  
```

### Tổng các hàng {#tbl_dplyr_totals .unnumbered}

Khi hàm `summarise()` vận hành trên dữ liệu đã được nhóm, nó không tính "tổng" một cách tự động. Sau đây là hai cách tiếp cận giúp bạn thêm tổng hàng:

#### **janitor**'s `adorn_totals()` {.unnumbered}

Nếu bảng của bạn chỉ chứa duy nhất số lượng hoặc tỷ lệ/tỷ lệ phần trăm có thể được tổng hợp thành một tổng, thì bạn có thể tính *tổng* sử dụng hàm `adorn_totals()` của package **janitor** như đã được mô tả bên trên. Lưu ý là hàm này chỉ có thể tính tổng của các cột định dạng là số - nếu bạn muốn tính các loại tổng khác, vui lòng xem cách tiếp cận tiếp theo bằng **dplyr**.

Dưới đây, bộ dữ liệu `linelist` được nhóm theo giới và tóm tắt thành một bảng mô tả số trường hợp có outcome đã biết, tử vong và phục hồi. Piping bảng tới hàm `adorn_totals()` để thêm tổng các hàng ở hàng dưới cùng thể hiện giá trị tổng của từng cột. Các hàm `adorn_*()` khác điều chỉnh cách kết quả được hiển thị như được comment trong phần code.

```{r}
linelist %>% 
  group_by(gender) %>%
  summarise(
    known_outcome = sum(!is.na(outcome)),           # Number of rows in group where outcome is not missing
    n_death  = sum(outcome == "Death", na.rm=T),    # Number of rows in group where outcome is Death
    n_recover = sum(outcome == "Recover", na.rm=T), # Number of rows in group where outcome is Recovered
  ) %>% 
  adorn_totals() %>%                                # Adorn total row (sums of each numeric column)
  adorn_percentages("col") %>%                      # Get column proportions
  adorn_pct_formatting() %>%                        # Convert proportions to percents
  adorn_ns(position = "front")                      # display % and counts (with counts in front)
```

#### `summarise()` trên dữ liệu "tổng" rồi sau đó `bind_rows()` {.unnumbered}

Nếu bảng của bạn chứa các phép tính thống kế chẳng hạn như `median()`, `mean()`, v.v, thì cách tiếp cận dùng hàm `adorn_totals()` bên trên sẽ *không* đủ. Thay vào đó, để có được thống kê tóm tắt cho toàn bộ tập dữ liệu, bạn phải tính toán chúng bằng lệnh `summarise()` một cách độc lập sau đó gắn các kết quả này với bảng tổng hợp theo nhóm ban đầu. Để làm điều này, bạn có thể sử dụng hàm `bind_rows()` từ **dplyr** như được mô tả trong chương [Nối dữ liệu]. Dưới đây là một ví dụ:

Bạn có thể tạo bảng tổng hợp của outcome *theo bệnh viện* với `group_by()` và `summarise()` như sau:

```{r, warning=F, message=F}
by_hospital <- linelist %>% 
  filter(!is.na(outcome) & hospital != "Missing") %>%  # Remove cases with missing outcome or hospital
  group_by(hospital, outcome) %>%                      # Group data
  summarise(                                           # Create new summary columns of indicators of interest
    N = n(),                                            # Number of rows per hospital-outcome group     
    ct_value = median(ct_blood, na.rm=T))               # median CT value per group
  
by_hospital # print table
```

Để tính tổng, vẫn sử dụng hàm `summarise()` nhưng chỉ nhóm dữ liệu theo outcome (không theo bệnh viện), như dưới đây:

```{r}
totals <- linelist %>% 
      filter(!is.na(outcome) & hospital != "Missing") %>%
      group_by(outcome) %>%                            # Grouped only by outcome, not by hospital    
      summarise(
        N = n(),                                       # These statistics are now by outcome only     
        ct_value = median(ct_blood, na.rm=T))

totals # print table
```

Bây giờ chúng ta có thể nối hai data frames này lại với nhau. Lưu ý là bảng `by_hospital` có 4 cột trong khi đó bảng kết quả `totals` có 3 cột. Bằng việc sử dụng `bind_rows()`, các cột được kết hợp theo tên, và bất kỳ khoảng trống nào sẽ được điền vào bằng giá trị `NA` (ví dụ ở cột `hospital` là các giá trị cho hai hàng `totals` mới). Sau khi gắn các hàng, chúng ta chuyển các khoảng trống đó thành "Tổng" bằng cách sử dụng `replace_na()` (xem chương [Làm sạch số liệu và các hàm quan trọng]).

```{r}
table_long <- bind_rows(by_hospital, totals) %>% 
  mutate(hospital = replace_na(hospital, "Total"))
```

Đây là bảng mới với các hàng "Tổng" ở các hàng dưới cùng của bảng.

```{r, message=FALSE, echo=F}
DT::datatable(table_long, rownames = FALSE, options = list(pageLength = 12, scrollX=T), class = 'white-space: nowrap' )
```

Bảng này đang có định dạng "dài", có thể là những gì bạn muốn. *Tuy nhiên*, bạn có thể *xoay* bảng này *rộng hơn* theo chiều ngang để dễ đọc. Xem thêm ở phần Xoay trục ngang (Pivot wider) bên trên, và chương [Xoay trục dữ liệu]. Bạn cũng có thêm nhiều cột nữa, và sắp xếp chúng một cách đẹp mắt. Phần code được trình bày bên dưới.

```{r}
table_long %>% 
  
  # Pivot wider and format
  ########################
  mutate(hospital = replace_na(hospital, "Total")) %>% 
  pivot_wider(                                         # Pivot from long to wide
    values_from = c(ct_value, N),                       # new values are from ct and count columns
    names_from = outcome) %>%                           # new column names are from outcomes
  mutate(                                              # Add new columns
    N_Known = N_Death + N_Recover,                               # number with known outcome
    Pct_Death = scales::percent(N_Death / N_Known, 0.1),         # percent cases who died (to 1 decimal)
    Pct_Recover = scales::percent(N_Recover / N_Known, 0.1)) %>% # percent who recovered (to 1 decimal)
  select(                                              # Re-order columns
    hospital, N_Known,                                   # Intro columns
    N_Recover, Pct_Recover, ct_value_Recover,            # Recovered columns
    N_Death, Pct_Death, ct_value_Death)  %>%             # Death columns
  arrange(N_Known)                                  # Arrange rows from lowest to highest (Total row at bottom)

```

Tiếp đó bạn có thể in bảng kết quả dưới dạng một bức ảnh đẹp - sau đây là output được in bằng **flextable**. Bạn có thể đọc chuyên sâu hơn về ví dụ này và cách tạo được bảng "đẹp" tương tự thế này trong chương [Trình bày bảng].

```{r echo=FALSE, fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}

linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds")) 

border_style = officer::fp_border(color="black", width=1)

pacman::p_load(
  rio,            # import/export
  here,           # file pathways
  flextable,      # make pretty images of tables 
  officer,        # helper functions for tables
  tidyverse)      # data management, summary, and visualization

table <- linelist %>% 
  # filter
  ########
  #filter(!is.na(outcome) & hospital != "Missing") %>%  # Remove cases with missing outcome or hospital
  
  # Get summary values per hospital-outcome group
  ###############################################
  group_by(hospital, outcome) %>%                      # Group data
  summarise(                                           # Create new summary columns of indicators of interest
    N = n(),                                            # Number of rows per hospital-outcome group     
    ct_value = median(ct_blood, na.rm=T)) %>%           # median CT value per group
  
  # add totals
  ############
  bind_rows(                                           # Bind the previous table with this mini-table of totals
    linelist %>% 
      filter(!is.na(outcome) & hospital != "Missing") %>%
      group_by(outcome) %>%                            # Grouped only by outcome, not by hospital    
      summarise(
        N = n(),                                       # Number of rows for whole dataset     
        ct_value = median(ct_blood, na.rm=T))) %>%     # Median CT for whole dataset
  
  # Pivot wider and format
  ########################
  mutate(hospital = replace_na(hospital, "Total")) %>% 
  pivot_wider(                                         # Pivot from long to wide
    values_from = c(ct_value, N),                       # new values are from ct and count columns
    names_from = outcome) %>%                           # new column names are from outcomes
  mutate(                                              # Add new columns
    N_Known = N_Death + N_Recover,                               # number with known outcome
    Pct_Death = scales::percent(N_Death / N_Known, 0.1),         # percent cases who died (to 1 decimal)
    Pct_Recover = scales::percent(N_Recover / N_Known, 0.1)) %>% # percent who recovered (to 1 decimal)
  select(                                              # Re-order columns
    hospital, N_Known,                                   # Intro columns
    N_Recover, Pct_Recover, ct_value_Recover,            # Recovered columns
    N_Death, Pct_Death, ct_value_Death)  %>%             # Death columns
  arrange(N_Known) %>%                                 # Arrange rows from lowest to highest (Total row at bottom)

  # formatting
  ############
  flextable() %>% 
  add_header_row(
    top = TRUE,                # New header goes on top of existing header row
    values = c("Hospital",     # Header values for each column below
               "Total cases with known outcome", 
               "Recovered",    # This will be the top-level header for this and two next columns
               "",
               "",
               "Died",         # This will be the top-level header for this and two next columns
               "",             # Leave blank, as it will be merged with "Died"
               "")) %>% 
    set_header_labels(         # Rename the columns in original header row
      hospital = "", 
      N_Known = "",                  
      N_Recover = "Total",
      Pct_Recover = "% of cases",
      ct_value_Recover = "Median CT values",
      N_Death = "Total",
      Pct_Death = "% of cases",
      ct_value_Death = "Median CT values")  %>% 
  merge_at(i = 1, j = 3:5, part = "header") %>% # Horizontally merge columns 3 to 5 in new header row
  merge_at(i = 1, j = 6:8, part = "header") %>%  
  border_remove() %>%  
  theme_booktabs() %>% 
  vline(part = "all", j = 2, border = border_style) %>%   # at column 2 
  vline(part = "all", j = 5, border = border_style) %>%   # at column 5
  merge_at(i = 1:2, j = 1, part = "header") %>% 
  merge_at(i = 1:2, j = 2, part = "header") %>% 
  width(j=1, width = 2.7) %>% 
  width(j=2, width = 1.5) %>% 
  width(j=c(4,5,7,8), width = 1) %>% 
  flextable::align(., align = "center", j = c(2:8), part = "all") %>% 
  bg(., part = "body", bg = "gray95")  %>% 
  colformat_num(., j = c(4,7), digits = 1) %>% 
  bold(i = 1, bold = TRUE, part = "header") %>% 
  bold(i = 6, bold = TRUE, part = "body")


table
```

## **gtsummary** package {#tbl_gt}

Nếu bạn muốn in các thống kê tóm tắt của mình dưới dạng đồ họa đẹp mắt, sẵn sàng xuất bản, bạn có thể sử dụng package **gtsummary** và hàm của nó `tbl_summary()`. Phần code ban đầu có thể trông phức tạp một chút, nhưng kết quả đầu ra trông rất đẹp và in ra Viewer panel của RStudio dưới dạng một ảnh HTML. Đọc [bản tóm tắt ở đây](http://www.danieldsjoberg.com/gtsummary/articles/tbl_summary.html).

Bạn cũng có thể thêm kết quả của các kiểm định thống kê vào các bảng của **gtsummary**. Quy trình này được trình bày ở mục **gtsummary** trong chương [Các kiểm định thống kê cơ bản](#stats_gt).

Để giới thiệu về `tbl_summary()`, trước tiên chúng ta sẽ chỉ ra các quy trình cơ bản nhất, giúp bạn thực sự tạo ra một bảng lớn và đẹp. Sau đó, chúng ta sẽ tìm hiểu chi tiết hơn về cách thực hiện các điều chỉnh và các bảng được thiết kế sẵn.

### Bảng tổng hợp {.unnumbered}

Cách làm việc mặc định của `tbl_summary()` khá kinh ngạc - nó lấy các cột bạn cung cấp và tạo một bảng tóm tắt chỉ trong một lệnh. Hàm in ra số liệu thống kê phù hợp với lớp cột: trung vị và khoảng tứ phân vị (IQR) cho các cột số, và số lượng (%) cho các cột danh mục. Giá trị missing được chuyển đổi thành "Unknown". Chú thích được thêm vào cuối bảng để giải thích các phép tính thống kê, trong khi tổng N được hiển thị ở trên cùng.

```{r, warning=F, message=F}
linelist %>% 
  select(age_years, gender, outcome, fever, temp, hospital) %>%  # keep only the columns of interest
  tbl_summary()                                                  # default
```

### Các điều chỉnh {.unnumbered}

Bây giờ chúng tôi sẽ giải thích cách hoạt động của hàm và cách điều chỉnh. Các đối số chính được trình bày chi tiết bên dưới:

**`by =`**\
Bạn có thể phân tầng bảng của mình theo một cột (ví dụ theo `outcome`), để tạo thành bảng 2 chiều.

**`statistic =`**\
Sử dụng phương trình để chỉ định thống kê nào sẽ được hiển thị và cách hiển thị chúng. Có hai vế của phương trình, được ngăn cách bởi dấu `~`. Ở vế phải, trong dấu ngoặc kép, là hiển thị phép toán thống kê mong muốn, và ở vế trái là các cột mà phép thống kê đó sẽ áp dụng.

-   Vế phải của phương trình sử dụng cú pháp của hàm `str_glue()` từ **stringr** (xem [Ký tự và chuỗi]), với chuỗi hiển thị mong muốn trong dấu ngoặc kép và các phép toán thống kê trong dấu ngoặc nhọn. Bạn có thể thêm các phép thống kê như là "n" (số lượng), "N" (mẫu số), "mean", "median", "sd", "max", "min", phân vị "p\#\#" như là  "p25", hoặc phần trăm của một tổng như là "p". Xem `?tbl_summary` để biết thêm chi tiết.\
-   Đối với phía bên trái của phương trình, bạn có thể chỉ định các cột theo tên (ví dụ: `age` hoặc `c(age, gender)`) hoặc sử dụng các hàm trợ giúp như `all_continuous()`, `all_categorical()`, `contains()`, `starts_with()`, v.v.

Một ví dụ đơn giản về phương trình `statistic =` có thể tham khảo ở bên dưới, để chỉ in giá trị trung bình của cột `age_years`:

```{r}
linelist %>% 
  select(age_years) %>%         # keep only columns of interest 
  tbl_summary(                  # create summary table
    statistic = age_years ~ "{mean}") # print mean of age
```

Một phương trình phức tạp hơn một chút có thể như`"({min}, {max})"`, kết hợp các giá trị max và min trong dấu ngoặc đơn và được phân tách bằng dấu phẩy:

```{r}
linelist %>% 
  select(age_years) %>%                       # keep only columns of interest 
  tbl_summary(                                # create summary table
    statistic = age_years ~ "({min}, {max})") # print min and max of age
```

Bạn cũng có thể phân biệt cú pháp cho các cột hoặc loại cột riêng biệt. Trong ví dụ phức tạp hơn bên dưới, giá trị được cung cấp cho `statistc =` là một **danh sách** chỉ ra rằng đối với tất cả các cột dạng số thì bảng sẽ in ra giá trị trung bình và độ lệch chuẩn bên trong ngoặc, trong khi các cột dạng danh sách thì sẽ in ra n, mẫu số, và phần trăm.

**`digits =`**\
Điều chỉnh các chữ số và làm tròn. Theo tùy chọn, điều này có thể được chỉ định chỉ dành cho các cột dạng số liên tục (như bên dưới).

**`label =`**\
Điều chỉnh cách hiển thị tên cột. Cung cấp tên cột và nhãn mong muốn của nó được phân tách bằng dấu ngã. Theo mặc định thì tên cột được hiển thị.

**`missing_text =`**\
Điều chỉnh cách giá trị missing được hiển thị. Mặc định hiển thị là "Unknown".

**`type =`**\
Sử dụng để điều chỉnh số lượng cấp độ của thống kê được hiển thị Cú pháp tương tự như `statistic =` trong đó bạn cung cấp một phương trình với các cột ở bên trái và một giá trị ở bên phải. Hai trường hợp phổ biến bao gồm:

-   `type = all_categorical() ~ "categorical"` Buộc các cột nhị phân (ví dụ: `fever` có/không) hiển thị tất cả các cấp độ thay vì chỉ hiện thị hàng "có"\
-   `type = all_continuous() ~ "continuous2"` Cho phép các kết quả thống kê được trình bày theo nhiều dòng cho mỗi biến, như được trình bày trong phần sau

Trong ví dụ dưới đây, mỗi đối số này được sử dụng để điều chỉnh bảng ban đầu:

```{r}
linelist %>% 
  select(age_years, gender, outcome, fever, temp, hospital) %>% # keep only columns of interest
  tbl_summary(     
    by = outcome,                                               # stratify entire table by outcome
    statistic = list(all_continuous() ~ "{mean} ({sd})",        # stats and format for continuous columns
                     all_categorical() ~ "{n} / {N} ({p}%)"),   # stats and format for categorical columns
    digits = all_continuous() ~ 1,                              # rounding for continuous columns
    type   = all_categorical() ~ "categorical",                 # force all categorical levels to display
    label  = list(                                              # display labels for column names
      outcome   ~ "Outcome",                           
      age_years ~ "Age (years)",
      gender    ~ "Gender",
      temp      ~ "Temperature",
      hospital  ~ "Hospital"),
    missing_text = "Missing"                                    # how missing values should display
  )
```

### Thống kê nhiều dòng cho các biến liên tục {.unnumbered}

Nếu bạn muốn in nhiều dòng thống kê cho các biến liên tục, bạn có thể thiết lập `type =` thành "continuous2". Bạn có thể kết hợp tất cả các yếu tố được hiển thị trước đó trong một bảng bằng cách chọn thống kê bạn muốn hiển thị. Để làm điều này, bạn cần cho hàm biết rằng bạn muốn khôi phục bảng bằng cách nhập type là "continuous2". Số lượng các giá trị missing được hiển thị là "Unknown".

```{r}
linelist %>% 
  select(age_years, temp) %>%                      # keep only columns of interest
  tbl_summary(                                     # create summary table
    type = all_continuous() ~ "continuous2",       # indicate that you want to print multiple statistics 
    statistic = all_continuous() ~ c(
      "{mean} ({sd})",                             # line 1: mean and SD
      "{median} ({p25}, {p75})",                   # line 2: median and IQR
      "{min}, {max}")                              # line 3: min and max
    )
```

Có nhiều cách khác để chỉnh sửa các bảng này, bao gồm thêm giá trị p, chỉnh sửa màu sắc và tiêu đề, v.v. Các phần này được đề cập trong tài liệu trợ giúp đính kèm (nhập `?tbl_summary` trong cửa sổ Console), và một số được đề cập trong chương [Các kiểm định thống kê cơ bản](#stat-tests).

## **base** R

Bạn có thể sử dụng hàm `table()` để tạo bảng đơn và bảng chéo các cột. Không giống như các cách ở trên, bạn phải chỉ định data frame mỗi khi bạn tham chiếu đến tên cột, như được trình bày dưới đây.

[***THẬN TRỌNG:*** Giá trị `NA` (missing) sẽ **không** sẽ không được lập bảng trừ khi bạn bao gồm đối số `useNA = "always"` (cũng có thể được đặt thành "no" hoặc "ifany").]{style="color: orange;"}

[***MẸO:*** Bạn có thể sử dụng `%$%` từ package **magrittr** để loại bỏ việc lặp lại các data frame trong các hàm **base**. Chẳng hạn, ví dụ bên dưới có thể được viết lại thành `linelist %$% table(outcome, useNA = "always")` ]{style="color: darkgreen;"}

```{r}
table(linelist$outcome, useNA = "always")
```

Có thể lập bảng chéo từ nhiều cột bằng cách liệt kê chúng nối tiếp nhau, phân tách bằng dấu phẩy. Hoặc là, bạn có thể gán cho mỗi cột một “tên” như `Outcome = linelist$outcome`.

```{r}
age_by_outcome <- table(linelist$age_cat, linelist$outcome, useNA = "always") # save table as object
age_by_outcome   # print table
```

### Tỷ lệ {.unnumbered}

Để trả về tỷ lệ, hãy chuyển bảng trên vào hàm `prop.table()`. Sử dụng đối số `margins =` để chỉ định xem bạn muốn tỷ lệ của hàng (1), của cột (2) hay của toàn bảng (3). Để dễ nhìn, chúng ta pipe bảng trên vào hàm `round()` của **base** R, chỉ định 2 chữ số sau dấu phẩy.

```{r}
# get proportions of table defined above, by rows, rounded
prop.table(age_by_outcome, 1) %>% round(2)
```

### Tổng {.unnumbered}

Để thêm tổng hàng và tổng cột, hãy chuyển bảng vào hàm `addmargins()`. Cách này hoạt động cho cả số lượng và tỷ lệ.

```{r}
addmargins(age_by_outcome)
```

### Chuyển đổi thành data frame {.unnumbered}

Chuyển đổi trực tiếp một đối tượng dạng `table()` sang một data frame không phải là một đường thẳng. Cách tiếp cận được trình bày như dưới đây:

1)  Tạo một bảng, mà *không sử dụng* `useNA = "always"`. Thay vào đó chuyển giá trị `NA` thành “(Missing)” với hàm `fct_explicit_na()` của package **forcats**.\
2)  Thêm tổng (tùy chọn) bằng cách piping tới `addmargins()`\
3)  Pipe tới hàm `as.data.frame.matrix()` của **base** R\
4)  Pipe bảng trên vào hàm `rownames_to_column()` của package **tibble**, ghi rõ tên cho cột đầu tiên\
5)  In, Xem hoặc xuất bảng như mong muốn. Trong ví dụ này, chúng ta sử dụng hàm `flextable()` từ package **flextable** như đã được mô tả trong chương Kết quả sẽ được in ra cửa sổ RStudio viewer dưới dạng một hình ảnh HTML đẹp.

```{r, warning=F, message=F}
table(fct_explicit_na(linelist$age_cat), fct_explicit_na(linelist$outcome)) %>% 
  addmargins() %>% 
  as.data.frame.matrix() %>% 
  tibble::rownames_to_column(var = "Age Category") %>% 
  flextable::flextable()
```

<!-- ======================================================= -->

## Nguồn

Phần lớn thông tin trong chương này được tham khảo từ các nguồn và bản tóm tắt trực tuyến dưới đây:

[gtsummary](http://www.danieldsjoberg.com/gtsummary/articles/tbl_summary.html)

[dplyr](https://dplyr.tidyverse.org/articles/grouping.html)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/tables_descriptive.Rmd-->

# Các kiểm định thống kê cơ bản {#stat-tests}


Chương này sẽ trình bày cách để thực hiện các phép kiểm định thống kê cơ bản bằng cách sử dụng **base** R, **rstatix**, và **gtsummary**.  

* Kiểm định t  
* Kiểm định Shapiro-Wilk  
* Kiểm định tổng thứ hạng Wilcoxon   
* Kiểm định Kruskal-Wallis  
* Kiểm định Chi-squared (Chi bình phương)  
* Tương quan giữa các biến định lượng 

...nhiều kiểm định khác có thể được thực hiện, nhưng chúng tôi chỉ trình bày các kiểm định thông dụng và kết nối với các phần khác trong cuốn sổ tay này.  

Mỗi package được đề cập bên trên đều có một số ưu điểm và khuyết điểm nhất định:  

* Sử dụng các câu lệnh của **base** để in các kết quả đầu ra thống kê trong R Console  
* Sử dụng các câu lệnh của **rstatix** để cho kết quả dưới dạng data frame hoặc khi muốn thực hiện các kiểm định theo nhóm  
* Sử dụng các câu lệnh của **gtsummary** khi muốn kết quả là các bảng biểu có thể sử dụng được ngay  



<!-- ======================================================= -->
## Các bước chuẩn bị {  }


### Gọi các packages {.unnumbered}

Đoạn code này hiển thị việc gọi các package cần thiết cho phân tích. Trong cuốn sổ tay này, chúng tôi nhấn mạnh đến hàm `p_load()` trong package **pacman**, cài đặt gói lệnh nếu cần thiết *và* gọi chúng ra để sử dụng. Các package đã cài đặt cũng có thể được gọi ra bằng `library()` từ **base** R. Xem thêm thông tin các package của R trong chương [R cơ bản].  


```{r}
pacman::p_load(
  rio,          # File import
  here,         # File locator
  skimr,        # get overview of data
  tidyverse,    # data management + ggplot2 graphics, 
  gtsummary,    # summary statistics and tests
  rstatix,      # statistics
  corrr,        # correlation analayis for numeric variables
  janitor,      # adding totals and percents to tables
  flextable     # converting tables to HTML
  )
```

### Nhập số liệu {.unnumbered}

Chúng ta nhập bộ số liệu của các ca bệnh về một vụ dịch Ebola mô phỏng. Để tiện theo dõi, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>bấm để tải bộ số liệu linelist "đã được làm sạch"</a> (as .rds file). Nhập số liệu bằng hàm `import()` từ package **rio** package (nó chấp nhận nhiều loại tập tin như .xlsx, .rds, .csv - xem thêm chương [Nhập xuất dữ liệu] để biết thêm chi tiết).  


```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.rds")
```

50 hàng đầu tiên của bộ dữ liệu linelist được hiển thị như dưới đây.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```





## Các kiểm định trong **base** R {}

Các lệnh trong **base** R functions to conduct statistical tests. có thể được sử dụng để thực hiện các kiểm định thống kê. Các câu lệnh tương đối đơn giản và kết quả sẽ hiển thị trong bảng điều khiển R Console. Tuy nhiên, kết quả đầu ra thường dưới dạng liệt kê, vì thế sẽ khó thao tác hơn nếu muốn sử dụng kết quả trong các thao tác tiếp theo. 

### Kiểm định t {.unnumbered} 

Một [kiểm định t](https://en.wikipedia.org/wiki/Student%27s_t-test), hay còn được gọi là "Student's t-Test", thường được sử dụng để xác định có sự khác biệt có ý nghĩa thống kê giữa giá trị trung bình của hai nhóm. Bên dưới là cú pháp để thực hiện kiểm định này tùy thuộc vào các cột có trong cùng một data frame hay không.

**Cú pháp 1:** Đây là cú pháp khi cột của biến liên tục và phân loại nằm trong cùng một data frame. Đặt biến liên tục bên trái và biến phân loại bên phải của phương trình. Ghi rõ bộ số liệu sau `data = `. Các tùy chọn khác như số liệu bắt cặp, viết thêm `paired = TRUE`,  khoảng tin cậy, viết thêm `conf.level = ` (mặc định là 0.95), và giả thuyết thay thế `alternative = ` (hai đuôi - “two.sided”, hoặc một đuôi nhỏ hơn hay lớn hơn - “less”, or “greater”). Gõ `?t.test` để biết thêm chi tiết.  

```{r}
## compare mean age by outcome group with a t-test
t.test(age_years ~ gender, data = linelist)
```

**Cú pháp 2:** Đây là cú pháp khi so sánh hai véc tơ dạng số. Ví dụ như hai cột nằm trong hai bộ số liệu khác nhau.  

```{r, eval=F}
t.test(df1$age_years, df2$age_years)
```

Kiểm định t cũng được sử dụng để xác định có sự khác biệt có ý nghĩa thống kê giữa giá trị trung bình của mẫu với một số giá trị cụ thể. Đây là phép kiểm định t cho một mẫu với trung bình quần thể giả thuyết/đã biết như `mu = `:  

```{r, eval=F}
t.test(linelist$age_years, mu = 45)
```

### Kiểm định Shapiro-Wilk {.unnumbered}  

[Kiểm định Shapiro-Wilk](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test) có thể được sử để xác định xem một mẫu có phân bố bình thường/phân bố chuản hay không (một giả định của nhiều kiểm định khác, ví dụ như kiểm định t). Tuy nhiên, phép kiểm định này chỉ có thể được sử dụng cho một mẫu có từ 3 đến 5000 quan sát. Đối với cỡ mẫu lớn hơn, nên sử dụng biểu đồ [quantile-quantile plot](https://ggplot2.tidyverse.org/reference/geom_qq.html). 


```{r, eval=F}
shapiro.test(linelist$age_years)
```

### Kiểm định tổng thứ hạng Wilcoxon {.unnumbered}

Kiểm định tổng thứ hạng Wilcoxon, hay còn gọi là [kiểm định Mann–Whitney U](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test), thường được sử dụng để giúp xác định xem hai mẫu có cùng phân bố hay không khi quần thể của chúng không có phân bố chuẩn hoặc có phương sai không bằng nhau.

```{r wilcox_base}

## compare age distribution by outcome group with a wilcox test
wilcox.test(age_years ~ outcome, data = linelist)

```


### Kiểm định Kruskal-Wallis {.unnumbered}


[Kiểm định Kruskal-Wallis](https://en.wikipedia.org/wiki/Kruskal%E2%80%93Wallis_one-way_analysis_of_variance) là một phần mở rộng của kiểm định tổng thứ hạng Wilcoxon mà có thể được sử dụng để kiểm định sự khác biệt trong phân bố của nhiều hơn hai mẫu. Khi có hai mẫu được sử dụng, nó cho kết quả giống như của kiểm định tổng thứ hạng Wilcoxon. 

```{r }

## compare age distribution by outcome group with a kruskal-wallis test
kruskal.test(age_years ~ outcome, linelist)

```

### Kiểm định Chi bình phương {.unnumbered} 

[Kiểm định Chi bình phương của Pearson](https://en.wikipedia.org/wiki/Chi-squared_test) được sử dụng trong kiểm tra sự khác biệt có ý nghĩa thống kê giữa các biến phân loại. 

```{r}

## compare the proportions in each group with a chi-squared test
chisq.test(linelist$gender, linelist$outcome)

```



## **rstatix** package {}

Package **rstatix** cho phép thực hiện các kiểm định thống kê và truy xuất kết quả "dễ sử dụng cho các tính toán tiếp theo". Có nghĩa là kết quả xuất tự động thành một data frame để có thể thực hiện các thao tác tiếp theo. Nó cũng dễ dàng để nhóm dữ liệu mà sẽ được chuyền vào các hàm, ở đó các thống kê được thực hiện cho từng nhóm.  


### Tóm tắt thống kê {.unnumbered}  

Hàm `get_summary_stats()` là một cách thực hiện tóm tắt thống kê nhanh. Chỉ cần đưa bộ số liệu và chỉ định các cột muốn phân tích vào hàm này. Nếu không có cột nào được cụ thể, tóm tắt thống kê sẽ tính toán cho tất cả các cột.  

Tóm tắt thống kê đầy đủ sẽ cho kết quả mặc định như sau: số quan sát (n), giá trị nhỏ nhất, giá trị lớn nhất, trung vị, giá trị tứ phân vị thứ nhất (25%), giá trị tứ phân vị thứ ba (75%), khoảng tứ phân vị, độ lệch tuyệt đối của trung vị (mad), trung bình, độ lệch chuẩn, sai số chuẩn và khoảng tin cậy của trung bình. 


```{r}
linelist %>%
  rstatix::get_summary_stats(age, temp)
```

Có thể tóm tắt một số giá trị thống kê bằng cách cung cấp một trong số các giá trị sau đến `type = `: "full", "common", "robust", "five_number", "mean_sd", "mean_se", "mean_ci", "median_iqr", "median_mad", "quantile", "mean", "median", "min", "max".  

Nó cũng có thể được sử dụng để nhóm số liệu, sao cho một hàng được trả về cho mỗi biến nhóm:  

```{r}
linelist %>%
  group_by(hospital) %>%
  rstatix::get_summary_stats(age, temp, type = "common")
```

Bạn cũng có thể sử dụng **rstatix** để thực hiện các kiểm định thống kê:  

### Kiểm định t {.unnumbered}  

USử dụng cú pháp để chỉ định cột biến liên tục và cột biến phân loại:  

```{r}
linelist %>% 
  t_test(age_years ~ gender)
```

Hoặc sử dụng `~ 1` và ghi rõ `mu = ` cho kiểm định t một mẫu. Cú pháp này có thể sử dụng để thực hiện cho nhóm.  

```{r}
linelist %>% 
  t_test(age_years ~ 1, mu = 30)
```

Nếu có thể, các kiểm định thống kê có thể thực hiện theo nhóm, như được trình bày bên dưới.  

```{r}
linelist %>% 
  group_by(gender) %>% 
  t_test(age_years ~ 1, mu = 18)
```

### Kiểm định Shapiro-Wilk {.unnumbered}  

Như đã đề cập bên trên, cỡ mẫu phải nằm trong khoảng từ 3 đến 5000.  

```{r}
linelist %>% 
  head(500) %>%            # first 500 rows of case linelist, for example only
  shapiro_test(age_years)
```

### Kiểm định tổng thứ hạng Wilcoxon {.unnumbered}  

```{r}
linelist %>% 
  wilcox_test(age_years ~ gender)
```


### Kiểm định Kruskal-Wallis {.unnumbered}  

Cũng được biết như kiểm định Mann-Whitney U.  

```{r}
linelist %>% 
  kruskal_test(age_years ~ outcome)
```


### Kiểm định Chi bình phương {.unnumbered}  

Hàm kiểm định Chi bình phương chấp nhận một bảng, vì vậy đầu tiên là tạo một bảng chéo. Có nhiều cách để tạo một bảng chéo (xem chương [Bảng mô tả]) nhưng ở đây chúng ta sử dụng hàm `tabyl()` từ **janitor** avà bỏ cột ngoài cùng bên trái của nhãn giá trị trước khi đưa vào hàm `chisq_test()`.  

```{r}
linelist %>% 
  tabyl(gender, outcome) %>% 
  select(-1) %>% 
  chisq_test()

```

Có rất nhiều hàm và kiểm định thống kê có thể được thực hiện bằng các hàm trong package **rstatix**. Đọc các tài liệu về **rstatix** [online ở đây](https://github.com/kassambara/rstatix) hoặc gõ ?rstatix.  





## `gtsummary` package {#stats_gt}

Sử dụng package **gtsummary** nếu bạn đang muốn thêm kết quả của một kiểm định thống kê vào một bảng đẹp được tạo ra bằng package này (như đã được mô tả trong phần **gtsummary** của chương [Bảng mô tả](#tbl_gt)).  

Khi thực hiện các kiểm định so sánh bằng hàm `tbl_summary`, dùng thêm hàm `add_p` để đưa cột giá trị p và kiểm định được sử dụng vào bảng. Có thể xuất nhiều giá trị p mà được hiệu chỉnh cho nhiều kiểm định bằng cách dùng thêm hàm `add_q`. Gõ lệnh `?tbl_summary` để biết thêm chi tiết.  

### Kiểm định Chi bình phương {.unnumbered}

Được sử dụng để so sánh các tỷ lệ của một biến phân loại trong hai nhóm. Kiểm định thống kê mặc định cho biến phân loại trong hàm `add_p()` là kiểm định Chi bình phương về tính độc lập với hiệu chỉnh liên tục, nhưng nếu có bất kỳ giá trị kỳ vọng nào nhỏ hơn 5 thì kiểm định chính xác của Fisher sẽ được sử dụng. 

```{r chi_gt}
linelist %>% 
  select(gender, outcome) %>%    # keep variables of interest
  tbl_summary(by = outcome) %>%  # produce summary table and specify grouping variable
  add_p()                        # specify what test to perform
```


### Kiểm định t {.unnumbered} 

Được sử dụng để so sánh sự khác biệt về trung bình của một biến trung bình trong hai nhóm. Ví dụ như so sánh tuổi trung bình với kết cục của bệnh nhân. 
```{r ttest_gt}

linelist %>% 
  select(age_years, outcome) %>%             # keep variables of interest
  tbl_summary(                               # produce summary table
    statistic = age_years ~ "{mean} ({sd})", # specify what statistics to show
    by = outcome) %>%                        # specify the grouping variable
  add_p(age_years ~ "t.test")                # specify what tests to perform


```

### Kiểm định tổng thứ hạng Wilcoxon {.unnumbered}

Được dùng để so sánh sự phân bố của một biến liên tục trong hai nhóm. Kiểm định mặc định là kiểm định tổng thứ hang Wilcoxon và trung vị (khoảng tứ phân vị IQR) khi so sánh hai nhóm. Tuy nhiên, đối với số liệu không có phân bố chuẩn hoặc so sánh nhiều nhóm, kiểm định Kruskal-wallis là kiểm định thích hợp hơn. 

```{r wilcox_gt}

linelist %>% 
  select(age_years, outcome) %>%                       # keep variables of interest
  tbl_summary(                                         # produce summary table
    statistic = age_years ~ "{median} ({p25}, {p75})", # specify what statistic to show (this is default so could remove)
    by = outcome) %>%                                  # specify the grouping variable
  add_p(age_years ~ "wilcox.test")                     # specify what test to perform (default so could leave brackets empty)


```

### Kiểm định Kruskal-wallis {.unnumbered}

Được sử dụng để so sánh sự phân bố của một biến liên tục trong hai hay nhiều nhóm, bất kể số liệu có phân bố chuẩn hay không. 

```{r kruskal_gt}

linelist %>% 
  select(age_years, outcome) %>%                       # keep variables of interest
  tbl_summary(                                         # produce summary table
    statistic = age_years ~ "{median} ({p25}, {p75})", # specify what statistic to show (default, so could remove)
    by = outcome) %>%                                  # specify the grouping variable
  add_p(age_years ~ "kruskal.test")                    # specify what test to perform


```




<!-- ## `dplyr` package {} -->

<!-- Performing statistical tests in `dplyr` alone is very dense, again because it  -->
<!-- does not fit within the tidy-data framework. It requires using `purrr` to create -->
<!-- a list of dataframes for each of the subgroups you want to compare. See the page on [Iteration, loops, and lists] to learn about **purrr**.   -->

<!-- An easier alternative may be the `rstatix` package.  -->

<!-- ### T-tests {.unnumbered}  -->

<!-- ```{r ttest_dplyr} -->

<!-- linelist %>%  -->
<!--   ## only keep variables of interest -->
<!--   select(age, outcome) %>%  -->
<!--   ## drop those missing outcome  -->
<!--   filter(!is.na(outcome)) %>%  -->
<!--   ## specify the grouping variable -->
<!--   group_by(outcome) %>%  -->
<!--   ## create a subset of data for each group (as a list) -->
<!--   nest() %>%  -->
<!--   ## spread in to wide format -->
<!--   pivot_wider(names_from = outcome, values_from = data) %>%  -->
<!--   mutate( -->
<!--     ## calculate the mean age for the death group -->
<!--     Death_mean = map(Death, ~mean(.x$age, na.rm = TRUE)), -->
<!--     ## calculate the sd among dead  -->
<!--     Death_sd = map(Death, ~sd(.x$age, na.rm = TRUE)), -->
<!--     ## calculate the mean age for the recover group -->
<!--     Recover_mean = map(Recover, ~mean(.x$age, na.rm = TRUE)),  -->
<!--     ## calculate the sd among recovered  -->
<!--     Recover_sd = map(Recover, ~sd(.x$age, na.rm = TRUE)), -->
<!--     ## using both grouped data sets compare mean age with a t-test -->
<!--     ## keep only the p.value -->
<!--     t_test = map2(Death, Recover, ~t.test(.x$age, .y$age)$p.value) -->
<!--   ) %>%  -->
<!--   ## drop datasets  -->
<!--   select(-Death, -Recover) %>%  -->
<!--   ## return a dataset with the medians and p.value (drop missing) -->
<!--   unnest(cols = everything()) -->

<!-- ``` -->


<!-- ### Wilcoxon rank sum test {.unnumbered} -->

<!-- ```{r wilcox_dplyr} -->

<!-- linelist %>%  -->
<!--   ## only keep variables of interest -->
<!--   select(age, outcome) %>%  -->
<!--   ## drop those missing outcome  -->
<!--   filter(!is.na(outcome)) %>%  -->
<!--   ## specify the grouping variable -->
<!--   group_by(outcome) %>%  -->
<!--   ## create a subset of data for each group (as a list) -->
<!--   nest() %>%  -->
<!--   ## spread in to wide format -->
<!--   pivot_wider(names_from = outcome, values_from = data) %>%  -->
<!--   mutate( -->
<!--     ## calculate the median age for the death group -->
<!--     Death_median = map(Death, ~median(.x$age, na.rm = TRUE)), -->
<!--     ## calculate the sd among dead  -->
<!--     Death_iqr = map(Death, ~str_c( -->
<!--       quantile(.x$age, probs = c(0.25, 0.75), na.rm = TRUE),  -->
<!--       collapse = ", " -->
<!--       )), -->
<!--     ## calculate the median age for the recover group -->
<!--     Recover_median = map(Recover, ~median(.x$age, na.rm = TRUE)),  -->
<!--     ## calculate the sd among recovered  -->
<!--     Recover_iqr = map(Recover, ~str_c( -->
<!--       quantile(.x$age, probs = c(0.25, 0.75), na.rm = TRUE),  -->
<!--       collapse = ", " -->
<!--       )), -->
<!--     ## using both grouped data sets compare age distribution with a wilcox test -->
<!--     ## keep only the p.value -->
<!--     wilcox = map2(Death, Recover, ~wilcox.test(.x$age, .y$age)$p.value) -->
<!--   ) %>%  -->
<!--   ## drop datasets  -->
<!--   select(-Death, -Recover) %>%  -->
<!--   ## return a dataset with the medians and p.value (drop missing) -->
<!--   unnest(cols = everything()) -->

<!-- ``` -->

<!-- ### Kruskal-wallis test {.unnumbered} -->


<!-- ```{r kruskal_dplyr} -->

<!-- linelist %>%  -->
<!--   ## only keep variables of interest -->
<!--   select(age, outcome) %>%  -->
<!--   ## drop those missing outcome  -->
<!--   filter(!is.na(outcome)) %>%  -->
<!--   ## specify the grouping variable -->
<!--   group_by(outcome) %>%  -->
<!--   ## create a subset of data for each group (as a list) -->
<!--   nest() %>%  -->
<!--   ## spread in to wide format -->
<!--   pivot_wider(names_from = outcome, values_from = data) %>%  -->
<!--   mutate( -->
<!--     ## calculate the median age for the death group -->
<!--     Death_median = map(Death, ~median(.x$age, na.rm = TRUE)), -->
<!--     ## calculate the sd among dead  -->
<!--     Death_iqr = map(Death, ~str_c( -->
<!--       quantile(.x$age, probs = c(0.25, 0.75), na.rm = TRUE),  -->
<!--       collapse = ", " -->
<!--       )), -->
<!--     ## calculate the median age for the recover group -->
<!--     Recover_median = map(Recover, ~median(.x$age, na.rm = TRUE)),  -->
<!--     ## calculate the sd among recovered  -->
<!--     Recover_iqr = map(Recover, ~str_c( -->
<!--       quantile(.x$age, probs = c(0.25, 0.75), na.rm = TRUE),  -->
<!--       collapse = ", " -->
<!--       )), -->
<!--     ## using the original data set compare age distribution with a kruskal test -->
<!--     ## keep only the p.value -->
<!--     kruskal = kruskal.test(linelist$age, linelist$outcome)$p.value -->
<!--   ) %>%  -->
<!--   ## drop datasets  -->
<!--   select(-Death, -Recover) %>%  -->
<!--   ## return a dataset with the medians and p.value (drop missing) -->
<!--   unnest(cols = everything()) -->

<!-- ``` -->

<!-- ### Chi-squared test {.unnumbered}  -->


<!-- ```{r} -->
<!-- linelist %>%  -->
<!--   ## do everything by gender  -->
<!--   group_by(outcome) %>%  -->
<!--   ## count the variable of interest -->
<!--   count(gender) %>%  -->
<!--   ## calculate proportion  -->
<!--   ## note that the denominator here is the sum of each gender -->
<!--   mutate(percentage = n / sum(n) * 100) %>%  -->
<!--   pivot_wider(names_from = outcome, values_from = c(n, percentage)) %>%  -->
<!--   filter(!is.na(gender)) %>%  -->
<!--   mutate(pval = chisq.test(linelist$gender, linelist$outcome)$p.value) -->
<!-- ``` -->


<!-- ======================================================= -->

## Tương quan 

Mối tương quan giữa các biến định lượng có thể được kiển bằng cách sử dụng lệnh **corrr** từ package **tidyverse**. Lệnh này cũng cho phép tính các hệ số tương quan bằng phương pháp Pearson, Kendall hoặc Spearman. Gói lệnh này tạo ra một bảng kết quả và cũng có chức năng tự động vẽ các giá trị. 

```{r, warning=F, message=F}

correlation_tab <- linelist %>% 
  select(generation, age, ct_blood, days_onset_hosp, wt_kg, ht_cm) %>%   # keep numeric variables of interest
  correlate()      # create correlation table (using default pearson)

correlation_tab    # print

## remove duplicate entries (the table above is mirrored) 
correlation_tab <- correlation_tab %>% 
  shave()

## view correlation table 
correlation_tab

## plot correlations 
rplot(correlation_tab)
```


<!-- ======================================================= -->

## Nguồn {  }

Phần lớn thông tin trong phần này được phỏng theo các nguồn sau:  

[gtsummary](http://www.danieldsjoberg.com/gtsummary/articles/tbl_summary.html)
[dplyr](https://dplyr.tidyverse.org/articles/grouping.html)
[corrr](https://corrr.tidymodels.org/articles/using-corrr.html)
[sthda correlation](http://www.sthda.com/english/wiki/correlation-test-between-two-variables-in-r)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/stat_tests.Rmd-->

# Hồi quy đơn và đa biến {#regression}

<!-- ======================================================= -->

Trong chương này, chúng tôi trình bày cách sử dụng các hàm hồi quy trong **base** R rnhư hàm `glm()` và package **gtsummary** để xem xét các mối liên quan giữa các biến (ví dụ như tỷ số chênh, tỷ số nguy cơ, tỷ số rủi ro). Chúng tôi cũng trình bày cách sử dụng các hàm như `tidy()` trong package **broom** để sắp xếp các kết quả hồi quy.

1.  Phân tích đơn biến: bảng 2 x 2
2.  Phân tích phân tầng: ước lượng của mantel-haenszel\
3.  Phân tích đa biến: lựa chọn biến số, lựa chọn mô hình, mô hình cuối cùng
4.  Biểu đồ Forest plot

Đối với hồi quy Cox, xem chương [Phân tích sống còn].

[***CHÚ Ý:*** Chúng tôi sử dụng thuật ngữ *đa biến (multivariable)* để nói đến một hồi quy có nhiều biến giải thích. Thuật ngữ này khác với mô hình *đa biến* (multivariate model), là một mô hình đa biến có nhiều biến kết cục -- xem chi tiết trong [bài xã luận](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3518362/) này ]{style="color: black;"}

<!-- ======================================================= -->

## Chuẩn bị

### Gọi packages {.unnumbered}

Đoạn mã này hiển thị cách tải các gói lệnh cần thiết cho phân tích. Trong cuốn sổ tay này, chúng tôi nhấn mạnh hàm `p_load()` thuộc package **pacman**, giúp cài đặt package khi cần thiết *và* gọi nó ra để sử dụng. Có thể gọi các package đã cài đặt bằng hàm `library()` trong **base** R. Xem thêm thông tin về các package của R trong chương [R cơ bản].

```{r}
pacman::p_load(
  rio,          # File import
  here,         # File locator
  tidyverse,    # data management + ggplot2 graphics, 
  stringr,      # manipulate text strings 
  purrr,        # loop over objects in a tidy way
  gtsummary,    # summary statistics and tests 
  broom,        # tidy up results from regressions
  lmtest,       # likelihood-ratio tests
  parameters,   # alternative to tidy up results from regressions
  see          # alternative to visualise forest plots
  )
```

### Nhập số liệu {.unnumbered}

Chúng tôi nhập bộ số liệu của các ca bệnh được mô phỏng từ một vụ dịch Ebola. Để tiện làm theo, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>bấm để tải số liệu linelist "đã được làm sạch"</a> (dưới dạng tệp .rds ). Nhập số liệu này bằng hàm `import()` trong package **rio** (nó chấp nhận nhiều loại tập tin như .xlsx, .rds, .csv -- xem chi tiết trong chương [Nhập xuất dữ liệu]).

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.rds")
```

Bên dưới là hiển thị của 50 hàng đầu tiên của bộ số liệu linelist.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T) )
```

### Làm sạch số liệu {.unnumbered}

#### Lưu trữ các biến giải thích {.unnumbered}

Tên của các biến giải thích sẽ được lưu trữ dưới dạng một véc tơ ký tự. Véc tơ này sẽ được đề cập về sau.

```{r}
## define variables of interest 
explanatory_vars <- c("gender", "fever", "chills", "cough", "aches", "vomit")
```

#### Chuyển đổi sang số 1 và số 0 {.unnumbered}

Sau đây, giá trị của các biến giải thích được chuyển đổi từ "có"/"không", "nam"/"nữ" và "chết"/"sống" thành 1 / 0, để hợp với các đặc tính của mô hình hồi quy logistic. TĐể thực hiện việc này một cách hiệu quả, sử dụng hàm `across()` từ **dplyr** để chuyển đổi nhiều biến cùng một lúc. Để áp dụng cho mỗi biến, dùng hàm `case_when()` (cũng trong package **dplyr**) để chuyển đổi các giá trị cụ thể thành 1 và 0. Xem các mục về `across()` và `case_when()` trong chương [Làm sạch số liệu và các hàm quan trọng](#clean_across)).

Chú ý: dấu "." bên dưới đại diện cho cột\`\`\`\`\``đang được xử lý trong hàm`across()\` tại thời điểm đó.

```{r}
## convert dichotomous variables to 0/1 
linelist <- linelist %>%  
  mutate(across(                                      
    .cols = all_of(c(explanatory_vars, "outcome")),  ## for each column listed and "outcome"
    .fns = ~case_when(                              
      . %in% c("m", "yes", "Death")   ~ 1,           ## recode male, yes and death to 1
      . %in% c("f", "no",  "Recover") ~ 0,           ## female, no and recover to 0
      TRUE                            ~ NA_real_)    ## otherwise set to missing
    )
  )

       
      
```

#### Loại bỏ các hàng có giá trị missing {.unnumbered}

Để bỏ các hàng có giá trị missing, dùng hàm `drop_na()` trong package **tidyr**. Tuy nhiên, chúng ta chỉ muốn thực hiện điều này cho các hàng có giá trị missing đối với các cột đang được quan tâm.

Trước hết, chúng ta phải đảm bảo rằng vectơ `explanatory_vars` bao gồm các biến `age` (`age` có thể tạo ra một lỗi trong thao tác của hàm `case_when()` trước đó, mà chỉ dành cho biến nhị phân). Sau đó chúng ta pipe bộ dữ liệu `linelist` tới hàm `drop_na()` để bỏ các hàng có giá trị missing cho biến `outcome` hoặc bất kỳ biển giải thích `explanatory_vars` nào.

Trước khi thực hiện các lệnh này, kiểm tra số hàng trong bộ số liệu `linelist` bằng hàm `nrow(linelist)`.

```{r}
## add in age_category to the explanatory vars 
explanatory_vars <- c(explanatory_vars, "age_cat")

## drop rows with missing information for variables of interest 
linelist <- linelist %>% 
  drop_na(any_of(c("outcome", explanatory_vars)))

```

Kiểm tra số hàng còn lại của `linelist` bằng hàm `nrow(linelist)`.

<!-- ======================================================= -->

## Phân tích đơn biến

Cũng giống như chương [Bảng mô tả](#tables-descriptive), chúng ta cần xác định packahe nào trong R mà chúng ta muốn sử dụng. Chúng tôi trình bày hai chọn lựa để thực hiện các phân tích đơn biến:

-   Dùng hàm có sẵn trong **base** để in nhanh kết quả ra console. Sử dụng package **broom** để làm gọn kết quả.\
-   Dùng package **gtsummary** để lập mô hình và nhận các kết quả đầu ra sẵn sàng để công bố

<!-- ======================================================= -->

### **base** R {.unnumbered}

#### Hồi quy tuyến tính {.unnumbered}

Hàm `lm()` trong **base** cho phép thực hiện hồi quy tuyến tính để đánh giá mối quan hệ giữa biến đầu ra dạng số (numeric) và các biến giải thích mà được giả định là có mối quan hệ tuyến tính.

Cung cấp phương trình dưới dạng công thức với tên của biến đầu ra và các biến giải thích được phân tách bằng dấu ngã `~`. Bên cạnh đó, chỉ rõ bộ số liệu nào được sử dụng với `data =`. Kết quả của mô hình được định nghĩa dưới dạng đối tượng của R để sử dụng về sau.

```{r lin_reg}
lm_results <- lm(ht_cm ~ age, data = linelist)
```

Sau đó tóm tắt kết quả của mô hình bằng hàm `summary()` để xem các hệ số (ước tính), P-value, phần dư và các đo lường khác.

```{r lin_reg_res}
summary(lm_results)
```

Ngoài ra, có thể dùng hàm `tidy()` trong package **broom** để xuất kết quả vào trong một bảng. Kết quả bên dưới cho chúng ta biết khi tăng thêm một tuổi thì chiều cao tăng 3,5 cm và mối quan hệ này có ý nghĩa thống kê.

```{r lin_reg_res_tidy}
tidy(lm_results)
```

Sau đó, có thể sử dụng kết quả hồi quy này để đưa vào **ggplot**. Để thực hiện điều này, trước tiên chúng ta đưa các giá trị quan sát và đường thẳng hồi quy (fitted line) vào một data frame bằng cách dùng hàm `augment()` trong package **broom**.

```{r lin_reg_res_plot}

## pull the regression points and observed data in to one dataset
points <- augment(lm_results)

## plot the data using age as the x-axis 
ggplot(points, aes(x = age)) + 
  ## add points for height 
  geom_point(aes(y = ht_cm)) + 
  ## add your regression line 
  geom_line(aes(y = .fitted), colour = "red")

```

Bạn cũng có thể vẽ đường hồi quy tuyến tính đơn bằng package **ggplot** thông qua hàm `geom_smooth()`.

```{r geom_smooth}

## add your data to a plot 
 ggplot(linelist, aes(x = age, y = ht_cm)) + 
  ## show points
  geom_point() + 
  ## add a linear regression 
  geom_smooth(method = "lm", se = FALSE)
```

Xem thêm các hướng dẫn chi tiết trong mục Nguồn ở cuối chương này.

#### Hồi quy Logistic {.unnumbered}

Hàm `glm()` trong package **stats** (một phần của **base** R) được sử dụng để fit (chọn mô hình dự đoán tối ưu dựa trên số liệu quan sát) đối với Mô hình Tuyến tính Tổng quát (GLM).

`glm()` có thể được sử dụng cho cả hồi quy logistic đơn biến và đa biến (ví dụ như để tính tỷ số chênh OR). Sau đây là những thành phần chính của hàm:

```{r, eval=F}
# arguments for glm()
glm(formula, family, data, weights, subset, ...)
```

-   `formula =` Mô hình được cung cấp cho `glm()` dưới dạng một phương trình với biến kết cục ở bên trái và biến giải thích ở bên phải dấu ngã `~`.\
-   `family =` Xác định loại mô hình sẽ thực hiện. Đối với hồi quy logistic, sử dụng `family = "binomial"`, đối với hồi quy poisson sử dụng `family = "poisson"`. Các ví dụ khác được trình bày trong bảng bên dưới.\
-   `data =` Cụ thể bộ số liệu

Nếu cần, có thể cụ thể hàm liên kết bằng cú pháp `family = familytype(link = "linkfunction"))`. Bạn có thể tìm đọc thêm về các họ hồi quy khác và các tùy chọn đối số như là `weights =` và `subset =` bằng cách gõ (`?glm`).

| Họ                   | Hàm liên kết mặc định                        |
|----------------------|----------------------------------------------|
| `"binomial"`         | `(link = "logit")`                           |
| `"gaussian"`         | `(link = "identity")`                        |
| `"Gamma"`            | `(link = "inverse")`                         |
| `"inverse.gaussian"` | `(link = "1/mu^2")`                          |
| `"poisson"`          | `(link = "log")`                             |
| `"quasi"`            | `(link = "identity", variance = "constant")` |
| `"quasibinomial"`    | `(link = "logit")`                           |
| `"quasipoisson"`     | `(link = "log")`                             |

Khi thực hiện `glm()` , phổ biến nhất là lưu kết quả dưới dạng một đối tượng của R được đặt tên. Sau đó, có thể xuất kết quả ra console bằng cách sử dụng hàm `summary()` như được trình bày bên dưới, hoặc thực hiện các thao tác khác từ kết quả (ví dụ như lấy lũy thừa).

Nếu cần thực hiện một hồi quy nhị thức âm, có thể sử dụng package **MASS**. Hàn `glm.nb()` uses cũng sử dụng cùng cú pháp như `glm()`. Để xem qua các hồi quy khác, xem trên [trang thống kê của UCLA](https://stats.idre.ucla.edu/other/dae/).

#### Phân tích đơn biến sử dụng `glm()` {.unnumbered}

Trong ví dụ này, chúng tôi đánh giá mối liên quan giữa nhóm tuổi và biến kết cục tử vong (được mã hóa là 1 trong phần chuẩn bị). Bên dưới là một mô hình đơn biến của biến kết cục `outcome` theo `age_cat`. Chúng tôi lưu kết quả đầu ra được đặt tên là `model` và sau đó in kết quả đến console bằng hàm `summary()`. Lưu ý, các ước tính được tạo ra là các giá trị *lôgarít của tỷ số chênh (log odds)* và giá trị tham chiếu là giá trị đầu tiên của biến `age_cat` ("0-4").

```{r}
model <- glm(outcome ~ age_cat, family = "binomial", data = linelist)
summary(model)
```

Để thay đổi giá trị tham chiếu của một biến Factor và chuyển giá trị mong muốn lên vị trí đầu tiên, dùng hàm `fct_relevel()` (xem chương [Factors]). Ở ví dụ bên dưới, chúng tôi lấy biến `age_cat` và đặt nhóm tuổi "20-29" làm giá trị tham chiếu trước khi chuyển số liệu đã sửa đổi vào hàm `glm()`.

```{r}
linelist %>% 
  mutate(age_cat = fct_relevel(age_cat, "20-29", after = 0)) %>% 
  glm(formula = outcome ~ age_cat, family = "binomial") %>% 
  summary()
```

#### In kết quả {.unnumbered}

Đối với hầu hết các mục đích sử dụng, kết quả đầu ra cần phải có một số sửa đổi. Hàm làm gọn `tidy()` trong package **broom** có những tiện lợi để hiển thị kết quả của mô hình.

Ở đây, chúng tôi trình bày cách để kết hợp các kết quả đầu ra của mô hình vào trong một bảng.

1)  Lấy *lũy thừa* logarit của ước lượng tỉ số chênh OR và khoảng tin cậy  bằng cách đưa mô hình vào hàm `tidy()` và thiết lập lũy thừa `exponentiate = TRUE` và `conf.int = TRUE`.

```{r odds_base_single}

model <- glm(outcome ~ age_cat, family = "binomial", data = linelist) %>% 
  tidy(exponentiate = TRUE, conf.int = TRUE) %>%        # exponentiate and produce CIs
  mutate(across(where(is.numeric), round, digits = 2))  # round all numeric columns
```

Bên dưới là bảng kết quả đầu ra của `model`:

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(model, rownames = FALSE, options = list(pageLength = nrow(model), scrollX=T), class = 'white-space: nowrap' )
```

2)  Kết hợp các kết quả của mô hình vào trong một bảng đếm. Dưới đây, chúng tôi tạo một bảng đếm bằng hàm `tabyl()` từ package **janitor**, như được đề cập trong chương [Bảng mô tả].

```{r}
counts_table <- linelist %>% 
  janitor::tabyl(age_cat, outcome)
```

<!-- * Group rows by outcome, and get counts by age category   -->

<!-- * Pivot wider so the column are `age_cat`, `0`, and `1`   -->

<!-- * Remove row for `NA` `age_cat`, if applicable, to align with the model results   -->

<!-- ```{r} -->

<!-- counts_table <- linelist %>%  -->

<!--   filter(!is.na(outcome) & !is.na(age_cat)) %>%    # ensure outcome and age_cat are present  -->

<!--   group_by(outcome) %>%                            # get counts of variable of interest grouped by outcome -->

<!--   count(age_cat) %>%   ## gets number or rows by unique outcome-age category combinations   -->

<!--   pivot_wider(names_from = outcome, values_from = n)    ## spread data to wide format (as in cross-tabulation) -->

<!-- ``` -->

Đây là cách mà bảng `counts_table` được hiển thị:

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(counts_table, rownames = FALSE, options = list(pageLength = nrow(counts_table), scrollX=T), class = 'white-space: nowrap' )
```

Bây giờ chúng ta có thể nối bảng `counts_table` và kết quả của mô hình `model` lại với nhau theo chiều ngang bằng hàm nối cột `bind_cols()` (**dplyr**). Hãy nhớ rằng đối với hàm `bind_cols()` các hàng trong hai cấu trúc dữ liệu trên phải được căn chỉnh hoàn hảo. Trong đoạn code này, bởi vì chúng ta đang thực hiện một chuỗi các thuật toán pipe, chúng ta sử dụng dấu `.` để đại diện cho đối tượng được nối trong bảng đếm `counts_table` khi chúng tôi nối nó với kết quả mô hình `model`. Để kết thúc quy trình này, chúng ta sử dụng hàm `select()` để chọn các cột mong muốn và thứ tự của nó, và cuối cùng áp dụng hàm `round()` trong **base** R để làm tròn với hai chữ số thập phân cho tất cả các cột.

```{r, message=F, warning=F}
combined <- counts_table %>%           # begin with table of counts
  bind_cols(., model) %>%              # combine with the outputs of the regression 
  select(term, 2:3, estimate,          # select and re-order cols
         conf.low, conf.high, p.value) %>% 
  mutate(across(where(is.numeric), round, digits = 2)) ## round to 2 decimal places
```

Đây là hiển thị của cấu trúc đã được kết hợp, nó được xuất gọn gẽ dưới dạng một hình bằng thông qua một hàm trong package **flextable**. Chương [Trình bày bảng] giải thích cách tùy chỉnh các bảng như vậy bằng **flextable**, hoặc có thể sử dụng các gói lệnh khác như **knitr** hoặc **GT**.

```{r}
combined <- combined %>% 
  flextable::qflextable()
```

#### Vòng lặp cho nhiều mô hình đơn biến {.unnumbered}

Sau đây chúng tôi trình bày một phương pháp sử dụng `glm()` và `tidy()` để có một cách tiếp cận đơn giản hơn, xem thêm ở phần **gtsummary**.

Để thực hiện các mô hình cho một số biến giải thích và cho ra các tỷ số chênh trong phân tích đơn biến (nghĩa là không có kiểm soát lẫn nhau), chúng ta có thể sử dụng các cách tiếp cận dưới đây. Sử dụng hàm `str_c()` từ package **stringr** để tạo ra các công thức cho phân tích đơn biến (xem chương [Ký tự và chuỗi]), thực hiện hàm `glm()` cho mỗi công thức, chuyển mỗi kết quả đầu ra của `glm()` đến hàm `tidy()` và cuối cùng thu gọn lại tất các kết quả đầu ra của mô hình bằng hàm nối dòng `bind_rows()` từ **tidyr**. Phương pháp này sử dụng hàm `map()` từ package **purrr** để lặp - xem chương [Lặp, vòng lặp và danh sách] để biết thêm thông tin về công cụ này.

1)  Tạo một véctơ tên các cột của biến giải thích. Chúng ta đã tạo biến này `explanatory_vars` trong phần chuẩn bị của chương này.

2)  Sử dụng hàm `str_c()` để tạo các công thức chuỗi với biến kết cục `outcome` ở bên trái và tên một cột của véctơ `explanatory_vars` ở bên phải. Dấu chấm `.` trong hàm này thay thế cho tên cột trong véctơ `explanatory_vars`.

```{r}
explanatory_vars %>% str_c("outcome ~ ", .)
```

3)  Đưa các công thức chuỗi này vào hàm `map()` và đặt `~glm()` làm hàm áp dụng cho mỗi đầu vào. Bên trong hàm `glm()`, thiết lập công thức hồi quy `as.formula(.x)` trong đó `.x` sẽ được thay thế bằng các công thức chuỗi đã được tạo bên trên. Hàm `map()` sẽ lặp từng công thức chuỗi và thực hiện hồi quy cho từng công thức.

4)  Kết quả đầu ra của hàm `map()` đầu tiên sẽ được chuyển đến hàm `map()` thứ hai mà sử dụng hàm `tidy()` để làm gọn các kết quả đầu ra.

5)  Cuối cùng, kết quả đầu ra của hàm `map()` thứ hai (một danh sách các data frames đã được làm gọn) được tóm tắt bằng hàm nối dòng `bind_rows()`, kết quả cho ra một data frame với tất cả các kết quả đơn biến.

```{r odds_base_multiple}

models <- explanatory_vars %>%       # begin with variables of interest
  str_c("outcome ~ ", .) %>%         # combine each variable into formula ("outcome ~ variable of interest")
  
  # iterate through each univariate formula
  map(                               
    .f = ~glm(                       # pass the formulas one-by-one to glm()
      formula = as.formula(.x),      # within glm(), the string formula is .x
      family = "binomial",           # specify type of glm (logistic)
      data = linelist)) %>%          # dataset
  
  # tidy up each of the glm regression outputs from above
  map(
    .f = ~tidy(
      .x, 
      exponentiate = TRUE,           # exponentiate 
      conf.int = TRUE)) %>%          # return confidence intervals
  
  # collapse the list of regression outputs in to one data frame
  bind_rows() %>% 
  
  # round all numeric columns
  mutate(across(where(is.numeric), round, digits = 2))
```

Lúc này, kết quả xuất ra của `models` dài hơn bởi vì kết quả bây giờ bao gồm các kết quả đầu ra của một số hồi quy đơn biến.  Nhấp nút tiếp theo để xem tất cả các hàng của `model`.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(models, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Như lúc trước, chúng ta có thể tạo một bảng đếm từ bộ số liệu `linelist` cho mỗi biến giải thích, gắn chúng với `models`, và tạo ra một bảng đẹp. Chúng ta bắt đầu với các biến giải thích này, và lặp lại các biến này thông qua hàm `map()`. Chúng ta lặp lại qua một hàm do người dùng tạo ra mà liên quan đến việc tạo ra một bảng đếm bằng cách dùng các hàm trong package **dplyr** Sau đó, kết quả được kết nối trình tự với kết quả của mô hình `models`.

```{r, warning=F, message=F}

## for each explanatory variable
univ_tab_base <- explanatory_vars %>% 
  map(.f = 
    ~{linelist %>%                ## begin with linelist
        group_by(outcome) %>%     ## group data set by outcome
        count(.data[[.x]]) %>%    ## produce counts for variable of interest
        pivot_wider(              ## spread to wide format (as in cross-tabulation)
          names_from = outcome,
          values_from = n) %>% 
        drop_na(.data[[.x]]) %>%         ## drop rows with missings
        rename("variable" = .x) %>%      ## change variable of interest column to "variable"
        mutate(variable = as.character(variable))} ## convert to character, else non-dichotomous (categorical) variables come out as factor and cant be merged
      ) %>% 
  
  ## collapse the list of count outputs in to one data frame
  bind_rows() %>% 
  
  ## merge with the outputs of the regression 
  bind_cols(., models) %>% 
  
  ## only keep columns interested in 
  select(term, 2:3, estimate, conf.low, conf.high, p.value) %>% 
  
  ## round decimal places
  mutate(across(where(is.numeric), round, digits = 2))

```

Bên dưới là cấu trúc số liệu kết nối được tạo ra. Xem chương [Trình bày bảng] để có thêm ý tưởng về cách chuyển đổi bảng số liệu này thành một bảng đẹp trên HTML (ví dụ như với package **flextable**).

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(univ_tab_base, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

<!-- ======================================================= -->

### **gtsummary** package {#reg_gt_uni .unnumbered}

Sau đây chúng tôi sẽ trình bày cách sử dụng hàm `tbl_uvregression()` từ package **gtsummary**. Cũng giống như trong chương [Bảng mô tả](https://epirhandbook.com/descriptive-tables.html), các hàm trong **gtsummary** thực hiện tốt các thống kê *và* xuất ra các kết quả khá chuyên nghiệp. Hàm này xuất ra một bảng kết quả của hồi quy đơn biến.

Chúng ta chỉ chọn các cột cần thiết từ bộ số liệu `linelist` (ecác biến giải thích và biến kết cục) và pipe chúng vào hàm `tbl_uvregression()`. Chúng ta sẽ thực hiện hồi quy đơn biến cho mỗi cột như được xác định trong véctơ `explanatory_vars` trong mục Chuẩn bị (gender, fever, chills, cough, aches, vomit, và age_cat).

Trong hàm này, chúng ta cung cấp thêm phương pháp thực hiện `method =` là `glm` (không có dấu ngoặc kép), biến kết cục `y =` cột kết quả (biến `outcome`), cụ thể `method.args =` mà chúng ta muốn thực hiện hồi quy logistic qua `family = binomial`, và lấy lũy thừa của kết quả.

Kết quả đầu ra dưới dạng HTML và chứa cột đếm

```{r odds_gt, message=F, warning=F}

univ_tab <- linelist %>% 
  dplyr::select(explanatory_vars, outcome) %>% ## select variables of interest

  tbl_uvregression(                         ## produce univariate table
    method = glm,                           ## define regression want to run (generalised linear model)
    y = outcome,                            ## define outcome variable
    method.args = list(family = binomial),  ## define what type of glm want to run (logistic)
    exponentiate = TRUE                     ## exponentiate to produce odds ratios (rather than log odds)
  )

## view univariate results table 
univ_tab
```

Chúng ta có thể sửa đổi đối với kết quả đầu ra của bảng này, ví dụ như điều chỉnh các nhãn, tô đậm các hàng theo giá trị p, .v.v. Xem hướng dẫn [tại đây](http://www.danieldsjoberg.com/gtsummary/articles/tbl_regression.html) và các tài liệu trực tuyến khác.

<!-- ======================================================= -->

## Phân tích phân tầng

Hiện tại, phân tích phần tầng sử dụng package **gtsummary** đang được xây dựng, phần này sẽ được cập nhật trong thời gian thích hợp.

## Phân tích đa biến

Đối với phân tích đa biến, chúng tôi trình bày hai cách tiếp cận:

-   `glm()` và `tidy()`\
-   Package **gtsummary** 

Quy trình thực hiện khá tương tự và chỉ khác ở bước cuối cùng để kết nối kết quả lại với nhau.

### Thực hiện phân tích đa biến {.unnumbered}

Ở đây chúng tôi sử dụng hàm `glm()` nhưng thêm nhiều biến hơn vào bên phải của phương trình và được phân tách với nhau bằng dấu cộng (`+`).

Để thực hiện mô hình với tất cả các biến giải thích, chúng ta thực hiện lệnh sau:

```{r}
mv_reg <- glm(outcome ~ gender + fever + chills + cough + aches + vomit + age_cat, family = "binomial", data = linelist)

summary(mv_reg)
```

Nếu muốn bao gồm hai biến và tương tác của hai biến này, chúng ta có thể phân tách chúng bằng dấu hoa thị `*` thay cho dấu `+`. Nếu chúng ta chỉ muốn cụ thể sự tương tác, phân tách chúng bằng dấu hai chấm `:`. Ví dụ:

```{r, eval=F}
glm(outcome ~ gender + age_cat * fever, family = "binomial", data = linelist)
```

*Một tùy chọn khác*, chúng ta có thể sử dụng đoạn mã này để sử dụng một véc tơ đã được định nghĩa trước của các cột và tạo lại lệnh trên bằng cách sử dụng hàm `str_c()`. Điều này có thể hữu ích nếu chúng ta thay đổi tên các biến giải thích, hoặc bạn không muốn gõ lại tất cả mọi thứ.

```{r mv_regression}

## run a regression with all variables of interest 
mv_reg <- explanatory_vars %>%  ## begin with vector of explanatory column names
  str_c(collapse = "+") %>%     ## combine all names of the variables of interest separated by a plus
  str_c("outcome ~ ", .) %>%    ## combine the names of variables of interest with outcome in formula style
  glm(family = "binomial",      ## define type of glm as logistic,
      data = linelist)          ## define your dataset
```

#### Xây dựng mô hình {.unnumbered}

Chúng ta có thể xây dựng mô hình theo từng bước, lưu các mô hình khác nhau với một số biến giải thích. Chúng ta có thể sử dụng kiểm định tỷ số khả dĩ (likelihood-ratio tests) để so sánh các mô hình này bằng cách sử dụng hàm `lrtest()` từ package **lmtest**, như dưới đây:

[***CHÚ Ý:*** Sử dụng hàn `anova(model1, model2, test = "Chisq)` trong **base** R cũng cho kết quả tương tự ]{style="color: black;"}

```{r}
model1 <- glm(outcome ~ age_cat, family = "binomial", data = linelist)
model2 <- glm(outcome ~ age_cat + gender, family = "binomial", data = linelist)

lmtest::lrtest(model1, model2)
```

Một tùy chọn khác là lấy đối tượng của mô hình và sử dụng hàm `step()` từ package **stats**. Chỉ rõ hướng lựa chọn biến mà chúng ta muốn sử dụng khi xây dựng mô hình.

```{r}
## choose a model using forward selection based on AIC
## you can also do "backward" or "both" by adjusting the direction
final_mv_reg <- mv_reg %>%
  step(direction = "forward", trace = FALSE)
```

Để hiển thị rõ số, chúng ta có thể tắt ký hiệu khoa học trong R bằng lệnh sau

```{r}
options(scipen=999)
```

Như được mô tả trong phần phân tích đơn biến, chuyển kết quả đầu ra của mô hình vào hàm `tidy()` để lấy lũy thừa cho các hệ số và khoảng tin cậy (CIs). Cuối cùng, làm tròn tất cả các cột số với hai số thập phân. Kéo qua để xem tất cả các hàng.

```{r mv_regression_base}

mv_tab_base <- final_mv_reg %>% 
  broom::tidy(exponentiate = TRUE, conf.int = TRUE) %>%  ## get a tidy dataframe of estimates 
  mutate(across(where(is.numeric), round, digits = 2))          ## round 
```

Đây là hiển thị kết quả dưới dạng data frame looks:

```{r, message=FALSE, echo=F}
DT::datatable(mv_tab_base, rownames = FALSE, options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )
```

<!-- ======================================================= -->

### Gộp kết quả phân tích đơn biến và đa biến {.unnumbered}

#### Gộp bằng package **gtsummary** {.unnumbered}

Hàm `tbl_regression()` trong package **gtsummary** sẽ lấy kết quả đầu tra từ một hồi quy (hàm `glm()` trong trường hợp này) và tạo ra một bảng tóm tắt đẹp.

```{r mv_regression_gt}
## show results table of final regression 
mv_tab <- tbl_regression(final_mv_reg, exponentiate = TRUE)
```

Hãy xem bảng sau:

```{r}
mv_tab
```

Chúng ta cũng có thể kết hợp một số bảng kết quả đầu ra bằng cách dùng hàm `tbl_merge()` trong package **gtsummary**. Bây giờ chúng ta hộp các kết quả đa biến với kết quả *đơn biến* đã được tạo [bên trên](#reg_gt_uni) bằng package **gtsummary**:

```{r}
## combine with univariate results 
tbl_merge(
  tbls = list(univ_tab, mv_tab),                          # combine
  tab_spanner = c("**Univariate**", "**Multivariable**")) # set header names
```

#### Gộp bằng package **dplyr** {.unnumbered}

Một cách khác để gộp các kết quả đơn biến và đa biến từ các hàm `glm()`/`tidy()` bằng cách sử dụng các hàm kết nối từ package **dplyr**.

-   Kết nối kết quả đơn biến trước đó (`univ_tab_base`, chứa được các cột đếm) với kết quả đa biến đã được làm gọn `mv_tab_base`\
-   Sử dụng hàm `select()` để giữ lại, sắp xếp lại thứ tự và đặt lại tên các cột mà chúng ta muốn\
-   Sử dụng hàm `round()` để làm tròn tất cả các cột với hai số thập phân

```{r, warning=F, message=F}
## combine univariate and multivariable tables 
left_join(univ_tab_base, mv_tab_base, by = "term") %>% 
  ## choose columns and rename them
  select( # new name =  old name
    "characteristic" = term, 
    "recovered"      = "0", 
    "dead"           = "1", 
    "univ_or"        = estimate.x, 
    "univ_ci_low"    = conf.low.x, 
    "univ_ci_high"   = conf.high.x,
    "univ_pval"      = p.value.x, 
    "mv_or"          = estimate.y, 
    "mvv_ci_low"     = conf.low.y, 
    "mv_ci_high"     = conf.high.y,
    "mv_pval"        = p.value.y 
  ) %>% 
  mutate(across(where(is.double), round, 2))   

```

<!-- ======================================================= -->

## Biểu đồ Forest plot

Phần này hướng dẫn cách tạo ra một biểu đồ của các kết quả hồi quy. Có hai lựa chọn để tạo biểu đồ, chúng ta có thể tự tạo một biểu đồ bằng cách sử dụng package **ggplot2** hoặc sử dụng một meta-package có tên **easystats** (một package gồm nhiều package).

Nếu chưa quen thuộc với gói lệnh tạo biểu đồ **ggplot2**, xem thêm chương [ggplot cơ bản].

<!-- ======================================================= -->

### **ggplot2** package {.unnumbered}

Bạn có thể xây dựng một forest plot với hàm `ggplot()` bằng cách vẽ các thành phần của kết quả hồi quy đa biến. Thêm các lớp của biều đồ bằng cách sử dụng các "geoms":

-   Các ước lượng bằng hàm `geom_point()`\
-   Khoảng tin cậy bằng hàm `geom_errorbar()`\
-   Đường thẳng đứng ở vị trí OR = 1 bằng hàm `geom_vline()`

Trước khi tạo biểu đồ, chúng ta sử dụng hàm `fct_relevel()` từ package **forcats** để đặt thứ tự các biến trên trục y. Hàm `ggplot()` cho phép hiển thị theo thứ tự chữ-số mà có thể không hiển thị tốt cho các giá trị của biến tuổi (“30” có thể hiển thị trước “5”). Xem chương [Factors] để biết thêm chi tiết.

```{r ggplot_forest}

## remove the intercept term from your multivariable results
mv_tab_base %>% 
  
  #set order of levels to appear along y-axis
  mutate(term = fct_relevel(
    term,
    "vomit", "gender", "fever", "cough", "chills", "aches",
    "age_cat5-9", "age_cat10-14", "age_cat15-19", "age_cat20-29",
    "age_cat30-49", "age_cat50-69", "age_cat70+")) %>%
  
  # remove "intercept" row from plot
  filter(term != "(Intercept)") %>% 
  
  ## plot with variable on the y axis and estimate (OR) on the x axis
  ggplot(aes(x = estimate, y = term)) +
  
  ## show the estimate as a point
  geom_point() + 
  
  ## add in an error bar for the confidence intervals
  geom_errorbar(aes(xmin = conf.low, xmax = conf.high)) + 
  
  ## show where OR = 1 is for reference as a dashed line
  geom_vline(xintercept = 1, linetype = "dashed")
  
```

<!-- ======================================================= -->

### **easystats** packages {.unnumbered}

Lựa chọn thứ hai là sử dụng một sự kết hợp của các package trong **easystats**, nếu chúng ta không muốn mức độ kiểm soát chặt chẽ mà package **ggplot2** cung cấp.

Hàm `model_parameters()` từ package **parameters** thực hiện tương đương với hàm `tidy()` trong package **broom** . Sau đó, package **see** chấp nhận các kết quả đầu ra và tạo một biểu đồ forest plot mặc định giống như cho một đối tượng `ggplot()`.

```{r easystats_forest}
pacman::p_load(easystats)

## remove the intercept term from your multivariable results
final_mv_reg %>% 
  model_parameters(exponentiate = TRUE) %>% 
  plot()
  
```

<!-- ======================================================= -->

## Nguồn

Nội dung của chương này được tham khảo từ các nguồn sau:

[Linear regression in R](https://www.datacamp.com/community/tutorials/linear-regression-R)

[gtsummary](http://www.danieldsjoberg.com/gtsummary/articles/tbl_regression.html)

[UCLA stats page](https://stats.idre.ucla.edu/other/dae/)

[sthda stepwise regression](http://www.sthda.com/english/articles/36-classification-methods-essentials/150-stepwise-logistic-regression-essentials-in-r/)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/regression.Rmd-->


# Dữ liệu Missing {#missing-data}

```{r, out.width=c("50%"), echo=F}
knitr::include_graphics(here::here("images", "missingness.png"))
knitr::include_graphics(here::here("images", "missingness_overview.png"))
```

This page will cover how to:  

1) Assess missingness  
2) Filter out rows by missingness  
3) Plot missingness over time  
4) Handle how `NA` is displayed in plots  
5) Perform missing value imputation: MCAR, MAR, MNAR  



<!-- ======================================================= -->
## Preparation { }

### Load packages {.unnumbered}  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  rio,           # import/export
  tidyverse,     # data mgmt and viz
  naniar,        # assess and visualize missingness
  mice           # missing data imputation
)
```


### Import data {.unnumbered}

We import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Import your data with the `import()` function from the **rio** package (it accepts many file types like .xlsx, .rds, .csv - see the [Import and export] page for details).  

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.rds")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


### Convert missing on import {.unnumbered}  

When importing your data, be aware of values that should be classified as missing. For example, 99, 999, "Missing", blank cells (""), or cells with an empty space (" "). You can convert these to `NA` (R's version of missing data) during the data import command.  
See the page on importing page section on [Missing data](#import_missing) for details, as the exact syntax varies by file type.  


<!-- ======================================================= -->
## Missing values in R { }

Below we explore ways that missingness is presented and assessed in R, along with some adjacent values and functions.  

### `NA` {.unnumbered}  

In R, missing values are represented by a reserved (special) value - `NA`. Note that this is typed *without* quotes. "NA" is different and is just a normal character value (also a Beatles lyric from the song Hey Jude).  

Your data may have other ways of representing missingness, such as "99", or "Missing", or "Unknown" - you may even have empty character value "" which looks "blank", or a single space " ". Be aware of these and consider whether to [convert them to `NA` during import](#import_missing) or during data cleaning with `na_if()`.  

In your data cleaning, you may also want to convert the other way - changing all `NA` to "Missing" or similar with `replace_na()` or with `fct_explicit_na()` for factors.  




### Versions of `NA` {.unnumbered}  

Most of the time, `NA` represents a missing value and everything works fine. However, in some circumstances you may encounter the need for *variations* of `NA` specific to an object class (character, numeric, etc). This will be rare, but you should be aware.    
The typical scenario for this is when creating a new column with the **dplyr** function `case_when()`. As described in the [Cleaning data and core functions](#clean_case_when) page, this function evaluates every row in the data frame, assess whether the rows meets specified logical criteria (right side of the code), and assigns the correct new value (left side of the code). *Importantly: all values on the right side must be the same class*.  

```{r, eval=F}
linelist <- linelist %>% 
  
  # Create new "age_years" column from "age" column
  mutate(age_years = case_when(
    age_unit == "years"  ~ age,       # if age is given in years, assign original value
    age_unit == "months" ~ age/12,    # if age is given in months, divide by 12
    is.na(age_unit)      ~ age,       # if age UNIT is missing, assume years
    TRUE                 ~ NA_real_)) # any other circumstance, assign missing
```

If you want `NA` on the right side, you may need to specify one of the special `NA` options listed below. If the other right side values are character, consider using "Missing" instead or otherwise use `NA_character_`. If they are all numeric, use `NA_real_`. If they are all dates or logical, you can use `NA`.  

* `NA` - use for dates or logical TRUE/FALSE 
* `NA_character_` - use for characters  
* `NA_real_`  - use for numeric

Again, it is not likely you will encounter these variations *unless* you are using `case_when()` to create a new column. See the [R documentation on NA](https://stat.ethz.ch/R-manual/R-devel/library/base/html/NA.html) for more information. 





### `NULL` {.unnumbered}  

`NULL` is another reserved value in R. It is the logical representation of a statement that is neither true nor false. It is returned by expressions or functions whose values are undefined. Generally do not assign NULL as a value, unless writing functions or perhaps writing a [**shiny** app][Dashboards with Shiny] to return `NULL` in specific scenarios.  

Null-ness can be assessed using `is.null()` and conversion can made with `as.null()`.  

See this [blog post](https://www.r-bloggers.com/2010/04/r-na-vs-null/) on the difference between `NULL` and `NA`.  




### `NaN` {.unnumbered}  

Impossible values are represented by the special value `NaN`. An example of this is when you force R to divide 0 by 0. You can assess this with `is.nan()`. You may also encounter complementary functions including `is.infinite()` and `is.finite()`.  


### `Inf` {.unnumbered}  

`Inf` represents an infinite value, such as when you divide a number by 0.  

As an example of how this might impact your work: let's say you have a vector/column `z` that contains these values: `z <- c(1, 22, NA, Inf, NaN, 5)`

If you want to use `max()` on the column to find the highest value, you can use the `na.rm = TRUE` to remove the `NA` from the calculation, but the `Inf` and `NaN` remain and `Inf` will be returned. To resolve this, you can use brackets `[ ]` and `is.finite()` to subset such that only finite values are used for the calculation: `max(z[is.finite(z)])`.  

```{r, eval=F}
z <- c(1, 22, NA, Inf, NaN, 5)
max(z)                           # returns NA
max(z, na.rm=T)                  # returns Inf
max(z[is.finite(z)])             # returns 22
```


### Examples {.unnumbered}  


R command | Outcome
----------|--------------
`5 / 0` | `Inf`  
`0 / 0` | `NaN`  
`5 / NA` | `NA`  
`5 / Inf | `0`  
`NA - 5` | `NA`  
`Inf / 5` | `Inf`  
`class(NA)` | "logical"  
`class(NaN)` | "numeric"  
`class(Inf)` | "numeric"  
`class(NULL)` | "NULL"  

"NAs introduced by coercion" is a common warning message. This can happen if you attempt to make an illegal conversion like inserting a character value into a vector that is otherwise numeric.  

```{r}
as.numeric(c("10", "20", "thirty", "40"))
```

`NULL` is ignored in a vector.  

```{r}
my_vector <- c(25, NA, 10, NULL)  # define
my_vector                         # print
```


Variance of one number results in `NA`.  

```{r}
var(22)
```


<!-- ======================================================= -->
## Useful functions { }

The following are useful **base** R functions when assessing or handling missing values:  


### `is.na()` and `!is.na()` {.unnumbered}  

Use `is.na()`to identify missing values, or use its opposite (with `!` in front) to identify non-missing values. These both return a logical value (`TRUE` or `FALSE`). Remember that you can `sum()` the resulting vector to count the number `TRUE`, e.g. `sum(is.na(linelist$date_outcome))`.    

```{r}
my_vector <- c(1, 4, 56, NA, 5, NA, 22)
is.na(my_vector)
!is.na(my_vector)
sum(is.na(my_vector))
```


### `na.omit()` {.unnumbered}  

This function, if applied to a data frame, will remove rows with *any* missing values. It is also from **base** R.  
If applied to a vector, it will remove `NA` values from the vector it is applied to. For example:  

```{r}
na.omit(my_vector)
```

### `drop_na()` {.unnumbered}  

This is a **tidyr** function that is useful in a [data cleaning pipeline][Cleaning data and core functions]. If run with the parentheses empty, it removes rows with *any* missing values. If column names are specified in the parentheses, rows with missing values in those columns will be dropped. You can also use "tidyselect" syntax to specify the columns.  

```{r, eval=F}
linelist %>% 
  drop_na(case_id, date_onset, age) # drops rows missing values for any of these columns
```


### `na.rm = TRUE` {.unnumbered}  

When you run a mathematical function such as `max()`, `min()`, `sum()` or `mean()`, if there are any `NA` values present the returned value will be `NA`. This default behavior is intentional, so that you are alerted if any of your data are missing.  

You can avoid this by removing missing values from the calculation. To do this, include the argument `na.rm = TRUE` ("na.rm" stands for "remove `NA`").  


```{r}
my_vector <- c(1, 4, 56, NA, 5, NA, 22)

mean(my_vector)     

mean(my_vector, na.rm = TRUE)
```



<!-- ======================================================= -->
## Assess missingness in a data frame { }

You can use the package **naniar** to assess and visualize missingness in the data frame `linelist`.  

```{r}
# install and/or load package
pacman::p_load(naniar)
```

### Quantifying missingness {.unnumbered}

To find the percent of all values that are missing use `pct_miss()`. Use `n_miss()` to get the number of missing values.  

```{r}
# percent of ALL data frame values that are missing
pct_miss(linelist)
```

The two functions below return the percent of rows with any missing value, or that are entirely complete, respectively. Remember that `NA` means missing, and that ``""` or `" "` will not be counted as missing.  

```{r}
# Percent of rows with any value missing
pct_miss_case(linelist)   # use n_complete() for counts
```

```{r}
# Percent of rows that are complete (no values missing)  
pct_complete_case(linelist) # use n_complete() for counts
```



### Visualizing missingness {.unnumbered}  

The `gg_miss_var()` function will show you the number (or %) of missing values in each column. A few nuances:  

* You can add a column name (not in quote) to the argument `facet = ` to see the plot by groups  
* By default, counts are shown instead of percents, change this with `show_pct = TRUE`  
* You can add axis and title labels as for a normal `ggplot()` with `+ labs(...)`  


```{r}
gg_miss_var(linelist, show_pct = TRUE)
```

Here the data are piped `%>%` into the function. The `facet = ` argument is also used to split the data.  

```{r}
linelist %>% 
  gg_miss_var(show_pct = TRUE, facet = outcome)
```


You can use `vis_miss()` to visualize the data frame as a heatmap, showing whether each value is missing or not. You can also `select()` certain columns from the data frame and provide only those columns to the function.    

```{r}
# Heatplot of missingness across the entire data frame  
vis_miss(linelist)
```


### Explore and visualize missingness relationships {.unnumbered} 

How do you visualize something that is not there??? By default, `ggplot()` removes points with missing values from plots.  

**naniar** offers a solution via `geom_miss_point()`. When creating a scatterplot of two columns, records with one of the values missing and the other value present are shown by setting the missing values to 10% lower than the lowest value in the column, and coloring them distinctly.  

In the scatterplot below, the red dots are records where the value for one column is present but the value for the other column is missing. This allows you to see the distribution of missing values in relation to the non-missing values.  



```{r}
ggplot(
  data = linelist,
  mapping = aes(x = age_years, y = temp)) +     
  geom_miss_point()
```

To assess missingness in the data frame *stratified by another column*, consider `gg_miss_fct()`, which returns a heatmap of percent missingness in the data frame *by a factor/categorical (or date) column*:  

```{r}
gg_miss_fct(linelist, age_cat5)
```


This function can also be used with a date column to see how missingness has changed over time:  

```{r}
gg_miss_fct(linelist, date_onset)
```




### "Shadow" columns {.unnumbered}

Another way to visualize missingness in one column by values in a second column is using the "shadow" that **naniar** can create. `bind_shadow()` creates a binary `NA`/not `NA` column for every existing column, and binds all these new columns to the original dataset with the appendix "_NA". This doubles the number of columns - see below:  


```{r}
shadowed_linelist <- linelist %>% 
  bind_shadow()

names(shadowed_linelist)
```

These "shadow" columns can be used to plot the proportion of values that are missing, by any another column.  

For example, the plot below shows the proportion of records missing `days_onset_hosp` (number of days from symptom onset to hospitalisation), by that record's value in `date_hospitalisation`. Essentially, you are plotting the density of the x-axis column, but stratifying the results (`color = `) by a shadow column of interest. This analysis works best if the x-axis is a numeric or date column.  


```{r, message = F}
ggplot(data = shadowed_linelist,          # data frame with shadow columns
  mapping = aes(x = date_hospitalisation, # numeric or date column
                colour = age_years_NA)) + # shadow column of interest
  geom_density()                          # plots the density curves
```

You can also use these "shadow" columns to stratify a statistical summary, as shown below:

```{r}
linelist %>%
  bind_shadow() %>%                # create the shows cols
  group_by(date_outcome_NA) %>%    # shadow col for stratifying
  summarise(across(
    .cols = age_years,             # variable of interest for calculations
    .fns = list("mean" = mean,     # stats to calculate
                "sd" = sd,
                "var" = var,
                "min" = min,
                "max" = max),  
    na.rm = TRUE))                 # other arguments for the stat calculations
```


An alternative way to plot the proportion of a column's values that are missing over time is shown below. It does *not* involve **naniar**. This example shows percent of weekly observations that are missing).  

1) Aggregate the data into a useful time unit (days, weeks, etc.), summarizing the proportion of observations with `NA` (and any other values of interest)  
2) Plot the proportion missing as a line using `ggplot()`  

Below, we take the linelist, add a new column for week, group the data by week, and then calculate the percent of that week's records where the value is missing. (note: if you want % of 7 days the calculation would be slightly different).  

```{r}
outcome_missing <- linelist %>%
  mutate(week = lubridate::floor_date(date_onset, "week")) %>%   # create new week column
  group_by(week) %>%                                             # group the rows by week
  summarise(                                                     # summarize each week
    n_obs = n(),                                                  # number of records
    
    outcome_missing = sum(is.na(outcome) | outcome == ""),        # number of records missing the value
    outcome_p_miss  = outcome_missing / n_obs,                    # proportion of records missing the value
  
    outcome_dead    = sum(outcome == "Death", na.rm=T),           # number of records as dead
    outcome_p_dead  = outcome_dead / n_obs) %>%                   # proportion of records as dead
  
  tidyr::pivot_longer(-week, names_to = "statistic") %>%         # pivot all columns except week, to long format for ggplot
  filter(stringr::str_detect(statistic, "_p_"))                  # keep only the proportion values
```

Then we plot the proportion missing as a line, by week. The [ggplot basics] page if you are unfamiliar with the **ggplot2** plotting package.  

```{r, message=F, warning=F}
ggplot(data = outcome_missing)+
    geom_line(
      mapping = aes(x = week, y = value, group = statistic, color = statistic),
      size = 2,
      stat = "identity")+
    labs(title = "Weekly outcomes",
         x = "Week",
         y = "Proportion of weekly records") + 
     scale_color_discrete(
       name = "",
       labels = c("Died", "Missing outcome"))+
    scale_y_continuous(breaks = c(seq(0,1,0.1)))+
  theme_minimal()+
  theme(legend.position = "bottom")
```





<!-- ======================================================= -->
## Using data with missing values  


### Filter out rows with missing values {.unnumbered}

To quickly remove rows with missing values, use the **dplyr** function `drop_na()`.  

The original `linelist` has ` nrow(linelist)` rows. The adjusted number of rows is shown below:  

```{r}
linelist %>% 
  drop_na() %>%     # remove rows with ANY missing values
  nrow()
```

You can specify to drop rows with missingness in certain columns:  

```{r}
linelist %>% 
  drop_na(date_onset) %>% # remove rows missing date_onset 
  nrow()
```

You can list columns one after the other, or use ["tidyselect" helper functions](#clean_tidyselect):  

```{r}
linelist %>% 
  drop_na(contains("date")) %>% # remove rows missing values in any "date" column 
  nrow()
```



<!-- ======================================================= -->
### Handling `NA` in `ggplot()` {.unnumbered}

It is often wise to report the number of values excluded from a plot in a caption. Below is an example:  

In `ggplot()`, you can add `labs()` and within it a `caption = `. In the caption, you can use `str_glue()` from **stringr** package to paste values together into a sentence dynamically so they will adjust to the data. An example is below:  

* Note the use of `\n` for a new line.  
* Note that if multiple column would contribute to values not being plotted (e.g. age or sex if those are reflected in the plot), then you must filter on those columns as well to correctly calculate the number not shown.  

```{r, eval=F}
labs(
  title = "",
  y = "",
  x = "",
  caption  = stringr::str_glue(
  "n = {nrow(central_data)} from Central Hospital;
  {nrow(central_data %>% filter(is.na(date_onset)))} cases missing date of onset and not shown."))  
```

Sometimes, it can be easier to save the string as an object in commands prior to the `ggplot()` command, and simply reference the named string object within the `str_glue()`.  


<!-- ======================================================= -->
### `NA` in factors {.unnumbered}

If your column of interest is a factor, use `fct_explicit_na()` from the **forcats** package to convert `NA` values to a character value. See more detail in the [Factors] page. By default, the new value is "(Missing)" but this can be adjusted via the `na_level =` argument.   

```{r}
pacman::p_load(forcats)   # load package

linelist <- linelist %>% 
  mutate(gender = fct_explicit_na(gender, na_level = "Missing"))

levels(linelist$gender)
```



<!-- ======================================================= -->
## Imputation { }


Sometimes, when analyzing your data, it will be important to "fill in the gaps" and impute missing data While you can always simply analyze a dataset after removing all missing values, this can cause problems in many ways. Here are two examples: 

1) By removing all observations with missing values or variables with a large amount of missing data, you might reduce your power or ability to do some types of analysis. For example, as we discovered earlier, only a small fraction of the observations in our linelist dataset have no missing data across all of our variables. If we removed the majority of our dataset we'd be losing a lot of information! And, most of our variables have some amount of missing data--for most analysis it's probably not reasonable to drop every variable that has a lot of missing data either.

2) Depending on why your data is missing, analysis of only non-missing data might lead to biased or misleading results. For example, as we learned earlier we are missing data for some patients about whether they've had some important symptoms like fever or cough. But, as one possibility, maybe that information wasn't recorded for people that just obviously weren't very sick. In that case, if we just removed these observations we'd be excluding some of the healthiest people in our dataset and that might really bias any results.

It's important to think about why your data might be missing in addition to seeing how much is missing. Doing this can help you decide how important it might be to impute missing data, and also which method of imputing missing data might be best in your situation.

### Types of missing data {.unnumbered}

Here are three general types of missing data:

1) **Missing Completely at Random** (MCAR). This means that there is no relationship between the probability of data being missing and any of the other variables in your data. The probability of being missing is the same for all cases This is a rare situation. But, if you have strong reason to believe your data is MCAR analyzing only non-missing data without imputing won't bias your results (although you may lose some power). [TODO: consider discussing statistical tests for MCAR]

2) **Missing at Random** (MAR). This name is actually a bit misleading as MAR means that your data is missing in a systematic, predictable way based on the other information you have. For example, maybe every observation in our dataset with a missing value for fever was actually not recorded because every patient with chills and and aches was just assumed to have a fever so their temperature was never taken. If true, we could easily predict that every missing observation with chills and aches has a fever as well and use this information to impute our missing data. In practice, this is more of a spectrum. Maybe if a patient had both chills and aches they were more likely to have a fever as well if they didn't have their temperature taken, but not always. This is still predictable even if it isn't perfectly predictable. This is a common type of missing data 

3) **Missing not at Random** (MNAR). Sometimes, this is also called **Not Missing at Random** (NMAR). This assumes that the probability of a value being missing is NOT systematic or predictable using the other information we have but also isn't missing randomly. In this situation data is missing for unknown reasons or for reasons you don't have any information about. For example, in our dataset maybe information on age is missing because some very elderly patients either don't know or refuse to say how old they are. In this situation, missing data on age is related to the value itself (and thus isn't random) and isn't predictable based on the other information we have. MNAR is complex and often the best way of dealing with this is to try to collect more data or information about why the data is missing rather than attempt to impute it. 

In general, imputing MCAR data is often fairly simple, while MNAR is very challenging if not impossible. Many of the common data imputation methods assume MAR. 

### Useful packages {.unnumbered}

Some useful packages for imputing missing data are Mmisc, missForest (which uses random forests to impute missing data), and mice (Multivariate Imputation by Chained Equations). For this section we'll just use the mice package, which implements a variety of techniques. The maintainer of the mice package has published an online book about imputing missing data that goes into more detail here (https://stefvanbuuren.name/fimd/).  

Here is the code to load the mice package:

```{r}
pacman::p_load(mice)
```

### Mean Imputation {.unnumbered}

Sometimes if you are doing a simple analysis or you have strong reason to think you can assume MCAR, you can simply set missing numerical values to the mean of that variable. Perhaps we can assume that missing temperature measurements in our dataset were either MCAR or were just normal values. Here is the code to create a new variable that replaces missing temperature values with the mean temperature value in our dataset. However, in many situations replacing data with the mean can lead to bias, so be careful.

```{r}
linelist <- linelist %>%
  mutate(temp_replace_na_with_mean = replace_na(temp, mean(temp, na.rm = T)))
```

You could also do a similar process for replacing categorical data with a specific value. For our dataset, imagine you knew that all observations with a missing value for their outcome (which can be "Death" or "Recover") were actually people that died (note: this is not actually true for this dataset):

```{r}
linelist <- linelist %>%
  mutate(outcome_replace_na_with_death = replace_na(outcome, "Death"))
```

### Regression imputation {.unnumbered}

A somewhat more advanced method is to use some sort of statistical model to predict what a missing value is likely to be and replace it with the predicted value. Here is an example of creating predicted values for all the observations where temperature is missing, but age and fever are not, using simple linear regression using fever status and age in years as predictors. In practice you'd want to use a better model than this sort of simple approach.

```{r, warning=F, message=F}
simple_temperature_model_fit <- lm(temp ~ fever + age_years, data = linelist)

#using our simple temperature model to predict values just for the observations where temp is missing
predictions_for_missing_temps <- predict(simple_temperature_model_fit,
                                        newdata = linelist %>% filter(is.na(temp))) 
```

Or, using the same modeling approach through the mice package to create imputed values for the missing temperature observations:

```{r}
model_dataset <- linelist %>%
  select(temp, fever, age_years)  

temp_imputed <- mice(model_dataset,
                            method = "norm.predict",
                            seed = 1,
                            m = 1,
                            print = F)

temp_imputed_values <- temp_imputed$imp$temp

```


This is the same type of approach by some more advanced methods like using the missForest package to replace missing data with predicted values. In that case, the prediction model is a random forest instead of a linear regression. You can use other types of models to do this as well. However, while this approach works well under MCAR you should be a bit careful if you believe MAR or MNAR more accurately describes your situation. The quality of your imputation will depend on how good your prediction model is and even with a very good model the variability of your imputed data may be underestimated. 

### LOCF and BOCF {.unnumbered}

Last observation carried forward (LOCF) and baseline observation carried forward (BOCF) are imputation methods for time series/longitudinal data. The idea is to take the previous observed value as a replacement for the missing data. When multiple values are missing in succession, the method searches for the last observed value.

The `fill()` function from the **tidyr** package can be used for both LOCF and BOCF imputation (however, other packages such as **HMISC**, **zoo**, and **data.table** also include methods for doing this). To show the `fill()` syntax we'll make up a simple time series dataset containing the number of cases of a disease for each quarter of the years 2000 and 2001. However, the year value for subsequent quarters after Q1 are missing so we'll need to impute them. The `fill()` junction is also demonstrated in the [Pivoting data] page.  

```{r}
#creating our simple dataset
disease <- tibble::tribble(
  ~quarter, ~year, ~cases,
  "Q1",    2000,    66013,
  "Q2",      NA,    69182,
  "Q3",      NA,    53175,
  "Q4",      NA,    21001,
  "Q1",    2001,    46036,
  "Q2",      NA,    58842,
  "Q3",      NA,    44568,
  "Q4",      NA,    50197)

#imputing the missing year values:
disease %>% fill(year)

```

Note: make sure your data are sorted correctly before using the `fill()` function. `fill()`  defaults to filling "down" but you can also impute values in different directions by changing the `.direction` parameter. We can make a similar dataset where the year value is recorded only at the end of the year and missing for earlier quarters: 

```{r}
#creating our slightly different dataset
disease <- tibble::tribble(
  ~quarter, ~year, ~cases,
  "Q1",      NA,    66013,
  "Q2",      NA,    69182,
  "Q3",      NA,    53175,
  "Q4",    2000,    21001,
  "Q1",      NA,    46036,
  "Q2",      NA,    58842,
  "Q3",      NA,    44568,
  "Q4",    2001,    50197)

#imputing the missing year values in the "up" direction:
disease %>% fill(year, .direction = "up")

```
In this example, LOCF and BOCF are clearly the right things to do, but in more complicated situations it may be harder to decide if these methods are appropriate. For example, you may have missing laboratory values for a hospital patient after the first day. Sometimes, this can mean the lab values didn't change...but it could also mean the patient recovered and their values would be very different after the first day! Use these methods with caution.


### Multiple Imputation {.unnumbered}

The online book we mentioned earlier by the author of the mice package (https://stefvanbuuren.name/fimd/) contains a detailed explanation of multiple imputation and why you'd want to use it. But, here is a basic explanation of the method:

When you do multiple imputation, you create multiple datasets with the missing values imputed to plausible data values (depending on your research data you might want to create more or less of these imputed datasets, but the mice package sets the default number to 5). The difference is that rather than a single, specific value each imputed value is drawn from an estimated distribution (so it includes some randomness). As a result, each of these datasets will have slightly different different imputed values (however, the non-missing data will be the same in each of these imputed datasets). You still use some sort of predictive model to do the imputation in each of these new datasets (mice has many options for prediction methods including *Predictive Mean Matching*, *logistic regression*, and *random forest*) but the mice package can take care of many of the modeling details. 

Then, once you have created these new imputed datasets, you can apply then apply whatever statistical model or analysis you were planning to do for each of these new imputed datasets and pool the results of these models together. This works very well to reduce bias in both MCAR and many MAR settings and often results in more accurate standard error estimates.

Here is an example of applying the Multiple Imputation process to predict temperature in our linelist dataset using a age and fever status (our simplified model_dataset from above):  

```{r}
# imputing missing values for all variables in our model_dataset, and creating 10 new imputed datasets
multiple_imputation = mice(
  model_dataset,
  seed = 1,
  m = 10,
  print = FALSE) 

model_fit <- with(multiple_imputation, lm(temp ~ age_years + fever))

base::summary(mice::pool(model_fit))
```

Here we used the mice default method of imputation, which is Predictive Mean Matching. We then used these imputed datasets to separately estimate and then pool results from simple linear regressions on each of these datasets. There are many details we've glossed over and many settings you can adjust during the Multiple Imputation process while using the mice package. For example, you won't always have numerical data and might need to use other imputation methods (you can still use the mice package for many other types of data and methods). But, for a more robust analysis when missing data is a significant concern, Multiple Imputation is good solution that isn't always much more work than doing a complete case analysis. 





<!-- ======================================================= -->
## Resources { }

Vignette on the [naniar package](https://cran.r-project.org/web/packages/naniar/vignettes/getting-started-w-naniar.html)

Gallery of [missing value visualizations](https://cran.r-project.org/web/packages/naniar/vignettes/naniar-visualisation.html)

[Online book](https://stefvanbuuren.name/fimd/) about multiple imputation in R by the maintainer of the **mice** package 
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/missing_data.Rmd-->


# Tỷ lệ chuẩn hóa {#standardization}  

This page will show you two ways to standardize an outcome, such as hospitalizations or mortality, by characteristics such as age and sex. 

* Using **dsr** package 
* Using **PHEindicatormethods** package  

We begin by extensively demonstrating the processes of data preparation/cleaning/joining, as this is common when combining population data from multiple countries, standard population data, deaths, etc.  

## Overview  

There are two main ways to standardize: direct and indirect standardization.
Let's say we would like to the standardize mortality rate by age and sex for country A and country B, and compare the standardized rates between these countries.

* For direct standardization, you will have to know the number of the at-risk population and the number of deaths for each stratum of age and sex, for country A and country B. One stratum in our example could be females between ages 15-44.  
* For indirect standardization, you only need to know the total number of deaths and the age- and sex structure of each country. This option is therefore feasible if age- and sex-specific mortality rates or population numbers are not available. Indirect standardization is furthermore preferable in case of small numbers per stratum, as estimates in direct standardization would be influenced by substantial sampling variation. 

<!-- ======================================================= -->
## Preparation {  }

To show how standardization is done, we will use fictitious population counts and death counts from  country A and country B, by age (in 5 year categories) and sex (female, male). To make the datasets ready for use, we will perform the following preparation steps:  

1. Load packages  
2. Load datasets  
3. Join the population and death data from the two countries
4. Pivot longer so there is one row per age-sex stratum
5. Clean the reference population (world standard population) and join it to the country data  

In your scenario, your data may come in a different format. Perhaps your data are by province, city, or other catchment area. You may have one row for each death and information on age and sex for each (or a significant proportion) of these deaths. In this case, see the pages on [Grouping data], [Pivoting data], and [Descriptive tables] to create a dataset with event and population counts per age-sex stratum.  

We also need a reference population, the standard population. For the purposes of this exercise we will use the `world_standard_population_by_sex`. The World standard population is based on the populations of 46 countries and was developed in 1960. There are many "standard" populations - as one example, the website of [NHS Scotland](https://www.opendata.nhs.scot/dataset/standard-populations) is quite informative on the European Standard Population, World Standard Population and Scotland Standard Population. 

<!-- ======================================================= -->
### Load packages {.unnumbered}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
     rio,                 # import/export data
     here,                # locate files
     tidyverse,           # data management and visualization
     stringr,             # cleaning characters and strings
     frailtypack,         # needed for dsr, for frailty models
     dsr,                 # standardise rates
     PHEindicatormethods) # alternative for rate standardisation
```


<span style="color: orange;">**_CAUTION:_** If you have a newer version of R, the **dsr** package cannot be directly downloaded from CRAN. However, it is still available from the CRAN archive. You can install and use this one. </span>

For non-Mac users:  

```{r, eval=F} 
packageurl <- "https://cran.r-project.org/src/contrib/Archive/dsr/dsr_0.2.2.tar.gz"
install.packages(packageurl, repos=NULL, type="source")
```

```{r, eval=FALSE}
# Other solution that may work
require(devtools)
devtools::install_version("dsr", version="0.2.2", repos="http:/cran.us.r.project.org")
```

For Mac users:  

```{r, eval=FALSE}
require(devtools)
devtools::install_version("dsr", version="0.2.2", repos="https://mac.R-project.org")
```




### Load population data {.unnumbered}  

See the [Download handbook and data] page for instructions on how to download all the example data in the handbook. You can import the Standardisation page data directly into R from our Github repository by running the following `import()` commands:  

```{r, eval=F}
# import demographics for country A directly from Github
A_demo <- import("https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/standardization/country_demographics.csv")

# import deaths for country A directly from Github
A_deaths <- import("https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/standardization/deaths_countryA.csv")

# import demographics for country B directly from Github
B_demo <- import("https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/standardization/country_demographics_2.csv")

# import deaths for country B directly from Github
B_deaths <- import("https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/standardization/deaths_countryB.csv")

# import demographics for country B directly from Github
standard_pop_data <- import("https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/standardization/world_standard_population_by_sex.csv")

```


First we load the demographic data (counts of males and females by 5-year age category) for the two countries that we will be comparing, "Country A" and "Country B".  

```{r, echo=F}
# Country A
A_demo <- rio::import(here::here("data", "standardization", "country_demographics.csv")) %>% 
     mutate(Country = "A") %>% 
     select(Country, everything()) %>% # re-arrange
     mutate(age_cat5 = str_replace_all(age_cat5, "\\+", "")) # remove + symbols
```

```{r, eval=F}
# Country A
A_demo <- import("country_demographics.csv")
```

```{r message=FALSE, echo=F}
DT::datatable(A_demo, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


```{r, echo=F}
# Country B
B_demo <- rio::import(here::here("data", "standardization", "country_demographics_2.csv")) %>% 
     mutate(Country = "B") %>% 
     select(Country, everything()) # re-arrange
```

```{r, eval=F}
# Country B
B_demo <- import("country_demographics_2.csv")
```

```{r message=FALSE, echo=F}
DT::datatable(B_demo, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```





### Load death counts {.unnumbered}  

Conveniently, we also have the counts of deaths during the time period of interest, by age and sex. Each country's counts are in a separate file, shown below.   

```{r, echo=F}
A_males <- c(224, 257, 251, 245, 334, 245, 154, 189, 334, 342, 565, 432, 543, 432, 245, 543, 234, 354) # for males of country A
B_males <- c(34, 37, 51, 145, 434, 120, 100, 143, 307, 354, 463, 639, 706, 232, 275, 543, 234, 274) # for males of country B
A_females <- c(194, 254, 232, 214, 316, 224, 163, 167, 354, 354, 463, 574, 493, 295, 175, 380, 177, 392) # for females of country A
B_females <- c(54, 24, 32, 154, 276, 254, 123, 164, 254, 354, 453, 654, 435, 354, 165, 432, 287, 395) # for females of country B

age_cat5 <- c("0-4", "5-9", "10-14", "15-19", "20-24", "25-29",  "30-34", "35-39", "40-44",
                                                                                "45-49", "50-54", "55-59",
                                                                                "60-64", "65-69", "70-74",
                                                                                "75-79", "80-84", "85")
A_deaths <- data.frame(Country = "A", AgeCat = age_cat5, Male = A_males, Female = A_females)
B_deaths <- data.frame(Country = "B", AgeCat = age_cat5, Male = B_males, Female = B_females)
```

Deaths in Country A
```{r message=FALSE, echo=F}
DT::datatable(A_deaths, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Deaths in Country B

```{r message=FALSE, echo=F}
DT::datatable(B_deaths, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


```{r, echo=F}
rio::export(A_deaths, here::here("data", "standardization", "deaths_countryA.csv"))
rio::export(B_deaths, here::here("data", "standardization", "deaths_countryB.csv"))
```



### Clean populations and deaths {.unnumbered}  


We need to join and transform these data in the following ways:  

* Combine country populations into one dataset and pivot "long" so that each age-sex stratum is one row  
* Combine country death counts into one dataset and pivot "long" so each age-sex stratum is one row  
* Join the deaths to the populations  

First, we combine the country populations datasets, pivot longer, and do minor cleaning. See the page on [Pivoting data] for more detail.  

```{r}
pop_countries <- A_demo %>%  # begin with country A dataset
     bind_rows(B_demo) %>%        # bind rows, because cols are identically named
     pivot_longer(                       # pivot longer
          cols = c(m, f),                   # columns to combine into one
          names_to = "Sex",                 # name for new column containing the category ("m" or "f") 
          values_to = "Population") %>%     # name for new column containing the numeric values pivoted
     mutate(Sex = recode(Sex,            # re-code values for clarity
          "m" = "Male",
          "f" = "Female"))
```

The combined population data now look like this (click through to see countries A and B):  

```{r message=FALSE, echo=F}
DT::datatable(pop_countries, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

And now we perform similar operations on the two deaths datasets.

```{r}
deaths_countries <- A_deaths %>%    # begin with country A deaths dataset
     bind_rows(B_deaths) %>%        # bind rows with B dataset, because cols are identically named
     pivot_longer(                  # pivot longer
          cols = c(Male, Female),        # column to transform into one
          names_to = "Sex",              # name for new column containing the category ("m" or "f") 
          values_to = "Deaths") %>%      # name for new column containing the numeric values pivoted
     rename(age_cat5 = AgeCat)      # rename for clarity
```

The deaths data now look like this, and contain data from both countries: 

```{r message=FALSE, echo=F}
DT::datatable(deaths_countries, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


We now join the deaths and population data based on common columns `Country`, `age_cat5`, and `Sex`. This adds the column `Deaths`.  

```{r}
country_data <- pop_countries %>% 
     left_join(deaths_countries, by = c("Country", "age_cat5", "Sex"))
```

We can now classify `Sex`, `age_cat5`, and `Country` as factors and set the level order using `fct_relevel()` function from the **forcats** package, as described in the page on [Factors]. Note, classifying the factor levels doesn't visibly change the data, but the `arrange()` command does sort it by Country, age category, and sex.  

```{r, warning=F, message=F}
country_data <- country_data %>% 
  mutate(
    Country = fct_relevel(Country, "A", "B"),
      
    Sex = fct_relevel(Sex, "Male", "Female"),
        
    age_cat5 = fct_relevel(
      age_cat5,
      "0-4", "5-9", "10-14", "15-19",
      "20-24", "25-29",  "30-34", "35-39",
      "40-44", "45-49", "50-54", "55-59",
      "60-64", "65-69", "70-74",
      "75-79", "80-84", "85")) %>% 
          
  arrange(Country, age_cat5, Sex)

```

```{r message=FALSE, echo=F}
DT::datatable(country_data, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

<span style="color: orange;">**_CAUTION:_** If you have few deaths per stratum, consider using 10-, or 15-year categories, instead of 5-year categories for age.</span>




### Load reference population {.unnumbered}  

Lastly, for the direct standardisation, we import the reference population (world "standard population" by sex)

```{r, echo=F}
# Reference population
standard_pop_data <- rio::import(here::here("data", "standardization", "world_standard_population_by_sex.csv")) %>% 
     rename(age_cat5 = AgeGroup)
```

```{r, eval=F}
# Reference population
standard_pop_data <- import("world_standard_population_by_sex.csv")
```

```{r message=FALSE, echo=F}
DT::datatable(standard_pop_data, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



<!-- ======================================================= -->
### Clean reference population {.unnumbered}

The age category values in the `country_data` and `standard_pop_data` data frames will need to be aligned.  

Currently, the values of the column `age_cat5` from the `standard_pop_data` data frame contain the word "years" and "plus", while those of the `country_data` data frame do not. We will have to make the age category values match. We use `str_replace_all()` from the **stringr** package, as described in the page on [Characters and strings], to replace these patterns with no space `""`.  

Furthermore, the package **dsr** expects that in the standard population, the column containing counts will be called `"pop"`. So we rename that column accordingly.  

```{r}
# Remove specific string from column values
standard_pop_clean <- standard_pop_data %>%
     mutate(
          age_cat5 = str_replace_all(age_cat5, "years", ""),   # remove "year"
          age_cat5 = str_replace_all(age_cat5, "plus", ""),    # remove "plus"
          age_cat5 = str_replace_all(age_cat5, " ", "")) %>%   # remove " " space
     
     rename(pop = WorldStandardPopulation)   # change col name to "pop", as this is expected by dsr package
```

<span style="color: orange;">**_CAUTION:_** If you try to use `str_replace_all()` to remove a plus *symbol*, it won't work because it is a special symbol. "Escape" the specialnes by putting two back slashes in front, as in `str_replace_call(column, "\\+", "")`. </span>

### Create dataset with standard population {#standard_all .unnumbered}  

Finally, the package **PHEindicatormethods**, detailed [below](#standard_phe), expects the standard populations joined to the country event and population counts. So, we will create a dataset `all_data` for that purpose.  

```{r}
all_data <- left_join(country_data, standard_pop_clean, by=c("age_cat5", "Sex"))
```

This complete dataset looks like this:  

```{r message=FALSE, echo=F}
DT::datatable(all_data, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



<!-- ======================================================= -->
## **dsr** package {  }
 
Below we demonstrate calculating and comparing directly standardized rates using the **dsr** package. The **dsr** package allows you to calculate and compare directly standardized rates (no indirectly standardized rates!).
  
In the data Preparation section, we made separate datasets for country counts and standard population:  

1) the `country_data` object, which is a population table with the number of population and number of deaths per stratum per country  
2) the `standard_pop_clean` object, containing the number of population per stratum for our reference population, the World Standard Population  

We will use these separate datasets for the **dsr** approach.  


<!-- ======================================================= -->
### Standardized rates {.unnumbered}

Below, we calculate rates per country directly standardized for age and sex. We use the `dsr()` function. 

Of note - `dsr()` expects one data frame for the country populations and event counts (deaths), *and a **separate** data frame with the reference population*. It also expects that in this reference population dataset the unit-time column name is "pop" (we assured this in the data Preparation section).  

There are many arguments, as annotated in the code below. Notably, `event = ` is set to the column `Deaths`, and the `fu = ` ("follow-up") is set to the `Population` column. We set the subgroups of comparison as the column `Country` and we standardize based on `age_cat5` and `Sex`. These last two columns are not assigned a particular named argument. See `?dsr` for details. 

```{r, warning=F, message=F}
# Calculate rates per country directly standardized for age and sex
mortality_rate <- dsr::dsr(
     data = country_data,  # specify object containing number of deaths per stratum
     event = Deaths,       # column containing number of deaths per stratum 
     fu = Population,      # column containing number of population per stratum
     subgroup = Country,   # units we would like to compare
     age_cat5,             # other columns - rates will be standardized by these
     Sex,
     refdata = standard_pop_clean, # reference population data frame, with column called pop
     method = "gamma",      # method to calculate 95% CI
     sig = 0.95,            # significance level
     mp = 100000,           # we want rates per 100.000 population
     decimals = 2)          # number of decimals)


# Print output as nice-looking HTML table
knitr::kable(mortality_rate) # show mortality rate before and after direct standardization
```

Above, we see that while country A had a lower crude mortality rate than country B, it has a higher standardized rate after direct age and sex standardization.




<!-- ======================================================= -->
### Standardized rate ratios {.unnumbered}

```{r,warning=F, message=F}
# Calculate RR
mortality_rr <- dsr::dsrr(
     data = country_data, # specify object containing number of deaths per stratum
     event = Deaths,      # column containing number of deaths per stratum 
     fu = Population,     # column containing number of population per stratum
     subgroup = Country,  # units we would like to compare
     age_cat5,
     Sex,                 # characteristics to which we would like to standardize 
     refdata = standard_pop_clean, # reference population, with numbers in column called pop
     refgroup = "B",      # reference for comparison
     estimate = "ratio",  # type of estimate
     sig = 0.95,          # significance level
     mp = 100000,         # we want rates per 100.000 population
     decimals = 2)        # number of decimals

# Print table
knitr::kable(mortality_rr) 
```

The standardized mortality rate is 1.22 times higher in country A compared to country B (95% CI 1.17-1.27).

<!-- ======================================================= -->
### Standardized rate difference {.unnumbered}

```{r, warning=F, message=F}
# Calculate RD
mortality_rd <- dsr::dsrr(
     data = country_data,       # specify object containing number of deaths per stratum
     event = Deaths,            # column containing number of deaths per stratum 
     fu = Population,           # column containing number of population per stratum
     subgroup = Country,        # units we would like to compare
     age_cat5,                  # characteristics to which we would like to standardize
     Sex,                        
     refdata = standard_pop_clean, # reference population, with numbers in column called pop
     refgroup = "B",            # reference for comparison
     estimate = "difference",   # type of estimate
     sig = 0.95,                # significance level
     mp = 100000,               # we want rates per 100.000 population
     decimals = 2)              # number of decimals

# Print table
knitr::kable(mortality_rd) 
```

Country A has 4.24 additional deaths per 100.000 population (95% CI 3.24-5.24) compared to country A.







<!-- ======================================================= -->
## **PHEindicatormethods** package {#standard_phe  }

Another way of calculating standardized rates is with the **PHEindicatormethods** package. This package allows you to calculate directly as well as indirectly standardized rates. We will show both.  

This section will use the `all_data` data frame created at the end of the Preparation section. This data frame includes the country populations, death events, and the world standard reference population. You can view it [here](#standard_all).  



<!-- ======================================================= -->
### Directly standardized rates {.unnumbered}

Below, we first group the data by Country and then pass it to the function `phe_dsr()` to get directly standardized rates per country.

Of note - the reference (standard) population can be provided as a **column within the country-specific data frame** or as a **separate vector**. If provided within the country-specific data frame, you have to set `stdpoptype = "field"`. If provided as a vector, set `stdpoptype = "vector"`. In the latter case, you have to make sure the ordering of rows by strata is similar in both the country-specific data frame and the reference population, as records will be matched by position. In our example below, we provided the reference population as a column within the country-specific data frame.

See the help with `?phr_dsr` or the links in the References section for more information.  

```{r}
# Calculate rates per country directly standardized for age and sex
mortality_ds_rate_phe <- all_data %>%
     group_by(Country) %>%
     PHEindicatormethods::phe_dsr(
          x = Deaths,                 # column with observed number of events
          n = Population,             # column with non-standard pops for each stratum
          stdpop = pop,               # standard populations for each stratum
          stdpoptype = "field")       # either "vector" for a standalone vector or "field" meaning std populations are in the data  

# Print table
knitr::kable(mortality_ds_rate_phe)
```

<!-- ======================================================= -->
### Indirectly standardized rates {#standard_indirect .unnumbered}

For indirect standardization, you need a reference population with the number of deaths and number of population per stratum. In this example, we will be calculating rates for country A *using country B as the reference population*, as the `standard_pop_clean` reference population does not include number of deaths per stratum. 

Below, we first create the reference population from country B. Then, we pass mortality and population data for country A, combine it with the reference population, and pass it to the function `phe_isr()`, to get indirectly standardized rates. Of course, you can do it also vice versa.

Of note - in our example below, the reference population is provided as a separate data frame. In this case, we make sure that `x = `, `n = `, `x_ref = ` and `n_ref = ` vectors are all ordered by the same standardization category (stratum) values as that in our country-specific data frame, as records will be matched by position.

See the help with `?phr_isr` or the links in the References section for more information.  

```{r}
# Create reference population
refpopCountryB <- country_data %>% 
  filter(Country == "B") 

# Calculate rates for country A indirectly standardized by age and sex
mortality_is_rate_phe_A <- country_data %>%
     filter(Country == "A") %>%
     PHEindicatormethods::phe_isr(
          x = Deaths,                 # column with observed number of events
          n = Population,             # column with non-standard pops for each stratum
          x_ref = refpopCountryB$Deaths,  # reference number of deaths for each stratum
          n_ref = refpopCountryB$Population)  # reference population for each stratum

# Print table
knitr::kable(mortality_is_rate_phe_A)
```

<!-- ======================================================= -->
## Resources {  }

If you would like to see another reproducible example using **dsr** please see [this vignette]( https://mran.microsoft.com/snapshot/2020-02-12/web/packages/dsr/vignettes/dsr.html)  

For another example using **PHEindicatormethods**, please go to [this website](https://mran.microsoft.com/snapshot/2018-10-22/web/packages/PHEindicatormethods/vignettes/IntroductiontoPHEindicatormethods.html)  

See the **PHEindicatormethods** [reference pdf file](https://cran.r-project.org/web/packages/PHEindicatormethods/PHEindicatormethods.pdf)  


```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/standardization.Rmd-->


# Đường trung bình động {#moving-average}  

```{r, out.width=c("100%"), echo=F}
knitr::include_graphics(here::here("images", "moving_avg_epicurve.png"))
```


This page will cover two methods to calculate and visualize moving averages:  

1) Calculate with the **slider** package  
2) Calculate *within* a `ggplot()` command with the **tidyquant** package  



<!-- ======================================================= -->
## Preparation {  }

### Load packages {.unnumbered}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages. 


```{r}
pacman::p_load(
  tidyverse,      # for data management and viz
  slider,         # for calculating moving averages
  tidyquant       # for calculating moving averages within ggplot
)
```


### Import data {.unnumbered}

We import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Import data with the `import()` function from the **rio** package (it handles many file types like .xlsx, .csv, .rds - see the [Import and export] page for details).  


```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.xlsx")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


<!-- ======================================================= -->
## Calculate with **slider** {  }

**Use this approach to calculate a moving average in a data frame prior to plotting.**  

The **slider** package provides several "sliding window" functions to compute rolling averages, cumulative sums, rolling regressions, etc. It treats a data frame as a vector of rows, allowing iteration row-wise over a data frame.   

Here are some of the common functions:  

* `slide_dbl()` - iterates through a *numeric* (hence "_dbl") column performing an operation using a sliding window  
  * `slide_sum()` - rolling sum shortcut function for `slide_dbl()`  
  * `slide_mean()` - rolling average shortcut function for `slide_dbl()` 
* `slide_index_dbl()` - applies the rolling window on a numeric column using a separate column to *index* the window progression (useful if rolling by date with some dates absent)  
  * `slide_index_sum()` - rolling sum shortcut function with indexing  
  * `slide_index_mean()` - rolling mean shortcut function with indexing  
  
The **slider** package has many other functions that are covered in the Resources section of this page. We briefly touch upon the most common.  

**Core arguments**  

* `.x`, the first argument by default, is the vector to iterate over and to apply the function to  
* `.i = ` for the "index" versions of the **slider** functions - provide a column to "index" the roll on (see section [below](#roll_index))  
* `.f = `, the second argument by default, either:  
  * A function, written without parentheses, like `mean`, or  
  * A formula, which will be converted into a function. For example `~ .x - mean(.x)` will return the result of the current value minus the mean of the window's value  
  
* For more details see this [reference material](https://davisvaughan.github.io/slider/reference/slide.html)



**Window size**  

Specify the size of the window by using either `.before`, `.after`, or both arguments:   

* `.before = ` - Provide an integer  
* `.after = ` - Provide an integer  
* `.complete = ` - Set this to `TRUE` if you only want calculation performed on complete windows  

For example, to achieve a 7-day window including the current value and the six previous, use `.before = 6`. To achieve a "centered" window provide the same number to both `.before = ` and `.after = `.    

By default, `.complete = ` will be FALSE so if the full window of rows does not exist, the functions will use available rows to perform the calculation. Setting to TRUE restricts so calculations are only performed on complete windows.  

**Expanding window**  

To achieve *cumulative* operations, set the `.before = ` argument to `Inf`. This will conduct the operation on the current value and all coming before.  





### Rolling by date  {#roll_index .unnumbered}  

The most likely use-case of a rolling calculation in applied epidemiology is to examine a metric *over time*. For example, a rolling measurement of case incidence, based on daily case counts. 

If you have clean time series data with values for every date, you may be OK to use `slide_dbl()`, as demonstrated here in the [Time series and outbreak detection](#timeseries_moving) page.  

However, in many applied epidemiology circumstances you may have dates absent from your data, where there are no events recorded. In these cases, it is best to use the "index" versions of the **slider** functions.  


### Indexed data {.unnumbered}  

Below, we show an example using `slide_index_dbl()` on the case linelist. Let us say that our objective is to calculate a rolling 7-day incidence - the sum of cases using a rolling 7-day window. If you are looking for an example of rolling average, see the section below on [grouped rolling](#roll_slider_group).    

To begin, the dataset `daily_counts` is created to reflect the daily case counts from the `linelist`, as calculated with `count()` from **dplyr**.  

```{r}
# make dataset of daily counts
daily_counts <- linelist %>% 
  count(date_hospitalisation, name = "new_cases")
```


Here is the `daily_counts` data frame - there are ` nrow(daily_counts)` rows, each day is represented by one row, but especially early in the epidemic *some days are not present (there were no cases admitted on those days)*.  


```{r, echo=F}
DT::datatable(daily_counts, rownames = FALSE, options = list(pageLength = 6, scrollX=T) )
```



It is crucial to recognize that a standard rolling function (like `slide_dbl()` would use a window of 7 *rows*, not 7 *days*. So, if there are any absent dates, some windows will actually extend more than 7 calendar days!  

A "smart" rolling window can be achieved with `slide_index_dbl()`. The "index" means that the function uses a *separate column* as an "index" for the rolling window. The window is not simply based on the rows of the data frame.  

If the index column is a date, you have the added ability to specify the window extent to `.before = ` and/or `.after = ` in units of **lubridate** `days()` or `months()`. If you do these things, the function will include absent days in the windows as if they were there (as `NA` values).  

Let's show a comparison. Below, we calculate rolling 7-day case incidence with regular and indexed windows.  


```{r}
rolling <- daily_counts %>% 
  mutate(                                # create new columns
    # Using slide_dbl()
    ###################
    reg_7day = slide_dbl(
      new_cases,                         # calculate on new_cases
      .f = ~sum(.x, na.rm = T),          # function is sum() with missing values removed
      .before = 6),                      # window is the ROW and 6 prior ROWS
    
    # Using slide_index_dbl()
    #########################
    indexed_7day = slide_index_dbl(
        new_cases,                       # calculate on new_cases
        .i = date_hospitalisation,       # indexed with date_onset 
        .f = ~sum(.x, na.rm = TRUE),     # function is sum() with missing values removed
        .before = days(6))               # window is the DAY and 6 prior DAYS
    )

```

Observe how in the regular column for the first 7 rows the count steadily increases *despite the rows not being within 7 days of each other*! The adjacent "indexed" column accounts for these absent calendar days, so its 7-day sums are much lower, at least in this period of the epidemic when the cases a farther between.  

```{r, echo=F}
DT::datatable(rolling, rownames = FALSE, options = list(pageLength = 12, scrollX=T) )
```



Now you can plot these data using `ggplot()`:  

```{r}
ggplot(data = rolling)+
  geom_line(mapping = aes(x = date_hospitalisation, y = indexed_7day), size = 1)
```




<!-- ### Rolling by month {.unnumbered}   -->

<!-- If you want to calculate statistics by month (e.g. sum, mean, max) you can do this with **dplyr** as described in the [Grouping data] page. Simply create a "month" column, group the data, and run your calculations with `summarise()`.   -->

<!-- If however, you want to calculate rolling statistics over several months (e.g a 2-month rolling window), you can use the `slide_period()` function from **slider**.   -->

<!-- ```{r} -->
<!-- monthly_mean = function(data){ -->
<!--   summarise(data, mean = mean(new_cases, na.rm=T)) -->
<!-- } -->

<!-- linelist %>%  -->
<!--   count(date_hospitalisation, name = "new_cases") %>%  -->
<!--   mutate( -->
<!--     slide_period_dfr( -->
<!--       new_cases,  -->
<!--       .i = date_hospitalisation, -->
<!--       .period = "month", -->
<!--       .f = monthly_mean))  #~mean(.x, na.rm=T))) -->

<!--       #values_col = new_cases, -->
<!--       #index_col = date_hospitalisation -->
<!--     )) -->



<!-- ``` -->


### Rolling by group {#roll_slider_group .unnumbered}  

If you group your data prior to using a **slider** function, the sliding windows will be applied by group. Be careful to arrange your rows in the desired order *by group*.  

Each time a new group begins, the sliding window will re-start. Therefore, one nuance to be aware of is that if your data are grouped *and* you have set `.complete = TRUE`, you will have empty values at each transition between groups. As the function moved downward through the rows, every transition in the grouping column will re-start the accrual of the minimum window size to allow a calculation.  

See handbook page on [Grouping data] for details on grouping data.

Below, we count linelist cases by date *and* by hospital. Then we arrange the rows in ascending order, first ordering by hospital and then within that by date. Next we set `group_by()`. Then we can create our new rolling average. 


```{r}
grouped_roll <- linelist %>%

  count(hospital, date_hospitalisation, name = "new_cases") %>% 

  arrange(hospital, date_hospitalisation) %>%   # arrange rows by hospital and then by date
  
  group_by(hospital) %>%              # group by hospital 
    
  mutate(                             # rolling average  
    mean_7day_hosp = slide_index_dbl(
      .x = new_cases,                 # the count of cases per hospital-day
      .i = date_hospitalisation,      # index on date of admission
      .f = mean,                      # use mean()                   
      .before = days(6)               # use the day and the 6 days prior
      )
  )

```

Here is the new dataset:  

```{r, echo=F}
DT::datatable(grouped_roll, rownames = FALSE, options = list(pageLength = 12, scrollX=T) )
```


We can now plot the moving averages, displaying the data by group by specifying `~ hospital` to `facet_wrap()` in `ggplot()`. For fun, we plot two geometries - a `geom_col()` showing the daily case counts and a `geom_line()` showing the 7-day moving average.  


```{r, warning=F, message=F}
ggplot(data = grouped_roll)+
  geom_col(                       # plot daly case counts as grey bars
    mapping = aes(
      x = date_hospitalisation,
      y = new_cases),
    fill = "grey",
    width = 1)+
  geom_line(                      # plot rolling average as line colored by hospital
    mapping = aes(
      x = date_hospitalisation,
      y = mean_7day_hosp,
      color = hospital),
    size = 1)+
  facet_wrap(~hospital, ncol = 2)+ # create mini-plots per hospital
  theme_classic()+                 # simplify background  
  theme(legend.position = "none")+ # remove legend
  labs(                            # add plot labels
    title = "7-day rolling average of daily case incidence",
    x = "Date of admission",
    y = "Case incidence")
```


<span style="color: red;">**_DANGER:_** If you get an error saying *"slide() was deprecated in tsibble 0.9.0 and is now defunct. Please use slider::slide() instead."*, it means that the `slide()` function from the **tsibble** package is masking the `slide()` function from **slider** package. Fix this by specifying the package in the command, such as `slider::slide_dbl()`.</span>




<!-- You can group the data prior to using a **slider** function. For example, if you want to calculate the same 7-day rolling sum as above, but by hospital. above rolling mean delay from symptom onset to hospital admission (column `days_onset_hosp`).   -->

<!-- You can group the data by the month of symptom onset using **lubridate**'s `floor_date()` as described in the [Grouping data] page. Then, use `slide_index_dbl()` as before but set your window extent using `months()` (also from **lubridate**).  -->

<!-- f you want a rolling average by *months*, you can use **lubridate** to group the data by month, and then apply `slide_index_dbl()` as below shown for a three-month rolling average:   -->

<!-- ```{r} -->
<!-- months_delay <- linelist %>% -->
<!--   arrange(date_onset) %>%    # drop rows missing date of onset -->
<!--   group_by(hospital) %>%  -->
<!--   #group_by(month_onset = floor_date(date_onset, "month")) %>% # create and group by month of onset  -->
<!--   mutate( -->
<!--     delay_7d = slide_index_dbl( -->
<!--       days_onset_hosp,                  # calculate avg based on value in new_cases column -->
<!--       .i = date_onset,                 # index column is date_onset, so non-present dates are included in 7day window  -->
<!--       .f = ~mean(.x, na.rm = TRUE),     # function is mean() with missing values removed -->
<!--       .before = days(7)), -->

<!--     delay_month = slide_index_dbl( -->
<!--       days_onset_hosp,                  # calculate avg based on value in new_cases column -->
<!--       .i = date_onset,                 # index column is date_onset, so non-present dates are included in 7day window  -->
<!--       .f = ~mean(.x, na.rm = TRUE),     # function is mean() with missing values removed -->
<!--       .before = months(1)))               # window is the month and the prior month -->


<!-- # window is the month and the prior month -->

<!-- ``` -->

<!-- ```{r} -->
<!-- ggplot(data = months_delay, mapping = aes(x = month_onset))+ -->
<!--   geom_line(mapping = aes(y = )) -->

<!-- ``` -->






<!-- ======================================================= -->
## Calculate with **tidyquant** within `ggplot()` {  }

The package **tidyquant** offers another approach to calculating moving averages - this time from *within* a `ggplot()` command itself.  

Below the `linelist` data are counted by date of onset, and this is plotted as a faded line (`alpha` < 1). Overlaid on top is a line created with `geom_ma()` from the package **tidyquant**, with a set window of 7 days (`n = 7`) with specified color and thickness.  

By default `geom_ma()` uses a simple moving average (`ma_fun = "SMA"`), but other types can be specified, such as:  

* "EMA" - exponential moving average (more weight to recent observations)  
* "WMA" - weighted moving average (`wts` are used to weight observations in the moving average)  
* Others can be found in the function documentation  

```{r}
linelist %>% 
  count(date_onset) %>%                 # count cases per day
  drop_na(date_onset) %>%               # remove cases missing onset date
  ggplot(aes(x = date_onset, y = n))+   # start ggplot
    geom_line(                          # plot raw values
      size = 1,
      alpha = 0.2                       # semi-transparent line
      )+             
    tidyquant::geom_ma(                 # plot moving average
      n = 7,           
      size = 1,
      color = "blue")+ 
  theme_minimal()                       # simple background
```

See this [vignette](https://cran.r-project.org/web/packages/tidyquant/vignettes/TQ04-charting-with-tidyquant.html) for more details on the options available within **tidyquant**.  


<!-- ## Rolling regression  -->

<!-- ```{r} -->
<!-- a <- linelist %>% -->
<!--   separate(time_admission, into = c("hour", "minute"), sep = ":") %>%  -->
<!--   count(days_onset_hosp, hour) %>%  -->
<!--   mutate(reg_admit_hour = slide(., ~lm(days_onset_hosp ~ hour), .before = 3, .complete = T)) %>%  -->
<!--   mutate(coeff = reg_admit_hour[[1]]) -->

<!-- ggplot()+ -->
<!--   geom_point(aes(x = hour, y = days_onset_hosp)) -->

<!-- ``` -->

<!-- ```{r} -->
<!-- linelist %>%  -->
<!--   mutate( -->

<!--   ) -->

<!-- ``` -->


<!-- ======================================================= -->
## Resources {  }


See the helpful online [vignette for the **slider** package](https://cran.r-project.org/web/packages/slider/vignettes/slider.html)  

The **slider** [github page](https://github.com/DavisVaughan/slider)

A **slider** [vignette](https://davisvaughan.github.io/slider/articles/slider.html)  

[tidyquant vignette](https://cran.r-project.org/web/packages/tidyquant/vignettes/TQ04-charting-with-tidyquant.html)

If your use case requires that you “skip over” weekends and even holidays, you might like **almanac** package.



```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/moving_average.Rmd-->


# Chuỗi thời gian và phát hiện ổ dịch {#time-series}  

<!-- ======================================================= -->
## Overview {  }

This tab demonstrates the use of several packages for time series analysis. 
It primarily relies on packages from the [**tidyverts**](https://tidyverts.org/) 
family, but will also use the RECON [**trending**](https://github.com/reconhub/trending) 
package to fit models that are more appropriate for infectious disease epidemiology. 

Note in the below example we use a dataset from the **surveillance** package 
on Campylobacter in Germany (see the [data chapter](https://epirhandbook.com/download-handbook-and-data.html), 
of the handbook for details). However, if you wanted to run the same code on a dataset
with multiple countries or other strata, then there is an example code template for this in the 
[r4epis github repo](https://github.com/R4EPI/epitsa). 

Topics covered include:  

1.  Time series data 
2.  Descriptive analysis 
3.  Fitting regressions
4.  Relation of two time series 
5.  Outbreak detection
6.  Interrupted time series


<!-- ======================================================= -->
## Preparation {  }

### Packages {.unnumbered}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics](https://epirhandbook.com/r-basics.html) for more information on R packages.  

```{r load_packages}
pacman::p_load(rio,          # File import
               here,         # File locator
               tidyverse,    # data management + ggplot2 graphics
               tsibble,      # handle time series datasets
               slider,       # for calculating moving averages
               imputeTS,     # for filling in missing values
               feasts,       # for time series decomposition and autocorrelation
               forecast,     # fit sin and cosin terms to data (note: must load after feasts)
               trending,     # fit and assess models 
               tmaptools,    # for getting geocoordinates (lon/lat) based on place names
               ecmwfr,       # for interacting with copernicus sateliate CDS API
               stars,        # for reading in .nc (climate data) files
               units,        # for defining units of measurement (climate data)
               yardstick,    # for looking at model accuracy
               surveillance  # for aberration detection
               )
``` 

### Load data {.unnumbered}

You can download all the data used in this handbook via the instructions in the [Download handbook and data] page.  

The example dataset used in this section is weekly counts of campylobacter cases reported in Germany between 2001 and 2011. <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/time_series/campylobacter_germany.xlsx' class='download-button'>
	You can click here to download<span> this data file (.xlsx).</span></a> 

This dataset is a reduced version of the dataset available in the [**surveillance**](https://cran.r-project.org/web/packages/surveillance/) package. 
(for details load the surveillance package and see `?campyDE`)

Import these data with the `import()` function from the **rio** package (it handles many file types like .xlsx, .csv, .rds - see the [Import and export] page for details).

```{r read_data_hide, echo=F}
# import the counts into R
counts <- rio::import(here::here("data", "time_series", "campylobacter_germany.xlsx"))
```

```{r read_data_show, eval=F}
# import the counts into R
counts <- rio::import("campylobacter_germany.xlsx")
```

The first 10 rows of the counts are displayed below.

```{r inspect_data, message=FALSE, echo=F}
# display the counts data as a table
DT::datatable(head(counts, 10), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

### Clean data {.unnumbered}

The code below makes sure that the date column is in the appropriate format. 
For this tab we will be using the **tsibble** package and so the `yearweek` 
function will be used to create a calendar week variable. There are several other
ways of doing this (see the [Working with dates](https://epirhandbook.com/working-with-dates.html)
page for details), however for time series its best to keep within one framework (**tsibble**). 

```{r clean_data}

## ensure the date column is in the appropriate format
counts$date <- as.Date(counts$date)

## create a calendar week variable 
## fitting ISO definitons of weeks starting on a monday
counts <- counts %>% 
     mutate(epiweek = yearweek(date, week_start = 1))

```

### Download climate data {.unnumbered} 

In the *relation of two time series* section of this page, we will be comparing 
campylobacter case counts to climate data. 

Climate data for anywhere in the world can be downloaded from the EU's Copernicus 
Satellite. These are not exact measurements, but based on a model (similar to 
interpolation), however the benefit is global hourly coverage as well as forecasts.  

You can download each of these climate data files from the [Download handbook and data] page.  

For purposes of demonstration here, we will show R code to use the **ecmwfr** package to pull these data from the Copernicus 
climate data store. You will need to create a free account in order for this to 
work. The package website has a useful [walkthrough](https://github.com/bluegreen-labs/ecmwfr#use-copernicus-climate-data-store-cds)
of how to do this. Below is example code of how to go about doing this, once you 
have the appropriate API keys. You have to replace the X's below with your account
IDs. You will need to download one year of data at a time otherwise the server times-out. 

If you are not sure of the coordinates for a location you want to download data 
for, you can use the **tmaptools** package to pull the coordinates off open street
maps. An alternative option is the [**photon**](https://github.com/rCarto/photon)
package, however this has not been released on to CRAN yet; the nice thing about 
**photon** is that it provides more contextual data for when there are several 
matches for your search.

```{r weather_data, eval = FALSE}

## retrieve location coordinates
coords <- geocode_OSM("Germany", geometry = "point")

## pull together long/lats in format for ERA-5 querying (bounding box) 
## (as just want a single point can repeat coords)
request_coords <- str_glue_data(coords$coords, "{y}/{x}/{y}/{x}")


## Pulling data modelled from copernicus satellite (ERA-5 reanalysis)
## https://cds.climate.copernicus.eu/cdsapp#!/software/app-era5-explorer?tab=app
## https://github.com/bluegreen-labs/ecmwfr

## set up key for weather data 
wf_set_key(user = "XXXXX",
           key = "XXXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXX",
           service = "cds") 

## run for each year of interest (otherwise server times out)
for (i in 2002:2011) {
  
  ## pull together a query 
  ## see here for how to do: https://bluegreen-labs.github.io/ecmwfr/articles/cds_vignette.html#the-request-syntax
  ## change request to a list using addin button above (python to list)
  ## Target is the name of the output file!!
  request <- request <- list(
    product_type = "reanalysis",
    format = "netcdf",
    variable = c("2m_temperature", "total_precipitation"),
    year = c(i),
    month = c("01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12"),
    day = c("01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12",
            "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24",
            "25", "26", "27", "28", "29", "30", "31"),
    time = c("00:00", "01:00", "02:00", "03:00", "04:00", "05:00", "06:00", "07:00",
             "08:00", "09:00", "10:00", "11:00", "12:00", "13:00", "14:00", "15:00",
             "16:00", "17:00", "18:00", "19:00", "20:00", "21:00", "22:00", "23:00"),
    area = request_coords,
    dataset_short_name = "reanalysis-era5-single-levels",
    target = paste0("germany_weather", i, ".nc")
  )
  
  ## download the file and store it in the current working directory
  file <- wf_request(user     = "XXXXX",  # user ID (for authentication)
                     request  = request,  # the request
                     transfer = TRUE,     # download the file
                     path     = here::here("data", "Weather")) ## path to save the data
  }

```

### Load climate data {.unnumbered}

Whether you downloaded the climate data via our handbook, or used the code above, you now should have 10 years of ".nc" climate data files stored in the same folder on your computer.  

Use the code below to import these files into R with the **stars** package. 

```{r read_climate, warning = FALSE, message = FALSE}

## define path to weather folder 
file_paths <- list.files(
  here::here("data", "time_series", "weather"), # replace with your own file path 
  full.names = TRUE)

## only keep those with the current name of interest 
file_paths <- file_paths[str_detect(file_paths, "germany")]

## read in all the files as a stars object 
data <- stars::read_stars(file_paths)
```

Once these files have been imported as the object `data`, we will convert them to a data frame.  

```{r}
## change to a data frame 
temp_data <- as_tibble(data) %>% 
  ## add in variables and correct units
  mutate(
    ## create an calendar week variable 
    epiweek = tsibble::yearweek(time), 
    ## create a date variable (start of calendar week)
    date = as.Date(epiweek),
    ## change temperature from kelvin to celsius
    t2m = set_units(t2m, celsius), 
    ## change precipitation from metres to millimetres 
    tp  = set_units(tp, mm)) %>% 
  ## group by week (keep the date too though)
  group_by(epiweek, date) %>% 
  ## get the average per week
  summarise(t2m = as.numeric(mean(t2m)), 
            tp = as.numeric(mean(tp)))

```




<!-- ======================================================= -->
## Time series data {  }

There are a number of different packages for structuring and handling time series
data. As said, we will focus on the **tidyverts** family of packages and so will
use the **tsibble** package to define our time series object. Having a data set
defined as a time series object means it is much easier to structure our analysis. 

To do this we use the `tsibble()` function and specify the "index", i.e. the variable
specifying the time unit of interest. In our case this is the `epiweek` variable. 

If we had a data set with weekly counts by province, for example, we would also 
be able to specify the grouping variable using the `key = ` argument. 
This would allow us to do analysis for each group. 


```{r ts_object}

## define time series object 
counts <- tsibble(counts, index = epiweek)

```

Looking at `class(counts)` tells you that on top of being a tidy data frame 
("tbl_df", "tbl", "data.frame"), it has the additional properties of a time series
data frame ("tbl_ts"). 

You can take a quick look at your data by using **ggplot2**. We see from the plot that
there is a clear seasonal pattern, and that there are no missings. However, there
seems to be an issue with reporting at the beginning of each year; cases drop 
in the last week of the year and then increase for the first week of the next year. 

```{r basic_plot}

## plot a line graph of cases by week
ggplot(counts, aes(x = epiweek, y = case)) + 
     geom_line()

```


<span style="color: red;">**_DANGER:_** Most datasets aren't as clean as this example. 
You will need to check for duplicates and missings as below. </span>

<!-- ======================================================= -->
### Duplicates {.unnumbered}

**tsibble** does not allow duplicate observations. So each row will need to be
unique, or unique within the group (`key` variable). 
The package has a few functions that help to identify duplicates. These include
`are_duplicated()` which gives you a TRUE/FALSE vector of whether the row is a 
duplicate, and `duplicates()` which gives you a data frame of the duplicated rows. 

See the page on [De-duplication](https://epirhandbook.com/de-duplication.html)
for more details on how to select rows you want. 

```{r duplicates, eval = FALSE}

## get a vector of TRUE/FALSE whether rows are duplicates
are_duplicated(counts, index = epiweek) 

## get a data frame of any duplicated rows 
duplicates(counts, index = epiweek) 

```

<!-- ======================================================= -->
### Missings {.unnumbered}

We saw from our brief inspection above that there are no missings, but we also 
saw there seems to be a problem with reporting delay around new year. 
One way to address this problem could be to set these values to missing and then 
to impute values. The simplest form of time series imputation is to draw
a straight line between the last non-missing and the next non-missing value. 
To do this we will use the **imputeTS** package function `na_interpolation()`. 

See the [Missing data](https://epirhandbook.com/missing-data.html) page for other options for imputation.  

Another alternative would be to calculate a moving average, to try and smooth
over these apparent reporting issues (see next section, and the page on [Moving averages](https://epirhandbook.com/moving-averages.html)). 

```{r missings}

## create a variable with missings instead of weeks with reporting issues
counts <- counts %>% 
     mutate(case_miss = if_else(
          ## if epiweek contains 52, 53, 1 or 2
          str_detect(epiweek, "W51|W52|W53|W01|W02"), 
          ## then set to missing 
          NA_real_, 
          ## otherwise keep the value in case
          case
     ))

## alternatively interpolate missings by linear trend 
## between two nearest adjacent points
counts <- counts %>% 
  mutate(case_int = imputeTS::na_interpolation(case_miss)
         )

## to check what values have been imputed compared to the original
ggplot_na_imputations(counts$case_miss, counts$case_int) + 
  ## make a traditional plot (with black axes and white background)
  theme_classic()

```




<!-- ======================================================= -->
## Descriptive analysis {  }



<!-- ======================================================= -->
### Moving averages {#timeseries_moving .unnumbered}

If data is very noisy (counts jumping up and down) then it can be helpful to 
calculate a moving average. In the example below, for each week we calculate the 
average number of cases from the four previous weeks. This smooths the data, to 
make it more interpretable. In our case this does not really add much, so we will
stick to the interpolated data for further analysis. 
See the [Moving averages](https://epirhandbook.com/moving-averages.html) page for more detail. 

```{r moving_averages}

## create a moving average variable (deals with missings)
counts <- counts %>% 
     ## create the ma_4w variable 
     ## slide over each row of the case variable
     mutate(ma_4wk = slider::slide_dbl(case, 
                               ## for each row calculate the name
                               ~ mean(.x, na.rm = TRUE),
                               ## use the four previous weeks
                               .before = 4))

## make a quick visualisation of the difference 
ggplot(counts, aes(x = epiweek)) + 
     geom_line(aes(y = case)) + 
     geom_line(aes(y = ma_4wk), colour = "red")

```


<!-- ======================================================= -->
### Periodicity {.unnumbered}

Below we define a custom function to create a periodogram. See the [Writing functions] page for information about how to write functions in R.  

First, the function is defined. Its arguments include a dataset with a column `counts`, `start_week = ` which is the first week of the dataset, a number to indicate how many periods per year (e.g. 52, 12), and lastly the output style (see details in the code below).  


```{r periodogram}
## Function arguments
#####################
## x is a dataset
## counts is variable with count data or rates within x 
## start_week is the first week in your dataset
## period is how many units in a year 
## output is whether you want return spectral periodogram or the peak weeks
  ## "periodogram" or "weeks"

# Define function
periodogram <- function(x, 
                        counts, 
                        start_week = c(2002, 1), 
                        period = 52, 
                        output = "weeks") {
  

    ## make sure is not a tsibble, filter to project and only keep columns of interest
    prepare_data <- dplyr::as_tibble(x)
    
    # prepare_data <- prepare_data[prepare_data[[strata]] == j, ]
    prepare_data <- dplyr::select(prepare_data, {{counts}})
    
    ## create an intermediate "zoo" time series to be able to use with spec.pgram
    zoo_cases <- zoo::zooreg(prepare_data, 
                             start = start_week, frequency = period)
    
    ## get a spectral periodogram not using fast fourier transform 
    periodo <- spec.pgram(zoo_cases, fast = FALSE, plot = FALSE)
    
    ## return the peak weeks 
    periodo_weeks <- 1 / periodo$freq[order(-periodo$spec)] * period
    
    if (output == "weeks") {
      periodo_weeks
    } else {
      periodo
    }
    
}

## get spectral periodogram for extracting weeks with the highest frequencies 
## (checking of seasonality) 
periodo <- periodogram(counts, 
                       case_int, 
                       start_week = c(2002, 1),
                       output = "periodogram")

## pull spectrum and frequence in to a dataframe for plotting
periodo <- data.frame(periodo$freq, periodo$spec)

## plot a periodogram showing the most frequently occuring periodicity 
ggplot(data = periodo, 
                aes(x = 1/(periodo.freq/52),  y = log(periodo.spec))) + 
  geom_line() + 
  labs(x = "Period (Weeks)", y = "Log(density)")


## get a vector weeks in ascending order 
peak_weeks <- periodogram(counts, 
                          case_int, 
                          start_week = c(2002, 1), 
                          output = "weeks")

```

<span style="color: black;">**_NOTE:_** It is possible to use the above weeks to add them to sin and cosine terms, however we will use a function to generate these terms (see regression section below) </span>

<!-- ======================================================= -->
### Decomposition {.unnumbered}

Classical decomposition is used to break a time series down several parts, which
when taken together make up for the pattern you see. 
These different parts are:  

* The trend-cycle (the long-term direction of the data)  
* The seasonality (repeating patterns)  
* The random (what is left after removing trend and season)  


```{r decomposition, warning=F, message=F}

## decompose the counts dataset 
counts %>% 
  # using an additive classical decomposition model
  model(classical_decomposition(case_int, type = "additive")) %>% 
  ## extract the important information from the model
  components() %>% 
  ## generate a plot 
  autoplot()

```

<!-- ======================================================= -->
### Autocorrelation {.unnumbered}

Autocorrelation tells you about the relation between the counts of each week 
and the weeks before it (called lags).  

Using the `ACF()` function, we can produce a plot which shows us a number of lines 
for the relation at different lags. Where the lag is 0 (x = 0), this line would 
always be 1 as it shows the relation between an observation and itself (not shown here). 
The first line shown here (x = 1) shows the relation between each observation 
and the observation before it (lag of 1), the second shows the relation between 
each observation and the observation before last (lag of 2) and so on until lag of
52 which shows the relation between each observation and the observation from 1 
year (52 weeks before).  

Using the `PACF()` function (for partial autocorrelation) shows the same type of relation 
but adjusted for all other weeks between. This is less informative for determining
periodicity. 

```{r autocorrelation}

## using the counts dataset
counts %>% 
  ## calculate autocorrelation using a full years worth of lags
  ACF(case_int, lag_max = 52) %>% 
  ## show a plot
  autoplot()

## using the counts data set 
counts %>% 
  ## calculate the partial autocorrelation using a full years worth of lags
  PACF(case_int, lag_max = 52) %>% 
  ## show a plot
  autoplot()

```

You can formally test the null hypothesis of independence in a time series (i.e. 
that it is not autocorrelated) using the Ljung-Box test (in the **stats** package). 
A significant p-value suggests that there is autocorrelation in the data.

```{r ljung_box}

## test for independance 
Box.test(counts$case_int, type = "Ljung-Box")

```


<!-- ======================================================= -->
## Fitting regressions {  }

It is possible to fit a large number of different regressions to a time series, 
however, here we will demonstrate how to fit a negative binomial regression - as 
this is often the most appropriate for counts data in infectious diseases. 

<!-- ======================================================= -->
### Fourier terms {.unnumbered}

Fourier terms are the equivalent of sin and cosin curves. The difference is that 
these are fit based on finding the most appropriate combination of curves to explain
your data.  

If only fitting one fourier term, this would be the equivalent of fitting a sin 
and a cosin for your most frequently occurring lag seen in your periodogram (in our 
case 52 weeks). We use the `fourier()` function from the **forecast** package.  

In the below code we assign using the `$`, as `fourier()` returns two columns (one 
for sin one for cosin) and so these are added to the dataset as a list, called 
"fourier" - but this list can then be used as a normal variable in regression. 

```{r fourier}

## add in fourier terms using the epiweek and case_int variabless
counts$fourier <- select(counts, epiweek, case_int) %>% 
  fourier(K = 1)
```

<!-- ======================================================= -->
### Negative binomial {.unnumbered}

It is possible to fit regressions using base **stats** or **MASS**
functions (e.g. `lm()`, `glm()` and `glm.nb()`). However we will be using those from 
the **trending** package, as this allows for calculating appropriate confidence
and prediction intervals (which are otherwise not available). 
The syntax is the same, and you specify an outcome variable then a tilde (~) 
and then add your various exposure variables of interest separated by a plus (+). 

The other difference is that we first define the model and then `fit()` it to the 
data. This is useful because it allows for comparing multiple different models 
with the same syntax. 

<span style="color: darkgreen;">**_TIP:_** If you wanted to use rates, rather than 
counts you could include the population variable as a logarithmic offset term, by adding 
`offset(log(population)`. You would then need to set population to be 1, before 
using `predict()` in order to produce a rate. </span>

<span style="color: darkgreen;">**_TIP:_** For fitting more complex models such 
as ARIMA or prophet, see the [**fable**](https://fable.tidyverts.org/index.html) package.</span>

```{r nb_reg, warning = FALSE}

## define the model you want to fit (negative binomial) 
model <- glm_nb_model(
  ## set number of cases as outcome of interest
  case_int ~
    ## use epiweek to account for the trend
    epiweek +
    ## use the fourier terms to account for seasonality
    fourier)

## fit your model using the counts dataset
fitted_model <- trending::fit(model, counts)

## calculate confidence intervals and prediction intervals 
observed <- predict(fitted_model, simulate_pi = FALSE)

## plot your regression 
ggplot(data = observed, aes(x = epiweek)) + 
  ## add in a line for the model estimate
  geom_line(aes(y = estimate),
            col = "Red") + 
  ## add in a band for the prediction intervals 
  geom_ribbon(aes(ymin = lower_pi, 
                  ymax = upper_pi), 
              alpha = 0.25) + 
  ## add in a line for your observed case counts
  geom_line(aes(y = case_int), 
            col = "black") + 
  ## make a traditional plot (with black axes and white background)
  theme_classic()


```

<!-- ======================================================= -->
### Residuals {.unnumbered}

To see how well our model fits the observed data we need to look at the residuals. 
The residuals are the difference between the observed counts and the counts 
estimated from the model. We could calculate this simply by using `case_int - estimate`, 
but the `residuals()` function extracts this directly from the regression for us.

What we see from the below, is that we are not explaining all of the variation 
that we could with the model. It might be that we should fit more fourier terms, 
and address the amplitude. However for this example we will leave it as is. 
The plots show that our model does worse in the peaks and troughs (when counts are
at their highest and lowest) and that it might be more likely to underestimate 
the observed counts. 

```{r, warning=F, message=F}

## calculate the residuals 
observed <- observed %>% 
  mutate(resid = residuals(fitted_model$fitted_model, type = "response"))

## are the residuals fairly constant over time (if not: outbreaks? change in practice?)
observed %>%
  ggplot(aes(x = epiweek, y = resid)) +
  geom_line() +
  geom_point() + 
  labs(x = "epiweek", y = "Residuals")

## is there autocorelation in the residuals (is there a pattern to the error?)  
observed %>% 
  as_tsibble(index = epiweek) %>% 
  ACF(resid, lag_max = 52) %>% 
  autoplot()

## are residuals normally distributed (are under or over estimating?)  
observed %>%
  ggplot(aes(x = resid)) +
  geom_histogram(binwidth = 100) +
  geom_rug() +
  labs(y = "count") 
  
## compare observed counts to their residuals 
  ## should also be no pattern 
observed %>%
  ggplot(aes(x = estimate, y = resid)) +
  geom_point() +
  labs(x = "Fitted", y = "Residuals")

## formally test autocorrelation of the residuals
## H0 is that residuals are from a white-noise series (i.e. random)
## test for independence 
## if p value significant then non-random
Box.test(observed$resid, type = "Ljung-Box")

```

<!-- ======================================================= -->
## Relation of two time series {  }

Here we look at using weather data (specifically the temperature) to explain 
campylobacter case counts. 

<!-- ======================================================= -->
### Merging datasets {.unnumbered}

We can join our datasets using the week variable. For more on merging see the 
handbook section on [joining](https://epirhandbook.com/joining-data.html).

```{r join}

## left join so that we only have the rows already existing in counts
## drop the date variable from temp_data (otherwise is duplicated)
counts <- left_join(counts, 
                    select(temp_data, -date),
                    by = "epiweek")

```

<!-- ======================================================= -->
### Descriptive analysis {.unnumbered}

First plot your data to see if there is any obvious relation. 
The plot below shows that there is a clear relation in the seasonality of the two
variables, and that temperature might peak a few weeks before the case number.
For more on pivoting data, see the handbook section on [pivoting data](https://epirhandbook.com/pivoting-data.html). 

```{r basic_plot_bivar}

counts %>% 
  ## keep the variables we are interested 
  select(epiweek, case_int, t2m) %>% 
  ## change your data in to long format
  pivot_longer(
    ## use epiweek as your key
    !epiweek,
    ## move column names to the new "measure" column
    names_to = "measure", 
    ## move cell values to the new "values" column
    values_to = "value") %>% 
  ## create a plot with the dataset above
  ## plot epiweek on the x axis and values (counts/celsius) on the y 
  ggplot(aes(x = epiweek, y = value)) + 
    ## create a separate plot for temperate and case counts 
    ## let them set their own y-axes
    facet_grid(measure ~ ., scales = "free_y") +
    ## plot both as a line
    geom_line()

```

<!-- ======================================================= -->
### Lags and cross-correlation {.unnumbered}

To formally test which weeks are most highly related between cases and temperature. 
We can use the cross-correlation function (`CCF()`) from the **feasts** package. 
You could also visualise (rather than using `arrange`) using the `autoplot()` function. 

```{r cross_correlation, warning=FALSE}

counts %>% 
  ## calculate cross-correlation between interpolated counts and temperature
  CCF(case_int, t2m,
      ## set the maximum lag to be 52 weeks
      lag_max = 52, 
      ## return the correlation coefficient 
      type = "correlation") %>% 
  ## arange in decending order of the correlation coefficient 
  ## show the most associated lags
  arrange(-ccf) %>% 
  ## only show the top ten 
  slice_head(n = 10)

```

We see from this that a lag of 4 weeks is most highly correlated, 
so we make a lagged temperature variable to include in our regression. 

<span style="color: red;">**_DANGER:_** Note that the first four weeks of our data
in the lagged temperature variable are missing (`NA`) - as there are not four 
weeks prior to get data from. In order to use this dataset with the **trending** 
`predict()` function, we need to use the the `simulate_pi = FALSE` argument within
`predict()` further down. If we did want to use the simulate option, then 
we have to drop these missings and store as a new data set by adding `drop_na(t2m_lag4)` 
to the code chunk below.</span>  
 

```{r lag_tempvar}

counts <- counts %>% 
  ## create a new variable for temperature lagged by four weeks
  mutate(t2m_lag4 = lag(t2m, n = 4))

```


<!-- ======================================================= -->
### Negative binomial with two variables {.unnumbered}

We fit a negative binomial regression as done previously. This time we add the 
temperature variable lagged by four weeks. 

<span style="color: orange;">**_CAUTION:_** Note the use of `simulate_pi = FALSE`
within the `predict()` argument. This is because the default behaviour of **trending** 
is to use the **ciTools** package to estimate a prediction interval. This does not 
work if there are `NA` counts, and also produces more granular intervals. 
See `?trending::predict.trending_model_fit` for details. </span>  

```{r nb_reg_bivar, warning = FALSE}

## define the model you want to fit (negative binomial) 
model <- glm_nb_model(
  ## set number of cases as outcome of interest
  case_int ~
    ## use epiweek to account for the trend
    epiweek +
    ## use the fourier terms to account for seasonality
    fourier + 
    ## use the temperature lagged by four weeks 
    t2m_lag4
    )

## fit your model using the counts dataset
fitted_model <- trending::fit(model, counts)

## calculate confidence intervals and prediction intervals 
observed <- predict(fitted_model, simulate_pi = FALSE)

```


To investigate the individual terms, we can pull the original negative binomial
regression out of the **trending** format using `get_model()` and pass this to the
**broom** package `tidy()` function to retrieve exponentiated estimates and associated
confidence intervals.  

What this shows us is that lagged temperature, after controlling for trend and seasonality, 
is similar to the case counts (estimate ~ 1) and significantly associated. 
This suggests that it might be a good variable for use in predicting future case
numbers (as climate forecasts are readily available). 

```{r results_nb_reg_bivar}

fitted_model %>% 
  ## extract original negative binomial regression
  get_model() %>% 
  ## get a tidy dataframe of results
  tidy(exponentiate = TRUE, 
       conf.int = TRUE)
```

A quick visual inspection of the model shows that it might do a better job of 
estimating the observed case counts. 

```{r plot_nb_reg_bivar, warning=F, message=F}

## plot your regression 
ggplot(data = observed, aes(x = epiweek)) + 
  ## add in a line for the model estimate
  geom_line(aes(y = estimate),
            col = "Red") + 
  ## add in a band for the prediction intervals 
  geom_ribbon(aes(ymin = lower_pi, 
                  ymax = upper_pi), 
              alpha = 0.25) + 
  ## add in a line for your observed case counts
  geom_line(aes(y = case_int), 
            col = "black") + 
  ## make a traditional plot (with black axes and white background)
  theme_classic()


```


#### Residuals {.unnumbered}

We investigate the residuals again to see how well our model fits the observed data. 
The results and interpretation here are similar to those of the previous regression, 
so it may be more feasible to stick with the simpler model without temperature. 

```{r}

## calculate the residuals 
observed <- observed %>% 
  mutate(resid = case_int - estimate)

## are the residuals fairly constant over time (if not: outbreaks? change in practice?)
observed %>%
  ggplot(aes(x = epiweek, y = resid)) +
  geom_line() +
  geom_point() + 
  labs(x = "epiweek", y = "Residuals")

## is there autocorelation in the residuals (is there a pattern to the error?)  
observed %>% 
  as_tsibble(index = epiweek) %>% 
  ACF(resid, lag_max = 52) %>% 
  autoplot()

## are residuals normally distributed (are under or over estimating?)  
observed %>%
  ggplot(aes(x = resid)) +
  geom_histogram(binwidth = 100) +
  geom_rug() +
  labs(y = "count") 
  
## compare observed counts to their residuals 
  ## should also be no pattern 
observed %>%
  ggplot(aes(x = estimate, y = resid)) +
  geom_point() +
  labs(x = "Fitted", y = "Residuals")

## formally test autocorrelation of the residuals
## H0 is that residuals are from a white-noise series (i.e. random)
## test for independence 
## if p value significant then non-random
Box.test(observed$resid, type = "Ljung-Box")

```

<!-- ======================================================= -->
## Outbreak detection {  }

We will demonstrate two (similar) methods of detecting outbreaks here. 
The first builds on the sections above. 
We use the **trending** package to fit regressions to previous years, and then
predict what we expect to see in the following year. If observed counts are above
what we expect, then it could suggest there is an outbreak. 
The second method is based on similar principles but uses the **surveillance** package,
which has a number of different algorithms for aberration detection.

<span style="color: orange;">**_CAUTION:_** Normally, you are interested in the current year (where you only know counts up to the present week). So in this example we are pretending to be in week 39 of 2011.</span>

<!-- ======================================================= -->
### **trending** package {.unnumbered}

For this method we define a baseline (which should usually be about 5 years of data). 
We fit a regression to the baseline data, and then use that to predict the estimates
for the next year. 

<!-- ======================================================= -->
#### Cut-off date { -}

It is easier to define your dates in one place and then use these throughout the
rest of your code.  

Here we define a start date (when our observations started) and a cut-off date 
(the end of our baseline period - and when the period we want to predict for starts). 
~We also define how many weeks are in our year of interest (the one we are going to
be predicting)~.
We also define how many weeks are between our baseline cut-off and the end date 
that we are interested in predicting for. 


<span style="color: black;">**_NOTE:_** In this example we pretend to currently be at the end of September 2011 ("2011 W39").</span>  

```{r cut_off}

## define start date (when observations began)
start_date <- min(counts$epiweek)

## define a cut-off week (end of baseline, start of prediction period)
cut_off <- yearweek("2010-12-31")

## define the last date interested in (i.e. end of prediction)
end_date <- yearweek("2011-12-31")

## find how many weeks in period (year) of interest
num_weeks <- as.numeric(end_date - cut_off)

```


<!-- ======================================================= -->
#### Add rows {.unnumbered}

To be able to forecast in a tidyverse format, we need to have the right number 
of rows in our dataset, i.e. one row for each week up to the `end_date`defined above. 
The code below allows you to add these rows for by a grouping variable - for example
if we had multiple countries in one dataset, we could group by country and then 
add rows appropriately for each. 
The `group_by_key()` function from **tsibble** allows us to do this grouping 
and then pass the grouped data to **dplyr** functions, `group_modify()` and 
`add_row()`. Then we specify the sequence of weeks between one after the maximum week 
currently available in the data and the end week. 

```{r add_rows}

## add in missing weeks till end of year 
counts <- counts %>%
  ## group by the region
  group_by_key() %>%
  ## for each group add rows from the highest epiweek to the end of year
  group_modify(~add_row(.,
                        epiweek = seq(max(.$epiweek) + 1, 
                                      end_date,
                                      by = 1)))

```



<!-- ======================================================= -->
#### Fourier terms {.unnumbered}

We need to redefine our fourier terms - as we want to fit them to the baseline 
date only and then predict (extrapolate) those terms for the next year. 
To do this we need to combine two output lists from the `fourier()` function together; 
the first one is for the baseline data, and the second one predicts for the 
year of interest (by defining the `h` argument).  

*N.b.* to bind rows we have to use `rbind()` (rather than tidyverse `bind_rows`) as
the fourier columns are a list (so not named individually). 

```{r fourier_terms_pred}


## define fourier terms (sincos) 
counts <- counts %>% 
  mutate(
    ## combine fourier terms for weeks prior to  and after 2010 cut-off date
    ## (nb. 2011 fourier terms are predicted)
    fourier = rbind(
      ## get fourier terms for previous years
      fourier(
        ## only keep the rows before 2011
        filter(counts, 
               epiweek <= cut_off), 
        ## include one set of sin cos terms 
        K = 1
        ), 
      ## predict the fourier terms for 2011 (using baseline data)
      fourier(
        ## only keep the rows before 2011
        filter(counts, 
               epiweek <= cut_off),
        ## include one set of sin cos terms 
        K = 1, 
        ## predict 52 weeks ahead
        h = num_weeks
        )
      )
    )

```

<!-- ======================================================= -->
#### Split data and fit regression {.unnumbered}

We now have to split our dataset in to the baseline period and the prediction 
period. This is done using the **dplyr** `group_split()` function after `group_by()`, 
and will create a list with two data frames, one for before your cut-off and one 
for after.  

We then use the **purrr** package `pluck()` function to pull the datasets out of the
list (equivalent of using square brackets, e.g. `dat[[1]]`), and can then fit 
our model to the baseline data, and then use the `predict()` function for our data
of interest after the cut-off.  

See the page on [Iteration, loops, and lists] to learn more about **purrr**.  

<span style="color: orange;">**_CAUTION:_** Note the use of `simulate_pi = FALSE`
within the `predict()` argument. This is because the default behaviour of **trending** 
is to use the **ciTools** package to estimate a prediction interval. This does not 
work if there are `NA` counts, and also produces more granular intervals. 
See `?trending::predict.trending_model_fit` for details. </span>  

```{r forecast_regression, warning = FALSE}
# split data for fitting and prediction
dat <- counts %>% 
  group_by(epiweek <= cut_off) %>%
  group_split()

## define the model you want to fit (negative binomial) 
model <- glm_nb_model(
  ## set number of cases as outcome of interest
  case_int ~
    ## use epiweek to account for the trend
    epiweek +
    ## use the furier terms to account for seasonality
    fourier
)

# define which data to use for fitting and which for predicting
fitting_data <- pluck(dat, 2)
pred_data <- pluck(dat, 1) %>% 
  select(case_int, epiweek, fourier)

# fit model 
fitted_model <- trending::fit(model, fitting_data)

# get confint and estimates for fitted data
observed <- fitted_model %>% 
  predict(simulate_pi = FALSE)

# forecast with data want to predict with 
forecasts <- fitted_model %>% 
  predict(pred_data, simulate_pi = FALSE)

## combine baseline and predicted datasets
observed <- bind_rows(observed, forecasts)

```

As previously, we can visualise our model with **ggplot**. We highlight alerts with
red dots for observed counts above the 95% prediction interval. 
This time we also add a vertical line to label when the forecast starts. 

```{r forecast_plot}

## plot your regression 
ggplot(data = observed, aes(x = epiweek)) + 
  ## add in a line for the model estimate
  geom_line(aes(y = estimate),
            col = "grey") + 
  ## add in a band for the prediction intervals 
  geom_ribbon(aes(ymin = lower_pi, 
                  ymax = upper_pi), 
              alpha = 0.25) + 
  ## add in a line for your observed case counts
  geom_line(aes(y = case_int), 
            col = "black") + 
  ## plot in points for the observed counts above expected
  geom_point(
    data = filter(observed, case_int > upper_pi), 
    aes(y = case_int), 
    colour = "red", 
    size = 2) + 
  ## add vertical line and label to show where forecasting started
  geom_vline(
           xintercept = as.Date(cut_off), 
           linetype = "dashed") + 
  annotate(geom = "text", 
           label = "Forecast", 
           x = cut_off, 
           y = max(observed$upper_pi) - 250, 
           angle = 90, 
           vjust = 1
           ) + 
  ## make a traditional plot (with black axes and white background)
  theme_classic()
```



<!-- ======================================================= -->
#### Prediction validation {.unnumbered}

Beyond inspecting residuals, it is important to investigate how good your model is
at predicting cases in the future. This gives you an idea of how reliable your 
threshold alerts are.  

The traditional way of validating is to see how well you can predict the latest 
year before the present one (because you don't yet know the counts for the "current year"). 
For example in our data set we would use the data from 2002 to 2009 to predict 2010, 
and then see how accurate those predictions are. Then refit the model to include
2010 data and use that to predict 2011 counts.  

As can be seen in the figure below by *Hyndman et al* in ["Forecasting principles 
and practice"](https://otexts.com/fpp3/). 

![](`r "https://otexts.com/fpp3/fpp_files/figure-html/traintest-1.png"`)
*figure reproduced with permission from the authors* 

The downside of this is that you are not using all the data available to you, and 
it is not the final model that you are using for prediction. 

An alternative is to use a method called cross-validation. In this scenario you 
roll over all of the data available to fit multiple models to predict one year ahead. 
You use more and more data in each model, as seen in the figure below from the 
same [*Hyndman et al* text]((https://otexts.com/fpp3/). 
For example, the first model uses 2002 to predict 2003, the second uses 2002 and 
2003 to predict 2004, and so on. 
![](`r "https://otexts.com/fpp2/fpp_files/figure-html/cv1-1.png"`)
*figure reproduced with permission from the authors*

In the below we use **purrr** package `map()` function to loop over each dataset. 
We then put estimates in one data set and merge with the original case counts, 
to use the **yardstick** package to compute measures of accuracy. 
We compute four measures including: Root mean squared error (RMSE), Mean absolute error	
(MAE), Mean absolute scaled error (MASE), Mean absolute percent error (MAPE).

<span style="color: orange;">**_CAUTION:_** Note the use of `simulate_pi = FALSE`
within the `predict()` argument. This is because the default behaviour of **trending** 
is to use the **ciTools** package to estimate a prediction interval. This does not 
work if there are `NA` counts, and also produces more granular intervals. 
See `?trending::predict.trending_model_fit` for details. </span>  

```{r cross_validation, warning = FALSE}

## Cross validation: predicting week(s) ahead based on sliding window

## expand your data by rolling over in 52 week windows (before + after) 
## to predict 52 week ahead
## (creates longer and longer chains of observations - keeps older data)

## define window want to roll over
roll_window <- 52

## define weeks ahead want to predict 
weeks_ahead <- 52

## create a data set of repeating, increasingly long data
## label each data set with a unique id
## only use cases before year of interest (i.e. 2011)
case_roll <- counts %>% 
  filter(epiweek < cut_off) %>% 
  ## only keep the week and case counts variables
  select(epiweek, case_int) %>% 
    ## drop the last x observations 
    ## depending on how many weeks ahead forecasting 
    ## (otherwise will be an actual forecast to "unknown")
    slice(1:(n() - weeks_ahead)) %>%
    as_tsibble(index = epiweek) %>% 
    ## roll over each week in x after windows to create grouping ID 
    ## depending on what rolling window specify
    stretch_tsibble(.init = roll_window, .step = 1) %>% 
  ## drop the first couple - as have no "before" cases
  filter(.id > roll_window)


## for each of the unique data sets run the code below
forecasts <- purrr::map(unique(case_roll$.id), 
                        function(i) {
  
  ## only keep the current fold being fit 
  mini_data <- filter(case_roll, .id == i) %>% 
    as_tibble()
  
  ## create an empty data set for forecasting on 
  forecast_data <- tibble(
    epiweek = seq(max(mini_data$epiweek) + 1,
                  max(mini_data$epiweek) + weeks_ahead,
                  by = 1),
    case_int = rep.int(NA, weeks_ahead),
    .id = rep.int(i, weeks_ahead)
  )
  
  ## add the forecast data to the original 
  mini_data <- bind_rows(mini_data, forecast_data)
  
  ## define the cut off based on latest non missing count data 
  cv_cut_off <- mini_data %>% 
    ## only keep non-missing rows
    drop_na(case_int) %>% 
    ## get the latest week
    summarise(max(epiweek)) %>% 
    ## extract so is not in a dataframe
    pull()
  
  ## make mini_data back in to a tsibble
  mini_data <- tsibble(mini_data, index = epiweek)
  
  ## define fourier terms (sincos) 
  mini_data <- mini_data %>% 
    mutate(
    ## combine fourier terms for weeks prior to  and after cut-off date
    fourier = rbind(
      ## get fourier terms for previous years
      forecast::fourier(
        ## only keep the rows before cut-off
        filter(mini_data, 
               epiweek <= cv_cut_off), 
        ## include one set of sin cos terms 
        K = 1
        ), 
      ## predict the fourier terms for following year (using baseline data)
      fourier(
        ## only keep the rows before cut-off
        filter(mini_data, 
               epiweek <= cv_cut_off),
        ## include one set of sin cos terms 
        K = 1, 
        ## predict 52 weeks ahead
        h = weeks_ahead
        )
      )
    )
  
  
  # split data for fitting and prediction
  dat <- mini_data %>% 
    group_by(epiweek <= cv_cut_off) %>%
    group_split()

  ## define the model you want to fit (negative binomial) 
  model <- glm_nb_model(
    ## set number of cases as outcome of interest
    case_int ~
      ## use epiweek to account for the trend
      epiweek +
      ## use the furier terms to account for seasonality
      fourier
  )

  # define which data to use for fitting and which for predicting
  fitting_data <- pluck(dat, 2)
  pred_data <- pluck(dat, 1)
  
  # fit model 
  fitted_model <- trending::fit(model, fitting_data)
  
  # forecast with data want to predict with 
  forecasts <- fitted_model %>% 
    predict(pred_data, simulate_pi = FALSE) %>% 
    ## only keep the week and the forecast estimate
    select(epiweek, estimate)
    
  }
  )

## make the list in to a data frame with all the forecasts
forecasts <- bind_rows(forecasts)

## join the forecasts with the observed
forecasts <- left_join(forecasts, 
                       select(counts, epiweek, case_int),
                       by = "epiweek")

## using {yardstick} compute metrics
  ## RMSE: Root mean squared error
  ## MAE:  Mean absolute error	
  ## MASE: Mean absolute scaled error
  ## MAPE: Mean absolute percent error
model_metrics <- bind_rows(
  ## in your forcasted dataset compare the observed to the predicted
  rmse(forecasts, case_int, estimate), 
  mae( forecasts, case_int, estimate),
  mase(forecasts, case_int, estimate),
  mape(forecasts, case_int, estimate),
  ) %>% 
  ## only keep the metric type and its output
  select(Metric  = .metric, 
         Measure = .estimate) %>% 
  ## make in to wide format so can bind rows after
  pivot_wider(names_from = Metric, values_from = Measure)

## return model metrics 
model_metrics

```


<!-- ======================================================= -->
### **surveillance** package {.unnumbered}

In this section we use the **surveillance** package to create alert thresholds 
based on outbreak detection algorithms. There are several different methods 
available in the package, however we will focus on two options here. 
For details, see these papers on the [application](https://cran.r-project.org/web/packages/surveillance/vignettes/monitoringCounts.pdf)
and [theory](https://cran.r-project.org/web/packages/surveillance/vignettes/glrnb.pdf)
of the alogirthms used. 

The first option uses the improved Farrington method. This fits a negative 
binomial glm (including trend) and down-weights past outbreaks (outliers) to 
create a threshold level. 

The second option use the glrnb method. This also fits a negative binomial glm 
but includes trend and fourier terms (so is favoured here). The regression is used
to calculate the "control mean" (~fitted values) - it then uses a computed 
generalized likelihood ratio statistic to assess if there is shift in the mean 
for each week. Note that the threshold for each week takes in to account previous
weeks so if there is a sustained shift an alarm will be triggered. 
(Also note that after each alarm the algorithm is reset)

In order to work with the **surveillance** package, we first need to define a 
"surveillance time series" object (using the `sts()` function) to fit within the 
framework. 

```{r surveillance_obj}

## define surveillance time series object
## nb. you can include a denominator with the population object (see ?sts)
counts_sts <- sts(observed = counts$case_int[!is.na(counts$case_int)],
                  start = c(
                    ## subset to only keep the year from start_date 
                    as.numeric(str_sub(start_date, 1, 4)), 
                    ## subset to only keep the week from start_date
                    as.numeric(str_sub(start_date, 7, 8))), 
                  ## define the type of data (in this case weekly)
                  freq = 52)

## define the week range that you want to include (ie. prediction period)
## nb. the sts object only counts observations without assigning a week or 
## year identifier to them - so we use our data to define the appropriate observations
weekrange <- cut_off - start_date

```

<!-- ======================================================= -->
#### Farrington method {.unnumbered}

We then define each of our parameters for the Farrington method in a `list`. 
Then we run the algorithm using `farringtonFlexible()` and then we can extract the 
threshold for an alert using `farringtonmethod@upperbound`to include this in our 
dataset. It is also possible to extract a TRUE/FALSE for each week if it triggered 
an alert (was above the threshold) using `farringtonmethod@alarm`. 

```{r farrington}

## define control
ctrl <- list(
  ## define what time period that want threshold for (i.e. 2011)
  range = which(counts_sts@epoch > weekrange),
  b = 9, ## how many years backwards for baseline
  w = 2, ## rolling window size in weeks
  weightsThreshold = 2.58, ## reweighting past outbreaks (improved noufaily method - original suggests 1)
  ## pastWeeksNotIncluded = 3, ## use all weeks available (noufaily suggests drop 26)
  trend = TRUE,
  pThresholdTrend = 1, ## 0.05 normally, however 1 is advised in the improved method (i.e. always keep)
  thresholdMethod = "nbPlugin",
  populationOffset = TRUE
  )

## apply farrington flexible method
farringtonmethod <- farringtonFlexible(counts_sts, ctrl)

## create a new variable in the original dataset called threshold
## containing the upper bound from farrington 
## nb. this is only for the weeks in 2011 (so need to subset rows)
counts[which(counts$epiweek >= cut_off & 
               !is.na(counts$case_int)),
              "threshold"] <- farringtonmethod@upperbound
```

We can then visualise the results in ggplot as done previously. 

```{r plot_farrington, warning=F, message=F}

ggplot(counts, aes(x = epiweek)) + 
  ## add in observed case counts as a line
  geom_line(aes(y = case_int, colour = "Observed")) + 
  ## add in upper bound of aberration algorithm
  geom_line(aes(y = threshold, colour = "Alert threshold"), 
            linetype = "dashed", 
            size = 1.5) +
  ## define colours
  scale_colour_manual(values = c("Observed" = "black", 
                                 "Alert threshold" = "red")) + 
  ## make a traditional plot (with black axes and white background)
  theme_classic() + 
  ## remove title of legend 
  theme(legend.title = element_blank())

```

<!-- ======================================================= -->
#### GLRNB method {.unnumbered}

Similarly for the GLRNB method we define each of our parameters for the in a `list`, 
then fit the algorithm and extract the upper bounds.

<span style="color: orange;">**_CAUTION:_** This method uses "brute force" (similar to bootstrapping) for calculating thresholds, so can take a long time!</span>

See the [GLRNB vignette](https://cran.r-project.org/web/packages/surveillance/vignettes/glrnb.pdf) 
for details. 

```{r glrnb, warning = FALSE, message = FALSE}

## define control options
ctrl <- list(
  ## define what time period that want threshold for (i.e. 2011)
  range = which(counts_sts@epoch > weekrange),
  mu0 = list(S = 1,    ## number of fourier terms (harmonics) to include
  trend = TRUE,   ## whether to include trend or not
  refit = FALSE), ## whether to refit model after each alarm
  ## cARL = threshold for GLR statistic (arbitrary)
     ## 3 ~ middle ground for minimising false positives
     ## 1 fits to the 99%PI of glm.nb - with changes after peaks (threshold lowered for alert)
   c.ARL = 2,
   # theta = log(1.5), ## equates to a 50% increase in cases in an outbreak
   ret = "cases"     ## return threshold upperbound as case counts
  )

## apply the glrnb method
glrnbmethod <- glrnb(counts_sts, control = ctrl, verbose = FALSE)

## create a new variable in the original dataset called threshold
## containing the upper bound from glrnb 
## nb. this is only for the weeks in 2011 (so need to subset rows)
counts[which(counts$epiweek >= cut_off & 
               !is.na(counts$case_int)),
              "threshold_glrnb"] <- glrnbmethod@upperbound

```

Visualise the outputs as previously. 

```{r plot_glrnb, message=F, warning=F}

ggplot(counts, aes(x = epiweek)) + 
  ## add in observed case counts as a line
  geom_line(aes(y = case_int, colour = "Observed")) + 
  ## add in upper bound of aberration algorithm
  geom_line(aes(y = threshold_glrnb, colour = "Alert threshold"), 
            linetype = "dashed", 
            size = 1.5) +
  ## define colours
  scale_colour_manual(values = c("Observed" = "black", 
                                 "Alert threshold" = "red")) + 
  ## make a traditional plot (with black axes and white background)
  theme_classic() + 
  ## remove title of legend 
  theme(legend.title = element_blank())

```

<!-- ======================================================= -->
## Interrupted timeseries {  }

Interrupted timeseries (also called segmented regression or intervention analysis), 
is often used in assessing the impact of vaccines on the incidence of disease. 
But it can be used for assessing impact of a wide range of interventions or introductions. 
For example changes in hospital procedures or the introduction of a new disease 
strain to a population. 
In this example we will pretend that a new strain of Campylobacter was introduced
to Germany at the end of 2008, and see if that affects the number of cases. 
We will use negative binomial regression again. The regression this time will be 
split in to two parts, one before the intervention (or introduction of new strain here) 
and one after (the pre and post-periods). This allows us to calculate an incidence rate ratio comparing the
two time periods. Explaining the equation might make this clearer (if not then just
ignore!). 

The negative binomial regression can be defined as follows: 

$$\log(Y_t)= β_0 + β_1 \times t+ β_2 \times δ(t-t_0) + β_3\times(t-t_0 )^+ + log(pop_t) + e_t$$

Where:
$Y_t$is the number of cases observed at time $t$  
$pop_t$ is the population size in 100,000s at time $t$ (not used here)  
$t_0$ is the last year of the of the pre-period (including transition time if any)  
$δ(x$ is the indicator function (it is 0 if x≤0 and 1 if x>0)  
$(x)^+$ is the cut off operator (it is x if x>0 and 0 otherwise)  
$e_t$ denotes the residual 
Additional terms trend and season can be added as needed. 

$β_2 \times δ(t-t_0) + β_3\times(t-t_0 )^+$ is the generalised linear 
part of the post-period and is zero in the pre-period. 
This means that the $β_2$ and $β_3$ estimates are the effects of the intervention. 

We need to re-calculate the fourier terms without forecasting here, as we will use
all the data available to us (i.e. retrospectively). Additionally we need to calculate
the extra terms needed for the regression. 

```{r define_terms_interrupted}

## add in fourier terms using the epiweek and case_int variabless
counts$fourier <- select(counts, epiweek, case_int) %>% 
  as_tsibble(index = epiweek) %>% 
  fourier(K = 1)

## define intervention week 
intervention_week <- yearweek("2008-12-31")

## define variables for regression 
counts <- counts %>% 
  mutate(
    ## corresponds to t in the formula
      ## count of weeks (could probably also just use straight epiweeks var)
    # linear = row_number(epiweek), 
    ## corresponds to delta(t-t0) in the formula
      ## pre or post intervention period
    intervention = as.numeric(epiweek >= intervention_week), 
    ## corresponds to (t-t0)^+ in the formula
      ## count of weeks post intervention
      ## (choose the larger number between 0 and whatever comes from calculation)
    time_post = pmax(0, epiweek - intervention_week + 1))

```

We then use these terms to fit a negative binomial regression, and produce a 
table with percentage change. What this example shows is that there was no 
significant change. 

<span style="color: orange;">**_CAUTION:_** Note the use of `simulate_pi = FALSE`
within the `predict()` argument. This is because the default behaviour of **trending** 
is to use the **ciTools** package to estimate a prediction interval. This does not 
work if there are `NA` counts, and also produces more granular intervals. 
See `?trending::predict.trending_model_fit` for details. </span>  

```{r interrupted_regression, warning = FALSE}


## define the model you want to fit (negative binomial) 
model <- glm_nb_model(
  ## set number of cases as outcome of interest
  case_int ~
    ## use epiweek to account for the trend
    epiweek +
    ## use the furier terms to account for seasonality
    fourier + 
    ## add in whether in the pre- or post-period 
    intervention + 
    ## add in the time post intervention 
    time_post
    )

## fit your model using the counts dataset
fitted_model <- trending::fit(model, counts)

## calculate confidence intervals and prediction intervals 
observed <- predict(fitted_model, simulate_pi = FALSE)



## show estimates and percentage change in a table
fitted_model %>% 
  ## extract original negative binomial regression
  get_model() %>% 
  ## get a tidy dataframe of results
  tidy(exponentiate = TRUE, 
       conf.int = TRUE) %>% 
  ## only keep the intervention value 
  filter(term == "intervention") %>% 
  ## change the IRR to percentage change for estimate and CIs 
  mutate(
    ## for each of the columns of interest - create a new column
    across(
      all_of(c("estimate", "conf.low", "conf.high")), 
      ## apply the formula to calculate percentage change
            .f = function(i) 100 * (i - 1), 
      ## add a suffix to new column names with "_perc"
      .names = "{.col}_perc")
    ) %>% 
  ## only keep (and rename) certain columns 
  select("IRR" = estimate, 
         "95%CI low" = conf.low, 
         "95%CI high" = conf.high,
         "Percentage change" = estimate_perc, 
         "95%CI low (perc)" = conf.low_perc, 
         "95%CI high (perc)" = conf.high_perc,
         "p-value" = p.value)
```

As previously we can visualise the outputs of the regression. 

```{r plot_interrupted}

ggplot(observed, aes(x = epiweek)) + 
  ## add in observed case counts as a line
  geom_line(aes(y = case_int, colour = "Observed")) + 
  ## add in a line for the model estimate
  geom_line(aes(y = estimate, col = "Estimate")) + 
  ## add in a band for the prediction intervals 
  geom_ribbon(aes(ymin = lower_pi, 
                  ymax = upper_pi), 
              alpha = 0.25) + 
  ## add vertical line and label to show where forecasting started
  geom_vline(
           xintercept = as.Date(intervention_week), 
           linetype = "dashed") + 
  annotate(geom = "text", 
           label = "Intervention", 
           x = intervention_week, 
           y = max(observed$upper_pi), 
           angle = 90, 
           vjust = 1
           ) + 
  ## define colours
  scale_colour_manual(values = c("Observed" = "black", 
                                 "Estimate" = "red")) + 
  ## make a traditional plot (with black axes and white background)
  theme_classic()

```


<!-- ======================================================= -->
## Resources {  }

[forecasting: principles and practice textbook](https://otexts.com/fpp3/)  
[EPIET timeseries analysis case studies](https://github.com/EPIET/TimeSeriesAnalysis)  
[Penn State course](https://online.stat.psu.edu/stat510/lesson/1) 
[Surveillance package manuscript](https://www.jstatsoft.org/article/view/v070i10)





```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/time_series.Rmd-->


# Mô hình hóa dịch bệnh {#epidemic-models}  


<!-- ======================================================= -->
## Overview {  }

There exists a growing body of tools for epidemic modelling that lets us conduct
fairly complex analyses with minimal effort. This section will provide an
overview on how to use these tools to:

* estimate the effective reproduction number R<sub>t</sub> and related statistics
  such as the doubling time
* produce short-term projections of future incidence

It is *not* intended as an overview of the methodologies and statistical methods
underlying these tools, so please refer to the Resources tab for links to some
papers covering this. Make sure you have an understanding of
the methods before using these tools; this will ensure you can accurately
interpret their results.

Below is an example of one of the outputs we'll be producing in this section.

```{r out.width=c('100%', '100%'), fig.show='hold', echo=F, fig.width = 12, fig.height = 9, message=F, warning=F}

## install and load packages
pacman::p_load(tidyverse, EpiNow2, EpiEstim, here, incidence2, epicontacts, rio, projections)

## load linelist
linelist <- import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

## generate contacts
contacts <- linelist %>%
  transmute(
    from = infector,
    to = case_id
  ) %>%
  drop_na()

## generate epicontacts
epic <- make_epicontacts(
  linelist = linelist,
  contacts = contacts, 
  directed = TRUE
)

## ## estimate gamma generation time
## generation_time <- bootstrapped_dist_fit(
##   get_pairwise(epic, "date_infection"),
##   dist = "gamma",
##   max_value = 20,
##   bootstraps = 1
## )

## ## export for caching
## export(
##   generation_time,
##   here("data/cache/epidemic_models/generation_time.rds")
## )

## import cached generation time
generation_time <- import(here("data/cache/epidemic_models/generation_time.rds"))

## ## estimate incubation period
## incubation_period <- bootstrapped_dist_fit(
##   linelist$date_onset - linelist$date_infection,
##   dist = "lognormal",
##   max_value = 100,
##   bootstraps = 1
## )

## ## export for caching
## export(
##   incubation_period,
##   here("data/cache/epidemic_models/incubation_period.rds")
## )

## import cached incubation period
incubation_period <- import(here("data/cache/epidemic_models/incubation_period.rds"))

## get incidence from onset date
cases <- linelist %>%
  group_by(date = date_onset) %>%
  summarise(confirm = n())

## ## run epinow
## epinow_res <- epinow(
##   reported_cases = cases,
##   generation_time = generation_time,
##   delays = delay_opts(incubation_period),
##   target_folder = here("data/cache/epidemic_models"),
##   return_output = TRUE,
##   output = "samples",
##   verbose = TRUE,
##   stan = stan_opts(samples = 750, chains = 4),
##   horizon = 21
## )

## ## export for caching
## export(
##   epinow_res,
##   here("data/cache/epidemic_models/epinow_res.rds")
## )

## import cached epinow results
epinow_res <- import(here("data/cache/epidemic_models/epinow_res.rds"))

## plot summary figure
plot(epinow_res)

```

<!-- ======================================================= -->
## Preparation {  }

We will use two different methods and packages for R<sub>t</sub> estimation,
namely **EpiNow** and **EpiEstim**, as well as the **projections** package for
forecasting case incidence.  

This code chunk shows the loading of packages required for the analyses. 
In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. 
You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

	
```{r epidemic_models_packages, }
pacman::p_load(
   rio,          # File import
   here,         # File locator
   tidyverse,    # Data management + ggplot2 graphics
   epicontacts,  # Analysing transmission networks
   EpiNow2,      # Rt estimation
   EpiEstim,     # Rt estimation
   projections,  # Incidence projections
   incidence2,   # Handling incidence data
   epitrix,      # Useful epi functions
   distcrete     # Discrete delay distributions
)
```
	
We will use the cleaned case linelist for all analyses in this section. If you want to follow along, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). See the [Download handbook and data] page to download all example data used in this handbook.  

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r eval=F}
# import the cleaned linelist
linelist <- import("linelist_cleaned.rds")
```


<!-- ======================================================= -->
## Estimating R<sub>t</sub> {  }

### EpiNow2 vs. EpiEstim {.unnumbered}

The reproduction number R is a measure of the transmissibility of a disease and
is defined as the expected number of secondary cases per infected case. In a
fully susceptible population, this value represents the basic reproduction
number R<sub>0</sub>. However, as the number of susceptible individuals in a
population changes over the course of an outbreak or pandemic, and as various
response measures are implemented, the most commonly used measure of
transmissibility is the effective reproduction number R<sub>t</sub>; this is
defined as the expected number of secondary cases per infected case at a given
time _t_.

The **EpiNow2** package provides the most sophisticated framework for estimating
R<sub>t</sub>. It has two key advantages over the other commonly used package,
**EpiEstim**:

* It accounts for delays in reporting and can therefore estimate R<sub>t</sub>
  even when recent data is incomplete.
* It estimates R<sub>t</sub> on _dates of infection_ rather than the dates of
  onset of reporting, which means that the effect of an intervention will
  be immediately reflected in a change in R<sub>t</sub>, rather than with a
  delay.

However, it also has two key disadvantages:

* It requires knowledge of the generation time distribution (i.e. distribution
  of delays between infection of a primary and secondary cases), incubation
  period distribution (i.e. distribution of delays between infection and symptom
  onset) and any further delay distribution relevant to your data (e.g. if you
  have dates of reporting, you require the distribution of delays from symptom
  onset to reporting). While this will allow more accurate estimation of
  R<sub>t</sub>, **EpiEstim** only requires the serial interval distribution
  (i.e. the distribution of delays between symptom onset of a primary and a
  secondary case), which may be the only distribution available to you.
* **EpiNow2** is significantly slower than **EpiEstim**, anecdotally by a factor
  of about 100-1000! For example, estimating R<sub>t</sub> for the sample outbreak
  considered in this section takes about four hours (this was run for a large
  number of iterations to ensure high accuracy and could probably be reduced if
  necessary, however the points stands that the algorithm is slow in
  general). This may be unfeasible if you are regularly updating your
  R<sub>t</sub> estimates.
  
Which package you choose to use will therefore depend on the data, time and
computational resources available to you.

### EpiNow2 {.unnumbered}

#### Estimating delay distributions {.unnumbered}

The delay distributions required to run **EpiNow2** depend on the data you
have. Essentially, you need to be able to describe the delay from the date of
infection to the date of the event you want to use to estimate R<sub>t</sub>. If
you are using dates of onset, this would simply be the incubation period
distribution. If you are using dates of reporting, you require the
delay from infection to reporting. As this distribution is unlikely to be known
directly, **EpiNow2** lets you chain multiple delay distributions together; in
this case, the delay from infection to symptom onset (e.g. the incubation
period, which is likely known) and from symptom onset to reporting (which you
can often estimate from the data).

As we have the dates of onset for all our cases in the example linelist, we will
only require the incubation period distribution to link our data (e.g. dates of
symptom onset) to the date of infection. We can either estimate this distribution
from the data or use values from the literature.

A literature estimate of the incubation period of Ebola (taken
from [this paper](https://www.nejm.org/doi/full/10.1056/nejmoa1411100)) with a
mean of 9.1, standard deviation of 7.3 and maximum value of 30 would be
specified as follows:

```{r epidemic_models_incubation_literature, eval=F}
incubation_period_lit <- list(
  mean = log(9.1),
  mean_sd = log(0.1),
  sd = log(7.3),
  sd_sd = log(0.1),
  max = 30
)
```
Note that **EpiNow2** requires these delay distributions to be provided on a **log**
scale, hence the `log` call around each value (except the `max` parameter which,
confusingly, has to be provided on a natural scale). The `mean_sd` and `sd_sd`
define the standard deviation of the mean and standard deviation estimates. As
these are not known in this case, we choose the fairly arbitrary value of 0.1.

In this analysis, we instead estimate the incubation period distribution
from the linelist itself using the function `bootstrapped_dist_fit`, which will
fit a lognormal distribution to the observed delays between infection and onset
in the linelist.

```{r epidemic_models_incubation_estimate, eval=F}
## estimate incubation period
incubation_period <- bootstrapped_dist_fit(
  linelist$date_onset - linelist$date_infection,
  dist = "lognormal",
  max_value = 100,
  bootstraps = 1
)
```

The other distribution we require is the generation time. As we have data on
infection times __and__ transmission links, we can estimate this
distribution from the linelist by calculating the delay between infection times
of infector-infectee pairs. To do this, we use the handy `get_pairwise` function
from the package **epicontacts**, which allows us to calculate pairwise
differences of linelist properties between transmission pairs. We first create an
epicontacts object (see [Transmission chains] page for further
details):

```{r epidemic_models_epicontacts, eval=F}
## generate contacts
contacts <- linelist %>%
  transmute(
    from = infector,
    to = case_id
  ) %>%
  drop_na()

## generate epicontacts object
epic <- make_epicontacts(
  linelist = linelist,
  contacts = contacts, 
  directed = TRUE
)
```

We then fit the difference in infection times between transmission pairs,
calculated using `get_pairwise`, to a gamma distribution:

```{r epidemic_models_generation_estimate, eval=F}
## estimate gamma generation time
generation_time <- bootstrapped_dist_fit(
  get_pairwise(epic, "date_infection"),
  dist = "gamma",
  max_value = 20,
  bootstraps = 1
)
```

#### Running **EpiNow2** {.unnumbered}

Now we just need to calculate daily incidence from the linelist, which we can do
easily with the **dplyr** functions `group_by()` and `n()`. Note
that **EpiNow2** requires the column names to  be `date` and `confirm`.

```{r epidemic_models_cases, eval=F}
## get incidence from onset dates
cases <- linelist %>%
  group_by(date = date_onset) %>%
  summarise(confirm = n())
```

We can then estimate R<sub>t</sub> using the `epinow` function. Some notes on
the inputs:

* We can provide any number of 'chained' delay distributions to the `delays`
  argument; we would simply insert them alongside the `incubation_period` object
  within the `delay_opts` function.
* `return_output` ensures the output is returned within R and not just saved to
  a file.
* `verbose` specifies that we want a readout of the progress.
* `horizon` indicates how many days we want to project future incidence for.
* We pass additional options to the `stan` argument to specify how long
  we want to run the inference for. Increasing `samples` and `chains` will give
  you a more accurate estimate that better characterises uncertainty, however
  will take longer to run.

```{r epidemic_models_run_epinow, eval=F}
## run epinow
epinow_res <- epinow(
  reported_cases = cases,
  generation_time = generation_time,
  delays = delay_opts(incubation_period),
  return_output = TRUE,
  verbose = TRUE,
  horizon = 21,
  stan = stan_opts(samples = 750, chains = 4)
)
```

#### Analysing outputs {.unnumbered}

Once the code has finished running, we can plot a summary very easily as follows. Scroll the image to see the full extent.  


```{r out.width=c('100%', '100%'), fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F }
## plot summary figure
plot(epinow_res)
```

We can also look at various summary statistics:

```{r epidemic_models_epinow_summary,}
## summary table
epinow_res$summary
```

For further analyses and custom plotting, you can access the summarised daily
estimates via `$estimates$summarised`. We will convert this from the default
`data.table` to a `tibble` for ease of use with **dplyr**.

```{r epidemic_models_to_tibble, eval=F}
## extract summary and convert to tibble
estimates <- as_tibble(epinow_res$estimates$summarised)
estimates
```

```{r epidemic_models_tibble_show,  echo = F}
## show outputs
estimates <- as_tibble(epinow_res$estimates$summarised)
DT::datatable(
  estimates,
  rownames = FALSE,
  filter = "top",
  options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap'
)
```

As an example, let's make a plot of the doubling time and R<sub>t</sub>. We will
only look at the first few months of the outbreak when R<sub>t</sub> is well
above one, to avoid plotting extremely high doublings times.

We use the formula `log(2)/growth_rate` to calculate the doubling time from the
estimated growth rate.

```{r epidemic_models_plot_epinow_cusotom, out.width=c('100%', '100%'), fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}

## make wide df for median plotting
df_wide <- estimates %>%
  filter(
    variable %in% c("growth_rate", "R"),
    date < as.Date("2014-09-01")
  ) %>%
  ## convert growth rates to doubling times
  mutate(
    across(
      c(median, lower_90:upper_90),
      ~ case_when(
        variable == "growth_rate" ~ log(2)/.x,
        TRUE ~ .x
      )
    ),
    ## rename variable to reflect transformation
    variable = replace(variable, variable == "growth_rate", "doubling_time")
  )

## make long df for quantile plotting
df_long <- df_wide %>%
  ## here we match matching quantiles (e.g. lower_90 to upper_90)
  pivot_longer(
    lower_90:upper_90,
    names_to = c(".value", "quantile"),
    names_pattern = "(.+)_(.+)"
  )

## make plot
ggplot() +
  geom_ribbon(
    data = df_long,
    aes(x = date, ymin = lower, ymax = upper, alpha = quantile),
    color = NA
  ) +
  geom_line(
    data = df_wide,
    aes(x = date, y = median)
  ) +
  ## use label_parsed to allow subscript label
  facet_wrap(
    ~ variable,
    ncol = 1,
    scales = "free_y",
    labeller = as_labeller(c(R = "R[t]", doubling_time = "Doubling~time"), label_parsed),
    strip.position = 'left'
  ) +
  ## manually define quantile transparency
  scale_alpha_manual(
    values = c(`20` = 0.7, `50` = 0.4, `90` = 0.2),
    labels = function(x) paste0(x, "%")
  ) +
  labs(
    x = NULL,
    y = NULL,
    alpha = "Credibel\ninterval"
  ) +
  scale_x_date(
    date_breaks = "1 month",
    date_labels = "%b %d\n%Y"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    strip.background = element_blank(),
    strip.placement = 'outside'
  )

```

<!-- ======================================================= -->
### EpiEstim {.unnumbered}

To run **EpiEstim**, we need to provide data on daily incidence and specify the
serial interval (i.e. the distribution of delays between symptom onset of
primary and secondary cases). 

Incidence data can be provided to **EpiEstim** as a vector, a data frame, or an `incidence`
object from the original **incidence** package. You can even distinguish between imports
and locally acquired infections; see the documentation at `?estimate_R` for
further details.  

We will create the input using **incidence2**. See the page on [Epidemic curves] for more examples with the **incidence2** package. Since there have been updates to the **incidence2** package that don't completely align with `estimateR()`'s expected input, there are some minor additional steps needed. The incidence object consists of a tibble with dates and their respective case counts. We use `complete()` from **tidyr** to ensure all dates are included (even those with no cases), and then `rename()` the columns to align with what is expected by `estimate_R()` in a later step.  

```{r epidemic_models_epiestim_incidence,}
## get incidence from onset date
cases <- incidence2::incidence(linelist, date_index = date_onset) %>% # get case counts by day
  tidyr::complete(date_index = seq.Date(                              # ensure all dates are represented
    from = min(date_index, na.rm = T),
    to = max(date_index, na.rm=T),
    by = "day"),
    fill = list(count = 0)) %>%                                       # convert NA counts to 0
  rename(I = count,                                                   # rename to names expected by estimateR
         dates = date_index)
```

The package provides several options for specifying the serial interval, the
details of which are provided in the documentation at `?estimate_R`. We will
cover two of them here.

#### Using serial interval estimates from the literature {.unnumbered}

Using the option `method = "parametric_si"`, we can manually specify the mean and
standard deviation of the serial interval in a `config` object created using the
function `make_config`. We use a mean and standard deviation of 12.0 and 5.2, respectively, defined in
[this paper](https://bmcmedicine.biomedcentral.com/articles/10.1186/s12916-014-0196-0):

```{r epidemic_models_epiestim_config,}
## make config
config_lit <- make_config(
  mean_si = 12.0,
  std_si = 5.2
)
```

We can then estimate R<sub>t</sub> with the `estimate_R` function:

```{r epidemic_models_epiestim_lit,  warning = FALSE}
epiestim_res_lit <- estimate_R(
  incid = cases,
  method = "parametric_si",
  config = config_lit
)
```

and plot a summary of the outputs:

```{r epidemic_models_epiestim_lit_plot,  warning = FALSE}
plot(epiestim_res_lit)
```

#### Using serial interval estimates from the data {.unnumbered}

As we have data on dates of symptom onset _and_ transmission links, we can
also estimate the serial interval from the linelist by calculating the delay
between onset dates of infector-infectee pairs. As we did in the **EpiNow2**
section, we will use the `get_pairwise` function from the **epicontacts**
package, which allows us to calculate pairwise differences of linelist
properties between transmission pairs. We first create an epicontacts object
(see [Transmission chains] page for further details):

```{r epidemic_models_epicontacts_epiestim, eval=F}
## generate contacts
contacts <- linelist %>%
  transmute(
    from = infector,
    to = case_id
  ) %>%
  drop_na()

## generate epicontacts object
epic <- make_epicontacts(
  linelist = linelist,
  contacts = contacts, 
  directed = TRUE
)
```

We then fit the difference in onset dates between transmission pairs, calculated
using `get_pairwise`, to a gamma distribution. We use the handy `fit_disc_gamma`
from the **epitrix** package for this fitting procedure, as we require a
_discretised_ distribution.

```{r epidemic_models_incubation_estimate_epiestim,  warning = FALSE}
## estimate gamma serial interval
serial_interval <- fit_disc_gamma(get_pairwise(epic, "date_onset"))
```

We then pass this information to the `config` object, run **EpiEstim**
again and plot the results:

```{r epidemic_models_epiestim_emp,  warning = FALSE}
## make config
config_emp <- make_config(
  mean_si = serial_interval$mu,
  std_si = serial_interval$sd
)

## run epiestim
epiestim_res_emp <- estimate_R(
  incid = cases,
  method = "parametric_si",
  config = config_emp
)

## plot outputs
plot(epiestim_res_emp)
```

#### Specifying estimation time windows {.unnumbered}

These default options will provide a weekly sliding estimate and might act as a
warning that you are estimating R<sub>t</sub> too early in the outbreak for a
precise estimate. You can change this by setting a later start date for the
estimation as shown below. Unfortunately, **EpiEstim** only provides a very
clunky way of specifying these estimations times, in that you have to provide a
vector of __integers__ referring to the start and end dates for each time
window.

```{r epidemic_models_epiestim_config_late,}

## define a vector of dates starting on June 1st
start_dates <- seq.Date(
  as.Date("2014-06-01"),
  max(cases$dates) - 7,
  by = 1
) %>%
  ## subtract the starting date to convert to numeric
  `-`(min(cases$dates)) %>%
  ## convert to integer
  as.integer()

## add six days for a one week sliding window
end_dates <- start_dates + 6
  
## make config
config_partial <- make_config(
  mean_si = 12.0,
  std_si = 5.2,
  t_start = start_dates,
  t_end = end_dates
)
```
Now we re-run **EpiEstim** and can see that the estimates only start from June:

```{r epidemic_models_epiestim_config_late_run,}

## run epiestim
epiestim_res_partial <- estimate_R(
  incid = cases,
  method = "parametric_si",
  config = config_partial
)

## plot outputs
plot(epiestim_res_partial)

```

#### Analysing outputs {.unnumbered}

The main outputs can be accessed via `$R`. As an example, we will create a plot of
R<sub>t</sub> and a measure of "transmission potential" given by the product of
R<sub>t</sub> and the number of cases reported on that day; this represents the
expected number of cases in the next generation of infection.

```{r epidemic_models_epiestim_plot_full, out.width=c('100%', '100%'), fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}

## make wide dataframe for median
df_wide <- epiestim_res_lit$R %>%
  rename_all(clean_labels) %>%
  rename(
    lower_95_r = quantile_0_025_r,
    lower_90_r = quantile_0_05_r,
    lower_50_r = quantile_0_25_r,
    upper_50_r = quantile_0_75_r,
    upper_90_r = quantile_0_95_r,
    upper_95_r = quantile_0_975_r,
    ) %>%
  mutate(
    ## extract the median date from t_start and t_end
    dates = epiestim_res_emp$dates[round(map2_dbl(t_start, t_end, median))],
    var = "R[t]"
  ) %>%
  ## merge in daily incidence data
  left_join(cases, "dates") %>%
  ## calculate risk across all r estimates
  mutate(
    across(
      lower_95_r:upper_95_r,
      ~ .x*I,
      .names = "{str_replace(.col, '_r', '_risk')}"
    )
  ) %>%
  ## seperate r estimates and risk estimates
  pivot_longer(
    contains("median"),
    names_to = c(".value", "variable"),
    names_pattern = "(.+)_(.+)"
  ) %>%
  ## assign factor levels
  mutate(variable = factor(variable, c("risk", "r")))

## make long dataframe from quantiles
df_long <- df_wide %>%
  select(-variable, -median) %>%
  ## seperate r/risk estimates and quantile levels
  pivot_longer(
    contains(c("lower", "upper")),
    names_to = c(".value", "quantile", "variable"),
    names_pattern = "(.+)_(.+)_(.+)"
  ) %>%
  mutate(variable = factor(variable, c("risk", "r")))

## make plot
ggplot() +
  geom_ribbon(
    data = df_long,
    aes(x = dates, ymin = lower, ymax = upper, alpha = quantile),
    color = NA
  ) +
  geom_line(
    data = df_wide,
    aes(x = dates, y = median),
    alpha = 0.2
  ) +
  ## use label_parsed to allow subscript label
  facet_wrap(
    ~ variable,
    ncol = 1,
    scales = "free_y",
    labeller = as_labeller(c(r = "R[t]", risk = "Transmission~potential"), label_parsed),
    strip.position = 'left'
  ) +
  ## manually define quantile transparency
  scale_alpha_manual(
    values = c(`50` = 0.7, `90` = 0.4, `95` = 0.2),
    labels = function(x) paste0(x, "%")
  ) +
  labs(
    x = NULL,
    y = NULL,
    alpha = "Credible\ninterval"
  ) +
  scale_x_date(
    date_breaks = "1 month",
    date_labels = "%b %d\n%Y"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    strip.background = element_blank(),
    strip.placement = 'outside'
  )
  
```

<!-- ======================================================= -->
## Projecting incidence {  }

### EpiNow2 {.unnumbered}

Besides estimating R<sub>t</sub>, **EpiNow2** also supports forecasting of
R<sub>t</sub> and projections of case numbers by integration with the
**EpiSoon** package under the hood. All you need to do is specify the `horizon`
argument in your `epinow` function call, indicating how many days you want to
project into the future; see the **EpiNow2** section under the "Estimating
R<sub>t</sub>" for details on how to get **EpiNow2** up and running. In this
section, we will just plot the outputs from that analysis, stored in the
`epinow_res` object.

```{r epidemic_models_episoon, out.width=c('100%', '100%'), fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}

## define minimum date for plot
min_date <- as.Date("2015-03-01")

## extract summarised estimates
estimates <-  as_tibble(epinow_res$estimates$summarised)

## extract raw data on case incidence
observations <- as_tibble(epinow_res$estimates$observations) %>%
  filter(date > min_date)

## extract forecasted estimates of case numbers
df_wide <- estimates %>%
  filter(
    variable == "reported_cases",
    type == "forecast",
    date > min_date
  )

## convert to even longer format for quantile plotting
df_long <- df_wide %>%
  ## here we match matching quantiles (e.g. lower_90 to upper_90)
  pivot_longer(
    lower_90:upper_90,
    names_to = c(".value", "quantile"),
    names_pattern = "(.+)_(.+)"
  )

## make plot
ggplot() +
  geom_histogram(
    data = observations,
    aes(x = date, y = confirm),
    stat = 'identity',
    binwidth = 1
  ) +
  geom_ribbon(
    data = df_long,
    aes(x = date, ymin = lower, ymax = upper, alpha = quantile),
    color = NA
  ) +
  geom_line(
    data = df_wide,
    aes(x = date, y = median)
  ) +
  geom_vline(xintercept = min(df_long$date), linetype = 2) +
  ## manually define quantile transparency
  scale_alpha_manual(
    values = c(`20` = 0.7, `50` = 0.4, `90` = 0.2),
    labels = function(x) paste0(x, "%")
  ) +
  labs(
    x = NULL,
    y = "Daily reported cases",
    alpha = "Credible\ninterval"
  ) +
  scale_x_date(
    date_breaks = "1 month",
    date_labels = "%b %d\n%Y"
  ) +
  theme_minimal(base_size = 14)

```

### projections {.unnumbered}

The **projections** package developed by RECON makes it very easy to make short
term incidence forecasts, requiring only knowledge of the effective reproduction
number R<sub>t</sub> and the serial interval. Here we will cover how to use
serial interval estimates from the literature and how to use our own estimates
from the linelist.

#### Using serial interval estimates from the literature {.unnumbered}

**projections** requires a discretised serial interval distribution of the class
`distcrete` from the package **distcrete**. We will use a gamma distribution
with a mean of 12.0 and and standard deviation of 5.2 defined in
[this paper](https://bmcmedicine.biomedcentral.com/articles/10.1186/s12916-014-0196-0). To
convert these values into the shape and scale parameters required for a gamma
distribution, we will use the function `gamma_mucv2shapescale` from the
**epitrix** package.

```{r epidemic_models_projections_distcrete,}

## get shape and scale parameters from the mean mu and the coefficient of
## variation (e.g. the ratio of the standard deviation to the mean)
shapescale <- epitrix::gamma_mucv2shapescale(mu = 12.0, cv = 5.2/12)

## make distcrete object
serial_interval_lit <- distcrete::distcrete(
  name = "gamma",
  interval = 1,
  shape = shapescale$shape,
  scale = shapescale$scale
)

```

Here is a quick check to make sure the serial interval looks correct. We
access the density of the gamma distribution we have just defined by `$d`, which
is equivalent to calling `dgamma`:

```{r epidemic_models_projections_distcrete_plot,}

## check to make sure the serial interval looks correct
qplot(
  x = 0:50, y = serial_interval_lit$d(0:50), geom = "area",
  xlab = "Serial interval", ylab = "Density"
)

```

#### Using serial interval estimates from the data {.unnumbered}

As we have data on dates of symptom onset _and_ transmission links, we can
also estimate the serial interval from the linelist by calculating the delay
between onset dates of infector-infectee pairs. As we did in the **EpiNow2**
section, we will use the `get_pairwise` function from the **epicontacts**
package, which allows us to calculate pairwise differences of linelist
properties between transmission pairs. We first create an epicontacts object
(see [Transmission chains] page for further details):

```{r epidemic_models_epicontacts_projections, eval=F}
## generate contacts
contacts <- linelist %>%
  transmute(
    from = infector,
    to = case_id
  ) %>%
  drop_na()

## generate epicontacts object
epic <- make_epicontacts(
  linelist = linelist,
  contacts = contacts, 
  directed = TRUE
)
```

We then fit the difference in onset dates between transmission pairs, calculated
using `get_pairwise`, to a gamma distribution. We use the handy `fit_disc_gamma`
from the **epitrix** package for this fitting procedure, as we require a
_discretised_ distribution.

```{r epidemic_models_incubation_estimate_projections,  warning = FALSE}
## estimate gamma serial interval
serial_interval <- fit_disc_gamma(get_pairwise(epic, "date_onset"))

## inspect estimate
serial_interval[c("mu", "sd")]
```

#### Projecting incidence {.unnumbered}

To project future incidence, we still need to provide historical incidence in
the form of an `incidence` object, as well as a sample of plausible
R<sub>t</sub> values. We will generate these values using the R<sub>t</sub>
estimates generated by **EpiEstim** in the previous section (under "Estimating
R<sub>t</sub>") and stored in the `epiestim_res_emp` object. In the code below,
we extract the mean and standard deviation estimates of R<sub>t</sub> for the
last time window of the outbreak (using the `tail` function to access the last
element in a vector), and simulate 1000 values from a gamma distribution using
`rgamma`. You can also provide your own vector of R<sub>t</sub> values that you
want to use for forward projections.

```{r epidemic_models_projection_setup,  warning = FALSE}

## create incidence object from dates of onset
inc <- incidence::incidence(linelist$date_onset)

## extract plausible r values from most recent estimate
mean_r <- tail(epiestim_res_emp$R$`Mean(R)`, 1)
sd_r <- tail(epiestim_res_emp$R$`Std(R)`, 1)
shapescale <- gamma_mucv2shapescale(mu = mean_r, cv = sd_r/mean_r)
plausible_r <- rgamma(1000, shape = shapescale$shape, scale = shapescale$scale)

## check distribution
qplot(x = plausible_r, geom = "histogram", xlab = expression(R[t]), ylab = "Counts")

```

We then use the `project()` function to make the actual forecast. We specify how
many days we want to project for via the `n_days` arguments, and specify the
number of simulations using the `n_sim` argument.

```{r epidemic_models_make_projection,}

## make projection
proj <- project(
  x = inc,
  R = plausible_r,
  si = serial_interval$distribution,
  n_days = 21,
  n_sim = 1000
)

```

We can then handily plot the incidence and projections using the `plot()` and
`add_projections()` functions. We can easily subset the `incidence` object to only
show the most recent cases by using the square bracket operator.

```{r epidemic_models_plot_projection, out.width=c('100%', '100%'), fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}

## plot incidence and projections
plot(inc[inc$dates > as.Date("2015-03-01")]) %>%
  add_projections(proj)

```

You can also easily extract the raw estimates of daily case numbers by
converting the output to a dataframe.

```{r epidemic_models_projection_df, eval=F, warning = FALSE}
## convert to data frame for raw data
proj_df <- as.data.frame(proj)
proj_df
```

```{r epidemic_models_projection_dt,  echo = F}

## convert to data frame for raw data
proj_df <- as.data.frame(proj)

## data table output
DT::datatable(
  proj_df[1:11],
  rownames = FALSE,
  filter = "top",
  options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap'
)

```


<!-- ======================================================= -->
## Resources {  }

* [Here is the paper](https://www.sciencedirect.com/science/article/pii/S1755436519300350) describing
  the methodology implemented in **EpiEstim**.
* [Here is the paper](https://wellcomeopenresearch.org/articles/5-112/v1) describing
  the methodology implemented in **EpiNow2**.
* [Here is a paper](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008409) describing
  various methodological and practical considerations for estimating R<sub>t</sub>.
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/epidemic_models.Rmd-->


# Truy vết tiếp xúc {#contact-tracing}


This page demonstrates descriptive analysis of contact tracing data, addessing some key considerations and approaches unique to these kinds of data.  

This page references many of the core R data management and visualisation competencies covered in other pages (e.g. data cleaning, pivoting, tables, time-series analyses), but we will highlight examples specific to contact tracing that have been useful for operational decision making. For example, this includes visualizing contact tracing follow-up data over time or across geographic areas, or producing clean Key Performance Indicator (KPI) tables for contact tracing supervisors.

For demonstration purposes we will use sample contact tracing data from the [Go.Data](https://www.who.int/tools/godata) platform. The principles covered here will apply for contact tracing data from other platforms - you may just need to undergo different data pre-processing steps depending on the structure of your data.  

You can read more about the Go.Data project on the [Github Documentation site](https://worldhealthorganization.github.io/godata/) or [Community of Practice](https://community-godata.who.int/). 

## Preparation


### Load packages {.unnumbered}  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r, message = F}
pacman::p_load(
  rio,          # importing data  
  here,         # relative file pathways  
  janitor,      # data cleaning and tables
  lubridate,    # working with dates
  epikit,       # age_categories() function
  apyramid,     # age pyramids
  tidyverse,    # data manipulation and visualization
  RColorBrewer, # color palettes
  formattable,  # fancy tables
  kableExtra    # table formatting
)
```


### Import data {.unnumbered}

We will import sample datasets of contacts, and of their "follow-up". These data have been retrieved and un-nested from the Go.Data API and stored as ".rds" files.  

You can download all the example data for this handbook from the [Download handbook and data] page. 

If you want to download the example contact tracing data specific to this page, use the three download links below:  

<a href='https://github.com/WorldHealthOrganization/godata/blob/master/analytics/r-reporting/data/cases_clean.rds?raw=true' class='download-button'>
	Click to download
	<span>the case investigation data (.rds file)</span>
</a>

<a href='https://github.com/WorldHealthOrganization/godata/blob/master/analytics/r-reporting/data/contacts_clean.rds?raw=true' class='download-button'>
	Click to download
	<span>the contact registration data (.rds file)</span>
</a>

<a href='https://github.com/WorldHealthOrganization/godata/blob/master/analytics/r-reporting/data/followups_clean.rds?raw=true' class='download-button'>
	Click to download
	<span>the contact follow-up data (.rds file)</span>
</a>

<!-- ```{r out.width = "100%", fig.align = "center", echo=F} -->
<!-- knitr::include_graphics(here::here("images", "godata_api_github.png")) -->
<!-- ``` -->


In their original form in the downloadable files, the data reflect data as provided by the Go.Data API (learn about [APIs here](#import_api)). For example purposes here, we will clean the data to make it easier to read on this page. If you are using a Go.Data instance, you can view complete instructions on how to retrieve your data [here](https://github.com/WorldHealthOrganization/godata/tree/master/analytics/r-reporting).  

Below, the datasets are imported using the `import()` function from the **rio** package. See the page on [Import and export] for various ways to import data. We use `here()` to specify the file path - you should provide the file path specific to your computer. We then use `select()` to select only certain columns of the data, to simplify for purposes of demonstration.  

#### Case data {.unnumbered}  

These data are a table of the cases, and information about them.  

```{r}
cases <- import(here("data", "godata", "cases_clean.rds")) %>% 
  select(case_id, firstName, lastName, gender, age, age_class,
         occupation, classification, was_contact, hospitalization_typeid)
```

Here are the ` nrow(cases)` cases:  

```{r, message=FALSE, echo=F}
DT::datatable(cases, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

#### Contacts data {.unnumbered}  

These data are a table of all the contacts and information about them. Again, provide your own file path. After importing we perform a few preliminary data cleaning steps including:  

* Set age_class as a factor and reverse the level order so that younger ages are first  
* Select only certain column, while re-naming a one of them  
* Artificially assign rows with missing admin level 2 to "Djembe", to improve clarity of some example visualisations  


```{r}
contacts <- import(here("data", "godata", "contacts_clean.rds")) %>% 
  mutate(age_class = forcats::fct_rev(age_class)) %>% 
  select(contact_id, contact_status, firstName, lastName, gender, age,
         age_class, occupation, date_of_reporting, date_of_data_entry,
         date_of_last_exposure = date_of_last_contact,
         date_of_followup_start, date_of_followup_end, risk_level, was_case, admin_2_name) %>% 
  mutate(admin_2_name = replace_na(admin_2_name, "Djembe"))
```

Here are the ` nrow(contacts)` rows of the `contacts` dataset:  

```{r, message=FALSE, echo=F}
DT::datatable(contacts, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

#### Follow-up data {.unnumbered}  

These data are records of the "follow-up" interactions with the contacts. Each contact is supposed to have an encounter each day for 14 days after their exposure.  

We import and perform a few cleaning steps. We select certain columns, and also convert a character column to all lowercase values.  

```{r}
followups <- rio::import(here::here("data", "godata", "followups_clean.rds")) %>% 
  select(contact_id, followup_status, followup_number,
         date_of_followup, admin_2_name, admin_1_name) %>% 
  mutate(followup_status = str_to_lower(followup_status))
```

Here are the first 50 rows of the ` nrow(followups)`-row `followups` dataset (each row is a follow-up interaction, with outcome status in the `followup_status` column):  

```{r, message=FALSE, echo=F}
DT::datatable(head(followups, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

#### Relationships data {.unnumbered}  

Here we import data showing the relationship between cases and contacts. We select certain column to show.  

```{r}
relationships <- rio::import(here::here("data", "godata", "relationships_clean.rds")) %>% 
  select(source_visualid, source_gender, source_age, date_of_last_contact,
         date_of_data_entry, target_visualid, target_gender,
         target_age, exposure_type)
```

Below are the first 50 rows of the `relationships` dataset, which records all relationships between cases and contacts.  

```{r, message=FALSE, echo=F}
DT::datatable(head(relationships, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```








## Descriptive analyses  

You can use the techniques covered in other pages of this handbook to conduct descriptive analyses of your cases, contacts, and their relationships. Below are some examples.  


### Demographics {.unnumbered}  

As demonstrated in the page covering [Demographic pyramids][Demographic pyramids and Likert-scales], you can visualise the age and gender distribution (here we use the **apyramid** package).  


#### Age and Gender of contacts {.unnumbered}  

The pyramid below compares the age distribution of contacts, by gender. Note that contacts missing age are included in their own bar at the top. You can change this default behavior, but then consider listing the number missing in a caption.  

```{r, warning=F, message=F}
apyramid::age_pyramid(
  data = contacts,                                   # use contacts dataset
  age_group = "age_class",                           # categorical age column
  split_by = "gender") +                             # gender for halfs of pyramid
  labs(
    fill = "Gender",                                 # title of legend
    title = "Age/Sex Pyramid of COVID-19 contacts")+ # title of the plot
  theme_minimal()                                    # simple background
```


With the Go.Data data structure, the `relationships` dataset contains the ages of both cases and contacts, so you could use that dataset and create an age pyramid showing the differences between these two groups of people. The `relationships` data frame will be mutated to transform the numberic age columns into categories (see the [Cleaning data and core functions] page). We also pivot the dataframe longer to facilitate easy plotting with **ggplot2** (see [Pivoting data]).  

```{r}
relation_age <- relationships %>% 
  select(source_age, target_age) %>% 
  transmute(                              # transmute is like mutate() but removes all other columns not mentioned
    source_age_class = epikit::age_categories(source_age, breakers = seq(0, 80, 5)),
    target_age_class = epikit::age_categories(target_age, breakers = seq(0, 80, 5)),
    ) %>% 
  pivot_longer(cols = contains("class"), names_to = "category", values_to = "age_class")  # pivot longer


relation_age
```


Now we can plot this transformed dataset with `age_pyramid()` as before, but replacing `gender` with `category` (contact, or case).  

```{r, warning=F, message=F}
apyramid::age_pyramid(
  data = relation_age,                               # use modified relationship dataset
  age_group = "age_class",                           # categorical age column
  split_by = "category") +                           # by cases and contacts
  scale_fill_manual(
    values = c("orange", "purple"),                  # to specify colors AND labels
    labels = c("Case", "Contact"))+
  labs(
    fill = "Legend",                                           # title of legend
    title = "Age/Sex Pyramid of COVID-19 contacts and cases")+ # title of the plot
  theme_minimal()                                              # simple background
```

We can also view other characteristics such as occupational breakdown (e.g. in form of a pie chart).

```{r, warning=F, message=F}
# Clean dataset and get counts by occupation
occ_plot_data <- cases %>% 
  mutate(occupation = forcats::fct_explicit_na(occupation),  # make NA missing values a category
         occupation = forcats::fct_infreq(occupation)) %>%   # order factor levels in order of frequency
  count(occupation)                                          # get counts by occupation
  
# Make pie chart
ggplot(data = occ_plot_data, mapping = aes(x = "", y = n, fill = occupation))+
  geom_bar(width = 1, stat = "identity") +
  coord_polar("y", start = 0) +
  labs(
    fill = "Occupation",
    title = "Known occupations of COVID-19 cases")+
  theme_minimal() +                    
  theme(axis.line = element_blank(),
        axis.title = element_blank(),
        axis.text = element_blank())
```


### Contacts per case {.unnumbered}  

The number of contacts per case can be an important metric to assess quality of contact enumeration and the compliance of the population toward public health response. 

Depending on your data structure, this can be assessed with a dataset that contains all cases and contacts. In the Go.Data datasets, the links between cases ("sources") and contacts ("targets") is stored in the `relationships` dataset.  

In this dataset, each row is a contact, and the source case is listed in the row. There are no contacts who have relationships with multiple cases, but if this exists you may need to account for those before plotting (and explore them too!).  

We begin by counting the number of rows (contacts) per source case. This is saved as a data frame.  

```{r}
contacts_per_case <- relationships %>% 
  count(source_visualid)

contacts_per_case
```

We use `geom_histogram()` to plot these data as a histogram.  

```{r, warning=F, message=F}
ggplot(data = contacts_per_case)+        # begin with count data frame created above
  geom_histogram(mapping = aes(x = n))+  # print histogram of number of contacts per case
  scale_y_continuous(expand = c(0,0))+   # remove excess space below 0 on y-axis
  theme_light()+                         # simplify background
  labs(
    title = "Number of contacts per case",
    y = "Cases",
    x = "Contacts per case"
  )
  

```



## Contact Follow Up  


Contact tracing data often contain "follow-up" data, which record outcomes of daily symptom checks of persons in quarantine. Analysis of this data can inform response strategy, identify contacts at-risk of loss-to-follow-up or at-risk of developing disease.  




### Data cleaning {.unnumbered}  

These data can exist in a variety of formats. They may exist as a "wide" format Excel sheet with one row per contact, and one column per follow-up "day". See [Pivoting data] for descriptions of "long" and "wide" data and how to pivot data wider or longer.  

In our Go.Data example, these data are stored in the `followups` data frame, which is in a "long" format  with one row per follow-up interaction. The first 50 rows look like this:   

```{r, message=FALSE, echo=FALSE}
# display the first 50 rows of contact linelist data as a table
DT::datatable(head(followups, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


<span style="color: orange;">**_CAUTION:_** Beware of duplicates when dealing with followup data; as there could be several erroneous followups on the same day for a given contact. Perhaps it seems to be an error but reflects reality - e.g. a contact tracer could submit a follow-up form early in the day when they could not reach the contact, and submit a second form when they were later reached. It will depend on the operational context for how you want to handle duplicates - just make sure to document your approach clearly. </span>

Let's *see* how many instances of "duplicate" rows we have:  

```{r}
followups %>% 
  count(contact_id, date_of_followup) %>%   # get unique contact_days
  filter(n > 1)                             # view records where count is more than 1  
```

In our example data, the only records that this applies to are ones missing an ID! We can remove those. But, for purposes of demonstration we will go show the steps for de-duplication so there is only one follow-up encoutner per person per day. See the page on [De-duplication] for more detail. We will assume that the most recent encounter record is the correct one. We also take the opportunity to clean the `followup_number` column (the "day" of follow-up which should range 1 - 14).  

```{r, warning=F, message=F}
followups_clean <- followups %>%
  
  # De-duplicate
  group_by(contact_id, date_of_followup) %>%        # group rows per contact-day
  arrange(contact_id, desc(date_of_followup)) %>%   # arrange rows, per contact-day, by date of follow-up (most recent at top)
  slice_head() %>%                                  # keep only the first row per unique contact id  
  ungroup() %>% 
  
  # Other cleaning
  mutate(followup_number = replace(followup_number, followup_number > 14, NA)) %>% # clean erroneous data
  drop_na(contact_id)                               # remove rows with missing contact_id
```

For each follow-up encounter, we have a follow-up status (such as whether the encounter occurred and if so, did the contact have symptoms or not). To see all the values we can run a quick `tabyl()` (from **janitor**) or `table()` (from **base** R) (see [Descriptive tables]) by `followup_status` to see the frequency of each of the outcomes.  

In this dataset, "seen_not_ok" means "seen with symptoms", and "seen_ok" means "seen without symptoms".  

```{r}
followups_clean %>% 
  tabyl(followup_status)
```


### Plot over time {.unnumbered}  

As the dates data are continuous, we will use a histogram to plot them with `date_of_followup` assigned to the x-axis. We can achieve a "stacked" histogram by specifying a `fill = ` argument within `aes()`, which we assign to the column `followup_status`. Consequently, you can set the legend title using the `fill = ` argument of `labs()`.  

We can see that the contacts were identified in waves (presumably corresponding with epidemic waves of cases), and that follow-up completion did not seemingly improve over the course of the epidemic.  

```{r, warning=F, message=F}
ggplot(data = followups_clean)+
  geom_histogram(mapping = aes(x = date_of_followup, fill = followup_status)) +
  scale_fill_discrete(drop = FALSE)+   # show all factor levels (followup_status) in the legend, even those not used
  theme_classic() +
  labs(
    x = "",
    y = "Number of contacts",
    title = "Daily Contact Followup Status",
    fill = "Followup Status",
    subtitle = str_glue("Data as of {max(followups$date_of_followup, na.rm=T)}"))   # dynamic subtitle
  
```


<span style="color: orange;">**_CAUTION:_** If you are preparing many plots (e.g. for multiple jurisdictions) you will want the legends to appear identically even with varying levels of data completion or data composition. There may be plots for which not all follow-up statuses are present in the data, but you still want those categories to appear the legends. In ggplots (like above), you can specify the `drop = FALSE` argument of the `scale_fill_discrete()`. In tables, use `tabyl()` which shows counts for all factor levels, or if using `count()` from **dplyr** add the argument `.drop = FALSE` to include counts for all factor levels.</span>  


### Daily individual tracking  {.unnumbered}  

If your outbreak is small enough, you may want to look at each contact individually and see their status over the course of their follow-up. Fortunately, this `followups` dataset already contains a column with the day "number" of follow-up (1-14). If this does not exist in your data, you could create it by calculating the difference between the encounter date and the date follow-up was intended to begin for the contact.  

A convenient visualisation mechanism (if the number of cases is not too large) can be a heat plot, made with `geom_tile()`. See more details in the [heat plot] page.  

```{r, warning=F, message=F}
ggplot(data = followups_clean)+
  geom_tile(mapping = aes(x = followup_number, y = contact_id, fill = followup_status),
            color = "grey")+       # grey gridlines
  scale_fill_manual( values = c("yellow", "grey", "orange", "darkred", "darkgreen"))+
  theme_minimal()+
  scale_x_continuous(breaks = seq(from = 1, to = 14, by = 1))
```


### Analyse by group {.unnumbered}  

Perhaps these follow-up data are being viewed on a daily or weekly basis for operational decision-making. You may want more meaningful disaggregations by geographic area or by contact-tracing team. We can do this by adjusting the columns provided to `group_by()`.  

```{r, warning=F, message=F}

plot_by_region <- followups_clean %>%                                        # begin with follow-up dataset
  count(admin_1_name, admin_2_name, followup_status) %>%   # get counts by unique region-status (creates column 'n' with counts)
  
  # begin ggplot()
  ggplot(                                         # begin ggplot
    mapping = aes(x = reorder(admin_2_name, n),     # reorder admin factor levels by the numeric values in column 'n'
                  y = n,                            # heights of bar from column 'n'
                  fill = followup_status,           # color stacked bars by their status
                  label = n))+                      # to pass to geom_label()              
  geom_col()+                                     # stacked bars, mapping inherited from above 
  geom_text(                                      # add text, mapping inherited from above
    size = 3,                                         
    position = position_stack(vjust = 0.5), 
    color = "white",           
    check_overlap = TRUE,
    fontface = "bold")+
  coord_flip()+
  labs(
    x = "",
    y = "Number of contacts",
    title = "Contact Followup Status, by Region",
    fill = "Followup Status",
    subtitle = str_glue("Data as of {max(followups_clean$date_of_followup, na.rm=T)}")) +
  theme_classic()+                                                                      # Simplify background
  facet_wrap(~admin_1_name, strip.position = "right", scales = "free_y", ncol = 1)      # introduce facets 

plot_by_region
```

<!-- If this was disaggregated by contact tracer, perhaps we would want to add a threshold line to display total # contacts that normally one person or area/team can handle, and how the current workload compares. We just do this by using `geom_hline()` function. -->

<!-- ```{r, warning=F, message=F} -->

<!-- plot_by_region +  -->
<!--      geom_hline(aes(yintercept=25), color="#C70039", linetype = "dashed") # fictitious threshold at 25 contacts -->

<!-- ``` -->



## KPI Tables  

There are a number of different Key Performance Indicators (KPIs) that can be calculated and tracked at varying levels of disaggregations and across different time periods to monitor contact tracing performance. Once you have the calculations down and the basic table format; it is fairly easy to swap in and out different KPIs. 

There are numerous sources of contact tracing KPIs, such as this one from [ResolveToSaveLives.org](https://contacttracingplaybook.resolvetosavelives.org/checklists/metrics). The majority of the work will be walking through your data structure and thinking through all of the inclusion/exclusion criteria. We show a few examples below; using Go.Data metadata structure:

Category          | Indicator                | Go.Data Numerator         | Go.Data Denominator
------------------|--------------------------|---------------------------|--------------------
Process Indicator - Speed of Contact Tracing|% cases interviewed and isolated within 24h of case report |COUNT OF `case_id` WHERE (`date_of_reporting` - `date_of_data_entry`) < 1 day AND (`isolation_startdate` - `date_of_data_entry`) < 1 day|COUNT OF  `case_id`
Process Indicator - Speed of Contact Tracing|% contacts notified and quarantined within 24h of elicitation|COUNT OF `contact_id` WHERE `followup_status` == "SEEN_NOT_OK" OR "SEEN_OK" AND `date_of_followup` -  `date_of_reporting` < 1 day|COUNT OF `contact_id`
Process Indicator - Completeness of Testing|% new symptomatic cases tested and interviewed within 3 days of onset of symptoms|COUNT OF `case_id` WHERE (`date_of_reporting` - `date_of_onset`) < =3 days|COUNT OF  `case_id`
Outcome Indicator - Overall|% new cases among existing contact list|COUNT OF `case_id` WHERE `was_contact` == "TRUE"|COUNT OF  `case_id`

Below we will walk through a sample exercise of creating a nice table visual to show contact follow-up across admin areas. At the end, we will make it fit for presentation with the **formattable** package (but you could use other packages like **flextable** - see [Tables for presentation]).  

How you create a table like this will depend on the structure of your contact tracing data. Use the [Descriptive tables] page to learn how to summarise data using **dplyr** functions. 

We will create a table that will be dynamic and change as the data change. To make the results interesting, we will set a `report_date` to allow us to simulate running the table on a certain day (we pick 10th June 2020). The data are filtered to that date.  

```{r, warning=F, message=F}
# Set "Report date" to simulate running the report with data "as of" this date
report_date <- as.Date("2020-06-10")

# Create follow-up data to reflect the report date.
table_data <- followups_clean %>% 
  filter(date_of_followup <= report_date)
```


Now, based on our data structure, we will do the following:  

1) Begin with the `followups` data and summarise it to contain, for each unique contact:  
  * The date of latest record (no matter the status of the encounter)  
  * The date of latest encounter where the contact was "seen"  
  * The encounter status at that final "seen" encounter (e.g. with symptoms, without symptoms)  
2) Join these data to the contacts data, which contains other information such as the overall contact status, date of last exposure to a case, etc. Also we will calculate metrics of interest for each contact such as days since last exposure  
3) We group the enhanced contact data by geographic region (`admin_2_name`) and calculate summary statistics per region  
4) Finally, we format the table nicely for presentation  


First we summarise the follow-up data to get the information of interest:  

```{r, warning=F, message=F}
followup_info <- table_data %>% 
  group_by(contact_id) %>% 
  summarise(
    date_last_record   = max(date_of_followup, na.rm=T),
    date_last_seen     = max(date_of_followup[followup_status %in% c("seen_ok", "seen_not_ok")], na.rm=T),
    status_last_record = followup_status[which(date_of_followup == date_last_record)]) %>% 
  ungroup()
```

Here is how these data look:  

```{r, echo=F}
DT::datatable(followup_info, rownames = FALSE, options = list(pageLength = 12, scrollX=T), class = 'white-space: nowrap' )
```


Now we will add this information to the `contacts` dataset, and calculate some additional columns.  

```{r}
contacts_info <- followup_info %>% 
  right_join(contacts, by = "contact_id") %>% 
  mutate(
    database_date       = max(date_last_record, na.rm=T),
    days_since_seen     = database_date - date_last_seen,
    days_since_exposure = database_date - date_of_last_exposure
    )
```

Here is how these data look. Note `contacts` column to the right, and new calculated column at the far right.  

```{r, echo=F}
DT::datatable(contacts_info, rownames = FALSE, options = list(pageLength = 12, scrollX=T), class = 'white-space: nowrap' )
```


Next we summarise the contacts data by region, to achieve a concise data frame of summary statistic columns.    

```{r}
contacts_table <- contacts_info %>% 
  
  group_by(`Admin 2` = admin_2_name) %>%
  
  summarise(
    `Registered contacts` = n(),
    `Active contacts`     = sum(contact_status == "UNDER_FOLLOW_UP", na.rm=T),
    `In first week`       = sum(days_since_exposure < 8, na.rm=T),
    `In second week`      = sum(days_since_exposure >= 8 & days_since_exposure < 15, na.rm=T),
    `Became case`         = sum(contact_status == "BECAME_CASE", na.rm=T),
    `Lost to follow up`   = sum(days_since_seen >= 3, na.rm=T),
    `Never seen`          = sum(is.na(date_last_seen)),
    `Followed up - signs` = sum(status_last_record == "Seen_not_ok" & date_last_record == database_date, na.rm=T),
    `Followed up - no signs` = sum(status_last_record == "Seen_ok" & date_last_record == database_date, na.rm=T),
    `Not Followed up`     = sum(
      (status_last_record == "NOT_ATTEMPTED" | status_last_record == "NOT_PERFORMED") &
        date_last_record == database_date, na.rm=T)) %>% 
    
  arrange(desc(`Registered contacts`))

```


```{r, echo=F}
DT::datatable(contacts_table, rownames = FALSE, options = list(pageLength = 12, scrollX=T), class = 'white-space: nowrap' )
```

And now we apply styling from the **formattable** and **knitr** packages, including a footnote that shows the "as of" date.  


```{r}
contacts_table %>%
  mutate(
    `Admin 2` = formatter("span", style = ~ formattable::style(
      color = ifelse(`Admin 2` == NA, "red", "grey"),
      font.weight = "bold",font.style = "italic"))(`Admin 2`),
    `Followed up - signs`= color_tile("white", "orange")(`Followed up - signs`),
    `Followed up - no signs`= color_tile("white", "#A0E2BD")(`Followed up - no signs`),
    `Became case`= color_tile("white", "grey")(`Became case`),
    `Lost to follow up`= color_tile("white", "grey")(`Lost to follow up`), 
    `Never seen`= color_tile("white", "red")(`Never seen`),
    `Active contacts` = color_tile("white", "#81A4CE")(`Active contacts`)
  ) %>%
  kable("html", escape = F, align =c("l","c","c","c","c","c","c","c","c","c","c")) %>%
  kable_styling("hover", full_width = FALSE) %>%
  add_header_above(c(" " = 3, 
                     "Of contacts currently under follow up" = 5,
                     "Status of last visit" = 3)) %>% 
  kableExtra::footnote(general = str_glue("Data are current to {format(report_date, '%b %d %Y')}"))

```


## Transmission Matrices  

As discussed in the [Heat plots] page, you can create a matrix of "who infected whom" using `geom_tile()`.

When new contacts are created, Go.Data stores this relationship information in the `relationships` API endpoint; and we can see the first 50 rows of this dataset below. This means that we can create a heat plot with relatively few steps given each contact is already joined to it's source case.

```{r, warning=F, message=F, echo=F}
# display the first 50 rows of relationships data as a table
DT::datatable(head(relationships, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

As done above for the age pyramid comparing cases and contacts, we can select the few variables we need and create columns with categorical age groupings for both sources (cases) and targets (contacts).

```{r}
heatmap_ages <- relationships %>% 
  select(source_age, target_age) %>% 
  mutate(                              # transmute is like mutate() but removes all other columns
    source_age_class = epikit::age_categories(source_age, breakers = seq(0, 80, 5)),
    target_age_class = epikit::age_categories(target_age, breakers = seq(0, 80, 5))) 
```

As described previously, we create cross-tabulation; 

```{r, warning=F, message=FALSE}

cross_tab <- table(
  source_cases = heatmap_ages$source_age_class,
  target_cases = heatmap_ages$target_age_class)

cross_tab
```

convert into long format with proportions;

```{r, warning=FALSE, message=FALSE}

long_prop <- data.frame(prop.table(cross_tab))

```

and create a heat-map for age.


```{r, warning=F, message=F}

ggplot(data = long_prop)+       # use long data, with proportions as Freq
  geom_tile(                    # visualize it in tiles
    aes(
      x = target_cases,         # x-axis is case age
      y = source_cases,     # y-axis is infector age
      fill = Freq))+            # color of the tile is the Freq column in the data
  scale_fill_gradient(          # adjust the fill color of the tiles
    low = "blue",
    high = "orange")+
  theme(axis.text.x = element_text(angle = 90))+
  labs(                         # labels
    x = "Target case age",
    y = "Source case age",
    title = "Who infected whom",
    subtitle = "Frequency matrix of transmission events",
    fill = "Proportion of all\ntranmsission events"     # legend title
  )

```


## Resources  

https://github.com/WorldHealthOrganization/godata/tree/master/analytics/r-reporting

https://worldhealthorganization.github.io/godata/

https://community-godata.who.int/
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/contact_tracing.Rmd-->


# Phân tích khảo sát {#survey-analysis}  

<!-- ======================================================= -->
## Overview {  }

This page demonstrates the use of several packages for survey analysis. 


Most survey R packages rely on the [**survey** package](https://cran.r-project.org/web/packages/survey/index.html) 
for doing weighted analysis. 
We will use **survey** as well as [**srvyr**](https://cran.r-project.org/web/packages/srvyr/index.html) 
(a wrapper for **survey** allowing for tidyverse-style coding) and
[**gtsummary**](https://cran.r-project.org/web/packages/gtsummary/index.html) 
(a wrapper for **survey** allowing for publication ready tables). 
While the original **survey** package does not allow for tidyverse-style coding, 
it does have the added benefit of allowing for survey-weighted generalised linear 
models (which will be added to this page at a later date). 
We will also demonstrate using a function from the [**sitrep**](https://github.com/R4EPI/sitrep)
package to create sampling weights (*n.b* this package is currently not yet on CRAN, 
but can be installed from github).

Most of this page is based off work done for the ["R4Epis" project](https://r4epis.netlify.app/); 
for detailed code and R-markdown templates see the ["R4Epis" github page](https://github.com/R4EPI/sitrep). 
Some of the **survey** package based code is based off early versions of 
[EPIET case studies](https://github.com/EPIET/RapidAssessmentSurveys).

At current this page does not address sample size calculations or sampling. 
For a simple to use sample size calculator see [OpenEpi](https://www.openepi.com/Menu/OE_Menu.htm). 
The [GIS basics](https://epirhandbook.com/gis-basics.html) page of the handbook 
will eventually have a section on spatial random sampling, and this page will 
eventually have a section on sampling frames as well as sample size calculations. 



1.  Survey data 
2.  Observation time 
3.  Weighting 
4.  Survey design objects
5.  Descriptive analysis 
6.  Weighted proportions
7.  Weighted rates 


<!-- ======================================================= -->
## Preparation {  }

### Packages {.unnumbered}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  
Here we also demonstrate using the `p_load_gh()` function from **pacman** to install a load a package from github which has not yet been published on CRAN. 

```{r}

## load packages from CRAN
pacman::p_load(rio,          # File import
               here,         # File locator
               tidyverse,    # data management + ggplot2 graphics
               tsibble,      # handle time series datasets
               survey,       # for survey functions
               srvyr,        # dplyr wrapper for survey package
               gtsummary,    # wrapper for survey package to produce tables
               apyramid,     # a package dedicated to creating age pyramids
               patchwork,    # for combining ggplots
               ggforce       # for alluvial/sankey plots
               ) 

## load packages from github
pacman::p_load_gh(
     "R4EPI/sitrep"          # for observation time / weighting functions
)

``` 

### Load data {.unnumbered}

The example dataset used in this section:

-   fictional mortality survey data.
-   fictional population counts for the survey area. 
-   data dictionary for the fictional mortality survey data. 

This is based off the MSF OCA ethical review board pre-approved survey. The 
fictional dataset was produced as part of the ["R4Epis" project](https://r4epis.netlify.app/). 
This is all based off data collected using [KoboToolbox](https://www.kobotoolbox.org/), 
which is a data collection software based off [Open Data Kit](https://opendatakit.org/).

Kobo allows you to export both the collected data, as well as the data dictionary 
for that dataset. We strongly recommend doing this as it simplifies data cleaning 
and is useful for looking up variables/questions. 


<span style="color: darkgreen;">**_TIP:_** The Kobo data dictionary has variable
names in the "name" column of the survey sheet. 
Possible values for each variable are specified in choices sheet. 
In the choices tab, "name" has the shortened value and the "label::english" and 
"label::french" columns have the appropriate long versions. 
Using the **epidict** package `msf_dict_survey()` function to import a Kobo 
dictionary excel file will re-format this for you so it can be used easily to recode.  </span>

<span style="color: orange;">**_CAUTION:_** The example dataset is not the same 
as an export (as in Kobo you export different questionnaire levels individually) 
- see the survey data section below to merge the different levels.</span>


The dataset is imported using the `import()` function from the **rio** package. See the page on [Import and export](https://epirhandbook.com/import-and-export.html) for various ways to import data.

```{r echo = FALSE}
# import the survey into R
survey_data <- rio::import(here::here("data", "surveys", "survey_data.xlsx"))

# import the dictionary into R
survey_dict <- rio::import(here::here("data", "surveys", "survey_dict.xlsx")) 

# import the population in to R 
population <- rio::import(here::here("data", "surveys", "population.xlsx"))
```

```{r eval = FALSE}
# import the survey data
survey_data <- rio::import("survey_data.xlsx")

# import the dictionary into R
survey_dict <- rio::import("survey_dict.xlsx") 
```

The first 10 rows of the survey are displayed below.

```{r, message = FALSE, echo = FALSE}
# display the survey data as a table
DT::datatable(head(survey_data, 10), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

We also want to import the data on sampling population so that we can produce 
appropriate weights. This data can be in different formats, however we would 
suggest to have it as seen below (this can just be typed in to an excel). 


```{r read_data_pop_show, eval = FALSE}
# import the population data
population <- rio::import("population.xlsx")
```

The first 10 rows of the survey are displayed below.

```{r message=FALSE, echo=F}
# display the survey data as a table
DT::datatable(head(population, 10), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

For cluster surveys you may want to add survey weights at the cluster level. 
You could read this data in as above. 
Alternatively if there are only a few counts, these could be entered as below
in to a tibble. 
In any case you will need to have one column with a cluster identifier which 
matches your survey data, and another column with the number of households in 
each cluster. 

```{r cluster_counts}

## define the number of households in each cluster
cluster_counts <- tibble(cluster = c("village_1", "village_2", "village_3", "village_4", 
                                     "village_5", "village_6", "village_7", "village_8",
                                     "village_9", "village_10"), 
                         households = c(700, 400, 600, 500, 300, 
                                        800, 700, 400, 500, 500))

```

### Clean data {.unnumbered}

The below makes sure that the date column is in the appropriate format. 
There are several other ways of doing this (see the [Working with dates](https://epirhandbook.com/working-with-dates.html)
page for details), however using the dictionary to define dates is quick and easy. 

We also create an age group variable using the `age_categories()` function from 
**epikit** - see [cleaning data](https://epirhandbook.com/cleaning-data-and-core-functions.html#num_cats)
handbook section for details. 
In addition, we create a character variable defining which district the various clusters
are in. 

Finally, we recode all of the yes/no variables to TRUE/FALSE variables - otherwise
these cant be used by the **survey** proportion functions. 

```{r cleaning}

## select the date variable names from the dictionary 
DATEVARS <- survey_dict %>% 
  filter(type == "date") %>% 
  filter(name %in% names(survey_data)) %>% 
  ## filter to match the column names of your data
  pull(name) # select date vars
  
## change to dates 
survey_data <- survey_data %>%
  mutate(across(all_of(DATEVARS), as.Date))


## add those with only age in months to the year variable (divide by twelve)
survey_data <- survey_data %>% 
  mutate(age_years = if_else(is.na(age_years), 
                             age_months / 12, 
                             age_years))

## define age group variable
survey_data <- survey_data %>% 
     mutate(age_group = age_categories(age_years, 
                                    breakers = c(0, 3, 15, 30, 45)
                                    ))


## create a character variable based off groups of a different variable 
survey_data <- survey_data %>% 
  mutate(health_district = case_when(
    cluster_number %in% c(1:5) ~ "district_a", 
    TRUE ~ "district_b"
  ))


## select the yes/no variable names from the dictionary 
YNVARS <- survey_dict %>% 
  filter(type == "yn") %>% 
  filter(name %in% names(survey_data)) %>% 
  ## filter to match the column names of your data
  pull(name) # select yn vars
  
## change to dates 
survey_data <- survey_data %>%
  mutate(across(all_of(YNVARS), 
                str_detect, 
                pattern = "yes"))

```



<!-- ======================================================= -->
## Survey data {  }

There numerous different sampling designs that can be used for surveys. Here 
we will demonstrate code for: 
- Stratified 
- Cluster 
- Stratified and cluster 

As described above (depending on how you design your questionnaire) the data for 
each level would be exported as a separate dataset from Kobo. In our example there
is one level for households and one level for individuals within those households. 

These two levels are linked by a unique identifier. 
For a Kobo dataset this variable is "_index" at the household level, which 
matches the "_parent_index" at the individual level.
This will create new rows for household with each matching individual, 
see the handbook section on [joining](https://epirhandbook.com/joining-data.html)
for details. 

```{r merge_data_levels, eval = FALSE}

## join the individual and household data to form a complete data set
survey_data <- left_join(survey_data_hh, 
                         survey_data_indiv,
                         by = c("_index" = "_parent_index"))


## create a unique identifier by combining indeces of the two levels 
survey_data <- survey_data %>% 
     mutate(uid = str_glue("{index}_{index_y}"))

```

<!-- ======================================================= -->
## Observation time {  }

For mortality surveys we want to now how long each individual was present for in 
the location to be able to calculate an appropriate mortality rate for our period 
of interest. This is not relevant to all surveys, but particularly for mortality
surveys this is important as they are conducted frequently among mobile or displaced
populations. 

To do this we first define our time period of interest, also known as a recall 
period (i.e. the time that participants are asked to report on when answering 
questions). 
We can then use this period to set inappropriate dates to missing, i.e. if deaths
are reported from outside the period of interest. 

```{r recall_period}

## set the start/end of recall period
## can be changed to date variables from dataset 
## (e.g. arrival date & date questionnaire)
survey_data <- survey_data %>% 
  mutate(recall_start = as.Date("2018-01-01"), 
         recall_end   = as.Date("2018-05-01")
  )


# set inappropriate dates to NA based on rules 
## e.g. arrivals before start, departures departures after end
survey_data <- survey_data %>%
      mutate(
           arrived_date = if_else(arrived_date < recall_start, 
                                 as.Date(NA),
                                  arrived_date),
           birthday_date = if_else(birthday_date < recall_start,
                                  as.Date(NA),
                                  birthday_date),
           left_date = if_else(left_date > recall_end,
                              as.Date(NA),
                               left_date),
           death_date = if_else(death_date > recall_end,
                               as.Date(NA),
                               death_date)
           )

```


We can then use our date variables to define start and end dates for each individual. 
We can use the `find_start_date()` function from **sitrep** to fine the causes for 
the dates and then use that to calculate the difference between days (person-time). 

start date: 
Earliest appropriate arrival event within your recall period
Either the beginning of your recall period (which you define in advance), or a 
date after the start of recall if applicable (e.g. arrivals or births)

end date: 
Earliest appropriate departure event within your recall period
Either the end of your recall period, or a date before the end of recall 
if applicable (e.g. departures, deaths)

```{r observation_time}

## create new variables for start and end dates/causes
survey_data <- survey_data %>% 
     ## choose earliest date entered in survey
     ## from births, household arrivals, and camp arrivals 
     find_start_date("birthday_date",
                  "arrived_date",
                  period_start = "recall_start",
                  period_end   = "recall_end",
                  datecol      = "startdate",
                  datereason   = "startcause" 
                 ) %>%
     ## choose earliest date entered in survey
     ## from camp departures, death and end of the study
     find_end_date("left_date",
                "death_date",
                period_start = "recall_start",
                period_end   = "recall_end",
                datecol      = "enddate",
                datereason   = "endcause" 
               )


## label those that were present at the start/end (except births/deaths)
survey_data <- survey_data %>% 
     mutate(
       ## fill in start date to be the beginning of recall period (for those empty) 
       startdate = if_else(is.na(startdate), recall_start, startdate), 
       ## set the start cause to present at start if equal to recall period 
       ## unless it is equal to the birth date 
       startcause = if_else(startdate == recall_start & startcause != "birthday_date",
                              "Present at start", startcause), 
       ## fill in end date to be end of recall period (for those empty) 
       enddate = if_else(is.na(enddate), recall_end, enddate), 
       ## set the end cause to present at end if equall to recall end 
       ## unless it is equal to the death date
       endcause = if_else(enddate == recall_end & endcause != "death_date", 
                            "Present at end", endcause))


## Define observation time in days
survey_data <- survey_data %>% 
  mutate(obstime = as.numeric(enddate - startdate))

```


<!-- ======================================================= -->
## Weighting {  }

It is important that you drop erroneous observations before adding survey weights. 
For example if you have observations with negative observation time, you will need
to check those (you can do this with the `assert_positive_timespan()` function 
from **sitrep**. 
Another thing is if you want to drop empty rows (e.g. with `drop_na(uid)`)
or remove duplicates (see handbook section on [De-duplication] 
for details). 
Those without consent need to be dropped too. 

In this example we filter for the cases we want to drop and store them in a separate
data frame - this way we can describe those that were excluded from the survey. 
We then use the `anti_join()` function from **dplyr** to remove these dropped cases
from our survey data. 

<span style="color: red;">**_DANGER:_** You cant have missing values in your weight variable, or any of the variables relevant to your survey design (e.g. age, sex, strata or cluster variables).</span>  

```{r remove_unused_data}

## store the cases that you drop so you can describe them (e.g. non-consenting 
## or wrong village/cluster)
dropped <- survey_data %>% 
  filter(!consent | is.na(startdate) | is.na(enddate) | village_name == "other")

## use the dropped cases to remove the unused rows from the survey data set  
survey_data <- anti_join(survey_data, dropped, by = names(dropped))

```

As mentioned above we demonstrate how to add weights for three different study 
designs (stratified, cluster and stratified cluster). These require information 
on the source population and/or the clusters surveyed. 
We will use the stratified cluster code for this example, but use whichever is
most appropriate for your study design. 

```{r survey_weights}

# stratified ------------------------------------------------------------------
# create a variable called "surv_weight_strata"
# contains weights for each individual - by age group, sex and health district
survey_data <- add_weights_strata(x = survey_data,
                                         p = population,
                                         surv_weight = "surv_weight_strata",
                                         surv_weight_ID = "surv_weight_ID_strata",
                                         age_group, sex, health_district)

## cluster ---------------------------------------------------------------------

# get the number of people of individuals interviewed per household
# adds a variable with counts of the household (parent) index variable
survey_data <- survey_data %>%
  add_count(index, name = "interviewed")


## create cluster weights
survey_data <- add_weights_cluster(x = survey_data,
                                          cl = cluster_counts,
                                          eligible = member_number,
                                          interviewed = interviewed,
                                          cluster_x = village_name,
                                          cluster_cl = cluster,
                                          household_x = index,
                                          household_cl = households,
                                          surv_weight = "surv_weight_cluster",
                                          surv_weight_ID = "surv_weight_ID_cluster",
                                          ignore_cluster = FALSE,
                                          ignore_household = FALSE)


# stratified and cluster ------------------------------------------------------
# create a survey weight for cluster and strata
survey_data <- survey_data %>%
  mutate(surv_weight_cluster_strata = surv_weight_strata * surv_weight_cluster)

```


<!-- ======================================================= -->
## Survey design objects {  }

Create survey object according to your study design. 
Used the same way as data frames to calculate weight proportions etc. 
Make sure that all necessary variables are created before this. 

There are four options, comment out those you do not use: 
- Simple random 
- Stratified 
- Cluster 
- Stratified cluster

For this template - we will pretend that we cluster surveys in two separate 
strata (health districts A and B). 
So to get overall estimates we need have combined cluster and strata weights. 

As mentioned previously, there are two packages available for doing this. The 
classic one is **survey** and then there is a wrapper package called **srvyr** 
that makes tidyverse-friendly objects and functions. We will demonstrate both, 
but note that most of the code in this chapter will use **srvyr** based objects. 
The one exception is that the **gtsummary** package only accepts **survey** objects. 

### **Survey** package  

The **survey** package effectively uses **base** *R* coding, and so it is not 
possible to use pipes (`%>%`) or other **dplyr** syntax. 
With the **survey** package we use the `svydesign()` function to define a survey
object with appropriate clusters, weights and strata. 

<span style="color: black;">**_NOTE:_** we need to use the tilde (`~`) in front of variables, this is because the package uses the **base** *R* syntax of assigning variables based on formulae. </span>

```{r survey_design}

# simple random ---------------------------------------------------------------
base_survey_design_simple <- svydesign(ids = ~1, # 1 for no cluster ids
                   weights = NULL,               # No weight added
                   strata = NULL,                # sampling was simple (no strata)
                   data = survey_data            # have to specify the dataset
                  )

## stratified ------------------------------------------------------------------
base_survey_design_strata <- svydesign(ids = ~1,  # 1 for no cluster ids
                   weights = ~surv_weight_strata, # weight variable created above
                   strata = ~health_district,     # sampling was stratified by district
                   data = survey_data             # have to specify the dataset
                  )

# cluster ---------------------------------------------------------------------
base_survey_design_cluster <- svydesign(ids = ~village_name, # cluster ids
                   weights = ~surv_weight_cluster, # weight variable created above
                   strata = NULL,                 # sampling was simple (no strata)
                   data = survey_data              # have to specify the dataset
                  )

# stratified cluster ----------------------------------------------------------
base_survey_design <- svydesign(ids = ~village_name,      # cluster ids
                   weights = ~surv_weight_cluster_strata, # weight variable created above
                   strata = ~health_district,             # sampling was stratified by district
                   data = survey_data                     # have to specify the dataset
                  )
```



### **Srvyr** package  

With the **srvyr** package we can use the `as_survey_design()` function, which 
has all the same arguments as above but allows pipes (`%>%`), and so we do not 
need to use the tilde (`~`). 

```{r survey_design_srvyr}
## simple random ---------------------------------------------------------------
survey_design_simple <- survey_data %>% 
  as_survey_design(ids = 1, # 1 for no cluster ids 
                   weights = NULL, # No weight added
                   strata = NULL # sampling was simple (no strata)
                  )
## stratified ------------------------------------------------------------------
survey_design_strata <- survey_data %>%
  as_survey_design(ids = 1, # 1 for no cluster ids
                   weights = surv_weight_strata, # weight variable created above
                   strata = health_district # sampling was stratified by district
                  )
## cluster ---------------------------------------------------------------------
survey_design_cluster <- survey_data %>%
  as_survey_design(ids = village_name, # cluster ids
                   weights = surv_weight_cluster, # weight variable created above
                   strata = NULL # sampling was simple (no strata)
                  )

## stratified cluster ----------------------------------------------------------
survey_design <- survey_data %>%
  as_survey_design(ids = village_name, # cluster ids
                   weights = surv_weight_cluster_strata, # weight variable created above
                   strata = health_district # sampling was stratified by district
                  )
```

<!-- ======================================================= -->
## Descriptive analysis {  }

Basic descriptive analysis and visualisation is covered extensively in other 
chapters of the handbook, so we will not dwell on it here. 
For details see the chapters on [descriptive tables](https://epirhandbook.com/descriptive-tables.html), 
[statistical tests](https://epirhandbook.com/simple-statistical-tests.html), 
[tables for presentation](https://epirhandbook.com/tables-for-presentation.html), 
[ggplot basics](https://epirhandbook.com/ggplot-basics.html) and 
[R markdown reports](https://epirhandbook.com/r-markdown-reports.html). 

In this section we will focus on how to investigate bias in your sample and visualise this. 
We will also look at visualising population flow in a survey setting using 
alluvial/sankey diagrams. 

In general, you should consider including the following descriptive analyses:  

- Final number of clusters, households and individuals included  
- Number of excluded individuals and the reasons for exclusion 
- Median (range) number of households per cluster and individuals per household 


### Sampling bias 

Compare the proportions in each age group between your sample and 
the source population. 
This is important to be able to highlight potential sampling bias. 
You could similarly repeat this looking at distributions by sex. 

Note that these p-values are just indicative, and a descriptive discussion (or
visualisation with age-pyramids below) of the distributions in your study sample 
compared to the source population is more important than the binomial test itself.
This is because increasing sample size will more often than not lead to 
differences that may be irrelevant after weighting your data.

```{r descriptive_sampling_bias, warning = FALSE}

## counts and props of the study population
ag <- survey_data %>% 
  group_by(age_group) %>% 
  drop_na(age_group) %>% 
  tally() %>% 
  mutate(proportion = n / sum(n), 
         n_total = sum(n))

## counts and props of the source population
propcount <- population %>% 
  group_by(age_group) %>%
    tally(population) %>%
    mutate(proportion = n / sum(n))

## bind together the columns of two tables, group by age, and perform a 
## binomial test to see if n/total is significantly different from population
## proportion.
  ## suffix here adds to text to the end of columns in each of the two datasets
left_join(ag, propcount, by = "age_group", suffix = c("", "_pop")) %>%
  group_by(age_group) %>%
  ## broom::tidy(binom.test()) makes a data frame out of the binomial test and
  ## will add the variables p.value, parameter, conf.low, conf.high, method, and
  ## alternative. We will only use p.value here. You can include other
  ## columns if you want to report confidence intervals
  mutate(binom = list(broom::tidy(binom.test(n, n_total, proportion_pop)))) %>%
  unnest(cols = c(binom)) %>% # important for expanding the binom.test data frame
  mutate(proportion_pop = proportion_pop * 100) %>%
  ## Adjusting the p-values to correct for false positives 
  ## (because testing multiple age groups). This will only make 
  ## a difference if you have many age categories
  mutate(p.value = p.adjust(p.value, method = "holm")) %>%
                      
  ## Only show p-values over 0.001 (those under report as <0.001)
  mutate(p.value = ifelse(p.value < 0.001, 
                          "<0.001", 
                          as.character(round(p.value, 3)))) %>% 
  
  ## rename the columns appropriately
  select(
    "Age group" = age_group,
    "Study population (n)" = n,
    "Study population (%)" = proportion,
    "Source population (n)" = n_pop,
    "Source population (%)" = proportion_pop,
    "P-value" = p.value
  )
```



### Demographic pyramids 

Demographic (or age-sex) pyramids are an easy way of visualising the distribution
in your survey population. It is also worth considering creating 
[descriptive tables](https://epirhandbook.com/descriptive-tables.html) of age 
and sex by survey strata. 
We will demonstrate using the **apyramid** package as it allows for weighted 
proportions using our survey design object created above. Other options for creating
[demographic pyramids](https://epirhandbook.com/demographic-pyramids-and-likert-scales.html)
are covered extensively in that chapter of the handbook. We will also use a 
wrapper function from **sitrep** called `plot_age_pyramid()` which saves a few lines
of coding for producing a plot with proportions. 

As with the formal binomial test of difference, seen above in the sampling bias 
section, we are interested here in visualising whether our sampled population 
is substantially different from the source population and whether weighting corrects
this difference. To do this we will use the **patchwork** package to show our 
**ggplot** visualisations side-by-side; for details see the section on 
combining plots in [ggplot tips](https://epirhandbook.com/ggplot-tips.html?q=patch#combine-plots)
chapter of the handbook. 
We will visualise our source population, our un-weighted survey population and 
our weighted survey population.
You may also consider visualising by each strata of your survey - in our example 
here that would be by using the argument `stack_by  = "health_district"` 
(see `?plot_age_pyramid` for details). 

<span style="color: black;">**_NOTE:_** The x and y axes are flipped in pyramids </span>

```{r weighted_age_pyramid, warning = FALSE, message = FALSE, fig.show = "hold", fig.width = 15}

## define x-axis limits and labels ---------------------------------------------
## (update these numbers to be the values for your graph)
max_prop <- 35      # choose the highest proportion you want to show 
step <- 5           # choose the space you want beween labels 

## this part defines vector using the above numbers with axis breaks
breaks <- c(
    seq(max_prop/100 * -1, 0 - step/100, step/100), 
    0, 
    seq(0 + step / 100, max_prop/100, step/100)
    )

## this part defines vector using the above numbers with axis limits
limits <- c(max_prop/100 * -1, max_prop/100)

## this part defines vector using the above numbers with axis labels
labels <-  c(
      seq(max_prop, step, -step), 
      0, 
      seq(step, max_prop, step)
    )


## create plots individually  --------------------------------------------------

## plot the source population 
## nb: this needs to be collapsed for the overall population (i.e. removing health districts)
source_population <- population %>%
  ## ensure that age and sex are factors
  mutate(age_group = factor(age_group, 
                            levels = c("0-2", 
                                       "3-14", 
                                       "15-29",
                                       "30-44", 
                                       "45+")), 
         sex = factor(sex)) %>% 
  group_by(age_group, sex) %>% 
  ## add the counts for each health district together 
  summarise(population = sum(population)) %>% 
  ## remove the grouping so can calculate overall proportion
  ungroup() %>% 
  mutate(proportion = population / sum(population)) %>% 
  ## plot pyramid 
  age_pyramid(
            age_group = age_group, 
            split_by = sex, 
            count = proportion, 
            proportional = TRUE) +
  ## only show the y axis label (otherwise repeated in all three plots)
  labs(title = "Source population", 
       y = "", 
       x = "Age group (years)") + 
  ## make the x axis the same for all plots 
  scale_y_continuous(breaks = breaks, 
    limits = limits, 
    labels = labels)
  
  
## plot the unweighted sample population 
sample_population <- plot_age_pyramid(survey_data, 
                 age_group = "age_group", 
                 split_by = "sex",
                 proportion = TRUE) + 
  ## only show the x axis label (otherwise repeated in all three plots)
  labs(title = "Unweighted sample population", 
       y = "Proportion (%)", 
       x = "") + 
  ## make the x axis the same for all plots 
  scale_y_continuous(breaks = breaks, 
    limits = limits, 
    labels = labels)


## plot the weighted sample population 
weighted_population <- survey_design %>% 
  ## make sure the variables are factors
  mutate(age_group = factor(age_group), 
         sex = factor(sex)) %>%
  plot_age_pyramid(
    age_group = "age_group",
    split_by = "sex", 
    proportion = TRUE) +
  ## only show the x axis label (otherwise repeated in all three plots)
  labs(title = "Weighted sample population", 
       y = "", 
       x = "")  + 
  ## make the x axis the same for all plots 
  scale_y_continuous(breaks = breaks, 
    limits = limits, 
    labels = labels)

## combine all three plots  ----------------------------------------------------
## combine three plots next to eachother using + 
source_population + sample_population + weighted_population + 
  ## only show one legend and define theme 
  ## note the use of & for combining theme with plot_layout()
  plot_layout(guides = "collect") & 
  theme(legend.position = "bottom",                    # move legend to bottom
        legend.title = element_blank(),                # remove title
        text = element_text(size = 18),                # change text size
        axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1) # turn x-axis text
       )
```


### Alluvial/sankey diagram

Visualising starting points and outcomes for individuals can be very helpful to 
get an overview. There is quite an obvious application for mobile populations, 
however there are numerous other applications such as cohorts or any other situation
where there are transitions in states for individuals. These diagrams have several
different names including alluvial, sankey and parallel sets - the details are 
in the handbook chapter on [diagrams and charts](https://epirhandbook.com/diagrams-and-charts.html#alluvialsankey-diagrams). 


```{r visualise_population_flow}

## summarize data
flow_table <- survey_data %>%
  count(startcause, endcause, sex) %>%  # get counts 
  gather_set_data(x = c("startcause", "endcause")) %>%     # change format for plotting
  mutate(x = fct_relevel(x, c("startcause", "endcause")),  # set startcause as first level
         x = fct_recode(x, 
                        "Start \n cause" = "startcause",   # add line break (\n) after start
                        "End \n cause"   = "endcause")
        )


## plot your dataset 
  ## on the x axis is the start and end causes
  ## gather_set_data generates an ID for each possible combination
  ## splitting by y gives the possible start/end combos
  ## value as n gives it as counts (could also be changed to proportion)
ggplot(flow_table, aes(x, id = id, split = y, value = n)) +
  ## colour lines by sex 
  geom_parallel_sets(aes(fill = sex), alpha = 0.5, axis.width = 0.2) +
  ## fill in the label boxes grey
  geom_parallel_sets_axes(axis.width = 0.15, fill = "grey80", color = "grey80") +
  ## change text colour and angle (needs to be adjusted)
  geom_parallel_sets_labels(color = "black", angle = 0, size = 5) +
  ## adjusted y and x axes (probably needs more vertical space)
  scale_x_discrete(name = NULL, expand = c(0, 0.2)) + 
  ## remove axis labels
  theme(
    title = element_text(size = 26),
    text = element_text(size = 26),
    axis.line = element_blank(),
    axis.ticks = element_blank(),
    axis.text.y = element_blank(),
    panel.background = element_blank(),
    legend.position = "bottom",                    # move legend to bottom
    legend.title = element_blank(),                # remove title
  )

```


<!-- ======================================================= -->
## Weighted proportions {  }

This section will detail how to produce tables for weighted counts and proportions,
with associated confidence intervals and design effect. 
There are four different options using functions from the following packages: 
**survey**, **srvyr**, **sitrep** and **gtsummary**. 
For minimal coding to produce a standard epidemiology style table, we would 
recommend the **sitrep** function - which is a wrapper for **srvyr** code; note 
however that this is not yet on CRAN and may change in the future. 
Otherwise, the **survey** code is likely to be the most stable long-term, whereas 
**srvyr** will fit most nicely within tidyverse work-flows. While **gtsummary** 
functions hold a lot of potential, they appear to be experimental and incomplete
at the time of writing. 


### **Survey** package 

We can use the `svyciprop()` function from **survey** to get weighted proportions 
and accompanying 95% confidence intervals. An appropriate design effect can be 
extracted using the `svymean()` rather than `svyprop()` function. 
It is worth noting that `svyprop()` only appears to accept variables between 0 and
1 (or TRUE/FALSE), so categorical variables will not work.

<span style="color: black;">**_NOTE:_** Functions from **survey** also accept **srvyr** design objects, but here we have used the **survey** design object just for consistency </span>


```{r survey_props}

## produce weighted counts 
svytable(~died, base_survey_design)

## produce weighted proportions
svyciprop(~died, base_survey_design, na.rm = T)

## get the design effect 
svymean(~died, base_survey_design, na.rm = T, deff = T) %>% 
  deff()

```

We can combine the functions from **survey** shown above in to a function which 
we define ourselves below, called `svy_prop`; and we can then use that function 
together with `map()` from the purrr package to iterate over several variables 
and create a table. See the handbook [iteration](https://epirhandbook.com/iteration-loops-and-lists.html) 
chapter for details on **purrr**. 

```{r survey_prop_fun}
# Define function to calculate weighted counts, proportions, CI and design effect
# x is the variable in quotation marks 
# design is your survey design object

svy_prop <- function(design, x) {
  
  ## put the variable of interest in a formula 
  form <- as.formula(paste0( "~" , x))
  ## only keep the TRUE column of counts from svytable
  weighted_counts <- svytable(form, design)[[2]]
  ## calculate proportions (multiply by 100 to get percentages)
  weighted_props <- svyciprop(form, design, na.rm = TRUE) * 100
  ## extract the confidence intervals and multiply to get percentages
  weighted_confint <- confint(weighted_props) * 100
  ## use svymean to calculate design effect and only keep the TRUE column
  design_eff <- deff(svymean(form, design, na.rm = TRUE, deff = TRUE))[[TRUE]]
  
  ## combine in to one data frame
  full_table <- cbind(
    "Variable"        = x,
    "Count"           = weighted_counts,
    "Proportion"      = weighted_props,
    weighted_confint, 
    "Design effect"   = design_eff
    )
  
  ## return table as a dataframe
  full_table <- data.frame(full_table, 
             ## remove the variable names from rows (is a separate column now)
             row.names = NULL)
  
  ## change numerics back to numeric
  full_table[ , 2:6] <- as.numeric(full_table[, 2:6])
  
  ## return dataframe
  full_table
}

## iterate over several variables to create a table 
purrr::map(
  ## define variables of interest
  c("left", "died", "arrived"), 
  ## state function using and arguments for that function (design)
  svy_prop, design = base_survey_design) %>% 
  ## collapse list in to a single data frame
  bind_rows() %>% 
  ## round 
  mutate(across(where(is.numeric), round, digits = 1))

```



### **Srvyr** package 

With **srvyr** we can use **dplyr** syntax to create a table. Note that the 
`survey_mean()` function is used and the proportion argument is specified, and 
also that the same function is used to calculate design effect. This is because 
**srvyr** wraps around both of the **survey** package functions `svyciprop()` and 
`svymean()`, which are used in the above section. 

<span style="color: black;">**_NOTE:_** It does not seem to be possible to get proportions from categorical variables using **srvyr** either, if you need this then check out the section below using **sitrep** </span>

```{r srvyr_prop}

## use the srvyr design object
survey_design %>% 
  summarise(
    ## produce the weighted counts 
    counts = survey_total(died), 
    ## produce weighted proportions and confidence intervals 
    ## multiply by 100 to get a percentage 
    props = survey_mean(died, 
                        proportion = TRUE, 
                        vartype = "ci") * 100, 
    ## produce the design effect 
    deff = survey_mean(died, deff = TRUE)) %>% 
  ## only keep the rows of interest
  ## (drop standard errors and repeat proportion calculation)
  select(counts, props, props_low, props_upp, deff_deff)

```

Here too we could write a function to then iterate over multiple variables using
the **purrr** package. 
See the handbook [iteration](https://epirhandbook.com/iteration-loops-and-lists.html) 
chapter for details on **purrr**. 

```{r srvyr_prop_fun}

# Define function to calculate weighted counts, proportions, CI and design effect
# design is your survey design object
# x is the variable in quotation marks 


srvyr_prop <- function(design, x) {
  
  summarise(
    ## using the survey design object
    design, 
    ## produce the weighted counts 
    counts = survey_total(.data[[x]]), 
    ## produce weighted proportions and confidence intervals 
    ## multiply by 100 to get a percentage 
    props = survey_mean(.data[[x]], 
                        proportion = TRUE, 
                        vartype = "ci") * 100, 
    ## produce the design effect 
    deff = survey_mean(.data[[x]], deff = TRUE)) %>% 
  ## add in the variable name
  mutate(variable = x) %>% 
  ## only keep the rows of interest
  ## (drop standard errors and repeat proportion calculation)
  select(variable, counts, props, props_low, props_upp, deff_deff)
  
}
  

## iterate over several variables to create a table 
purrr::map(
  ## define variables of interest
  c("left", "died", "arrived"), 
  ## state function using and arguments for that function (design)
  ~srvyr_prop(.x, design = survey_design)) %>% 
  ## collapse list in to a single data frame
  bind_rows()
  

```



### **Sitrep** package 

The `tab_survey()` function from **sitrep** is a wrapper for **srvyr**, allowing 
you to create weighted tables with minimal coding. It also allows you to calculate
weighted proportions for categorical variables. 

```{r sitrep_props}

## using the survey design object
survey_design %>% 
  ## pass the names of variables of interest unquoted
  tab_survey(arrived, left, died, education_level,
             deff = TRUE,   # calculate the design effect
             pretty = TRUE  # merge the proportion and 95%CI
             )

```



### **Gtsummary** package

With **gtsummary** there does not seem to be inbuilt functions yet to add confidence
intervals or design effect. 
Here we show how to define a function for adding confidence intervals and then 
add confidence intervals to a **gtsummary** table created using the `tbl_svysummary()` 
function. 


```{r gtsummary_table}


confidence_intervals <- function(data, variable, by, ...) {
  
  ## extract the confidence intervals and multiply to get percentages
  props <- svyciprop(as.formula(paste0( "~" , variable)),
              data, na.rm = TRUE)
  
  ## extract the confidence intervals 
  as.numeric(confint(props) * 100) %>% ## make numeric and multiply for percentage
    round(., digits = 1) %>%           ## round to one digit
    c(.) %>%                           ## extract the numbers from matrix
    paste0(., collapse = "-")          ## combine to single character
}

## using the survey package design object
tbl_svysummary(base_survey_design, 
               include = c(arrived, left, died),   ## define variables want to include
               statistic = list(everything() ~ c("{n} ({p}%)"))) %>% ## define stats of interest
  add_n() %>%  ## add the weighted total 
  add_stat(fns = everything() ~ confidence_intervals) %>% ## add CIs
  ## modify the column headers
  modify_header(
    list(
      n ~ "**Weighted total (N)**",
      stat_0 ~ "**Weighted Count**",
      add_stat_1 ~ "**95%CI**"
    )
    )

```



<!-- ======================================================= -->
## Weighted ratios {  }

Similarly for weighted ratios (such as for mortality ratios) you can use the 
**survey** or the **srvyr** package. 
You could similarly write functions (similar to those above) to iterate over 
several variables. You could also create a function for **gtsummary** as above
but currently it does not have inbuilt functionality. 


### **Survey** package 

```{r survey_ratio}

ratio <- svyratio(~died, 
         denominator = ~obstime, 
         design = base_survey_design)

ci <- confint(ratio)

cbind(
  ratio$ratio * 10000, 
  ci * 10000
)

```


### **Srvyr** package 

```{r srvyr_ratio}

survey_design %>% 
  ## survey ratio used to account for observation time 
  summarise(
    mortality = survey_ratio(
      as.numeric(died) * 10000, 
      obstime, 
      vartype = "ci")
    )

```




<!-- ======================================================= -->
## Resources {  }

[UCLA stats page](https://stats.idre.ucla.edu/r/seminars/survey-data-analysis-with-r/)  

[Analyze survey data free](http://asdfree.com/)  

[srvyr packge](http://gdfe.co/srvyr/)  

[gtsummary package](http://www.danieldsjoberg.com/gtsummary/reference/index.html) 

[EPIET survey case studies](https://github.com/EPIET/RapidAssessmentSurveys)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/survey_analysis.Rmd-->


<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
# Phân tích sống còn {#survival-analysis}  


```{r out.width = c('75%'), fig.align='center', fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "survival_analysis.png"))
```

<!-- ======================================================= -->
## Overview {}


*Survival analysis* focuses on describing for a given individual or group of individuals, a defined point of event called **_the failure_** (occurrence of a disease, cure from a disease, death, relapse after response to treatment...) that occurs after a period of time called **_failure time_** (or  **_follow-up time_** in cohort/population-based studies) during which individuals are observed. To determine the failure time, it is then necessary to define a time of origin (that can be the inclusion date, the date of diagnosis...). 

The target of inference for survival analysis is then the time between an origin and an event.
In current medical research, it is widely used in clinical studies to assess the effect of a treatment for instance, or in cancer epidemiology to assess a large variety of cancer survival measures. 


It is usually expressed through the **_survival probability_** which is the probability that the event of interest has not occurred by a duration t.


**_Censoring_**: Censoring occurs when at the end of follow-up, some of the individuals have not had the event of interest, and thus their true time to event is unknown. We will mostly focus on right censoring here but for more details on censoring and survival analysis in general, you can see references. 


```{r echo=F, eval=F, out.width = "80%", out.height="80%", fig.align = "center"}
 
#Add a figure from the following chunks for the last version of the page
#do not forget to save the output figure in "images"
# knitr::include_graphics(here::here("images", "survanalysis.png"))

```  

<!-- ======================================================= -->
## Preparation {  }

### Load packages {.unnumbered}  

To run survival analyses in R, one the most widely used package is the **survival** package. We first install it and then load it as well as the other packages that will be used in this section:

In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r, echo=F, message=FALSE, warning=FALSE}

# install/load the different packages needed for this page
pacman::p_load(
  survival,      # survival analysis 
  survminer,     # survival analysis
  rio,           # importing data  
  here,          # relative file pathways  
  janitor,       # tabulations
  SemiCompRisks, # dataset examples and advanced tools for working with Semi-Competing Risks data
  tidyverse,     # data manipulation and visualization
  Epi,           # stat analyses in Epi
  survival,      # survival analysis
  survminer      # survival analysis: advanced KM curves
)


```


This page explores survival analyses using the linelist used in most of the previous pages and on which we apply some changes to have a proper survival data.


### Import dataset {.unnumbered}  

We import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Import data with the `import()` function from the **rio** package (it handles many file types like .xlsx, .csv, .rds - see the [Import and export] page for details).  

```{r echo=F}
# import linelist
linelist_case_data <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r eval=F}
# import linelist
linelist_case_data <- rio::import("linelist_cleaned.rds")
```

### Data management and transformation {.unnumbered}

In short, survival data can be described as having the following three characteristics:

1) the dependent variable or response is the waiting time until the occurrence of a well-defined event,
2) observations are censored, in the sense that for some units the event of interest has not occurred at the time the data are analyzed, and 
3) there are predictors or explanatory variables whose effect on the waiting time we wish to assess or control. 

Thus, we will create different variables needed to respect that structure and run the survival analysis.

We define:

- a new data frame `linelist_surv` for this analysis  
- our event of interest as being "death" (hence our survival probability will be the probability of being alive after a certain time after the time of origin),
- the follow-up time (`futime`) as the time between the time of onset and the time of outcome *in days*,
- censored patients as those who recovered or for whom the final outcome is not known ie the event "death" was not observed (`event=0`).

<span style="color: orange;">**_CAUTION:_** Since in a real cohort study, the information on the time of origin and the end of the follow-up is known given individuals are observed, we will remove observations where the date of onset or the date of outcome is unknown. Also the cases where the date of onset is later than the date of outcome will be removed since they are considered as wrong.</span>

<span style="color: darkgreen;">**_TIP:_** Given that filtering to greater than (>) or less than (<) a date can remove rows with missing values, applying the filter on the wrong dates will also remove the rows with missing dates.</span>

We then use `case_when()` to create a column `age_cat_small` in which there are only 3 age categories.

```{r }
#create a new data called linelist_surv from the linelist_case_data

linelist_surv <-  linelist_case_data %>% 
     
  dplyr::filter(
       # remove observations with wrong or missing dates of onset or date of outcome
       date_outcome > date_onset) %>% 
  
  dplyr::mutate(
       # create the event var which is 1 if the patient died and 0 if he was right censored
       event = ifelse(is.na(outcome) | outcome == "Recover", 0, 1), 
    
       # create the var on the follow-up time in days
       futime = as.double(date_outcome - date_onset), 
    
       # create a new age category variable with only 3 strata levels
       age_cat_small = dplyr::case_when( 
            age_years < 5  ~ "0-4",
            age_years >= 5 & age_years < 20 ~ "5-19",
            age_years >= 20   ~ "20+"),
       
       # previous step created age_cat_small var as character.
       # now convert it to factor and specify the levels.
       # Note that the NA values remain NA's and are not put in a level "unknown" for example,
       # since in the next analyses they have to be removed.
       age_cat_small = fct_relevel(age_cat_small, "0-4", "5-19", "20+")
       )
```


<span style="color: darkgreen;">**_TIP:_** We can verify the new columns we have created by doing a summary on the `futime` and a cross-tabulation between `event` and `outcome` from which it was created. Besides this verification it is a good habit to communicate the median follow-up time when interpreting survival analysis results.</span>

```{r }

summary(linelist_surv$futime)

# cross tabulate the new event var and the outcome var from which it was created
# to make sure the code did what it was intended to
linelist_surv %>% 
  tabyl(outcome, event)
```

Now we cross-tabulate the new age_cat_small var and the old age_cat col to ensure correct assingments  

```{r}
linelist_surv %>% 
  tabyl(age_cat_small, age_cat)
```

Now we review the 10 first observations of the `linelist_surv` data looking at specific variables (including those newly created).  


```{r}
linelist_surv %>% 
  select(case_id, age_cat_small, date_onset, date_outcome, outcome, event, futime) %>% 
  head(10)
```

We can also cross-tabulate the columns `age_cat_small` and `gender` to have more details on the distribution of this new column by gender. We use `tabyl()` and the *adorn* functions from **janitor** as described in the [Descriptive tables] page. 

<!-- For this we use the `stat.table()` function of the **Epi** package. -->

```{r}

linelist_surv %>% 
  tabyl(gender, age_cat_small, show_na = F) %>% 
  adorn_totals(c("row", "col")) %>% 
  adorn_percentages() %>% 
  adorn_pct_formatting() %>% 
  adorn_ns(position = "front")

```

<!-- Epi::stat.table(  -->
<!--   #give variables for the cross tabulation -->
<!--   list( -->
<!--     gender,  -->
<!--     age_cat_small -->
<!--     ), -->

<!--   #precise the function you want to call (mean,count..) -->
<!--   list(  -->
<!--     count(), -->
<!--     percent(age_cat_small) -->
<!--     ),  -->

<!--   #add margins -->
<!--   margins=T,  -->

<!--   #data used -->
<!--   data = linelist_surv  -->
<!--   ) -->

<!-- ``` -->


<!-- ======================================================= -->
## Basics of survival analysis {}


### Building a surv-type object {.unnumbered}

We will first use `Surv()` from **survival** to build a survival object from the follow-up time and event columns.  

The result of such a step is to produce an object of type *Surv* that condenses the time information and whether the event of interest (death) was observed. This object will ultimately be used in the right-hand side of subsequent model formulae (see [documentation](https://cran.r-project.org/web/packages/survival/vignettes/survival.pdf)).  


```{r survobj }
# Use Suv() syntax for right-censored data
survobj <- Surv(time = linelist_surv$futime,
                event = linelist_surv$event)
```

<!-- ```{r} -->
<!-- survobj <- with(linelist_surv, -->

<!--                 survival::Surv(futime, event) -->

<!--                 ) -->
<!-- ``` -->


To review, here are the first 10 rows of the `linelist_surv` data, viewing only some important columns.  

```{r}
linelist_surv %>% 
  select(case_id, date_onset, date_outcome, futime, outcome, event) %>% 
  head(10)
```

And here are the first 10 elements of `survobj`. It prints as essentially a vector of follow-up time, with "+" to represent if an observation was right-censored. See how the numbers align above and below.  

```{r}
#print the 50 first elements of the vector to see how it presents
head(survobj, 10)
```


### Running initial analyses {.unnumbered}

We then start our analysis using the `survfit()` function to produce a *survfit object*, which fits the default calculations for **_Kaplan Meier_** (KM) estimates of the overall (marginal) survival curve, which are in fact a step function with jumps at observed event times. The final *survfit object*  contains one or more survival curves and is created using the *Surv* object as a response variable in the model formula.  

<span style="color: black;">**_NOTE:_** The Kaplan-Meier estimate is a nonparametric maximum likelihood estimate (MLE) of the survival function. . (see resources for more information).</span>

The summary of this *survfit object* will give what is called a *life table*. For each time step of the follow-up (`time`) where an event happened (in ascending order):  

* the number of people who were at risk of developing the event (people who did not have the event yet nor were censored: `n.risk`)  
* those who did develop the event  (`n.event`)  
* and from the above: the probability of *not* developing the event (probability of not dying, or of surviving past that specific time)  
* finally, the standard error and the confidence interval for that probability are derived and displayed  

We fit the KM estimates using the formula where the previously Surv object "survobj" is the response variable. "~ 1" precises we run the model for the overall survival.  

```{r fit}
# fit the KM estimates using a formula where the Surv object "survobj" is the response variable.
# "~ 1" signifies that we run the model for the overall survival  
linelistsurv_fit <-  survival::survfit(survobj ~ 1)

#print its summary for more details
summary(linelistsurv_fit)

```


While using `summary()` we can add the option `times` and  specify certain times at which we want to see the survival information 

```{r print_spec_times}

#print its summary at specific times
summary(linelistsurv_fit, times = c(5,10,20,30,60))

```


We can also use the `print()` function. The `print.rmean = TRUE` argument is used to obtain the mean survival time and its standard error (se).

<span style="color: black;">**_NOTE:_** The restricted mean survival time (RMST) is a specific survival measure more and more used in cancer survival analysis and which is often defined as the area under the survival curve, given we observe patients up to restricted time T (more details in Resources section).


```{r, mean_survtime}
# print linelistsurv_fit object with mean survival time and its se. 
print(linelistsurv_fit, print.rmean = TRUE)

```


<span style="color: darkgreen;">**_TIP:_** We can create the *surv object* directly in the `survfit()` function and save a line of code. This will then look like: `linelistsurv_quick <-  survfit(Surv(futime, event) ~ 1, data=linelist_surv)`.</span>


### Cumulative hazard {.unnumbered}  

Besides the `summary()` function, we can also use the `str()` function that gives more details on the structure of the `survfit()` object. It is a list of 16 elements.  

Among these elements is an important one: `cumhaz`, which is a numeric vector. This could be plotted to allow show the **_cumulative hazard_**, with the **_hazard_** being the **_instantaneous rate of event occurrence_** (see references).

```{r fit_struct}

str(linelistsurv_fit)

```

<!-- ======================================================= -->
### Plotting Kaplan-Meir curves  {.unnumbered}

Once the KM estimates are fitted, we can visualize the probability of being alive through a given time using the basic `plot()` function that draws the "Kaplan-Meier curve". In other words, the curve below is a conventional illustration of the survival experience in the whole patient group.

We can quickly verify the follow-up time min and max on the curve.  

An easy way to interpret is to say that at time zero, all the participants are still alive and survival probability is then 100%. This probability decreases over time as patients die. The proportion of participants surviving past 60 days of follow-up is around 40%.

```{r }

plot(linelistsurv_fit, 
     xlab = "Days of follow-up",    # x-axis label
     ylab="Survival Probability",   # y-axis label
     main= "Overall survival curve" # figure title
     )

```

The confidence interval of the KM survival estimates are also plotted by default and can be dismissed by adding the option `conf.int = FALSE` to the `plot()` command.

Since the event of interest is "death", drawing a curve describing the complements of the survival proportions will lead to drawing the cumulative mortality proportions. This can be done with `lines()`, which adds information to an existing plot.  


```{r}

# original plot
plot(
  linelistsurv_fit,
  xlab = "Days of follow-up",       
  ylab = "Survival Probability",       
  mark.time = TRUE,              # mark events on the curve: a "+" is printed at every event
  conf.int = FALSE,              # do not plot the confidence interval
  main = "Overall survival curve and cumulative mortality"
  )

# draw an additional curve to the previous plot
lines(
  linelistsurv_fit,
  lty = 3,             # use different line type for clarity
  fun = "event",       # draw the cumulative events instead of the survival 
  mark.time = FALSE,
  conf.int = FALSE
  )

# add a legend to the plot
legend(
  "topright",                               # position of legend
  legend = c("Survival", "Cum. Mortality"), # legend text 
  lty = c(1, 3),                            # line types to use in the legend
  cex = .85,                                # parametes that defines size of legend text
  bty = "n"                                 # no box type to be drawn for the legend
  )

```

<!-- ======================================================= -->
## Comparison of survival curves 

To compare the survival within different groups of our observed participants or patients, we might need to first look at their respective survival curves and then run tests to evaluate the difference between independent groups. This comparison can concern groups based on gender, age, treatment, comorbidity...

### Log rank test {.unnumbered}

The log rank test is a popular test that compares the entire survival experience between two or more *independent* groups and can be thought of as a test of whether the survival curves are identical (overlapping) or not (null hypothesis of no difference in survival between the groups). The `survdiff()` function of the **survival package** allows running the log-rank test when we specify `rho = 0` (which is the default). The test results gives a chi-square statistic along with a p-value since the log rank statistic is approximately distributed as a chi-square test statistic.

We first try to compare the survival curves by gender group. For this, we first try to visualize it (check whether the two survival curves are overlapping). A new *survfit object*  will be created with a slightly different formula. Then the *survdiff object* will be created.

By supplying ` ~ gender` as the right side of the formula, we no longer plot the overall survival but instead by gender.  


```{r comp_surv, warning=FALSE}

# create the new survfit object based on gender
linelistsurv_fit_sex <-  survfit(Surv(futime, event) ~ gender, data = linelist_surv)
```

Now we can plot the survival curves by gender. Have a look at the *order* of the strata levels in the gender column before defining your colors and legend.  

```{r}
# set colors
col_sex <- c("lightgreen", "darkgreen")

# create plot
plot(
  linelistsurv_fit_sex,
  col = col_sex,
  xlab = "Days of follow-up",
  ylab = "Survival Probability")

# add legend
legend(
  "topright",
  legend = c("Female","Male"),
  col = col_sex,
  lty = 1,
  cex = .9,
  bty = "n")
```

And now we can compute the test of the difference between the survival curves using `survdiff()`

```{r}
#compute the test of the difference between the survival curves
survival::survdiff(
  Surv(futime, event) ~ gender, 
  data = linelist_surv
  )

```

We see that the survival curve for women and the one for men overlap and the log-rank test does not give evidence of a survival difference between women and men.

Some other R packages allow illustrating survival curves for different groups and testing the difference all at once. Using the `ggsurvplot()` function from the **survminer** package, we can also include in our curve the printed risk tables for each group, as well the p-value from the log-rank test. 

<span style="color: orange;">**_CAUTION:_** **survminer** functions require that you specify the survival object *and* again specify the data used to fit the survival object. Remember to do this to avoid non-specific error messages. </span>

```{r, warning=F, message=F}

survminer::ggsurvplot(
    linelistsurv_fit_sex, 
    data = linelist_surv,          # again specify the data used to fit linelistsurv_fit_sex 
    conf.int = FALSE,              # do not show confidence interval of KM estimates
    surv.scale = "percent",        # present probabilities in the y axis in %
    break.time.by = 10,            # present the time axis with an increment of 10 days
    xlab = "Follow-up days",
    ylab = "Survival Probability",
    pval = T,                      # print p-value of Log-rank test 
    pval.coord = c(40,.91),        # print p-value at these plot coordinates
    risk.table = T,                # print the risk table at bottom 
    legend.title = "Gender",       # legend characteristics
    legend.labs = c("Female","Male"),
    font.legend = 10, 
    palette = "Dark2",             # specify color palette 
    surv.median.line = "hv",       # draw horizontal and vertical lines to the median survivals
    ggtheme = theme_light()        # simplify plot background
)

```


We may also want to test for differences in survival by the source of infection (source of contamination).  

In this case, the Log rank test gives enough evidence of a difference in the survival probabilities at `alpha= 0.005`. The survival probabilities for patients that were infected at funerals are higher than the survival probabilities for patients that got infected in other places, suggesting a survival benefit.

```{r}

linelistsurv_fit_source <-  survfit(
  Surv(futime, event) ~ source,
  data = linelist_surv
  )

# plot
ggsurvplot( 
  linelistsurv_fit_source,
  data = linelist_surv,
  size = 1, linetype = "strata",   # line types
  conf.int = T,
  surv.scale = "percent",  
  break.time.by = 10, 
  xlab = "Follow-up days",
  ylab= "Survival Probability",
  pval = T,
  pval.coord = c(40,.91),
  risk.table = T,
  legend.title = "Source of \ninfection",
  legend.labs = c("Funeral", "Other"),
  font.legend = 10,
  palette = c("#E7B800","#3E606F"),
  surv.median.line = "hv", 
  ggtheme = theme_light()
)

```

<!-- ======================================================= -->
## Cox regression analysis {}

Cox proportional hazards regression is one of the most popular regression techniques for survival analysis. Other models  can also be used since the Cox model requires *important assumptions* that need to be verified for an appropriate use such as the proportional hazards assumption: see references. 

In a Cox proportional hazards regression model, the measure of effect is the **_hazard rate_** (HR), which is the risk of failure (or the risk of death in our example), given that the participant has survived up to a specific time.  Usually, we are interested in comparing *independent* groups with respect to their hazards, and we use a hazard ratio, which is analogous to an odds ratio in the setting of multiple logistic regression analysis. The `cox.ph()` function from the **survival** package is used to fit the model. The function `cox.zph()` from **survival** package may be used to test the proportional hazards assumption for a Cox regression model fit. 

<span style="color: black;">**_NOTE:_** A probability must lie in the range 0 to 1. However, the hazard represents the expected number of events per one unit of time. 

* If the hazard ratio for a predictor is close to 1 then that predictor does not affect survival,
* if the HR is less than 1, then the predictor is protective (i.e., associated with improved survival),
* and if the HR is greater than 1, then the predictor is associated with increased risk (or decreased survival).</span> 

### Fitting a Cox model {.unnumbered}

We can first fit a model to assess the effect of age and gender on the survival. By just printing the model, we have the information on:

  + the estimated regression coefficients `coef` which quantifies the association between the predictors and the outcome,
  + their exponential (for interpretability, `exp(coef)`) which produces the *hazard ratio*,
  + their standard error `se(coef)`,
  + the z-score: how many standard errors is the estimated coefficient away from  0,
  + and the p-value:  the probability that the estimated coefficient could be 0.
  
The `summary()` function applied to the cox model object gives more information, such as the confidence interval of the estimated HR and the different test scores.

The effect of the first covariate `gender`  is presented in the first row. `genderm` (male) is printed, implying that the first strata level ("f"), i.e the female group, is the reference group for the gender. Thus the interpretation of the test parameter is that of men compared to women. The p-value indicates there was not enough evidence of an effect of the gender on the expected hazard or of an association between gender and all-cause mortality.

The same lack of evidence is noted regarding age-group.

```{r coxmodel_agesex}

#fitting the cox model
linelistsurv_cox_sexage <-  survival::coxph(
              Surv(futime, event) ~ gender + age_cat_small, 
              data = linelist_surv
              )


#printing the model fitted
linelistsurv_cox_sexage


#summary of the model
summary(linelistsurv_cox_sexage)

```


It was interesting to run the model and look at the results but a first look to verify whether the proportional hazards assumptions is respected could help saving time.

```{r test_assumption}

test_ph_sexage <- survival::cox.zph(linelistsurv_cox_sexage)
test_ph_sexage

```


<span style="color: black;">**_NOTE:_** A second argument called *method* can be specified when computing the cox model, that determines how ties are handled. The *default* is "efron", and the other options are "breslow" and "exact".</span>

In another model we add more risk factors such as the source of infection and the number of days between date of onset and admission. This time, we first  verify the proportional hazards assumption before going forward.

In this model, we have included a continuous predictor (`days_onset_hosp`). In this case we interpret the parameter estimates as the increase in the expected log of the relative hazard for each one unit increase in the predictor, holding other predictors constant. We first verify the proportional hazards assumption.  

```{r coxmodel_fit_ph,  message=FALSE}

#fit the model
linelistsurv_cox <-  coxph(
                        Surv(futime, event) ~ gender + age_years+ source + days_onset_hosp,
                        data = linelist_surv
                        )


#test the proportional hazard model
linelistsurv_ph_test <- cox.zph(linelistsurv_cox)
linelistsurv_ph_test
```

The graphical verification of this assumption may be performed with the function `ggcoxzph()` from the **survminer** package. 

```{r}
survminer::ggcoxzph(linelistsurv_ph_test)

```


The model results indicate there is a negative association between onset to admission duration and all-cause mortality. The expected hazard is 0.9 times lower in a person who who is one day later admitted than another, holding gender constant. Or in a more straightforward explanation, a one unit increase in the duration of onset to admission is associated with a 10.7% (`coef *100`) decrease in the risk of death.

Results show also a positive association between the source of infection and the all-cause mortality. Which is to say there is an increased risk of death (1.21x) for patients that got a source of infection other than funerals.


```{r coxmodel_summary,  message=FALSE}

#print the summary of the model
summary(linelistsurv_cox)

```


We can verify this relationship with a table:  


```{r}
linelist_case_data %>% 
  tabyl(days_onset_hosp, outcome) %>% 
  adorn_percentages() %>%  
  adorn_pct_formatting()

```


We would need to consider and investigate why this association exists in the data. One possible explanation could be that patients who live long enough to be admitted later had less severe disease to begin with. Another perhaps more likely explanation is that since we used a simulated fake dataset, this pattern does not reflect reality!  


<!-- ======================================================= -->

### Forest plots {.unnumbered}

We can then visualize the results of the cox model using the practical forest plots with the `ggforest()` function of the **survminer package**.

```{r forestp}

ggforest(linelistsurv_cox, data = linelist_surv)

```

<!-- ======================================================= -->
## Time-dependent covariates in survival models {}

Some of the following sections have been adapted with permission from an excellent [introduction to survival analysis in R](https://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html) by [Dr. Emily Zabor](https://www.emilyzabor.com/) 

In the last section we covered using Cox regression to examine associations between covariates of interest and survival outcomes.But these analyses rely on the covariate being measured at baseline, that is, before follow-up time for the event begins.

What happens if you are interested in a covariate that is measured **after** follow-up time begins? Or, what if you have a covariate that can change over time?

For example, maybe you are working with clinical data where you repeated measures of hospital laboratory values that can change over time. This is an example of a **Time Dependent Covariate**. In order to address this you need a special setup, but fortunately the cox model is very flexible and this type of data can also be modeled with tools from the **survival** package. 

### Time-dependent covariate setup {.unnumbered} 

Analysis of time-dependent covariates in R requires setup of a special dataset. If interested, see the more detailed paper on this by the author of the **survival** package [Using Time Dependent Covariates and Time Dependent Coefficients in the Cox Model](https://cran.r-project.org/web/packages/survival/vignettes/timedep.pdf).

For this, we'll use a new dataset from the `SemiCompRisks` package named `BMT`, which includes data on 137 bone marrow transplant patients. The variables we'll focus on are:  

* `T1`  - time (in days) to death or last follow-up  
* `delta1` - death indicator; 1-Dead, 0-Alive  
* `TA` -  time (in days) to acute graft-versus-host disease  
* `deltaA` -  acute graft-versus-host disease indicator;  
  * 1 - Developed acute graft-versus-host disease  
  * 0 - Never developed acute graft-versus-host disease

We'll load this dataset from the **survival** package using the **base** R command `data()`, which can be used for loading data that is already included in a R package that is loaded. The data frame `BMT` will appear in your R environment.  

```{r}
data(BMT, package = "SemiCompRisks")
```

#### Add unique patient identifier {.unnumbered}  

There is no unique ID column in the `BMT` data, which is needed to create the type of dataset we want. So we use the function `rowid_to_column()` from the **tidyverse** package **tibble** to create a new id column called `my_id` (adds column at start of data frame with sequential row ids, starting at 1). We name the data frame `bmt`.  

```{r}
bmt <- rowid_to_column(BMT, "my_id")
```

The dataset now looks like this:  

```{r message=FALSE, echo=F}
DT::datatable(bmt, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

#### Expand patient rows {.unnumbered}  

Next, we'll use the `tmerge()` function with the `event()` and `tdc()` helper functions to create the restructured dataset. Our goal is to restructure the dataset to create a separate row for each patient for each time interval where they have a different value for `deltaA`. In this case, each patient can have at most two rows depending on whether they developed acute graft-versus-host disease during the data collection period. We'll call our new indicator for the development of acute graft-versus-host disease `agvhd`.

- `tmerge()` creates a long dataset with multiple time intervals for the different covariate values for each patient
- `event()` creates the new event indicator to go with the newly-created time intervals
- `tdc()` creates the time-dependent covariate column, `agvhd`, to go with the newly created time intervals

```{r}
td_dat <- 
  tmerge(
    data1 = bmt %>% select(my_id, T1, delta1), 
    data2 = bmt %>% select(my_id, T1, delta1, TA, deltaA), 
    id = my_id, 
    death = event(T1, delta1),
    agvhd = tdc(TA)
    )
```

To see what this does, let's look at the data for the first 5 individual patients.

The variables of interest in the original data looked like this:

```{r}
bmt %>% 
  select(my_id, T1, delta1, TA, deltaA) %>% 
  filter(my_id %in% seq(1, 5))
```

The new dataset for these same patients looks like this:

```{r}
td_dat %>% 
  filter(my_id %in% seq(1, 5))
```

Now some of our patients have two rows in the dataset corresponding to intervals where they have a different value of our new variable, `agvhd`. For example, Patient 1 now has two rows with a `agvhd` value of zero from time 0 to time 67, and a value of 1 from time 67 to time 2081. 

### Cox regression with time-dependent covariates {.unnumbered} 

Now that we've reshaped our data and added the new time-dependent `aghvd` variable, let's fit a simple single variable cox regression model. We can use the same `coxph()` function as before, we just need to change our `Surv()` function to specify both the start and stop time for each interval using the `time1 = ` and `time2 = ` arguments. 


```{r}
bmt_td_model = coxph(
  Surv(time = tstart, time2 = tstop, event = death) ~ agvhd, 
  data = td_dat
  )

summary(bmt_td_model)
```

Again, we'll visualize our cox model results using the `ggforest()` function from the **survminer package**.:

```{r}

ggforest(bmt_td_model, data = td_dat)

```

As you can see from the forest plot, confidence interval, and p-value, there does not appear to be a strong association between death and acute graft-versus-host disease in the context of our simple model. 

<!-- ======================================================= -->
## Resources {  }

[Survival Analysis Part I: Basic concepts and first analyses](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2394262/)

[Survival Analysis in R](https://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html)

[Survival analysis in infectious disease research: Describing events in time](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2954271/)

[Chapter on advanced survival models Princeton](https://data.princeton.edu/wws509/notes/c7.pdf)

[Using Time Dependent Covariates and Time Dependent Coefficients in the Cox Model](https://cran.r-project.org/web/packages/survival/vignettes/timedep.pdf)

[Survival analysis cheatsheet R](https://publicifsv.sund.ku.dk/~ts/survival/survival-cheat.pdf)

[Survminer cheatsheet](https://paulvanderlaken.files.wordpress.com/2017/08/survminer_cheatsheet.pdf)

[Paper on different survival measures for cancer registry data with Rcode provided as supplementary materials](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6322561/)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/survival_analysis.Rmd-->

# GIS cơ bản {#gis}  

```{r, out.width=c('100%', '100%'), echo=F}
knitr::include_graphics(here::here("images", "gis_head_image.png"))
```

<!-- ======================================================= -->
## Overview {  }

Spatial aspects of your data can provide a lot of insights into the situation of the outbreak, and to answer questions such as: 

* Where are the current disease hotspots?
* How have the hotspots have changed over time?
* How is the access to health facilities? Are any improvements needed?

The current focus of this GIS page to address the needs of applied epidemiologists in outbreak response. We will explore basic spatial data visualization methods using **tmap** and **ggplot2** packages. We will also walk through some of the basic spatial data management and querying methods with the **sf** package. Lastly, we will briefly touch upon concepts of *spatial statistics* such as spatial relationships, spatial autocorrelation, and spatial regression using the **spdep** package.  



## Key terms {}  

Below we introduce some key terminology. For a thorough introduction to GIS and spatial analysis, we suggest that you review one of the longer tutorials or courses listed in the References section.  

**Geographic Information System (GIS)** - A GIS is a framework or environment for gathering, managing, analyzing, and visualizing spatial data.

### GIS software {.unnumbered}

Some popular GIS software allow point-and-click interaction for map development and spatial analysis. These tools comes with advantages such as not needing to learn code and the ease of manually selecting and placing icons and features on a map. Here are two popular ones:  

**ArcGIS** - A commercial GIS software developed by the company ESRI, which is very popular but quite expensive  

**QGIS** - A free open-source GIS software that can do almost anything that ArcGIS can do. You can [download QGIS here](https://qgis.org/en/site/forusers/download.html)  

Using R as a GIS can seem more intimidating at first because instead of "point-and-click", it has a "command-line interface" (you must code to acquire the desired outcome). However, this is a major advantage if you need to repetitively produce maps or create an analysis that is reproducible.  

### Spatial data {.unnumbered}

The two primary forms of spatial data used in GIS are vector and raster data:

**Vector Data** - The most common format of spatial data used in GIS, vector data are comprised of geometric features of vertices and paths. Vector spatial data can be further divided into three widely-used types:

  * *Points* - A point consists of a coordinate pair (x,y)  representing a specific location in a coordinate system. Points are the most basic form of spatial data, and may be used to denote a case (i.e. patient home) or a location (i.e. hospital) on a map.

  * *Lines* - A line is composed of two connected points. Lines have a length, and may be used to denote things like roads or rivers.

  * *Polygons* - A polygon is composed of at least three line segments connected by points. Polygon features have a length (i.e. the perimeter of the area) as well as an area measurement. Polygons may be used to note an area (i.e. a village) or a structure (i.e. the actual area of a hospital). 

**Raster Data** - An alternative format for spatial data, raster data is a matrix of cells (e.g. pixels) with each cell containing information such as height, temperature, slope, forest cover, etc. These are often aerial photographs, satellite imagery, etc. Rasters can also be used as “base maps” below vector data.

### Visualizing spatial data {.unnumbered}

To visually represent spatial data on a map, GIS software requires you to provide sufficient information about where different features should be, in relation to one another. If you are using vector data, which will be true for most use cases, this information will typically be stored in a shapefile:

**Shapefiles** - A shapefile is a common data format for storing "vector" spatial data consisting or lines, points, or polygons. A single shapefile is actually a collection of at least three files - .shp, .shx, and .dbf. All of these sub-component files must be present in a given directory (folder) for the shapefile to be readable. These associated files can be compressed into a ZIP folder to be sent via email or download from a website.  

The shapefile will contain information about the features themselves, as well as where to locate them on the Earth’s surface. This is important because while the Earth is a globe, maps are typically two-dimensional; choices about how to “flatten” spatial data can have a big impact on the look and interpretation of the resulting map.

**Coordinate Reference Systems (CRS)** - A CRS is a coordinate-based system used to locate geographical features on the Earth's surface. It has a few key components:  

  * *Coordinate System* - There are many many different coordinate systems, so make sure you know which system your coordinates are from. Degrees of latitude/longitude are common, but you could also see [UTM](https://www.maptools.com/tutorials/utm/quick_guide) coordinates.  
  
  * *Units* - Know what the units are for your coordinate system (e.g. decimal degrees, meters)  

  * *Datum* - A particular modeled version of the Earth. These have been revised over the years, so ensure that your map layers are using the same datum.  

  * *Projection* - A reference to the mathematical equation that was used to project the truly round earth onto a flat surface (map).  

Remember that you can summarise spatial data without using the mapping tools shown below. Sometimes a simple table by geography (e.g. district, country, etc.) is all that is needed!  

## Getting started with GIS  


There are a couple of key items you will need to have and to think about to make a map. These include:

  * A **dataset** -- this can be in a spatial data format (such as shapefiles, as noted above) or it may not be in a spatial format (for instance just as a csv).
  
  * If your dataset is not in a spatial format you will also need a **reference dataset**. Reference data consists of the spatial representation of the data and the related **attributes**, which would include material containing the location and address information of specific features.
  
    + If you are working with pre-defined geographic boundaries (for example, administrative regions), reference shapefiles are often freely available to download from a government agency or data sharing organization. When in doubt, a good place to start is to Google “[regions] shapefile”
    
    + If you have address information, but no latitude and longitude, you may need to use a **geocoding engine** to get the spatial reference data for your records. 
    
  * An idea about **how you want to present** the information in your datasets to your target audience. There are many different types of maps, and it is important to think about which type of map best fits your needs.

### Types of maps for visualizing your data {.unnumbered}

**Choropleth map** - a type of thematic map where colors, shading, or patterns are used to represent geographic regions in relation to their value of an attribute. For instance a larger value could be indicated by a darker colour than a smaller value. This type of map is particularly useful when visualizing a variable and how it changes across defined regions or geopolitical areas.

```{r, out.width = '50%', fig.align = "center", fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "gis_choropleth.png"))
```

**Case density heatmap** - a type of thematic map where colours are used to represent intensity of a value, however, it does not use defined regions or geopolitical boundaries to group data. This type of map is typically used for showing ‘hot spots’ or areas with a high density or concentration of points. 

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "gis_heatmap.png"))
```

**Dot density map** - a thematic map type that uses dots to represent attribute values in your data. This type of map is best used to visualize the scatter of your data and visually scan for clusters.

```{r, fig.align = "center", echo=F}
# dot density img here
```

**Proportional symbols map (graduated symbols map)** - a thematic map similar to a choropleth map, but instead of using colour to indicate the value of an attribute it uses a symbol (usually a circle) in relation to the value. For instance a larger value could be indicated by a larger symbol than a smaller value. This type of map is best used when you want to visualize the size or quantity of your data across geographic regions. 

```{r, fig.align = "center", echo=F}
# proportional symbols img here
```

You can also combine several different types of visualizations to show complex geographic patterns. For example, the cases (dots) in the map below are colored according to their closest health facility (see legend). The large red circles show *health facility catchment areas* of a certain radius, and the bright red case-dots those that were outside any catchment range:

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "gis_hf_catchment.png"))
```

Note: The primary focus of this GIS page is based on the context of field outbreak response. Therefore the contents of the page will cover the basic spatial data manipulations, visualizations, and analyses.


<!-- ======================================================= -->
## Preparation {  }

### Load packages {.unnumbered}  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  rio,           # to import data
  here,          # to locate files
  tidyverse,     # to clean, handle, and plot the data (includes ggplot2 package)
  sf,            # to manage spatial data using a Simple Feature format
  tmap,          # to produce simple maps, works for both interactive and static maps
  janitor,       # to clean column names
  OpenStreetMap, # to add OSM basemap in ggplot map
  spdep          # spatial statistics
  ) 
                  
```

You can see an overview of all the R packages that deal with spatial data at the [CRAN "Spatial Task View"](https://cran.r-project.org/web/views/Spatial.html).  


### Sample case data {.unnumbered}

For demonstration purposes, we will work with a random sample of 1000 cases from the simulated Ebola epidemic `linelist` dataframe (computationally, working with fewer cases is easier to display in this handbook). If you want to follow along, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file).  

Since we are taking a random sample of the cases, your results may look slightly different from what is demonstrated here when you run the codes on your own.

Import data with the `import()` function from the **rio** package (it handles many file types like .xlsx, .csv, .rds - see the [Import and export] page for details).  

```{r, echo=F}
# import clean case linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))  
```

```{r, eval=F}
# import clean case linelist
linelist <- import("linelist_cleaned.rds")  
```

Next we select a random sample of 1000 rows using `sample()` from **base** R.   

```{r}
# generate 1000 random row numbers, from the number of rows in linelist
sample_rows <- sample(nrow(linelist), 1000)

# subset linelist to keep only the sample rows, and all columns
linelist <- linelist[sample_rows,]
```

Now we want to convert this `linelist` which is class dataframe, to an object of class "sf" (spatial features). Given that the linelist has two columns "lon" and "lat" representing the longitude and latitude of each case's residence, this will be easy.  

We use the package **sf** (spatial features) and its function `st_as_sf()` to create the new object we call `linelist_sf`. This new object looks essentially the same as the linelist, but the columns `lon` and `lat` have been designated as coordinate columns, and a coordinate reference system (CRS) has been assigned for when the points are displayed. 4326 identifies our coordinates as based on the [World Geodetic System 1984 (WGS84)](https://gisgeography.com/wgs84-world-geodetic-system/) - which is standard for GPS coordinates.  

```{r}
# Create sf object
linelist_sf <- linelist %>%
     sf::st_as_sf(coords = c("lon", "lat"), crs = 4326)
```

This is how the original `linelist` dataframe looks like. In this demonstration, we will only use the column `date_onset` and `geometry` (which was constructed from the longitude and latitude fields above and is the last column in the data frame).  

```{r}
DT::datatable(head(linelist_sf, 10), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

### Admin boundary shapefiles {.unnumbered}  

**Sierra Leone: Admin boundary shapefiles**  

In advance, we have downloaded all administrative boundaries for Sierra Leone from the Humanitarian Data Exchange (HDX) [website here](https://data.humdata.org/dataset/sierra-leone-all-ad-min-level-boundaries). Alternatively, you can download these and all other example data for this handbook via our R package, as explained in the [Download handbook and data] page.  

Now we are going to do the following to save the Admin Level 3 shapefile in R:  

1) Import the shapefile  
2) Clean the column names  
3) Filter rows to keep only areas of interest  

To import a shapefile we use the `read_sf()` function from **sf**. It is provided the filepath via `here()`. - in our case the file is within our R project in the "data", "gis", and "shp" subfolders, with filename "sle_adm3.shp" (see pages on [Import and export] and [R projects] for more information). You will need to provide your own file path.  

```{r, echo=F}
sle_adm3_raw <- sf::read_sf(here("data", "gis", "shp", "sle_adm3.shp"))
```


Next we use `clean_names()` from the **janitor** package to standardize the column names of the shapefile. We also use `filter()` to keep only the rows with admin2name of "Western Area Urban" or "Western Area Rural".    

```{r}
# ADM3 level clean
sle_adm3 <- sle_adm3_raw %>% 
  janitor::clean_names() %>% # standardize column names
  filter(admin2name %in% c("Western Area Urban", "Western Area Rural")) # filter to keep certain areas
```

Below you can see the how the shapefile looks after import and cleaning. *Scroll to the right* to see how there are columns with admin level 0 (country), admin level 1, admin level 2, and finally admin level 3. Each level has a character name and a unique identifier "pcode". The pcode expands with each increasing admin level e.g. SL (Sierra Leone) -> SL04 (Western) -> SL0410 (Western Area Rural) -> SL040101 (Koya Rural).  

```{r message=FALSE, echo=F}
# display the shapefile as a table
DT::datatable(head(sle_adm3, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



### Population data {.unnumbered}  

**Sierra Leone: Population by ADM3**  

These data can again be downloaded from HDX (link [here](https://data.humdata.org/dataset/sierra-leone-population)) or via our **epirhandbook** R package as explained [in this page][Download handbook and data]. We use `import()` to load the .csv file. We also pass the imported file to `clean_names()` to standardize the column name syntax.   

```{r}
# Population by ADM3
sle_adm3_pop <- import(here("data", "gis", "population", "sle_admpop_adm3_2020.csv")) %>%
  janitor::clean_names()
```

Here is what the population file looks like. Scroll to the right to see how each jurisdiction has columns with `male` population, `female` populaton, `total` population, and the population break-down in columns by age group.  

```{r message=FALSE, echo=F}
# display the population as a table
DT::datatable(head(sle_adm3_pop, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


### Health Facilities {.unnumbered}

**Sierra Leone: Health facility data from OpenStreetMap**  

Again we have downloaded the locations of health facilities from HDX [here](https://data.humdata.org/dataset/hotosm_sierra_leone_health_facilities) or via instructions in the [Download handbook and data] page.   

We import the facility points shapefile with `read_sf()`, again clean the column names, and then filter to keep only the points tagged as either "hospital", "clinic", or "doctors".  


```{r}
# OSM health facility shapefile
sle_hf <- sf::read_sf(here("data", "gis", "shp", "sle_hf.shp")) %>% 
  janitor::clean_names() %>%
  filter(amenity %in% c("hospital", "clinic", "doctors"))
```

Here is the resulting dataframe - *scroll right* to see the facility name and `geometry` coordinates.  

```{r message=FALSE, echo=F}
# display the population as a table
DT::datatable(head(sle_hf, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```





<!-- ======================================================= -->
## Plotting coordinates {  }

The easiest way to plot X-Y coordinates (longitude/latitude, points), in this case of cases, is to draw them as points directly from the `linelist_sf` object which we created in the preparation section.

The package **tmap** offers simple mapping capabilities for both static ("plot" mode) and interactive ("view" mode) with just a few lines of code. The **tmap** syntax is similar to that of **ggplot2**, such that commands are added to each other with `+`. Read more detail in this [vignette](https://cran.r-project.org/web/packages/tmap/vignettes/tmap-getstarted.html). 


1) Set the **tmap** mode. In this case we will use "plot" mode, which produces static outputs.  

```{r, warning = F, message=F}
tmap_mode("plot") # choose either "view" or "plot"
```

Below, the points are plotted alone.`tm_shape()` is provided with the `linelist_sf` objects. We then add points via `tm_dots()`, specifying the size and color. Because `linelist_sf` is an sf object, we have already designated the two columns that contain the lat/long coordinates and the coordinate reference system (CRS): 


```{r, warning = F, message=F}
# Just the cases (points)
tm_shape(linelist_sf) + tm_dots(size=0.08, col='blue')
```

Alone, the points do not tell us much. So we should also map the administrative boundaries:  

Again we use `tm_shape()` (see [documentation](https://www.rdocumentation.org/packages/tmap/versions/3.3/topics/tm_shape)) but instead of providing the case points shapefile, we provide the administrative boundary shapefile (polygons).  

With the `bbox = ` argument (bbox stands for "bounding box") we can specify the coordinate boundaries. First we show the map display without `bbox`, and then with it.  

```{r, out.width = c('50%', '50%'), fig.show='hold', warning=F, message=F}
# Just the administrative boundaries (polygons)
tm_shape(sle_adm3) +               # admin boundaries shapefile
  tm_polygons(col = "#F7F7F7")+    # show polygons in light grey
  tm_borders(col = "#000000",      # show borders with color and line weight
             lwd = 2) +
  tm_text("admin3name")            # column text to display for each polygon


# Same as above, but with zoom from bounding box
tm_shape(sle_adm3,
         bbox = c(-13.3, 8.43,    # corner
                  -13.2, 8.5)) +  # corner
  tm_polygons(col = "#F7F7F7") +
  tm_borders(col = "#000000", lwd = 2) +
  tm_text("admin3name")

```


And now both points and polygons together:  

```{r, warning=F, message=FALSE}
# All together
tm_shape(sle_adm3, bbox = c(-13.3, 8.43, -13.2, 8.5)) +     #
  tm_polygons(col = "#F7F7F7") +
  tm_borders(col = "#000000", lwd = 2) +
  tm_text("admin3name")+
tm_shape(linelist_sf) +
  tm_dots(size=0.08, col='blue', alpha = 0.5) +
  tm_layout(title = "Distribution of Ebola cases")   # give title to map

```


To read a good comparison of mapping options in R, see this [blog post](https://rstudio-pubs-static.s3.amazonaws.com/324400_69a673183ba449e9af4011b1eeb456b9.html).  




<!-- ======================================================= -->
## Spatial joins {}

You may be familiar with *joining* data from one dataset to another one. Several methods are discussed in the [Joining data] page of this handbook. A spatial join serves a similar purpose but leverages spatial relationships. Instead of relying on common values in columns to correctly match observations, you can utilize their spatial relationships, such as one feature being *within* another, or *the nearest neighbor* to another, or within a *buffer* of a certain radius from another, etc.  


The **sf** package offers various methods for spatial joins. See more documentation about the st_join method and spatial join types in this [reference](https://r-spatial.github.io/sf/reference/geos_binary_pred.html).  


### Points in polygon {.unnumbered}
**Spatial assign administrative units to cases**

Here is an interesting conundrum: the case linelist does not contain any information about the administrative units of the cases. Although it is ideal to collect such information during the initial data collection phase, we can also assign administrative units to individual cases based on their spatial relationships (i.e. point intersects with a polygon).  

Below, we will spatially intersect our case locations (points) with the ADM3 boundaries (polygons):  

1) Begin with the linelist (points)  
2) Spatial join to the boundaries, setting the type of join at "st_intersects"  
3) Use `select()` to keep only certain of the new administrative boundary columns  

```{r, warning=F, message=F}
linelist_adm <- linelist_sf %>%
  
  # join the administrative boundary file to the linelist, based on spatial intersection
  sf::st_join(sle_adm3, join = st_intersects)
```

All the columns from `sle_adms` have been added to the linelist! Each case now has columns detailing the administrative levels that it falls within. In this example, we only want to keep two of the new columns (admin level 3), so we `select()` the old column names and just the two additional of interest:  

```{r, warning=F, message=F}
linelist_adm <- linelist_sf %>%
  
  # join the administrative boundary file to the linelist, based on spatial intersection
  sf::st_join(sle_adm3, join = st_intersects) %>% 
  
  # Keep the old column names and two new admin ones of interest
  select(names(linelist_sf), admin3name, admin3pcod)
```

Below, just for display purposes you can see the first ten cases and that their admin level 3 (ADM3) jurisdictions that have been attached, based on where the point spatially intersected with the polygon shapes.    

```{r, warning=F, message=F}
# Now you will see the ADM3 names attached to each case
linelist_adm %>% select(case_id, admin3name, admin3pcod)
```

Now we can describe our cases by administrative unit - something we were not able to do before the spatial join!  

```{r, warning=F, message=F}
# Make new dataframe containing counts of cases by administrative unit
case_adm3 <- linelist_adm %>%          # begin with linelist with new admin cols
  as_tibble() %>%                      # convert to tibble for better display
  group_by(admin3pcod, admin3name) %>% # group by admin unit, both by name and pcode 
  summarise(cases = n()) %>%           # summarize and count rows
  arrange(desc(cases))                     # arrange in descending order

case_adm3
```

We can also create a bar plot of case counts by administrative unit.  

In this example, we begin the `ggplot()` with the `linelist_adm`, so that we can apply factor functions like `fct_infreq()` which orders the bars by frequency (see page on [Factors] for tips).  

```{r, warning=F, message=F}
ggplot(
    data = linelist_adm,                       # begin with linelist containing admin unit info
    mapping = aes(
      x = fct_rev(fct_infreq(admin3name))))+ # x-axis is admin units, ordered by frequency (reversed)
  geom_bar()+                                # create bars, height is number of rows
  coord_flip()+                              # flip X and Y axes for easier reading of adm units
  theme_classic()+                           # simplify background
  labs(                                      # titles and labels
    x = "Admin level 3",
    y = "Number of cases",
    title = "Number of cases, by adminstative unit",
    caption = "As determined by a spatial join, from 1000 randomly sampled cases from linelist"
  )
```


<!-- ======================================================= -->
### Nearest neighbor {.unnumbered}

**Finding the nearest health facility / catchment area**  

It might be useful to know where the health facilities are located in relation to the disease hot spots.

We can use the *st_nearest_feature* join method from the `st_join()` function (**sf** package) to visualize the closest health facility to individual cases.  

1) We begin with the shapefile linelist `linelist_sf`  
2) We spatially join with `sle_hf`, which is the locations of health facilities and clinics (points)  

```{r, warning=F, message=F}
# Closest health facility to each case
linelist_sf_hf <- linelist_sf %>%                  # begin with linelist shapefile  
  st_join(sle_hf, join = st_nearest_feature) %>%   # data from nearest clinic joined to case data 
  select(case_id, osm_id, name, amenity) %>%       # keep columns of interest, including id, name, type, and geometry of healthcare facility
  rename("nearest_clinic" = "name")                # re-name for clarity
```

We can see below (first 50 rows) that the each case now has data on the nearest clinic/hospital  

```{r message=FALSE, echo=F}
DT::datatable(head(linelist_sf_hf, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


We can see that "Den Clinic" is the closest health facility for about ~30% of the cases.

```{r}
# Count cases by health facility
hf_catchment <- linelist_sf_hf %>%   # begin with linelist including nearest clinic data
  as.data.frame() %>%                # convert from shapefile to dataframe
  count(nearest_clinic,              # count rows by "name" (of clinic)
        name = "case_n") %>%         # assign new counts column as "case_n"
  arrange(desc(case_n))              # arrange in descending order

hf_catchment                         # print to console
```

To visualize the results, we can use **tmap** - this time interactive mode for easier viewing  

```{r, warning=F, message=F}
tmap_mode("view")   # set tmap mode to interactive  

# plot the cases and clinic points 
tm_shape(linelist_sf_hf) +            # plot cases
  tm_dots(size=0.08,                  # cases colored by nearest clinic
          col='nearest_clinic') +    
tm_shape(sle_hf) +                    # plot clinic facilities in large black dots
  tm_dots(size=0.3, col='black', alpha = 0.4) +      
  tm_text("name") +                   # overlay with name of facility
tm_view(set.view = c(-13.2284, 8.4699, 13), # adjust zoom (center coords, zoom)
        set.zoom.limits = c(13,14))+
tm_layout(title = "Cases, colored by nearest clinic")
```


### Buffers {.unnumbered} 

We can also explore how many cases are located within 2.5km (~30 mins) walking distance from the closest health facility.

*Note: For more accurate distance calculations, it is better to re-project your sf object to the respective local map projection system such as UTM (Earth projected onto a planar surface). In this example, for simplicity we will stick to the World Geodetic System (WGS84) Geograhpic coordinate system (Earth represented in a spherical / round surface, therefore the units are in decimal degrees). We will use a general conversion of: 1 decimal degree = ~111km.*  

See more information about map projections and coordinate systems at this [esri article](https://www.esri.com/arcgis-blog/products/arcgis-pro/mapping/gcs_vs_pcs/). This [blog](http://www.geo.hunter.cuny.edu/~jochen/gtech201/lectures/lec6concepts/map%20coordinate%20systems/how%20to%20choose%20a%20projection.htm) talks about different types of map projection and how one can choose a suitable projection depending on the area of interest and the context of your map / analysis.


**First**, create a circular buffer with a radius of ~2.5km around each health facility. This is done with the function `st_buffer()` from **tmap**. Because the unit of the map is in lat/long decimal degrees, that is how "0.02" is interpreted. If your map coordinate system is in meters, the number must be provided in meters.  

```{r, warning=F, message=F}
sle_hf_2k <- sle_hf %>%
  st_buffer(dist=0.02)       # decimal degrees translating to approximately 2.5km 
```

Below we plot the buffer zones themselves, with the :  

```{r, warning=F, message=F}
tmap_mode("plot")
# Create circular buffers
tm_shape(sle_hf_2k) +
  tm_borders(col = "black", lwd = 2)+
tm_shape(sle_hf) +                    # plot clinic facilities in large red dots
  tm_dots(size=0.3, col='black')      
```


**Second*, we intersect these buffers with the cases (points) using `st_join()` and the join type of *st_intersects*. That is, the data from the buffers are joined to the points that they intersect with. 

```{r, warning=F, message=F}
# Intersect the cases with the buffers
linelist_sf_hf_2k <- linelist_sf_hf %>%
  st_join(sle_hf_2k, join = st_intersects, left = TRUE) %>%
  filter(osm_id.x==osm_id.y | is.na(osm_id.y)) %>%
  select(case_id, osm_id.x, nearest_clinic, amenity.x, osm_id.y)
```

Now we can count the results: ` nrow(linelist_sf_hf_2k[is.na(linelist_sf_hf_2k$osm_id.y),])` out of 1000 cases did not intersect with any buffer (that value is missing), and so live more than 30 mins walk from the nearest health facility.

```{r}
# Cases which did not get intersected with any of the health facility buffers
linelist_sf_hf_2k %>% 
  filter(is.na(osm_id.y)) %>%
  nrow()
```

We can visualize the results such that cases that did not intersect with any buffer appear in red.  

```{r, out.width = '100%', warning=F, message=F}
tmap_mode("view")

# First display the cases in points
tm_shape(linelist_sf_hf) +
  tm_dots(size=0.08, col='nearest_clinic') +

# plot clinic facilities in large black dots
tm_shape(sle_hf) +                    
  tm_dots(size=0.3, col='black')+   

# Then overlay the health facility buffers in polylines
tm_shape(sle_hf_2k) +
  tm_borders(col = "black", lwd = 2) +

# Highlight cases that are not part of any health facility buffers
# in red dots  
tm_shape(linelist_sf_hf_2k %>%  filter(is.na(osm_id.y))) +
  tm_dots(size=0.1, col='red') +
tm_view(set.view = c(-13.2284,8.4699, 13), set.zoom.limits = c(13,14))+

# add title  
tm_layout(title = "Cases by clinic catchment area")

```


### Other spatial joins {.unnumbered}  

Alternative values for argument `join` include (from the [documentation](https://r-spatial.github.io/sf/reference/st_join.html))

* st_contains_properly  
* st_contains  
* st_covered_by  
* st_covers  
* st_crosses  
* st_disjoint  
* st_equals_exact  
* st_equals  
* st_is_within_distance  
* st_nearest_feature  
* st_overlaps  
* st_touches  
* st_within  





## Choropleth maps {}  


Choropleth maps can be useful to visualize your data by pre-defined area, usually administrative unit or health area. In outbreak response this can help to target resource allocation for specific areas with high incidence rates, for example.

Now that we have the administrative unit names assigned to all cases (see section on spatial joins, above), we can start mapping the case counts by area (choropleth maps).

Since we also have population data by ADM3, we can add this information to the *case_adm3* table created previously.

We begin with the dataframe created in the previous step `case_adm3`, which is a summary table of each administrative unit and its number of cases.  

1) The population data `sle_adm3_pop` are joined using a `left_join()` from **dplyr** on the basis of common values across column `admin3pcod` in the `case_adm3` dataframe, and column `adm_pcode` in the `sle_adm3_pop` dataframe. See page on [Joining data]).  
2) `select()` is applied to the new dataframe, to keep only the useful columns - `total` is total population  
3) Cases per 10,000 populaton is calculated as a new column with `mutate()`  


```{r}
# Add population data and calculate cases per 10K population
case_adm3 <- case_adm3 %>% 
     left_join(sle_adm3_pop,                             # add columns from pop dataset
               by = c("admin3pcod" = "adm3_pcode")) %>%  # join based on common values across these two columns
     select(names(case_adm3), total) %>%                 # keep only important columns, including total population
     mutate(case_10kpop = round(cases/total * 10000, 3)) # make new column with case rate per 10000, rounded to 3 decimals

case_adm3                                                # print to console for viewing
```

Join this table with the ADM3 polygons shapefile for mapping

```{r, warning=F, message=F}
case_adm3_sf <- case_adm3 %>%                 # begin with cases & rate by admin unit
  left_join(sle_adm3, by="admin3pcod") %>%    # join to shapefile data by common column
  select(objectid, admin3pcod,                # keep only certain columns of interest
         admin3name = admin3name.x,           # clean name of one column
         admin2name, admin1name,
         cases, total, case_10kpop,
         geometry) %>%                        # keep geometry so polygons can be plotted
  st_as_sf()                                  # convert to shapefile

```


Mapping the results

```{r, message=F, warning=F}
# tmap mode
tmap_mode("plot")               # view static map

# plot polygons
tm_shape(case_adm3_sf) + 
        tm_polygons("cases") +  # color by number of cases column
        tm_text("admin3name")   # name display
```

We can also map the incidence rates  


```{r, warning=F, message=F}
# Cases per 10K population
tmap_mode("plot")             # static viewing mode

# plot
tm_shape(case_adm3_sf) +                # plot polygons
  tm_polygons("case_10kpop",            # color by column containing case rate
              breaks=c(0, 10, 50, 100), # define break points for colors
              palette = "Purples"       # use a purple color palette
              ) +
  tm_text("admin3name")                 # display text

```

## Mapping with ggplot2
If you are already familiar with using **ggplot2**, you can use that package instead to create static maps of your data. The `geom_sf()` function will draw different objects based on which features (points, lines, or polygons) are in your data. For example, you can use `geom_sf()` in a `ggplot()` using `sf` data with polygon geometry to create a choropleth map.

To illustrate how this works, we can start with the ADM3 polygons shapefile that we used earlier. Recall that these are Admin Level 3 regions in Sierra Leone:

```{r}
sle_adm3
```
We can use the `left_join()` function from **dplyr** to add the data we would like to map to the shapefile object. In this case, we are going to use the `case_adm3` data frame that we created earlier to summarize case counts by administrative region; however, we can use this same approach to map any data stored in a data frame.
```{r}
sle_adm3_dat <- sle_adm3 %>% 
  inner_join(case_adm3, by = "admin3pcod") # inner join = retain only if in both data objects

select(sle_adm3_dat, admin3name.x, cases) # print selected variables to console
```

To make a column chart of case counts by region, using **ggplot2**, we could then call `geom_col()` as follows:

```{r, fig.align = "center"}
ggplot(data=sle_adm3_dat) +
  geom_col(aes(x=fct_reorder(admin3name.x, cases, .desc=T),   # reorder x axis by descending 'cases'
               y=cases)) +                                  # y axis is number of cases by region
  theme_bw() +
  labs(                                                     # set figure text
    title="Number of cases, by administrative unit",
    x="Admin level 3",
    y="Number of cases"
  ) + 
  guides(x=guide_axis(angle=45))                            # angle x-axis labels 45 degrees to fit better

```

If we want to use **ggplot2** to instead make a choropleth map of case counts, we can use similar syntax to call the `geom_sf()` function:

```{r, fig.align = "center"}
ggplot(data=sle_adm3_dat) + 
  geom_sf(aes(fill=cases))    # set fill to vary by case count variable

```

We can then customize the appearance of our map using grammar that is consistent across **ggplot2**, for example:
```{r, fig.align = "center"}
ggplot(data=sle_adm3_dat) +                           
  geom_sf(aes(fill=cases)) +						
  scale_fill_continuous(high="#54278f", low="#f2f0f7") +    # change color gradient
  theme_bw() +
  labs(title = "Number of cases, by administrative unit",   # set figure text
       subtitle = "Admin level 3"
  )
```

For R users who are comfortable working with **ggplot2**, `geom_sf()` offers a simple and direct implementation that is suitable for basic map visualizations. To learn more, read the [geom_sf() vignette](https://ggplot2.tidyverse.org/reference/ggsf.html) or the [ggplot2 book](https://ggplot2-book.org/maps.html). 





<!-- ======================================================= -->
## Basemaps { }

### OpenStreetMap {.unnumbered} 

Below we describe how to achieve a basemap for a **ggplot2** map using OpenStreetMap features. Alternative methods include using **ggmap** which requires free registration with Google ([details](https://www.earthdatascience.org/courses/earth-analytics/lidar-raster-data-r/ggmap-basemap/)).  

[**OpenStreetMap**](https://en.wikipedia.org/wiki/OpenStreetMap) is a collaborative project to create a free editable map of the world. The underlying geolocation data (e.g. locations of cities, roads, natural features, airports, schools, hospitals, roads etc) are considered the primary output of the project.

First we load the **OpenStreetMap** package, from which we will get our basemap.  

Then, we create the object `map`, which we define using the function `openmap()` from **OpenStreetMap** package ([documentation](https://www.rdocumentation.org/packages/OpenStreetMap/versions/0.3.4/topics/openmap)). We provide the following:  

* `upperLeft` and `lowerRight` Two coordinate pairs specifying the limits of the basemap tile  
  * In this case we've put in the max and min from the linelist rows, so the map will respond dynamically to the data  
* `zoom = ` (if null it is determined automatically)  
* `type =` which type of basemap - we have listed several possibilities here and the code is currently using the first one (`[1]`) "osm"  
* `mergeTiles = ` we chose TRUE so the basetiles are all merged into one


```{r, message=FALSE, warning=FALSE}
# load package
pacman::p_load(OpenStreetMap)

# Fit basemap by range of lat/long coordinates. Choose tile type
map <- openmap(
  upperLeft = c(max(linelist$lat, na.rm=T), max(linelist$lon, na.rm=T)),   # limits of basemap tile
  lowerRight = c(min(linelist$lat, na.rm=T), min(linelist$lon, na.rm=T)),
  zoom = NULL,
  type = c("osm", "stamen-toner", "stamen-terrain", "stamen-watercolor", "esri","esri-topo")[1])
```

If we plot this basemap right now, using `autoplot.OpenStreetMap()` from **OpenStreetMap** package, you see that the units on the axes are not latitude/longitude coordinates. It is using a different coordinate system. To correctly display the case residences (which are stored in lat/long), this must be changed.  

```{r, warning=F, message=F}
autoplot.OpenStreetMap(map)
```
Thus, we want to convert the map to latitude/longitude with the `openproj()` function from **OpenStreetMap** package. We provide the basemap `map` and also provide the Coordinate Reference System (CRS) we want. We do this by providing the "proj.4" character string for the WGS 1984 projection, but you can provide the CRS in other ways as well. (see [this page](https://www.earthdatascience.org/courses/earth-analytics/spatial-data-r/understand-epsg-wkt-and-other-crs-definition-file-types/) to better understand what a proj.4 string is)  

```{r, warning=F, message=F}
# Projection WGS84
map_latlon <- openproj(map, projection = "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs")
```

Now when we create the plot we see that along the axes are latitude and longitude coordinate. The coordinate system has been converted. Now our cases will plot correctly if overlaid!  

```{r, warning=F, message=F}
# Plot map. Must use "autoplot" in order to work with ggplot
autoplot.OpenStreetMap(map_latlon)
```

See the tutorials [here](http://data-analytics.net/cep/Schedule_files/geospatial.html) and [here](https://www.rdocumentation.org/packages/OpenStreetMap/versions/0.3.4/topics/autoplot.OpenStreetMap) for more info.  





## Contoured density heatmaps {}

Below we describe how to achieve a contoured density heatmap of cases, over a basemap, beginning with a linelist (one row per case).  

1) Create basemap tile from OpenStreetMap, as described above  
2) Plot the cases from `linelist` using the latitude and longitude columns  
3) Convert the points to a density heatmap with `stat_density_2d()` from **ggplot2**, 


When we have a basemap with lat/long coordinates, we can plot our cases on top using the lat/long coordinates of their residence. 

Building on the function `autoplot.OpenStreetMap()` to create the basemap, **ggplot2** functions will easily add on top, as shown with `geom_point()` below:  

```{r, warning=F, message=F}
# Plot map. Must be autoplotted to work with ggplot
autoplot.OpenStreetMap(map_latlon)+                 # begin with the basemap
  geom_point(                                       # add xy points from linelist lon and lat columns 
    data = linelist,                                
    aes(x = lon, y = lat),
    size = 1, 
    alpha = 0.5,
    show.legend = FALSE) +                          # drop legend entirely
  labs(x = "Longitude",                             # titles & labels
       y = "Latitude",
       title = "Cumulative cases")

```
The map above might be difficult to interpret, especially with the points overlapping. So you can instead plot a 2d density map using the **ggplot2** function `stat_density_2d()`. You are still using the linelist lat/lon coordinates, but a 2D kernel density estimation is performed and the results are displayed with contour lines - like a topographical map. Read the full [documentation here](https://ggplot2.tidyverse.org/reference/geom_density_2d.html).  


```{r, warning=F, message=F}
# begin with the basemap
autoplot.OpenStreetMap(map_latlon)+
  
  # add the density plot
  ggplot2::stat_density_2d(
        data = linelist,
        aes(
          x = lon,
          y = lat,
          fill = ..level..,
          alpha = ..level..),
        bins = 10,
        geom = "polygon",
        contour_var = "count",
        show.legend = F) +                          
  
  # specify color scale
  scale_fill_gradient(low = "black", high = "red")+
  
  # labels 
  labs(x = "Longitude",
       y = "Latitude",
       title = "Distribution of cumulative cases")

```





<!-- ======================================================= -->
### Time series heatmap {.unnumbered}

The density heatmap above shows *cumulative cases*. We can examine the outbreak over time and space by faceting the heatmap based on the *month of symptom onset*, as derived from the linelist.  

We begin in the `linelist`, creating a new column with the Year and Month of onset. The `format()` function from **base** R changes how a date is displayed. In this case we want "YYYY-MM".  

```{r, warning=F, message=F}
# Extract month of onset
linelist <- linelist %>% 
  mutate(date_onset_ym = format(date_onset, "%Y-%m"))

# Examine the values 
table(linelist$date_onset_ym, useNA = "always")
```

Now, we simply introduce facetting via **ggplot2** to the density heatmap. `facet_wrap()` is applied, using the new column as rows. We set the number of facet columns to 3 for clarity.  


```{r, warning=F, message=F}
# packages
pacman::p_load(OpenStreetMap, tidyverse)

# begin with the basemap
autoplot.OpenStreetMap(map_latlon)+
  
  # add the density plot
  ggplot2::stat_density_2d(
        data = linelist,
        aes(
          x = lon,
          y = lat,
          fill = ..level..,
          alpha = ..level..),
        bins = 10,
        geom = "polygon",
        contour_var = "count",
        show.legend = F) +                          
  
  # specify color scale
  scale_fill_gradient(low = "black", high = "red")+
  
  # labels 
  labs(x = "Longitude",
       y = "Latitude",
       title = "Distribution of cumulative cases over time")+
  
  # facet the plot by month-year of onset
  facet_wrap(~ date_onset_ym, ncol = 4)               

```



<!-- SPATIAL STATISTICS SECTION IS UNDER DEVELOPMENT --> 
## Spatial statistics
Most of our discussion so far has focused on visualization of spatial data. In some cases, you may also be interested in using *spatial statistics* to quantify the spatial relationships of attributes in your data. This section will provide a very brief overview of some key concepts in spatial statistics, and suggest some resources that will be helpful to explore if you wish to do more comprehensive spatial analyses. 

### Spatial relationships {.unnumbered}  

Before we can calculate any spatial statistics, we need to specify the relationships between features in our data. There are many ways to conceptualize spatial relationships, but a simple and commonly-applicable model to use is that of *adjacency* - specifically, that we expect a geographic relationship between areas that share a border or “neighbour” one another. 

We can quantify adjacency relationships between administrative region polygons in the `sle_adm3` data we have been using with the **spdep** package. We will specify *queen* contiguity, which means that regions will be neighbors if they share at least one point along their borders. The alternative would be *rook* contiguity, which requires that regions share an edge - in our case, with irregular polygons, the distinction is trivial, but in some cases the choice between queen and rook can be influential.  
 
```{r}
sle_nb <- spdep::poly2nb(sle_adm3_dat, queen=T) # create neighbors 
sle_adjmat <- spdep::nb2mat(sle_nb)    # create matrix summarizing neighbor relationships
sle_listw <- spdep::nb2listw(sle_nb)   # create listw (list of weights) object -- we will need this later

sle_nb
round(sle_adjmat, digits = 2)
```

The matrix printed above shows the relationships between the 9 regions in our `sle_adm3` data. A score of 0 indicates two regions are not neighbors, while any value other than 0 indicates a neighbor relationship. The values in the matrix are scaled so that each region has a total row weight of 1.

A better way to visualize these neighbor relationships is by plotting them:
```{r, fig.align='center', results='hide'}
plot(sle_adm3_dat$geometry) +                                           # plot region boundaries
  spdep::plot.nb(sle_nb,as(sle_adm3_dat, 'Spatial'), col='grey', add=T) # add neighbor relationships
```

We have used an adjacency approach to identify neighboring polygons; the neighbors we identified are also sometimes called **contiguity-based neighbors**. But this is just one way of choosing which regions are expected to have a geographic relationship. The most common alternative approaches for identifying geographic relationships generate **distance-based neighbors**; briefly, these are:
  
  * **K-nearest neighbors** - Based on the distance between centroids (the geographically-weighted center of each polygon region), select the *n* closest regions as neighbors. A maximum-distance proximity threshold may also be specified. In **spdep**, you can use `knearneigh()` (see [documentation](https://r-spatial.github.io/spdep/reference/knearneigh.html)).
  
  * **Distance threshold neighbors** - Select all neighbors within a distance threshold. In **spdep**, these neighbor relationships can be identified using `dnearneigh()` (see [documentation](https://www.rdocumentation.org/packages/spdep/versions/1.1-7/topics/dnearneigh)).

### Spatial autocorrelation {.unnumbered}  

Tobler's oft-cited first law of geography states that "everything is related to everything else, but near things are more related than distant things." In epidemiology, this often means that risk of a particular health outcome in a given region is more similar to its neighboring regions than to those far away. This concept has been formalized as **spatial autocorrelation** - the statistical property that geographic features with similar values are clustered together in space. Statistical measures of spatial autocorrelation can be used to *quantify the extent of spatial clustering* in your data, *locate where clustering occurs*, and *identify shared patterns of spatial autocorrelation* between distinct variables in your data. This section gives an overview of some common measures of spatial autocorrelation and how to calculate them in R.

**Moran's I** - This is a global summary statistic of the correlation between the value of a variable in one region, and the values of the same variable in neighboring regions. The Moran's I statistic typically ranges from -1 to 1. A value of 0 indicates no pattern of spatial correlation, while values closer to 1 or -1 indicate stronger spatial autocorrelation (similar values close together) or spatial dispersion (dissimilar values close together), respectively.

For an example, we will calculate a Moran's I statistic to quantify the spatial autocorrelation in Ebola cases we mapped earlier (remember, this is a subset of cases from the simulated epidemic `linelist` dataframe). The **spdep** package has a function, `moran.test`, that can do this calculation for us:

```{r}
moran_i <-spdep::moran.test(sle_adm3_dat$cases,    # numeric vector with variable of interest
                            listw=sle_listw)       # listw object summarizing neighbor relationships

moran_i                                            # print results of Moran's I test
```
The output from the `moran.test()` function shows us a Moran I statistic of ` round(moran_i$estimate[1],2)`. This indicates the presence of spatial autocorrelation in our data - specifically, that regions with similar numbers of Ebola cases are likely to be close together. The p-value provided by `moran.test()` is generated by comparison to the expectation under null hypothesis of no spatial autocorrelation, and can be used if you need to report the results of a formal hypothesis test.

**Local Moran's I** - We can decompose the (global) Moran's I statistic calculated above to identify *localized* spatial autocorrelation; that is, to identify specific clusters in our data. This statistic, which is sometimes called a **Local Indicator of Spatial Association (LISA)** statistic, summarizes the extent of spatial autocorrelation around each individual region. It can be useful for finding "hot" and "cold" spots on the map.

To show an example, we can calculate and map Local Moran's I for the Ebola case counts used above, with the `local_moran()` function from **spdep**:
```{r, fig.align='center'}
# calculate local Moran's I
local_moran <- spdep::localmoran(                  
  sle_adm3_dat$cases,                              # variable of interest
  listw=sle_listw                                  # listw object with neighbor weights
)

# join results to sf data
sle_adm3_dat<- cbind(sle_adm3_dat, local_moran)    

# plot map
ggplot(data=sle_adm3_dat) +
  geom_sf(aes(fill=Ii)) +
  theme_bw() +
  scale_fill_gradient2(low="#2c7bb6", mid="#ffffbf", high="#d7191c",
                       name="Local Moran's I") +
  labs(title="Local Moran's I statistic for Ebola cases",
       subtitle="Admin level 3 regions, Sierra Leone")

```

**Getis-Ord Gi\*** - This is another statistic that is commonly used for hotspot analysis; in large part, the popularity of this statistic relates to its use in the Hot Spot Analysis tool in ArcGIS. It is based on the assumption that typically, the difference in a variable's value between neighboring regions should follow a normal distribution. It uses a z-score approach to identify regions that have significantly higher (hot spot) or significantly lower (cold spot) values of a specified variable, compared to their neighbors. 

We can calculate and map the Gi* statistic using the `localG()` function from **spdep**:  

```{r}
# Perform local G analysis
getis_ord <- spdep::localG(
  sle_adm3_dat$cases,
  sle_listw
)

# join results to sf data
sle_adm3_dat$getis_ord <- getis_ord

# plot map
ggplot(data=sle_adm3_dat) +
  geom_sf(aes(fill=getis_ord)) +
  theme_bw() +
  scale_fill_gradient2(low="#2c7bb6", mid="#ffffbf", high="#d7191c",
                       name="Gi*") +
  labs(title="Getis-Ord Gi* statistic for Ebola cases",
       subtitle="Admin level 3 regions, Sierra Leone")

```


As you can see, the map of Getis-Ord Gi* looks slightly different from the map of Local Moran's I produced earlier. This reflects that the method used to calculate these two statistics are slightly different; which one you should use depends on your specific use case and the research question of interest.

**Lee's L test** - This is a statistical test for bivariate spatial correlation. It allows you to test whether the spatial pattern for a given variable *x* is similar to the spatial pattern of another variable, *y*, that is hypothesized to be related spatially to *x*. 

To give an example, let's test whether the spatial pattern of Ebola cases from the simulated epidemic is correlated with the spatial pattern of population. To start, we need to have a `population` variable in our `sle_adm3` data. We can use the `total` variable from the `sle_adm3_pop` dataframe that we loaded earlier.

```{r}
sle_adm3_dat <- sle_adm3_dat %>% 
  rename(population = total)                          # rename 'total' to 'population'
```

We can quickly visualize the spatial patterns of the two variables side by side, to see whether they look similar:
```{r, fig.align='center', warning=F, message=F}
tmap_mode("plot")

cases_map <- tm_shape(sle_adm3_dat) + tm_polygons("cases") + tm_layout(main.title="Cases")
pop_map <- tm_shape(sle_adm3_dat) + tm_polygons("population") + tm_layout(main.title="Population")

tmap_arrange(cases_map, pop_map, ncol=2)   # arrange into 2x1 facets
```

Visually, the patterns seem dissimilar. We can use the `lee.test()` function in **spdep** to test statistically whether the pattern of spatial autocorrelation in the two variables is related. The L statistic will be close to 0 if there is no correlation between the patterns, close to 1 if there is a strong positive correlation (i.e. the patterns are similar), and close to -1 if there is a strong negative correlation (i.e. the patterns are inverse). 

```{r, warning=F, message=F}
lee_test <- spdep::lee.test(
  x=sle_adm3_dat$cases,          # variable 1 to compare
  y=sle_adm3_dat$population,     # variable 2 to compare
  listw=sle_listw                # listw object with neighbor weights
)

lee_test
```

The output above shows that the Lee's L statistic for our two variables was ` round(lee_test$estimate[1],2)`, which indicates weak negative correlation. This confirms our visual assessment that the pattern of cases and population are not related to one another, and provides evidence that the spatial pattern of cases is not strictly a result of population density in high-risk areas.

The Lee L statistic can be useful for making these kinds of inferences about the relationship between spatially distributed variables; however, to describe the nature of the relationship between two variables in more detail, or adjust for confounding, *spatial regression* techniques will be needed. These are described briefly in the following section.

### Spatial regression {.unnumbered}  

You may wish to make statistical inferences about the relationships between variables in your spatial data. In these cases, it is useful to consider *spatial regression* techniques - that is, approaches to regression that explicitly consider the spatial organization of units in your data. Some reasons that you may need to consider spatial regression models, rather than standard regression models such as GLMs, include:

  * Standard regression models assume that residuals are independent from one another. In the presence of strong *spatial autocorrelation*, the residuals of a standard regression model are likely to be spatially autocorrelated as well, thus violating this assumption. This can lead to problems with interpreting the model results, in which case a spatial model would be preferred.
  
  * Regression models also typically assume that the effect of a variable *x* is constant over all observations. In the case of *spatial heterogeneity*, the effects we wish to estimate may vary over space, and we may be interested in quantifying those differences. In this case, spatial regression models offer more flexibility for estimating and interpreting effects.
  
The details of spatial regression approaches are beyond the scope of this handbook. This section will instead provide an overview of the most common spatial regression models and their uses, and refer you to references that may of use if you wish to explore this area further.

**Spatial error models** - These models assume that the error terms across spatial units are correlated, in which case the data would violate the assumptions of a standard OLS model. Spatial error models are also sometimes referred to as **simultaneous autoregressive (SAR) models**. They can be fit using the `errorsarlm()` function in the **spatialreg** package (spatial regression functions which used to be a part of **spdep**). 

**Spatial lag models** - These models assume that the dependent variable for a region *i* is influenced not only by value of independent variables in *i*, but also by the values of those variables in regions neighboring *i*. Like spatial error models, spatial lag models are also sometimes described as **simultaneous autoregressive (SAR) models**.  They can be fit using the `lagsarlm()` function in the **spatialreg** package.

The **spdep** package contains several useful diagnostic tests for deciding between standard OLS, spatial lag, and spatial error models. These tests, called *Lagrange Multiplier diagnostics*, can be used to identify the type of spatial dependence in your data and choose which model is most appropriate. The function `lm.LMtests()` can be used to calculate all of the Lagrange Multiplier tests. Anselin (1988) also provides a useful flow chart tool to decide which spatial regression model to use based on the results of the Lagrange Multiplier tests:

```{r, fig.align='center', echo=F}
knitr::include_graphics(here::here("images", "gis_lmflowchart.jpg"))
```

**Bayesian hierarchical models** - Bayesian approaches are commonly used for some applications in spatial analysis, most commonly for [disease mapping](https://pubmed.ncbi.nlm.nih.gov/15690999/). They are preferred in cases where case data are sparsely distributed (for example, in the case of a rare outcome) or statistically "noisy", as they can be used to generate "smoothed" estimates of disease risk by accounting for the underlying latent spatial process. This may improve the quality of estimates. They also allow investigator pre-specification (via choice of prior) of complex spatial correlation patterns that may exist in the data, which can account for spatially-dependent and -independent variation in both independent and dependent variables. In R, Bayesian hierarchical models can be fit using the **CARbayes** package (see [vignette](https://cran.r-project.org/web/packages/CARBayes/vignettes/CARBayes.pdf)) or R-INLA (see [website](https://www.r-inla.org/home) and [textbook](https://becarioprecario.bitbucket.io/inla-gitbook/)). R can also be used to call external software that does Bayesian estimation, such as JAGS or WinBUGS.

<!-- ======================================================= -->
## Resources {  }

* R Simple Features and sf package [vignette](https://cran.r-project.org/web/packages/sf/vignettes/sf1.html)


* R tmap package [vignette](https://cran.r-project.org/web/packages/tmap/vignettes/tmap-getstarted.html)


* ggmap: [Spatial Visualization with ggplot2](https://journal.r-project.org/archive/2013-1/kahle-wickham.pdf)


* [Intro to making maps with R, overview of different packages](https://bookdown.org/nicohahn/making_maps_with_r5/docs/introduction.html)  

* Spatial Data in R [(EarthLab course)](https://www.earthdatascience.org/courses/earth-analytics/spatial-data-r/)

* Applied Spatial Data Analysis in R [textbook](https://link.springer.com/book/10.1007/978-1-4614-7618-4)

* **SpatialEpiApp** - a [Shiny app that is downloadable as an R package](https://github.com/Paula-Moraga/SpatialEpiApp), allowing you to provide your own data and conduct mapping, cluster analysis, and spatial statistics.  


* An Introduction to Spatial Econometrics in R [workshop](http://www.econ.uiuc.edu/~lab/workshop/Spatial_in_R.html)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/gis.Rmd-->

# (PART) Trực quan hóa dữ liệu {.unnumbered}
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/cat_data_viz.Rmd-->


# Trình bày bảng {#tables-presentation}  


```{r echo=FALSE, fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}

linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds")) 

border_style = officer::fp_border(color="black", width=1)

pacman::p_load(
  rio,            # import/export
  here,           # file pathways
  flextable,      # make HTML tables 
  officer,        # helper functions for tables
  tidyverse)      # data management, summary, and visualization

table <- linelist %>% 
  # filter
  ########
  #filter(!is.na(outcome) & hospital != "Missing") %>%  # Remove cases with missing outcome or hospital
  
  # Get summary values per hospital-outcome group
  ###############################################
  group_by(hospital, outcome) %>%                      # Group data
  summarise(                                           # Create new summary columns of indicators of interest
    N = n(),                                            # Number of rows per hospital-outcome group     
    ct_value = median(ct_blood, na.rm=T)) %>%           # median CT value per group
  
  # add totals
  ############
  bind_rows(                                           # Bind the previous table with this mini-table of totals
    linelist %>% 
      filter(!is.na(outcome) & hospital != "Missing") %>%
      group_by(outcome) %>%                            # Grouped only by outcome, not by hospital    
      summarise(
        N = n(),                                       # Number of rows for whole dataset     
        ct_value = median(ct_blood, na.rm=T))) %>%     # Median CT for whole dataset
  
  # Pivot wider and format
  ########################
  mutate(hospital = replace_na(hospital, "Total")) %>% 
  pivot_wider(                                         # Pivot from long to wide
    values_from = c(ct_value, N),                       # new values are from ct and count columns
    names_from = outcome) %>%                           # new column names are from outcomes
  mutate(                                              # Add new columns
    N_Known = N_Death + N_Recover,                               # number with known outcome
    Pct_Death = scales::percent(N_Death / N_Known, 0.1),         # percent cases who died (to 1 decimal)
    Pct_Recover = scales::percent(N_Recover / N_Known, 0.1)) %>% # percent who recovered (to 1 decimal)
  select(                                              # Re-order columns
    hospital, N_Known,                                   # Intro columns
    N_Recover, Pct_Recover, ct_value_Recover,            # Recovered columns
    N_Death, Pct_Death, ct_value_Death)  %>%             # Death columns
  arrange(N_Known) %>%                                 # Arrange rows from lowest to highest (Total row at bottom)

  # formatting
  ############
  flextable() %>% 
  add_header_row(
    top = TRUE,                # New header goes on top of existing header row
    values = c("Hospital",     # Header values for each column below
               "Total cases with known outcome", 
               "Recovered",    # This will be the top-level header for this and two next columns
               "",
               "",
               "Died",         # This will be the top-level header for this and two next columns
               "",             # Leave blank, as it will be merged with "Died"
               "")) %>% 
    set_header_labels(         # Rename the columns in original header row
      hospital = "", 
      N_Known = "",                  
      N_Recover = "Total",
      Pct_Recover = "% of cases",
      ct_value_Recover = "Median CT values",
      N_Death = "Total",
      Pct_Death = "% of cases",
      ct_value_Death = "Median CT values")  %>% 
  merge_at(i = 1, j = 3:5, part = "header") %>% # Horizontally merge columns 3 to 5 in new header row
  merge_at(i = 1, j = 6:8, part = "header") %>%  
  border_remove() %>%  
  theme_booktabs() %>% 
  vline(part = "all", j = 2, border = border_style) %>%   # at column 2 
  vline(part = "all", j = 5, border = border_style) %>%   # at column 5
  merge_at(i = 1:2, j = 1, part = "header") %>% 
  merge_at(i = 1:2, j = 2, part = "header") %>% 
  width(j=1, width = 2.7) %>% 
  width(j=2, width = 1.5) %>% 
  width(j=c(4,5,7,8), width = 1) %>% 
  flextable::align(., align = "center", j = c(2:8), part = "all") %>% 
  bg(., part = "body", bg = "gray95")  %>% 
  #bg(., j=c(1:8), i= ~ hospital == "Military Hospital", part = "body", bg = "#91c293") %>% 
  bg(j = 7, i = ~ Pct_Death >= 55, part = "body", bg = "red") %>% 
  colformat_num(., j = c(4,7), digits = 1) %>%
  bold(i = 1, bold = TRUE, part = "header") %>% 
  bold(i = 7, bold = TRUE, part = "body")

table
```


Chương này sẽ trình bày cách chuyển đổi một bảng tóm tắt dữ liệu thành các bảng sẵn sàng cho mục đích trình bày với package **flextable**. Các bảng này có thể được chèn vào slide powerpoint, trang HTML, tài liệu PDF hoặc Word, v.v.  

Hãy hiểu rằng *trước khi* sử dụng **flextable**, bạn phải tạo bảng tóm tắt dữ liệu dưới dạng một data frame. Sử dụng các phương pháp trong các chương [Bảng mô tả] và [Xoay trục dữ liệu] như tạo bảng đơn, tạo bảng chéo, xoay trục, và tính toán các thống kê mô tả. Kết quả là một data frame sau đó có thể được chuyển đến **flextable** để định dạng hiển thị.  


Có nhiều các R packages khác có thể được sử dụng để tạo bảng cho mục đích trình bày - trong chương này chúng tôi nhấm mạnh vào package **flextable**. Một ví dụ sử dụng **knitr** package và hàm của nó `kable()` có thể được tìm thấy trong chương [Truy vết tiếp xúc]. Tương tự như vậy, package **DT** cũng được nhấn mạnh trong chương [Dashboards với Shiny]. Các package khác như **GT** và **huxtable** được đề cập trong chương [Package đề xuất].  



<!-- ======================================================= -->
## Chuẩn bị {  }

### Gọi packages {.unnumbered} 

Hãy cài đặt và gọi package **flextable**. Trong sổ tay này chúng tôi nhấn mạnh việc sử dụng hàm `p_load()` từ package **pacman**, giúp cài đặt package nếu cần thiết *và* gọi chúng ra để sử dụng. Bạn cũng có thể gọi package bằng lệnh `library()` từ **base** R. Xem thêm chương [R cơ bản] để biết thêm các thông tin về các package trong R.  

```{r}
pacman::p_load(
  rio,            # import/export
  here,           # file pathways
  flextable,      # make HTML tables 
  officer,        # helper functions for tables
  tidyverse)      # data management, summary, and visualization

```

### Nhập dữ liệu {.unnumbered}  

Để bắt đầu, chúng ta nhập bộ dữ liệu linelist đã được làm sạch về các ca bệnh Ebola mô phỏng. Để tiện theo dõi, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>bấm để tải dữ liệu linelist "đã làm sạch"</a> (as .rds file). Nhập dữ liệu bằng hàm `import()` từ package **rio** (chấp nhận nhiều loại tập tin như .xlsx, .csv, .rds - xem thêm chương [Nhập xuất dữ liệu] để biết thêm chi tiết). 


```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.rds")
```

50 hàng đầu tiên của bộ dữ liệu linelist được hiển thị như dưới đây.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

### Chuẩn bị bảng {.unnumbered}  

*Trước khi* bắt đầu sử dụng **flextable** bạn cần phải *tạo* bảng của bạn dưới một data frame. Xem chương [Bảng mô tả] và [Xoay trục dữ liệu] để biết cách tạo một data frame sử dụng các packages như **janitor** và **dplyr**. Đầu tiên, bạn phải sắp xếp nội dung theo hàng và cột như cách bạn muốn nội dung hiển thị. Sau đó, data frame sẽ được chuyển đến **flextable** để hiển thị nó với màu sắc, tiêu đề, phông chữ, v.v. 
  
Dưới đây là một ví dụ trong chương [Bảng mô tả] về cách biến đổi các trường hợp bệnh trong `linelist` thành một data frame để tóm tắt các outcomes của bệnh nhân và giá trị CT theo bệnh viện, với hàng Tổng ở cuối bảng. Đầu ra được lưu dưới dạng `table`.  

```{r message=FALSE, warning=FALSE}
table <- linelist %>% 
  
  # Get summary values per hospital-outcome group
  ###############################################
  group_by(hospital, outcome) %>%                      # Group data
  summarise(                                           # Create new summary columns of indicators of interest
    N = n(),                                            # Number of rows per hospital-outcome group     
    ct_value = median(ct_blood, na.rm=T)) %>%           # median CT value per group
  
  # add totals
  ############
  bind_rows(                                           # Bind the previous table with this mini-table of totals
    linelist %>% 
      filter(!is.na(outcome) & hospital != "Missing") %>%
      group_by(outcome) %>%                            # Grouped only by outcome, not by hospital    
      summarise(
        N = n(),                                       # Number of rows for whole dataset     
        ct_value = median(ct_blood, na.rm=T))) %>%     # Median CT for whole dataset
  
  # Pivot wider and format
  ########################
  mutate(hospital = replace_na(hospital, "Total")) %>% 
  pivot_wider(                                         # Pivot from long to wide
    values_from = c(ct_value, N),                       # new values are from ct and count columns
    names_from = outcome) %>%                           # new column names are from outcomes
  mutate(                                              # Add new columns
    N_Known = N_Death + N_Recover,                               # number with known outcome
    Pct_Death = scales::percent(N_Death / N_Known, 0.1),         # percent cases who died (to 1 decimal)
    Pct_Recover = scales::percent(N_Recover / N_Known, 0.1)) %>% # percent who recovered (to 1 decimal)
  select(                                              # Re-order columns
    hospital, N_Known,                                   # Intro columns
    N_Recover, Pct_Recover, ct_value_Recover,            # Recovered columns
    N_Death, Pct_Death, ct_value_Death)  %>%             # Death columns
  arrange(N_Known)                                    # Arrange rows from lowest to highest (Total row at bottom)

table  # print

```




<!-- ======================================================= -->
## Cơ bản về flextable {  }

### Tạo một flextable {.unnumbered}  

Để tạo và quản lý các đối tượng của **flextable**, đầu tiên chúng ta đẩy data frame vào hàm `flextable()`, sau đó lưu kết quả là `my_table`.  

```{r}

my_table <- flextable(table) 
my_table

```

Sau khi thực hiện việc này, chúng ta có thể pipe dần đối tượng `my_table` thông qua các hàm định dạng **flextable** khác.  

Trong trang này để rõ ràng, chúng tôi sẽ lưu bảng ở các bước trung gian vào đối tượng `my_table`, thêm các hàm **flextable** theo từng bước. Nếu bạn muốn xem *tất cả* code từ đầu đến cuối được viết trong một đoạn, hãy xem mục [Kết hợp tất cả các code](#tbl_pres_all) phía dưới đây.  

Cú pháp chung của mỗi dòng code **flextable** như sau:

* `function(table, i = X, j = X, part = "X")`, where:
  * 'function' có thể là một trong số rất nhiều hàm khác nhau, ví dụ như `width()` để xác định độ rộng cột, `bg()` để thiết lập màu nền, `align()` để điều chỉnh văn bản căn giữa / phải / trái, v.v. 
  * `table = ` tên của data frame, có thể bỏ qua nếu như data frame được piping vào trong hàm.
  * `part = ` đề cập đến phần nào của bảng mà hàm đang được áp dụng. Ví dụ. "tiêu đề", "nội dung" hoặc "tất cả". 
  * `i = ` chỉ định *hàng* mà hàm sẽ được áp dụng, trong đó 'X' là số thứ tự hàng. Nếu nhiều hàng được chọn, ví dụ: từ hàng đầu tiên đến hàng thứ ba, ta có thể viết: `i = c (1: 3)`. Lưu ý nếu chọn 'body', hàng đầu tiên bắt đầu từ bên dưới phần tiêu đề.
  * `j = ` chỉ định *cột* mà hàm sẽ được áp dụng, trong đó 'X' là số thứ tự cột hoặc tên cột. Nếu nhiều cột được chọn, ví dụ: từ hàng thứ năm đến hàng thứ sáu, ta có thể viết: `j = c(5,6)`. 
  
Bạn có thể tìm thấy danh sách đầy đủ các hàm định dạng trong package **flextable** [tại đây](https://davidgohel.github.io/flextable/reference/index.html) hoặc xem tài liệu hướng dẫn bằng cách gõ `?flextable`.  


### Độ rộng cột {.unnumbered}

Chúng ta có thể sử dụng hàm `autofit()` để điều chỉnh bảng sao cho mỗi ô chỉ có một hàng văn bản. Hàm `qflextable()` là một cách viết tắt thuận tiện cho `flextable()` và `autofit()`.  

```{r}

my_table %>% autofit()

```

Tuy nhiên, điều này có thể không phải lúc nào cũng phù hợp, đặc biệt nếu có các giá trị rất dài trong các ô, nghĩa là bảng có thể không vừa trong độ rộng của trang.

Thay vào đó, chúng ta có thể điều chỉnh độ rộng cột bằng hàm `width()`. Điều này có thể tốn một chút thời gian để tìm giá trị chiều rộng phù hợp cho các cột. Trong ví dụ dưới đây, chúng ta chỉ định các độ rộng khác nhau cho cột 1, cột 2 và cột 4 đến 8. 

```{r}

my_table <- my_table %>% 
  width(j=1, width = 2.7) %>% 
  width(j=2, width = 1.5) %>% 
  width(j=c(4,5,7,8), width = 1)

my_table
  
```

### Tiêu đề cột {.unnumbered}

Một bảng có nhiều tiêu đề cột sẽ giúp giải thích nội dung bảng một cách dễ dàng hơn.

Đối với bảng này, chúng ta cần thêm một lớp tiêu đề thứ hai để các cột bao gồm các nhóm con giống nhau có thể được nhóm lại với nhau. Chúng ta thực hiện điều này bằng hàm `add_header_row()` với `top = TRUE`. Chúng ta cung cấp tên mới của mỗi cột bằng `values = `, bỏ trống `""` đối với các cột chúng ta dự định sẽ ghép lại với nhau sau này.  

Chúng ta cũng đổi tên các tên tiêu đề phụ ở hàng thứ hai bằng lệnh `set_header_labels()`.  

Cuối cùng, chúng ta sử dụng hàm `merge_at ()` để hợp nhất các tiêu đề cột trong hàng tiêu đề trên cùng.  

```{r}
my_table <- my_table %>% 
  
  add_header_row(
    top = TRUE,                # New header goes on top of existing header row
    values = c("Hospital",     # Header values for each column below
               "Total cases with known outcome", 
               "Recovered",    # This will be the top-level header for this and two next columns
               "",
               "",
               "Died",         # This will be the top-level header for this and two next columns
               "",             # Leave blank, as it will be merged with "Died"
               "")) %>% 
    
  set_header_labels(         # Rename the columns in original header row
      hospital = "", 
      N_Known = "",                  
      N_Recover = "Total",
      Pct_Recover = "% of cases",
      ct_value_Recover = "Median CT values",
      N_Death = "Total",
      Pct_Death = "% of cases",
      ct_value_Death = "Median CT values")  %>% 
  
  merge_at(i = 1, j = 3:5, part = "header") %>% # Horizontally merge columns 3 to 5 in new header row
  merge_at(i = 1, j = 6:8, part = "header")     # Horizontally merge columns 6 to 8 in new header row

my_table  # print

```

### Đường viền và nền {.unnumbered}  

Bạn có thể điều chỉnh đường viền, đường bên trong, v.v. bằng các hàm khác nhau trong **flextable**. Để dễ dàng, thông thường đầu tiên bạn cần loại bỏ hết các đường viền trong bảng bằng hàm `border_remove()`.  

Sau đó, bạn có thể áp dụng các theme đường viền mặc định bằng cách đưa bảng tới hàm `theme_box()`, `theme_booktabs()`, hoặc `theme_alafoli()`.  

Bạn có thể thêm các đường dọc và ngang bằng nhiều hàm khác nhau. `hline()` và `vline()` sẽ thêm các dòng vào một hàng hoặc cột cụ thể. Bên trong hàm, bạn cần chỉ định phần mà bảng sẽ áp dụng qua đối số `part = ` với các tùy chọn "all", "body", hoặc "header". Đối với các đường dọc, ghi rõ cột được áp dụng với `j = `, đối với các đường ngang, ghi rõ hàng được áp dụng với `i = `. Các hàm khác như `vline_right()`, `vline_left()`, `hline_top()`, và `hline_bottom()` chỉ thêm các đường viền ở bên ngoài.  

Bên trong tất cả các hàm này, kiểu đường phải được định nghĩa thông qua đối số `border = ` và phải là đầu ra của một lệnh riêng biệt bằng cách sử dụng hàm `fp_border()` từ package **officer**. Hàm này giúp bạn xác định độ rộng và màu sắc của đường. Bạn có thể định nghĩa các thông tin này phía trên trước khi thực hiện các lệnh liên quan tới bảng, như được trình bày dưới đây:  

```{r}
# define style for border line
border_style = officer::fp_border(color="black", width=1)

# add border lines to table
my_table <- my_table %>% 

  # Remove all existing borders
  border_remove() %>%  
  
  # add horizontal lines via a pre-determined theme setting
  theme_booktabs() %>% 
  
  # add vertical lines to separate Recovered and Died sections
  vline(part = "all", j = 2, border = border_style) %>%   # at column 2 
  vline(part = "all", j = 5, border = border_style)       # at column 5

my_table
```

### Phông chữ và căn chỉnh {.unnumbered}

Chúng ta căn giữa tất cả các cột ngoại trừ cột ngoài cùng bên trái với tên các bệnh viện, bằng cách sử dụng hàm `align()` từ **flextable**.

```{r}
my_table <- my_table %>% 
   flextable::align(align = "center", j = c(2:8), part = "all") 
my_table
```

Ngoài ra, chúng ta có thể tăng kích thước phông chữ tiêu đề và sau đó thay đổi thành in đậm. Chúng ta cũng có thể thay đổi hàng "Total" thành in đậm.  

```{r}

my_table <-  my_table %>%  
  fontsize(i = 1, size = 12, part = "header") %>%   # adjust font size of header
  bold(i = 1, bold = TRUE, part = "header") %>%     # adjust bold face of header
  bold(i = 7, bold = TRUE, part = "body")           # adjust bold face of total row (row 7 of body)

my_table

```


Chúng ta cũng có thể thiết lập để các cột tỷ lệ chỉ hiển thị một chữ số thập phân bằng cách sử dụng hàm `colformat_num()`. Lưu ý rằng điều này cũng có thể được thực hiện ở giai đoạn quản lý dữ liệu với hàm `round()`. 

```{r}
my_table <- colformat_num(my_table, j = c(4,7), digits = 1)
my_table
```

### Hợp nhất ô {.unnumbered}  

Cũng giống như khi chúng ta hợp nhất các ô theo chiều ngang trong hàng tiêu đề, chúng ta cũng có thể hợp nhất các ô theo chiều dọc bằng cách sử dụng `merge_at()` và chỉ rõ các hàng (`i`) và cột (`j`). Ở đây chúng ta hợp nhất ô "Hospital" và "Total cases with known outcome" theo chiều dọc để cung cấp thêm không gian cho chúng.   

```{r}
my_table <- my_table %>% 
  merge_at(i = 1:2, j = 1, part = "header") %>% 
  merge_at(i = 1:2, j = 2, part = "header")

my_table
```

### Màu nền {.unnumbered}

Để phân biệt nội dung của bảng với các tiêu đề, chúng ta có thể muốn thêm định dạng bổ sung, ví dụ như thay đổi màu nền. Trong ví dụ này, chúng ta sẽ thay đổi nội dung bảng thành màu xám.

```{r}
my_table <- my_table %>% 
    bg(part = "body", bg = "gray95")  

my_table 
```


<!-- ======================================================= -->
## Định dạng có điều kiện {  }

Chúng ta có thể highlight tất cả các giá trị trong một cột đáp ứng một quy tắc nhất định, ví dụ các ô có hơn 55% trường hợp tử vong. Đơn giản chỉ cần đặt điều kiện so sánh vào trong đối số `i = ` hoặc `j = `, phía sau dấu `~`. Bạn cần tham chiếu tới thứ tự cột cần highlight trong trong data frame, không phải tiêu đề cột.  

```{r}

my_table %>% 
  bg(j = 7, i = ~ Pct_Death >= 55, part = "body", bg = "red") 

```



Hoặc, chúng ta có thể highlight toàn bộ hàng đáp ứng một tiêu chí nhất định, chẳng hạn như tên một bệnh viện. Để làm điều này đơn giản chỉ cần không định danh thông số ở đối số (`j`), để các tiêu chí được áp dụng cho tất cả các cột.


```{r}

my_table %>% 
  bg(., i= ~ hospital == "Military Hospital", part = "body", bg = "#91c293") 

```

## Kết hợp tất cả các code {#tbl_pres_all}  


Dưới đây, chúng tôi ghép tất cả code từ các phần trên lại với nhau. 

```{r}  

border_style = officer::fp_border(color="black", width=1)

pacman::p_load(
  rio,            # import/export
  here,           # file pathways
  flextable,      # make HTML tables 
  officer,        # helper functions for tables
  tidyverse)      # data management, summary, and visualization

table <- linelist %>% 

  # Get summary values per hospital-outcome group
  ###############################################
  group_by(hospital, outcome) %>%                      # Group data
  summarise(                                           # Create new summary columns of indicators of interest
    N = n(),                                            # Number of rows per hospital-outcome group     
    ct_value = median(ct_blood, na.rm=T)) %>%           # median CT value per group
  
  # add totals
  ############
  bind_rows(                                           # Bind the previous table with this mini-table of totals
    linelist %>% 
      filter(!is.na(outcome) & hospital != "Missing") %>%
      group_by(outcome) %>%                            # Grouped only by outcome, not by hospital    
      summarise(
        N = n(),                                       # Number of rows for whole dataset     
        ct_value = median(ct_blood, na.rm=T))) %>%     # Median CT for whole dataset
  
  # Pivot wider and format
  ########################
  mutate(hospital = replace_na(hospital, "Total")) %>% 
  pivot_wider(                                         # Pivot from long to wide
    values_from = c(ct_value, N),                       # new values are from ct and count columns
    names_from = outcome) %>%                           # new column names are from outcomes
  mutate(                                              # Add new columns
    N_Known = N_Death + N_Recover,                               # number with known outcome
    Pct_Death = scales::percent(N_Death / N_Known, 0.1),         # percent cases who died (to 1 decimal)
    Pct_Recover = scales::percent(N_Recover / N_Known, 0.1)) %>% # percent who recovered (to 1 decimal)
  select(                                              # Re-order columns
    hospital, N_Known,                                   # Intro columns
    N_Recover, Pct_Recover, ct_value_Recover,            # Recovered columns
    N_Death, Pct_Death, ct_value_Death)  %>%             # Death columns
  arrange(N_Known) %>%                                 # Arrange rows from lowest to highest (Total row at bottom)

  # formatting
  ############
  flextable() %>%              # table is piped in from above
  add_header_row(
    top = TRUE,                # New header goes on top of existing header row
    values = c("Hospital",     # Header values for each column below
               "Total cases with known outcome", 
               "Recovered",    # This will be the top-level header for this and two next columns
               "",
               "",
               "Died",         # This will be the top-level header for this and two next columns
               "",             # Leave blank, as it will be merged with "Died"
               "")) %>% 
    set_header_labels(         # Rename the columns in original header row
      hospital = "", 
      N_Known = "",                  
      N_Recover = "Total",
      Pct_Recover = "% of cases",
      ct_value_Recover = "Median CT values",
      N_Death = "Total",
      Pct_Death = "% of cases",
      ct_value_Death = "Median CT values")  %>% 
  merge_at(i = 1, j = 3:5, part = "header") %>% # Horizontally merge columns 3 to 5 in new header row
  merge_at(i = 1, j = 6:8, part = "header") %>%  
  border_remove() %>%  
  theme_booktabs() %>% 
  vline(part = "all", j = 2, border = border_style) %>%   # at column 2 
  vline(part = "all", j = 5, border = border_style) %>%   # at column 5
  merge_at(i = 1:2, j = 1, part = "header") %>% 
  merge_at(i = 1:2, j = 2, part = "header") %>% 
  width(j=1, width = 2.7) %>% 
  width(j=2, width = 1.5) %>% 
  width(j=c(4,5,7,8), width = 1) %>% 
  flextable::align(., align = "center", j = c(2:8), part = "all") %>% 
  bg(., part = "body", bg = "gray95")  %>% 
  bg(., j=c(1:8), i= ~ hospital == "Military Hospital", part = "body", bg = "#91c293") %>% 
  colformat_num(., j = c(4,7), digits = 1) %>%
  bold(i = 1, bold = TRUE, part = "header") %>% 
  bold(i = 7, bold = TRUE, part = "body")

table
```


<!-- ======================================================= -->
## Lưu bảng của bạn {  }

Có nhiều cách khác nhau mà bảng có thể được tích hợp vào kết quả đầu ra của bạn. 

### Lưu bảng đơn {.unnumbered}

Bạn có thể xuất các bảng ra file Word, PowerPoint hoặc HTML hoặc dưới tệp tin ảnh (PNG). Để thực hiện điều này, hãy sử dụng một trong các hàm sau:

* `save_as_docx()`  
* `save_as_pptx()`  
* `save_as_image()`  
* `save_as_html()`  

Ví dụ dưới đây, chúng ta sẽ lưu bảng dưới dạng tài liệu word. Lưu ý cú pháp của đối số đầu tiên - bạn chỉ có thể cung cấp tên của đối tượng flextable, ví dụ: `my_table`, hoặc bạn có thể gán một "tên" cho bảng (ví dụ đặt tên là "my table"). Nếu đặt tên thì tên này sẽ xuất hiện dưới dạng tiêu đề của bảng trong Word. Code để lưu bảng dưới dạng ảnh PNG cũng được minh họa như dưới đây.  

```{r message=FALSE, warning=FALSE, eval=F}
# Edit the 'my table' as needed for the title of table.  
save_as_docx("my table" = my_table, path = "file.docx")

save_as_image(my_table, path = "file.png")
```

Lưu ý là bạn cần cài đặt package `webshot` hoặc `webshot2` để lưu bảng từ flextable dưới dạng ảnh. Hình ảnh xuất ra sẽ có nền trong suốt.

Nếu bạn muốn xem thử kết quả đầu ra của bảng **flextable** , sử dụng lệnh `print()` và chỉ định định dạng muốn xem trước với `preview = `. Tài liệu sẽ được "mở lên" trên máy tính của bạn bằng phần mềm đã chỉ định, nhưng sẽ không được lưu. Điều này có thể hữu ích để kiểm tra xem bảng có vừa với một trang/slide hay không hoặc bạn có thể nhanh chóng copy kết quả sang một tài liệu khác. Bạn có thể sử dụng phương pháp này với đối số preview đặt là “pptx” hoặc “docx”.  

```{r, eval=F}
print(my_table, preview = "docx") # Word document example
print(my_table, preview = "pptx") # Powerpoint example
```

### In bảng trong R markdown {.unnumbered}  

Bảng này có thể được tích hợp vào R markdown, một dạng báo cáo tự động của bạn, nếu đối tượng bảng được gọi trong phần code chunk của R markdown. Điều này có nghĩa là bảng có thể được cập nhật như một phần của báo cáo trong đó dữ liệu có thể thay đổi, do đó, các con số có thể được làm mới.

Xem thêm chi tiết trong chương [Báo cáo với R Markdown] của cuốn sổ tay này. 

<!-- ======================================================= -->
## Nguồn {  }

Sách đầy đủ về **flextable** có thể xem ở đây: https://ardata-fr.github.io/flextable-book/
Trang Github xem ở [đây](https://davidgohel.github.io/flextable/)  
Có thể tìm thấy sách hướng dẫn về tất cả các hàm **flextable** ở [đây](https://davidgohel.github.io/flextable/reference/index.html)

Thư viên các ví dụ về mẫu bảng **flextable** cùng code có thể truy cập tại [đây](https://ardata-fr.github.io/flextable-gallery/gallery/)  
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/tables_presentation.Rmd-->


# ggplot cơ bản {#ggplot-basics}

```{r, out.width=c('100%', '100%'), fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "ggplot_basics_top.png"))
```


**ggplot2** is the most popular data visualisation R package. Its `ggplot()` function is at the core of this package, and this whole approach is colloquially known as *"ggplot"* with the resulting figures sometimes affectionately called "ggplots". The "gg" in these names reflects the "**g**rammar of **g**raphics" used to construct the figures. **ggplot2** benefits from a wide variety of supplementary R packages that further enhance its functionality.  

The syntax is significantly different from **base** `R` plotting, and has a learning curve associated with it. Using **ggplot2** generally requires the user to format their data in a way that is highly **tidyverse** compatible, which ultimately makes using these packages together very effective.

In this page we will cover the fundamentals of plotting with **ggplot2**. See the page [ggplot tips] for suggestions and advanced techniques to make your plots really look nice.  

There are several extensive **ggplot2** tutorials linked in the resources section. You can also download this [data visualization with ggplot cheatsheet](https://github.com/rstudio/cheatsheets/raw/master/data-visualization-2.1.pdf) from the RStudio website. If you want inspiration for ways to creatively visualise your data, we suggest reviewing websites like the [R graph gallery](https://www.r-graph-gallery.com/) and [Data-to-viz](https://www.data-to-viz.com/caveats.html). 



<!-- ======================================================= -->
## Preparation {}

### Load packages {.unnumbered}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  tidyverse,      # includes ggplot2 and other data management tools
  rio,            # import/export
  here,           # file locator
  stringr         # working with characters   
)
```

### Import data {.unnumbered}  

We import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Import your data with the `import()` function from the **rio** package (it accepts many file types like .xlsx, .rds, .csv - see the [Import and export] page for details).

```{r,  echo=F}
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

```

```{r, eval=F}
linelist <- rio::import("linelist_cleaned.rds")
```

The first 50 rows of the linelist are displayed below. We will focus on the continuous variables `age`, `wt_kg` (weight in kilos), `ct_blood` (CT values), and `days_onset_hosp` (difference between onset date and hospitalisation).  

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



### General cleaning {.unnumbered}

When preparing data to plot, it is best to make the data adhere to ["tidy" data standards](https://r4ds.had.co.nz/tidy-data.html) as much as possible. How to achieve this is expanded on in the data management pages of this handbook, such as [Cleaning data and core functions]. 

Some simple ways we can prepare our data to make it better for plotting can include making the contents of the data better for display - which does not necessarily equate to better for data manipulation. For example:  

* Replace `NA` values in a character column with the character string "Unknown"  
* Consider converting column to class *factor* so their values have prescribed ordinal levels  
* Clean some columns so that their "data friendly" values with underscores etc are changed to normal text or title case (see [Characters and strings])  

Here are some examples of this in action:

```{r, }
# make display version of columns with more friendly names
linelist <- linelist %>%
  mutate(
    gender_disp = case_when(gender == "m" ~ "Male",        # m to Male 
                            gender == "f" ~ "Female",      # f to Female,
                            is.na(gender) ~ "Unknown"),    # NA to Unknown
    
    outcome_disp = replace_na(outcome, "Unknown")          # replace NA outcome with "unknown"
  )
```

### Pivoting longer {.unnumbered}

As a matter of data structure, for **ggplot2** we often also want to pivot our data into *longer* formats. Read more about this is the page on [Pivoting data].  


```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "pivoting", "pivot_longer_new.png"))
```


For example, say that we want to plot data that are in a "wide" format, such as for each case in the `linelist` and their symptoms. Below we create a mini-linelist called `symptoms_data` that contains only the `case_id` and symptoms columns.  

```{r}
symptoms_data <- linelist %>% 
  select(c(case_id, fever, chills, cough, aches, vomit))
```

Here is how the first 50 rows of this mini-linelist look - see how they are formatted "wide" with each symptom as a column: 

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(symptoms_data, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

If we wanted to plot the number of cases with specific symptoms, we are limited by the fact that each symptom is a specific column. However, we can *pivot* the symptoms columns to a longer format like this:

```{r, }
symptoms_data_long <- symptoms_data %>%    # begin with "mini" linelist called symptoms_data
  
  pivot_longer(
    cols = -case_id,                       # pivot all columns except case_id (all the symptoms columns)
    names_to = "symptom_name",             # assign name for new column that holds the symptoms
    values_to = "symptom_is_present") %>%  # assign name for new column that holds the values (yes/no)
  
  mutate(symptom_is_present = replace_na(symptom_is_present, "unknown")) # convert NA to "unknown"

```


Here are the first 50 rows. Note that case has 5 rows - one for each possible symptom. The new columns `symptom_name` and `symptom_is_present` are the result of the pivot. Note that this format may not be very useful for other operations, but is useful for plotting.

```{r, message=FALSE, echo=F}
DT::datatable(head(symptoms_data_long, 50), rownames = FALSE, filter="top", options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )
```






<!-- ======================================================= -->
## Basics of ggplot {}

**"Grammar of graphics" - ggplot2**  

Plotting with **ggplot2** is based on "adding" plot layers and design elements on top of one another, with each command added to the previous ones with a plus symbol (`+`). The result is a multi-layer plot object that can be saved, modified, printed, exported, etc.  

ggplot objects can be highly complex, but the basic order of layers will usually look like this:  

1. Begin with the baseline `ggplot()` command - this "opens" the ggplot and allow subsequent functions to be added with `+`. Typically the dataset is also specified in this command  
2. Add "geom" layers - these functions visualize the data as *geometries* (*shapes*), e.g. as a bar graph, line plot, scatter plot, histogram (or a combination!). These functions all start with `geom_` as a prefix.  
3. Add design elements to the plot such as axis labels, title, fonts, sizes, color schemes, legends, or axes rotation  

A simple example of skeleton code is as follows. We will explain each component in the sections below.  

```{r, eval=F}
# plot data from my_data columns as red points
ggplot(data = my_data)+                   # use the dataset "my_data"
  geom_point(                             # add a layer of points (dots)
    mapping = aes(x = col1, y = col2),    # "map" data column to axes
    color = "red")+                       # other specification for the geom
  labs()+                                 # here you add titles, axes labels, etc.
  theme()                                 # here you adjust color, font, size etc of non-data plot elements (axes, title, etc.) 
```

 


## `ggplot()`  

The opening command of any ggplot2 plot is `ggplot()`. This command simply creates a blank canvas upon which to add layers. It "opens" the way for further layers to be added with a `+` symbol.

Typically, the command `ggplot()` includes the `data = ` argument for the plot. This sets the default dataset to be used for subsequent layers of the plot.  

This command will end with a `+` after its closing parentheses. This leaves the command "open". The ggplot will only execute/appear when the full command includes a final layer *without* a `+` at the end.  

```{r, eval=F}
# This will create plot that is a blank canvas
ggplot(data = linelist)
```


## Geoms  

A blank canvas is certainly not sufficient - we need to create geometries (shapes) from our data (e.g. bar plots, histograms, scatter plots, box plots).  

This is done by adding layers "geoms" to the initial `ggplot()` command. There are many **ggplot2** functions that create "geoms". Each of these functions begins with "geom_", so we will refer to them generically as `geom_XXXX()`. There are over 40 geoms in **ggplot2** and many others created by fans. View them at the [ggplot2 gallery](https://exts.ggplot2.tidyverse.org/gallery/). Some common geoms are listed below:  

* Histograms - `geom_histogram()`  
* Bar charts - `geom_bar()` or `geom_col()` (see ["Bar plot" section](#ggplot_basics_bars))  
* Box plots - `geom_boxplot()`  
* Points (e.g. scatter plots) - `geom_point()`  
* Line graphs - `geom_line()` or `geom_path()`  
* Trend lines - `geom_smooth()`  

In one plot you can display one or multiple geoms. Each is added to previous **ggplot2** commands with a `+`, and they are plotted sequentially such that later geoms are plotted on top of previous ones.  



## Mapping data to the plot {#ggplot_basics_mapping}  

Most geom functions must be told *what to use* to create their shapes - so you must tell them how they should *map (assign) columns in your data* to components of the plot like the axes, shape colors, shape sizes, etc. For most geoms, the *essential* components that must be mapped to columns in the data are the x-axis, and (if necessary) the y-axis.  

This "mapping" occurs with the `mapping = ` argument. The mappings you provide to `mapping` must be wrapped in the `aes()` function, so you would write something like `mapping = aes(x = col1, y = col2)`, as shown below.

Below, in the `ggplot()` command the data are set as the case `linelist`. In the `mapping = aes()` argument the column `age` is mapped to the x-axis, and the column `wt_kg` is mapped to the y-axis.  

After a `+`, the plotting commands continue. A shape is created with the "geom" function `geom_point()`. This geom *inherits* the mappings from the `ggplot()` command above - it knows the axis-column assignments and proceeds to visualize those relationships as *points* on the canvas.  

```{r, warning=F, message=F}
ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+
  geom_point()
```

As another example, the following commands utilize the same data, a slightly different mapping, and a different geom. The `geom_histogram()` function only requires a column mapped to the x-axis, as the counts y-axis is generated automatically.  

```{r, warning=F, message=F}
ggplot(data = linelist, mapping = aes(x = age))+
  geom_histogram()
```


### Plot aesthetics {.unnumbered}  

In ggplot terminology a plot "aesthetic" has a specific meaning. It refers to a visual property of *plotted data*. Note that "aesthetic" here refers to the *data being plotted in geoms/shapes* - not the surrounding display such as titles, axis labels, background color, that you might associate with the word "aesthetics" in common English. In ggplot those details are called "themes" and are adjusted within a `theme()` command (see [this section](#ggplot_basics_themes)).  

Therefore, plot object *aesthetics* can be colors, sizes, transparencies, placement, etc. *of the plotted data*. Not all geoms will have the same aesthetic options, but many can be used by most geoms. Here are some examples:  

* `shape =` Display a point with `geom_point()` as a dot, star, triangle, or square...  
* `fill = ` The interior color (e.g. of a bar or boxplot)  
* `color =` The exterior line of a bar, boxplot, etc., or the point color if using `geom_point()`  
* `size = ` Size (e.g. line thickness, point size)  
* `alpha = ` Transparency (1 = opaque, 0 = invisible)  
* `binwidth = ` Width of histogram bins  
* `width = ` Width of "bar plot" columns  
* `linetype =` Line type (e.g. solid, dashed, dotted) 

These plot object aesthetics can be assigned values in two ways:  

1) Assigned a static value (e.g. `color = "blue"`) to apply across all plotted observations  
2) Assigned to a column of the data (e.g. `color = hospital`) such that display of each observation depends on its value in that column  

<!-- *These non-axis aesthetics can be assigned static values (e.g. `size = 1`) or can be mapped to a column (e.g. `size = age`).* If you want the aesthetic to be assigned a static value, the assignment is placed *outside* the `mapping = aes()`. If you want the aesthetic to be scaled/depend on the value in each row of data, the assignment is made *inside* the `mapping = aes()`.   -->

### Set to a static value {.unnumbered}  

If you want the plot object aesthetic to be static, that is - to be the same for every observation in the data, you write its assignment within the geom but *outside* of any `mapping = aes()` statement. These assignments could look like `size = 1` or `color = "blue"`. Here are two examples:  

* In the first example, the `mapping = aes()` is in the `ggplot()` command and the axes are mapped to age and weight columns in the data. The plot aesthetics `color = `, `size = `, and `alpha = ` (transparency) are assigned to static values. For clarity, this is done in the `geom_point()` function, as you may add other geoms afterward that would take different values for their plot aesthetics.  
* In the second example, the histogram requires only the x-axis mapped to a column. The histogram `binwidth = `, `color = `, `fill = ` (internal color), and `alpha = ` are again set within the geom to static values.  

```{r, out.width=c('50%', '50%'), fig.show='hold', warning=F, message=F}
# scatterplot
ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  # set data and axes mapping
  geom_point(color = "darkgreen", size = 0.5, alpha = 0.2)         # set static point aesthetics

# histogram
ggplot(data = linelist, mapping = aes(x = age))+       # set data and axes
  geom_histogram(              # display histogram
    binwidth = 7,                # width of bins
    color = "red",               # bin line color
    fill = "blue",               # bin interior color
    alpha = 0.1)                 # bin transparency
```


### Scaled to column values {.unnumbered}  

The alternative is to scale the plot object aesthetic by the values in a column. In this approach, the display of this aesthetic will depend on that observation's value in that column of the data. If the column values are continuous, the display scale (legend) for that aesthetic will be continuous. If the column values are discrete, the legend will display each value and the plotted data will appear as distinctly "grouped" (read more in the [grouping](#ggplotgroups) section of this page).  

To achieve this, you map that plot aesthetic to a *column name* (not in quotes). This must be done *within a `mapping = aes()` function* (note: there are several places in the code you can make these mapping assignments, as discussed [below](##ggplot_basics_map_loc)).  

Two examples are below.  

* In the first example, the `color = ` aesthetic (of each point) is mapped to the column `age` - and a scale has appeared in a legend! For now just note that the scale exists - we will show how to modify it in later sections.  
* In the second example two new plot aesthetics are also mapped to columns (`color = ` and `size = `), while the plot aesthetics `shape = ` and `alpha = ` are mapped to static values outside of any `mapping = aes()` function.  

```{r, out.width=c('50%', '50%'), fig.show='hold', warning=F, message=F}
# scatterplot
ggplot(data = linelist,   # set data
       mapping = aes(     # map aesthetics to column values
         x = age,           # map x-axis to age            
         y = wt_kg,         # map y-axis to weight
         color = age)
       )+     # map color to age
  geom_point()         # display data as points 

# scatterplot
ggplot(data = linelist,   # set data
       mapping = aes(     # map aesthetics to column values
         x = age,           # map x-axis to age            
         y = wt_kg,         # map y-axis to weight
         color = age,       # map color to age
         size = age))+      # map size to age
  geom_point(             # display data as points
    shape = "diamond",      # points display as diamonds
    alpha = 0.3)            # point transparency at 30%


```



Note: Axes assignments are always assigned to columns in the data (not to static values), and this is always done within `mapping = aes()`.  


It becomes important to keep track of your plot layers and aesthetics when making more complex plots - for example plots with multiple geoms. In the example below, the `size = ` aesthetic is assigned twice - once for `geom_point()` and once for `geom_smooth()` - both times as a static value.  

```{r, warning=F, message=F}
ggplot(data = linelist,
       mapping = aes(           # map aesthetics to columns
         x = age,
         y = wt_kg,
         color = age_years)
       ) + 
  geom_point(                   # add points for each row of data
    size = 1,
    alpha = 0.5) +  
  geom_smooth(                  # add a trend line 
    method = "lm",              # with linear method
    size = 2)                   # size (width of line) of 2
```






### Where to make mapping assignments {#ggplot_basics_map_loc .unnumbered}


Aesthetic mapping within `mapping = aes()` can be written in several places in your plotting commands and can even be written more than once. This can be written in the top `ggplot()` command, and/or for each individual geom beneath. The nuances include:  

* Mapping assignments made in the top `ggplot()` command will be inherited as defaults across any geom below, like how `x = ` and `y = ` are inherited 
* Mapping assignments made within one geom apply only to that geom  

Likewise, `data = ` specified in the top `ggplot()` will apply by default to any geom below, but you could also specify data for each geom (but this is more difficult).  

Thus, each of the following commands will create the same plot:  

```{r, eval=F, warning=F, message=F}
# These commands will produce the exact same plot
ggplot(data = linelist, mapping = aes(x = age))+
  geom_histogram()

ggplot(data = linelist)+
  geom_histogram(mapping = aes(x = age))

ggplot()+
  geom_histogram(data = linelist, mapping = aes(x = age))
```




### Groups {#ggplotgroups .unnumbered}  

You can easily group the data and "plot by group". In fact, you have already done this!  

Assign the "grouping" column to the appropriate plot aesthetic, within a `mapping = aes()`. Above, we demonstrated this using continuous values when we assigned point `size = ` to the column `age`. However this works the same way for discrete/categorical columns.  

For example, if you want points to be displayed by gender, you would set `mapping = aes(color = gender)`. A legend automatically appears. This assignment can be made within the `mapping = aes()` in the top `ggplot()` command (and be inherited by the geom), or it could be set in a separate `mapping = aes()` within the geom. Both approaches are shown below:  


```{r, warning=F, message=F}
ggplot(data = linelist,
       mapping = aes(x = age, y = wt_kg, color = gender))+
  geom_point(alpha = 0.5)
```


```{r, eval=F}
# This alternative code produces the same plot
ggplot(data = linelist,
       mapping = aes(x = age, y = wt_kg))+
  geom_point(
    mapping = aes(color = gender),
    alpha = 0.5)

```


Note that depending on the geom, you will need to use different arguments to group the data. For `geom_point()` you will most likely use `color =`, `shape = ` or `size = `. Whereas for `geom_bar()` you are more likely to use `fill = `. This just depends on the geom and what plot aesthetic you want to reflect the groupings.  

For your information - the most basic way of grouping the data is by using only the `group = ` argument within `mapping = aes()`. However, this by itself will not change the colors, fill, or shapes. Nor will it create a legend. Yet the data are grouped, so statistical displays may be affected.  

To adjust the order of groups in a plot, see the [ggplot tips] page or the page on [Factors]. There are many examples of grouped plots in the sections below on plotting continuous and categorical data.   



## Facets / Small-multiples {#ggplot_basics_facet}  

Facets, or "small-multiples", are used to split one plot into a multi-panel figure, with one panel ("facet") per group of data. The same type of plot is created multiple times, each one using a sub-group of the same dataset.  

Faceting is a functionality that comes with **ggplot2**, so the legends and axes of the facet "panels" are automatically aligned. There are other packages discussed in the [ggplot tips] page that are used to combine completely different plots (**cowplot** and **patchwork**) into one figure.  

Faceting is done with one of the following **ggplot2** functions:

  1. `facet_wrap()` To show a different panel for each level of a *single* variable. One example of this could be showing a different epidemic curve for each hospital in a region. Facets are ordered alphabetically, unless the variable is a factor with other ordering defined.  
  + You can invoke certain options to determine the layout of the facets, e.g. `nrow = 1` or `ncol = 1` to control the number of rows or columns that the faceted plots are arranged within.  
  
  2. `facet_grid()` This is used when you want to bring a second variable into the faceting arrangement. Here each panel of a grid shows the intersection between values in *two columns*. For example, epidemic curves for each hospital-age group combination with hospitals along the top (columns) and age groups along the sides (rows).  
  + `nrow` and `ncol` are not relevant, as the subgroups are presented in a grid  

Each of these functions accept a formula syntax to specify the column(s) for faceting. Both accept up to two columns, one on each side of a tilde `~`.  

* For `facet_wrap()` most often you will write only one column preceded by a tilde `~` like `facet_wrap(~hospital)`. However you can write two columns `facet_wrap(outcome ~ hospital)` - each unique combination will display in a separate panel, but they will not be arranged in a grid. The headings will show combined terms and these won't be specific logic to the columns vs. rows.  If you are providing only one faceting variable, a period `.` is used as a placeholder on the other side of the formula - see the code examples.  

* For `facet_grid()` you can also specify one or two columns to the formula (grid `rows ~ columns`). If you only want to specify one, you can place a period `.` on the other side of the tilde like `facet_grid(. ~ hospital)` or `facet_grid(hospital ~ .)`.  

Facets can quickly contain an overwhelming amount of information - its good to ensure you don't have too many levels of each variable that you choose to facet by. Here are some quick examples with the malaria dataset (see [Download handbook and data]) which consists of daily case counts of malaria for facilities, by age group.  

Below we import and do some quick modifications for simplicity:  

```{r, , warning=F, message=F}
# These data are daily counts of malaria cases, by facility-day
malaria_data <- import(here("data", "malaria_facility_count_data.rds")) %>%  # import
  select(-submitted_date, -Province, -newid)                                 # remove unneeded columns

```

The first 50 rows of the malaria data are below. Note there is a column `malaria_tot`, but also columns for counts by age group (these will be used in the second, `facet_grid()` example).  

```{r, message=FALSE, echo=F}
DT::datatable(head(malaria_data, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



### `facet_wrap()` {.unnumbered}

For the moment, let's focus on the columns `malaria_tot` and `District`. Ignore the age-specific count columns for now. We will plot epidemic curves with `geom_col()`, which produces a column for each day at the specified y-axis height given in column `malaria_tot` (the data are already daily counts, so we use `geom_col()` - see [the "Bar plot" section below](#ggplot_basics_bars)).  

When we add the command `facet_wrap()`, we specify a tilde and then the column to facet on (`District` in this case). You can place another column on the left side of the tilde, - this will create one facet for each combination - but we recommend you do this with `facet_grid()` instead. In this use case, one facet is created for each unique value of `District`.  

```{r, warning=F, message=F}
# A plot with facets by district
ggplot(malaria_data, aes(x = data_date, y = malaria_tot)) +
  geom_col(width = 1, fill = "darkred") +       # plot the count data as columns
  theme_minimal()+                              # simplify the background panels
  labs(                                         # add plot labels, title, etc.
    x = "Date of report",
    y = "Malaria cases",
    title = "Malaria cases by district") +
  facet_wrap(~District)                       # the facets are created
```

### `facet_grid()` {.unnumbered}  

We can use a `facet_grid()` approach to cross two variables. Let's say we want to cross `District` and age. Well, we need to do some data transformations on the age columns to get these data into ggplot-preferred "long" format. The age groups all have their own columns - we want them in a single column called `age_group` and another called `num_cases`. See the page on [Pivoting data] for more information on this process.  

```{r, message=F, warning=F}
malaria_age <- malaria_data %>%
  select(-malaria_tot) %>% 
  pivot_longer(
    cols = c(starts_with("malaria_rdt_")),  # choose columns to pivot longer
    names_to = "age_group",      # column names become age group
    values_to = "num_cases"      # values to a single column (num_cases)
  ) %>%
  mutate(
    age_group = str_replace(age_group, "malaria_rdt_", ""),
    age_group = forcats::fct_relevel(age_group, "5-14", after = 1))
```

Now the first 50 rows of data look like this:  

```{r, message=FALSE, echo=F}
DT::datatable(head(malaria_age, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


When you pass the two variables to `facet_grid()`, easiest is to use formula notation (e.g. `x ~ y`) where x is rows and y is columns. Here is the plot, using `facet_grid()` to show the plots for each combination of the columns `age_group` and `District`.

```{r, message=F, warning=F}
ggplot(malaria_age, aes(x = data_date, y = num_cases)) +
  geom_col(fill = "darkred", width = 1) +
  theme_minimal()+
  labs(
    x = "Date of report",
    y = "Malaria cases",
    title = "Malaria cases by district and age group"
  ) +
  facet_grid(District ~ age_group)
```

### Free or fixed axes {.unnumbered}  

The axes scales displayed when faceting are by default the same (fixed) across all the facets. This is helpful for cross-comparison, but not always appropriate.  

When using `facet_wrap()` or `facet_grid()`, we can add `scales = "free_y"` to "free" or release the y-axes of the panels to scale appropriately to their data subset. This is particularly useful if the actual counts are small for one of the subcategories and trends are otherwise hard to see. Instead of "free_y" we can also write "free_x" to do the same for the x-axis (e.g. for dates) or "free" for both axes. Note that in `facet_grid`, the y scales will be the same for facets in the same row, and the x scales will be the same for facets in the same column.

When using `facet_grid` only, we can add `space = "free_y"` or `space = "free_x"` so that the actual height or width of the facet is weighted to the values of the figure within. This only works if `scales = "free"` (y or x) is already applied. 

```{r, message=FALSE, warning=FALSE}

# Free y-axis
ggplot(malaria_data, aes(x = data_date, y = malaria_tot)) +
  geom_col(width = 1, fill = "darkred") +       # plot the count data as columns
  theme_minimal()+                              # simplify the background panels
  labs(                                         # add plot labels, title, etc.
    x = "Date of report",
    y = "Malaria cases",
    title = "Malaria cases by district - 'free' x and y axes") +
  facet_wrap(~District, scales = "free")        # the facets are created
```


<!-- ```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')} -->
<!-- # A) Facet hospitalsation date by hospital, free y axis -->
<!-- ggplot(data = linelist %>% filter(hospital != "Missing"), # filter removes unknown hospital -->
<!--        aes(x = date_hospitalisation ))+ -->
<!--   geom_histogram(binwidth=7) + # Bindwidth = 7 days -->
<!--   labs(title = "A) Histogram with free y axis scales")+ -->
<!--   facet_grid(hospital~., # Facet with hospital as the row  -->
<!--              scales = "free_y") # Free the y scale of each facet -->

<!-- # B) Facet hospitalisation date by hospital, free y axis and vertical spacing -->
<!-- ggplot(data = linelist %>% filter(hospital != "Missing"), # filter removes unknown hospital -->
<!--        aes(x = date_hospitalisation ))+ -->
<!--   geom_histogram(binwidth=7) + # Bindwidth = 7 days -->
<!--   labs(title = "B) Histogram with free y axis scales and spacing")+ -->
<!--   facet_grid(hospital~., # Facet with hospital as the row  -->
<!--              scales = "free_y", # Free the y scale of each facet -->
<!--              space = "free_y") # Free the vertical spacing of each facet to optimise space -->

<!-- ``` -->

### Factor level order in facets {.unnumbered}  

See this [post](https://juliasilge.com/blog/reorder-within/) on how to re-order factor levels *within* facets.  


## Storing plots  

### Saving plots {.unnumbered}

By default when you run a `ggplot()` command, the plot will be printed to the Plots RStudio pane. However, you can also save the plot as an object by using the assignment operator `<-` and giving it a name. Then it will not print unless the object name itself is run. You can also print it by wrapping the plot name with `print()`, but this is only necessary in certain circumstances such as if the plot is created inside a *for loop* used to print multiple plots at once (see [Iteration, loops, and lists] page).  

```{r, warning=F, message=F}
# define plot
age_by_wt <- ggplot(data = linelist, mapping = aes(x = age_years, y = wt_kg, color = age_years))+
  geom_point(alpha = 0.1)

# print
age_by_wt    
```


### Modifying saved plots {.unnumbered}  

One nice thing about **ggplot2** is that you can define a plot (as above), and then add layers to it starting with its name. You do not have to repeat all the commands that created the original plot! 

For example, to modify the plot `age_by_wt` that was defined above, to include a vertical line at age 50, we would just add a `+` and begin adding additional layers to the plot.  

```{r, warning=F, message=F}
age_by_wt+
  geom_vline(xintercept = 50)
```


### Exporting plots {.unnumbered}   

Exporting ggplots is made easy with the `ggsave()` function from **ggplot2**. It can work in two ways, either:  

* Specify the name of the plot object, then the file path and name with extension  
  * For example: `ggsave(my_plot, here("plots", "my_plot.png"))`  
* Run the command with only a file path, to save the last plot that was printed  
  * For example: `ggsave(here("plots", "my_plot.png"))`  
  
You can export as png, pdf, jpeg, tiff, bmp, svg, or several other file types, by specifying the file extension in the file path.  

You can also specify the arguments `width = `, `height = `, and `units = ` (either "in", "cm", or "mm"). You can also specify `dpi = ` with a number for plot resolution (e.g. 300). See the function details by entering `?ggsave` or reading the [documentation online](https://ggplot2.tidyverse.org/reference/ggsave.html). 

Remember that you can use `here()` syntax to provide the desired file path. See the [Import and export] page for more information.  


## Labels 

Surely you will want to add or adjust the plot's labels. These are most easily done within the `labs()` function which is added to the plot with `+` just as the geoms were.  

Within `labs()` you can provide character strings to these arguements:  

* `x = ` and `y = ` The x-axis and y-axis title (labels)  
* `title = ` The main plot title  
* `subtitle = ` The subtitle of the plot, in smaller text below the title  
* `caption = ` The caption of the plot, in bottom-right by default  

Here is a plot we made earlier, but with nicer labels:  

```{r, warning=F, message=F}
age_by_wt <- ggplot(
  data = linelist,   # set data
  mapping = aes(     # map aesthetics to column values
         x = age,           # map x-axis to age            
         y = wt_kg,         # map y-axis to weight
         color = age))+     # map color to age
  geom_point()+           # display data as points
  labs(
    title = "Age and weight distribution",
    subtitle = "Fictional Ebola outbreak, 2014",
    x = "Age in years",
    y = "Weight in kilos",
    color = "Age",
    caption = stringr::str_glue("Data as of {max(linelist$date_hospitalisation, na.rm=T)}"))

age_by_wt
```

Note how in the caption assignment we used `str_glue()` from the **stringr** package to implant dynamic R code within the string text. The caption will show the "Data as of: " date that reflects the maximum hospitalization date in the linelist. Read more about this in the page on [Characters and strings].  

A note on specifying the *legend* title: There is no one "legend title" argument, as you could have multiple scales in your legend. Within `labs()`, you can write the argument for the plot aesthetic used to create the legend, and provide the title this way. For example, above we assigned `color = age` to create the legend. Therefore, we provide `color = ` to `labs()` and assign the legend title desired ("Age" with capital A). If you create the legend with `aes(fill = COLUMN)`, then in `labs()` you would write `fill = ` to adjust the title of that legend. The section on color scales in the [ggplot tips] page provides more details on editing legends, and an alternative approach using `scales_()` functions.  



## Themes {#ggplot_basics_themes} 

One of the best parts of **ggplot2** is the amount of control you have over the plot - you can define anything! As mentioned above, the design of the plot that is *not* related to the data shapes/geometries are adjusted within the `theme()` function. For example, the plot background color, presence/absence of gridlines, and the font/size/color/alignment of text (titles, subtitles, captions, axis text...). These adjustments can be done in one of two ways:  

* Add a [*complete theme*](https://ggplot2.tidyverse.org/reference/ggtheme.html) `theme_()` function to make sweeping adjustments - these include `theme_classic()`, `theme_minimal()`, `theme_dark()`, `theme_light()` `theme_grey()`, `theme_bw()` among others  
* Adjust each tiny aspect of the plot individually within `theme()`  


### Complete themes {.unnumbered}  

As they are quite straight-forward, we will demonstrate the complete theme functions below and will not describe them further here. Note that any micro-adjustments with `theme()` should be made *after* use of a complete theme.  

Write them with empty parentheses.  

```{r, out.width=c('50%', '50%'), fig.show='hold', warning=F, message=F}

ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  
  geom_point(color = "darkgreen", size = 0.5, alpha = 0.2)+
  labs(title = "Theme classic")+
  theme_classic()

ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  
  geom_point(color = "darkgreen", size = 0.5, alpha = 0.2)+
  labs(title = "Theme bw")+
  theme_bw()

ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  
  geom_point(color = "darkgreen", size = 0.5, alpha = 0.2)+
  labs(title = "Theme minimal")+
  theme_minimal()

ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  
  geom_point(color = "darkgreen", size = 0.5, alpha = 0.2)+
  labs(title = "Theme gray")+
  theme_gray()
  


```

### Modify theme {.unnumbered}  

The `theme()` function can take a large number of arguments, each of which edits a very specific aspect of the plot. There is no way we could cover all of the arguments, but we will describe the general pattern for them and show you how to find the argument name that you need. The basic syntax is this:

1. Within `theme()` write the argument name for the plot element you want to edit, like `plot.title = `  
3. Provide an `element_()` function to the argument  
  + Most often, use `element_text()`, but others include `element_rect()` for canvas background colors, or `element_blank()` to remove plot elements  
4. Within the `element_()` function, write argument assignments to make the fine adjustments you desire  

So, that description was quite abstract, so here are some examples.  

The below plot looks quite silly, but it serves to show you a variety of the ways you can adjust your plot.  

* We begin with the plot `age_by_wt` defined just above and add `theme_classic()`  
* For finer adjustments we add `theme()` and include one argument for each plot element to adjust  

It can be nice to organize the arguments in logical sections. To describe just some of those used below:  

* `legend.position = ` is unique in that it accepts simple values like "bottom", "top", "left", and "right". But generally, text-related arguments require that you place the details *within* `element_text()`.  
* Title size with `element_text(size = 30)`  
* The caption horizontal alignment with `element_text(hjust = 0)` (from right to left)  
* The subtitle is italicized with `element_text(face = "italic")`  

```{r, , warning=F, message=F}
age_by_wt + 
  theme_classic()+                                 # pre-defined theme adjustments
  theme(
    legend.position = "bottom",                    # move legend to bottom
    
    plot.title = element_text(size = 30),          # size of title to 30
    plot.caption = element_text(hjust = 0),        # left-align caption
    plot.subtitle = element_text(face = "italic"), # italicize subtitle
    
    axis.text.x = element_text(color = "red", size = 15, angle = 90), # adjusts only x-axis text
    axis.text.y = element_text(size = 15),         # adjusts only y-axis text
    
    axis.title = element_text(size = 20)           # adjusts both axes titles
    )     
```

Here are some especially common `theme()` arguments. You will recognize some patterns, such as appending `.x` or `.y` to apply the change only to one axis.  


`theme()` argument                 |What it adjusts
-----------------------------------|----------------------------------
`plot.title = element_text()`      |The title
`plot.subtitle = element_text()`   |The subtitle
`plot.caption = element_text()`    |The caption (family, face, color, size, angle, vjust, hjust...) 
`axis.title = element_text()`      |Axis titles (both x and y) (size, face, angle, color...)
`axis.title.x = element_text()`    |Axis title x-axis only (use `.y` for y-axis only)
`axis.text = element_text()`       |Axis text (both x and y)
`axis.text.x = element_text()`     |Axis text x-axis only (use `.y` for y-axis only)  
`axis.ticks = element_blank()`     |Remove axis ticks
`axis.line = element_line()`       |Axis lines (colour, size, linetype: solid dashed dotted etc)
`strip.text = element_text()`      |Facet strip text (colour, face, size, angle...)
`strip.background = element_rect()`|facet strip (fill, colour, size...)  

But there are so many theme arguments! How could I remember them all? Do not worry - it is impossible to remember them all. Luckily there are a few tools to help you:  

The **tidyverse** documentation on [modifying theme](https://ggplot2.tidyverse.org/reference/theme.html), which has a complete list.  

<span style="color: darkgreen;">**_TIP:_** Run `theme_get()` from **ggplot2** to print a list of all 90+ `theme()` arguments to the console.</span>  

<span style="color: darkgreen;">**_TIP:_** If you ever want to remove an element of a plot, you can also do it through `theme()`. Just pass `element_blank()` to an argument to have it disappear completely. For legends, set `legend.position = "none".`</span>  




## Colors  


Please see this [section on color scales of the ggplot tips page](#ggplot_tips_colors).  



## Piping into **ggplot2**   

When using pipes to clean and transform your data, it is easy to pass the transformed data into `ggplot()`.  

The pipes that pass the dataset from function-to-function will transition to `+` once the `ggplot()` function is called. Note that in this case, there is no need to specify the `data = ` argument, as this is automatically defined as the piped-in dataset.  

This is how that might look:  

```{r, warning=F, message=F}
linelist %>%                                                     # begin with linelist
  select(c(case_id, fever, chills, cough, aches, vomit)) %>%     # select columns
  pivot_longer(                                                  # pivot longer
    cols = -case_id,                                  
    names_to = "symptom_name",
    values_to = "symptom_is_present") %>%
  mutate(                                                        # replace missing values
    symptom_is_present = replace_na(symptom_is_present, "unknown")) %>% 
  
  ggplot(                                                        # begin ggplot!
    mapping = aes(x = symptom_name, fill = symptom_is_present))+
  geom_bar(position = "fill", col = "black") +                    
  theme_classic() +
  labs(
    x = "Symptom",
    y = "Symptom status (proportion)"
  )
```









## Plot continuous data

Throughout this page, you have already seen many examples of plotting continuous data. Here we briefly consolidate these and present a few variations.  
Visualisations covered here include:

* Plots for one continuous variable:  
  * **Histogram**, a classic graph to present the distribution of a continuous variable. 
  * **Box plot** (also called box and whisker), to show the 25th, 50th, and 75th percentiles, tail ends of the distribution, and outliers ([important limitations](https://www.data-to-viz.com/caveat/boxplot.html)).  
  * **Jitter plot**, to show all values as points that are 'jittered' so they can (mostly) all be seen, even where two have the same value.  
  * **Violin plot**, show the distribution of a continuous variable based on the symmetrical width of the 'violin'. 
  * **Sina plot**, are a combination of jitter and violin plots, where individual points are shown but in the symmetrical shape of the distribution (via **ggforce** package).  
* **Scatter plot** for two continuous variables.  
* **Heat plots** for three continuous variables (linked to [Heat plots] page)  



### Histograms {.unnumbered}

Histograms may look like bar charts, but are distinct because they measure the distribution of a *continuous* variable. There are no spaces between the "bars", and only one column is provided to `geom_histogram()`.

Below is code for generating **histograms**, which group continuous data into ranges and display in adjacent bars of varying height. This is done using `geom_histogram()`. See the ["Bar plot" section](#ggplot_basics_bars) of the ggplot basics page to understand difference between `geom_histogram()`, `geom_bar()`, and `geom_col()`.  

We will show the distribution of ages of cases. Within `mapping = aes()` specify which column you want to see the distribution of. You can assign this column to either the x or the y axis. 

The rows will be assigned to "bins" based on their numeric age, and these bins will be graphically represented by bars. If you specify a number of bins with the `bins = ` plot aesthetic, the break points are evenly spaced between the minimum and maximum values of the histogram. If `bins = ` is unspecified, an appropriate number of bins will be guessed and this message displayed after the plot:  

```
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
``` 

If you do not want to specify a number of bins to `bins = `, you could alternatively specify `binwidth = ` in the units of the axis. We give a few examples showing different bins and bin widths:  

```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# A) Regular histogram
ggplot(data = linelist, aes(x = age))+  # provide x variable
  geom_histogram()+
  labs(title = "A) Default histogram (30 bins)")

# B) More bins
ggplot(data = linelist, aes(x = age))+  # provide x variable
  geom_histogram(bins = 50)+
  labs(title = "B) Set to 50 bins")

# C) Fewer bins
ggplot(data = linelist, aes(x = age))+  # provide x variable
  geom_histogram(bins = 5)+
  labs(title = "C) Set to 5 bins")


# D) More bins
ggplot(data = linelist, aes(x = age))+  # provide x variable
  geom_histogram(binwidth = 1)+
  labs(title = "D) binwidth of 1")

```



To get smoothed proportions, you can use `geom_density()`:  

```{r, fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# Frequency with proportion axis, smoothed
ggplot(data = linelist, mapping = aes(x = age)) +
  geom_density(size = 2, alpha = 0.2)+
  labs(title = "Proportional density")

# Stacked frequency with proportion axis, smoothed
ggplot(data = linelist, mapping = aes(x = age, fill = gender)) +
  geom_density(size = 2, alpha = 0.2, position = "stack")+
  labs(title = "'Stacked' proportional densities")
```


To get a "stacked" histogram (of a continuous column of data), you can do one of the following:  

1) Use `geom_histogram()` with the `fill = ` argument within `aes()` and assigned to the grouping column, or  
2) Use `geom_freqpoly()`, which is likely easier to read (you can still set `binwidth = `)  
3) To see proportions of all values, set the `y = after_stat(density)` (use this syntax exactly - not changed for your data). Note: these proportions will show *per group*.  

Each is shown below (*note use of `color = ` vs. `fill = ` in each):  

```{r, fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# "Stacked" histogram
ggplot(data = linelist, mapping = aes(x = age, fill = gender)) +
  geom_histogram(binwidth = 2)+
  labs(title = "'Stacked' histogram")

# Frequency 
ggplot(data = linelist, mapping = aes(x = age, color = gender)) +
  geom_freqpoly(binwidth = 2, size = 2)+
  labs(title = "Freqpoly")

# Frequency with proportion axis
ggplot(data = linelist, mapping = aes(x = age, y = after_stat(density), color = gender)) +
  geom_freqpoly(binwidth = 5, size = 2)+
  labs(title = "Proportional freqpoly")

# Frequency with proportion axis, smoothed
ggplot(data = linelist, mapping = aes(x = age, y = after_stat(density), fill = gender)) +
  geom_density(size = 2, alpha = 0.2)+
  labs(title = "Proportional, smoothed with geom_density()")
```

If you want to have some fun, try `geom_density_ridges` from the **ggridges** package ([vignette here](https://cran.r-project.org/web/packages/ggridges/vignettes/introduction.html).  

Read more in detail about histograms at the **tidyverse** [page on geom_histogram()](https://ggplot2.tidyverse.org/reference/geom_histogram.html).  



### Box plots {.unnumbered}

Box plots are common, but have important limitations. They can obscure the actual distribution - e.g. a bi-modal distribution. See this [R graph gallery](https://www.r-graph-gallery.com/boxplot.html) and this [data-to-viz article](https://www.data-to-viz.com/caveat/boxplot.html) for more details. However, they do nicely display the inter-quartile range and outliers - so they can be overlaid on top of other types of plots that show the distribution in more detail.  

Below we remind you of the various components of a boxplot:  

```{r, out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "boxplot.png"))
```

When using `geom_boxplot()` to create a box plot, you generally map only one axis (x or y) within `aes()`. The axis specified determines if the plots are horizontal or vertical. 

In most geoms, you create a plot per group by mapping an aesthetic like `color = ` or `fill = ` to a column within `aes()`. However, for box plots achieve this by assigning the grouping column to the un-assigned axis (x or y). Below is code for a boxplot of *all* age values in the dataset, and second is code to display one box plot for each (non-missing) gender in the dataset. Note that `NA` (missing) values will appear as a separate box plot unless removed. In this example we also set the `fill` to the column `outcome` so each plot is a different color - but this is not necessary.  

```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# A) Overall boxplot
ggplot(data = linelist)+  
  geom_boxplot(mapping = aes(y = age))+   # only y axis mapped (not x)
  labs(title = "A) Overall boxplot")

# B) Box plot by group
ggplot(data = linelist, mapping = aes(y = age, x = gender, fill = gender)) + 
  geom_boxplot()+                     
  theme(legend.position = "none")+   # remove legend (redundant)
  labs(title = "B) Boxplot by gender")      
```

For code to add a box plot to the edges of a scatter plot ("marginal" plots) see the page [ggplot tips].  





### Violin, jitter, and sina plots {.unnumbered}

Below is code for creating **violin plots** (`geom_violin`) and **jitter plots** (`geom_jitter`) to show distributions. You can specify that the fill or color is also determined by the data, by inserting these options within `aes()`. 

```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}


# A) Jitter plot by group
ggplot(data = linelist %>% drop_na(outcome),      # remove missing values
       mapping = aes(y = age,                     # Continuous variable
           x = outcome,                           # Grouping variable
           color = outcome))+                     # Color variable
  geom_jitter()+                                  # Create the violin plot
  labs(title = "A) jitter plot by gender")     



# B) Violin plot by group
ggplot(data = linelist %>% drop_na(outcome),       # remove missing values
       mapping = aes(y = age,                      # Continuous variable
           x = outcome,                            # Grouping variable
           fill = outcome))+                       # fill variable (color)
  geom_violin()+                                   # create the violin plot
  labs(title = "B) violin plot by gender")    
```


You can combine the two using the `geom_sina()` function from the **ggforce** package. The sina plots the jitter points in the shape of the violin plot. When overlaid on the violin plot (adjusting the transparencies) this can be easier to visually interpret.  

```{r, warning=F, message=F}

# A) Sina plot by group
ggplot(
  data = linelist %>% drop_na(outcome), 
  aes(y = age,           # numeric variable
      x = outcome)) +    # group variable
  geom_violin(
    aes(fill = outcome), # fill (color of violin background)
    color = "white",     # white outline
    alpha = 0.2)+        # transparency
  geom_sina(
    size=1,                # Change the size of the jitter
    aes(color = outcome))+ # color (color of dots)
  scale_fill_manual(       # Define fill for violin background by death/recover
    values = c("Death" = "#bf5300", 
              "Recover" = "#11118c")) + 
  scale_color_manual(      # Define colours for points by death/recover
    values = c("Death" = "#bf5300", 
              "Recover" = "#11118c")) + 
  theme_minimal() +                                # Remove the gray background
  theme(legend.position = "none") +                # Remove unnecessary legend
  labs(title = "B) violin and sina plot by gender, with extra formatting")      


```



### Two continuous variables  {.unnumbered}

Following similar syntax, `geom_point()` will allow you to plot two continuous variables against each other in a **scatter plot**. This is useful for showing actual values rather than their distributions. A basic scatter plot of age vs weight is shown in (A). In (B) we again use `facet_grid()` to show the relationship between two continuous variables in the linelist. 

```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# Basic scatter plot of weight and age
ggplot(data = linelist, 
       mapping = aes(y = wt_kg, x = age))+
  geom_point() +
  labs(title = "A) Scatter plot of weight and age")

# Scatter plot of weight and age by gender and Ebola outcome
ggplot(data = linelist %>% drop_na(gender, outcome), # filter retains non-missing gender/outcome
       mapping = aes(y = wt_kg, x = age))+
  geom_point() +
  labs(title = "B) Scatter plot of weight and age faceted by gender and outcome")+
  facet_grid(gender ~ outcome) 

```


### Three continuous variables {.unnumbered}  

You can display three continuous variables by utilizing the `fill = ` argument to create a *heat plot*. The color of each "cell" will reflect the value of the third continuous column of data. See the [ggplot tips] page and the page on on [Heat plots] for more details and several examples. 

There are ways to make 3D plots in R, but for applied epidemiology these are often difficult to interpret and therefore less useful for decision-making.  









## Plot categorical data  

Categorical data can be character values, could be logical (TRUE/FALSE), or factors (see the [Factors] page). 

### Preparation  {.unnumbered}

#### Data structure {.unnumbered}  

The first thing to understand about your categorical data is whether it exists as raw observations like a linelist of cases, or as a summary or aggregate data frame that holds counts or proportions. The state of your data will impact which plotting function you use:  

* If your data are raw observations with one row per observation, you will likely use `geom_bar()`  
* If your data are already aggregated into counts or proportions, you will likely use `geom_col()`  


#### Column class and value ordering {.unnumbered}  

Next, examine the class of the columns you want to plot. We look at `hospital`, first with `class()` from **base** R, and with `tabyl()` from **janitor**.  

```{r}
# View class of hospital column - we can see it is a character
class(linelist$hospital)

# Look at values and proportions within hospital column
linelist %>% 
  tabyl(hospital)
```

We can see the values within are characters, as they are hospital names, and by default they are ordered alphabetically. There are 'other' and 'missing' values, which we would prefer to be the last subcategories when presenting breakdowns. So we change this column into a factor and re-order it. This is covered in more detail in the [Factors] page.

```{r}
# Convert to factor and define level order so "Other" and "Missing" are last
linelist <- linelist %>% 
  mutate(
    hospital = fct_relevel(hospital, 
      "St. Mark's Maternity Hospital (SMMH)",
      "Port Hospital", 
      "Central Hospital",
      "Military Hospital",
      "Other",
      "Missing"))

```


```{r}
levels(linelist$hospital)
```

### `geom_bar()` {#ggplot_basics_bars .unnumbered}  

Use `geom_bar()` if you want bar height (or the height of stacked bar components) to reflect *the number of relevant rows in the data*. These bars will have gaps between them, unless the `width = ` plot aesthetic is adjusted.  

* Provide only one axis column assignment (typically x-axis). If you provide x and y, you will get `Error: stat_count() can only have an x or y aesthetic.`  
* You can create stacked bars by adding a `fill = ` column assignment within `mapping = aes()`  
* The opposite axis will be titled "count" by default, because it represents the number of rows  

Below, we have assigned outcome to the y-axis, but it could just as easily be on the x-axis. If you have longer character values, it can sometimes look better to flip the bars sideways and put the legend on the bottom. This may impact how your factor levels are ordered - in this case we reverse them with `fct_rev()` to put missing and other at the bottom.    

```{r, out.width=c('50%', '50%'), fig.show='hold'}
# A) Outcomes in all cases
ggplot(linelist %>% drop_na(outcome)) + 
  geom_bar(aes(y = fct_rev(hospital)), width = 0.7) +
  theme_minimal()+
  labs(title = "A) Number of cases by hospital",
       y = "Hospital")


# B) Outcomes in all cases by hosptial
ggplot(linelist %>% drop_na(outcome)) + 
  geom_bar(aes(y = fct_rev(hospital), fill = outcome), width = 0.7) +
  theme_minimal()+
  theme(legend.position = "bottom") +
  labs(title = "B) Number of recovered and dead Ebola cases, by hospital",
       y = "Hospital")

```





### `geom_col()` {.unnumbered}  

Use `geom_col()` if you want bar height (or height of stacked bar components) to reflect pre-calculated *values* that exists in the data. Often, these are summary or "aggregated" counts, or proportions.  

Provide column assignments for *both* axes to `geom_col()`. Typically your x-axis column is discrete and your y-axis column is numeric. 

Let's say we have this dataset `outcomes`:  

```{r, echo = F}
outcomes <- linelist %>% 
  drop_na() %>% 
  group_by(outcome) %>% 
  count %>% 
  ungroup() %>% # Ungroup so proportion is out of total
  mutate(proportion = n/sum(n)*100) # Caculate percentage
  
outcomes #View full table
```



Below is code using `geom_col` for creating  simple bar charts to show the distribution of Ebola patient outcomes. With geom_col, both x and y need to be specified. Here x is the categorical variable along the x axis, and y is the generated proportions column `proportion`. 

```{r, fig.height = 3, fig.width=4.5}
# Outcomes in all cases
ggplot(outcomes) + 
  geom_col(aes(x=outcome, y = proportion)) +
  labs(subtitle = "Number of recovered and dead Ebola cases")

```

To show breakdowns by hospital, we would need our table to contain more information, and to be in "long" format. We create this table with the frequencies of the combined categories `outcome` and `hospital` (see [Grouping data] page for grouping tips). 

```{r, fig.height = 4, fig.width=6}
outcomes2 <- linelist %>% 
  drop_na(outcome) %>% 
  count(hospital, outcome) %>%  # get counts by hospital and outcome
  group_by(hospital) %>%        # Group so proportions are out of hospital total
  mutate(proportion = n/sum(n)*100) # calculate proportions of hospital total

head(outcomes2) # Preview data
```

We then create the ggplot with some added formatting:

  * **Axis flip**: Swapped the axis around with `coord_flip()` so that we can read the hospital names.
  * **Columns side-by-side**: Added a `position = "dodge"` argument so that the bars for death and recover are presented side by side rather than stacked. Note stacked bars are the default.
  * **Column width**: Specified 'width', so the columns are half as thin as the full possible width.
  * **Column order**: Reversed the order of the categories on the y axis so that 'Other' and 'Missing' are at the bottom, with `scale_x_discrete(limits=rev)`. Note that we used that rather than `scale_y_discrete` because hospital is stated in the `x` argument of `aes()`, even if visually it is on the y axis. We do this because Ggplot seems to present categories backwards unless we tell it not to.  
  * **Other details**: Labels/titles and colours added within `labs` and `scale_fill_color` respectively.
  
```{r, fig.height = 4, fig.width=8}

# Outcomes in all cases by hospital
ggplot(outcomes2) +  
  geom_col(
    mapping = aes(
      x = proportion,                 # show pre-calculated proportion values
      y = fct_rev(hospital),          # reverse level order so missing/other at bottom
      fill = outcome),                # stacked by outcome
    width = 0.5)+                    # thinner bars (out of 1)
  theme_minimal() +                  # Minimal theme 
  theme(legend.position = "bottom")+
  labs(subtitle = "Number of recovered and dead Ebola cases, by hospital",
       fill = "Outcome",             # legend title
       y = "Count",                  # y axis title
       x = "Hospital of admission")+ # x axis title
  scale_fill_manual(                 # adding colors manually
    values = c("Death"= "#3B1c8C",
               "Recover" = "#21908D" )) 

```


Note that the proportions are binary, so we may prefer to drop 'recover' and just show the proportion who died. This is just for illustration purposes.  


If using `geom_col()` with dates data (e.g. an epicurve from aggregated data) - you will want to adjust the `width = ` argument to remove the "gap" lines between the bars. If using daily data set `width = 1`. If weekly, `width = 7`. Months are not possible because each month has a different number of days.  


### `geom_histogram()` {.unnumbered}  

Histograms may look like bar charts, but are distinct because they measure the distribution of a *continuous* variable. There are no spaces between the "bars", and only one column is provided to `geom_histogram()`. There are arguments specific to histograms such as `bin_width = ` and `breaks = ` to specify how the data should be binned. The section above on continuous data and the page on [Epidemic curves] provide additional detail.  



## Resources  

There is a huge amount of help online, especially with ggplot. See:

* [ggplot2 cheat sheet](http://r-statistics.co/ggplot2-cheatsheet.html)
* [another cheat sheet](https://biostats.w.uib.no/the-ggplot2-cheat-sheet-by-rstudio/)
* [tidyverse ggplot basics page](https://ggplot2.tidyverse.org/reference/)  
* [plotting continuous variables](http://www.sthda.com/english/articles/32-r-graphics-essentials/131-plot-two-continuous-variables-scatter-graph-and-alternatives/)  
* R for Data Science pages on [data visualization](https://r4ds.had.co.nz/data-visualisation.html)
* [graphics for communicaton](https://r4ds.had.co.nz/graphics-for-communication.html)  

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/ggplot_basics.Rmd-->


# Các tips với ggplot {#ggplot-tips}

In this page we will cover tips and tricks to make your ggplots sharp and fancy. See the page on [ggplot basics] for the fundamentals.  

There a several extensive [**ggplot2** tutorials](https://ggplot2.tidyverse.org/) linked in the Resources section. You can also download this [data visualization with ggplot cheatsheet](https://rstudio.com/resources/cheatsheets/) from the RStudio website. We strongly recommend that you peruse for inspiration at the [R graph gallery](https://www.r-graph-gallery.com/) and [Data-to-viz](https://www.data-to-viz.com/caveats.html). 



<!-- ======================================================= -->
## Preparation {}

### Load packages {.unnumbered}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  tidyverse,      # includes ggplot2 and other
  rio,            # import/export
  here,           # file locator
  stringr,        # working with characters   
  scales,         # transform numbers
  ggrepel,        # smartly-placed labels
  gghighlight,    # highlight one part of plot
  RColorBrewer    # color scales
)
```

### Import data {.unnumbered}  

For this page, we import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Import data with the `import()` function from the **rio** package (it handles many file types like .xlsx, .csv, .rds - see the [Import and export] page for details).  

```{r,  echo=F}
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

```

```{r, eval=F}
linelist <- rio::import("linelist_cleaned.rds")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```




<!-- ======================================================= -->
## Scales for color, fill, axes, etc. {#ggplot_tips_colors}

In **ggplot2**, when aesthetics of plotted data (e.g. size, color, shape, fill, plot axis) are mapped to columns in the data, the exact display can be adjusted with the corresponding "scale" command. In this section we explain some common scale adjustments.  



### Color schemes

One thing that can initially be difficult to understand with **ggplot2** is control of color schemes. Note that this section discusses the color of *plot objects* (geoms/shapes) such as points, bars, lines, tiles, etc. To adjust color of accessory text, titles, or background color see the [Themes](#ggplot_basics_themes) section of the [ggplot basics] page. 

To control "color" of *plot objects* you will be adjusting either the `color = ` aesthetic (the *exterior* color) or the `fill = ` aesthetic (the *interior* color). One exception to this pattern is `geom_point()`, where you really only get to control `color = `, which controls the color of the point (interior and exterior).  

When setting colour or fill you can use colour names recognized by R like `"red"` (see [complete list](http://sape.inf.usi.ch/quick-reference/ggplot2/colour) or enter `?colors`), or a specific hex colour such as `"#ff0505"`.

```{r, warning=F, message=F}
# histogram - 
ggplot(data = linelist, mapping = aes(x = age))+       # set data and axes
  geom_histogram(              # display histogram
    binwidth = 7,                # width of bins
    color = "red",               # bin line color
    fill = "lightblue")          # bin interior color (fill) 
```



As explained the [ggplot basics] section on [mapping data to the plot](#ggplot_basics_mapping), aesthetics such as `fill = ` and `color = ` can be defined either *outside* of a `mapping = aes()` statement or *inside* of one. If *outside* the `aes()`, the assigned value should be static (e.g. `color = "blue"`) and will apply for *all* data plotted by the geom. If *inside*, the aesthetic should be mapped to a column, like `color = hospital`, and the expression will vary by the value for that row in the data. A few examples:  

```{r, out.width=c('50%', '50%'), fig.show='hold', warning=F, message=F}
# Static color for points and for line
ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+     
  geom_point(color = "purple")+
  geom_vline(xintercept = 50, color = "orange")+
  labs(title = "Static color for points and line")

# Color mapped to continuous column
ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+     
  geom_point(mapping = aes(color = temp))+         
  labs(title = "Color mapped to continuous column")

# Color mapped to discrete column
ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+     
  geom_point(mapping = aes(color = gender))+         
  labs(title = "Color mapped to discrete column")

# bar plot, fill to discrete column, color to static value
ggplot(data = linelist, mapping = aes(x = hospital))+     
  geom_bar(mapping = aes(fill = gender), color = "yellow")+         
  labs(title = "Fill mapped to discrete column, static color")

```


### Scales {#ggplot_tips_scales .unnumbered}  

Once you map a column to a plot aesthetic (e.g. `x = `, `y = `, `fill = `, `color = `...), your plot will gain a scale/legend. See above how the scale can be continuous, discrete, date, etc. values depending on the class of the assigned column. If you have multiple aesthetics mapped to columns, your plot will have multiple scales.  

You can control the scales with the appropriate `scales_()` function. The scale functions of **ggplot()** have 3 parts that are written like this: `scale_AESTHETIC_METHOD()`.  

1) The first part, `scale_()`, is fixed.  
2) The second part, the AESTHETIC, should be the aesthetic that you want to adjust the scale for (`_fill_`, `_shape_`, `_color_`, `_size_`, `_alpha_`...) - the options here also include `_x_` and `_y_`.  
3) The third part, the METHOD, will be either `_discrete()`, `continuous()`, `_date()`, `_gradient()`, or `_manual()` depending on the class of the column and *how* you want to control it. There are others, but these are the most-often used.  

Be sure that you use the correct function for the scale! Otherwise your scale command will not appear to change anything. If you have multiple scales, you may use multiple scale functions to adjust them! For example:  

### Scale arguments {.unnumbered}  

Each kind of scale has its own arguments, though there is some overlap. Query the function like `?scale_color_discrete` in the R console to see the function argument documentation.  

For continuous scales, use `breaks = ` to provide a sequence of values with `seq()` (take `to = `, `from = `, and `by = ` as shown in the example below. Set `expand = c(0,0)` to eliminate padding space around the axes (this can be used on any `_x_` or `_y_` scale.  

For discrete scales, you can adjust the order of level appearance with `breaks = `, and how the values display with the `labels = ` argument. Provide a character vector to each of those (see example below). You can also drop `NA` easily by setting `na.translate = FALSE`.  

The nuances of date scales are covered more extensively in the [Epidemic curves] page.  


### Manual adjustments {.unnumbered}  

One of the most useful tricks is using "manual" scaling functions to explicitly assign colors as you desire. These are functions with the syntax `scale_xxx_manual()` (e.g. `scale_colour_manual()` or `scale_fill_manual()`). Each of the below arguments are demonstrated in the code example below.  

* Assign colors to data values with the `values = ` argument  
* Specify a color for `NA` with `na.value = `  
* Change how the values are *written* in the legend with the `labels = ` argument  
* Change the legend title with `name = `  


Below, we create a bar plot and show how it appears by default, and then with three scales adjusted - the continuous y-axis scale, the discrete x-axis scale, and manual adjustment of the fill (interior bar color).  


```{r, warning=F, message=F}
# BASELINE - no scale adjustment
ggplot(data = linelist)+
  geom_bar(mapping = aes(x = outcome, fill = gender))+
  labs(title = "Baseline - no scale adjustments")

# SCALES ADJUSTED
ggplot(data = linelist)+
  
  geom_bar(mapping = aes(x = outcome, fill = gender), color = "black")+
  
  theme_minimal()+                   # simplify background
  
  scale_y_continuous(                # continuous scale for y-axis (counts)
    expand = c(0,0),                 # no padding
    breaks = seq(from = 0,
                 to = 3000,
                 by = 500))+
  
  scale_x_discrete(                   # discrete scale for x-axis (gender)
    expand = c(0,0),                  # no padding
    drop = FALSE,                     # show all factor levels (even if not in data)
    na.translate = FALSE,             # remove NA outcomes from plot
    labels = c("Died", "Recovered"))+ # Change display of values
    
  
  scale_fill_manual(                  # Manually specify fill (bar interior color)
    values = c("m" = "violetred",     # reference values in data to assign colors
               "f" = "aquamarine"),
    labels = c("m" = "Male",          # re-label the legend (use "=" assignment to avoid mistakes)
              "f" = "Female",
              "Missing"),
    name = "Gender",                  # title of legend
    na.value = "grey"                 # assign a color for missing values
  )+
  labs(title = "Adjustment of scales") # Adjust the title of the fill legend
```

### Continuous axes scales {.unnumbered}  

When data are mapping to the plot axes, these too can be adjusted with scales commands. A common example is adjusting the display of an axis (e.g. y-axis) that is mapped to a column with continuous data. 

We may want to adjust the breaks or display of the values in the ggplot using `scale_y_continuous()`. As noted above, use the argument `breaks = ` to provide a sequence of values that will serve as "breaks" along the scale. These are the values at which numbers will display. To this argument, you can provide a `c()` vector containing the desired break values, or you can provide a regular sequence of numbers using the **base** R function `seq()`. This `seq()` function accepts `to = `, `from = `, and `by = `.

```{r, warning=F, message=F, out.width=c('50%', '50%'), fig.show='hold'}
# BASELINE - no scale adjustment
ggplot(data = linelist)+
  geom_bar(mapping = aes(x = outcome, fill = gender))+
  labs(title = "Baseline - no scale adjustments")

# 
ggplot(data = linelist)+
  geom_bar(mapping = aes(x = outcome, fill = gender))+
  scale_y_continuous(
    breaks = seq(
      from = 0,
      to = 3000,
      by = 100)
  )+
  labs(title = "Adjusted y-axis breaks")

```



#### Display percents {.unnumbered}  

If your original data values are proportions, you can easily display them as percents with "%" by providing `labels = scales::percent` in your scales command, as shown below.  

While an alternative would be to convert the values to character and add a "%" character to the end, this approach will cause complications because your data will no longer be continuous numeric values. 


```{r, warning=F, message=F, out.width=c('50%', '50%'), fig.show='hold'}
# Original y-axis proportions
#############################
linelist %>%                                   # start with linelist
  group_by(hospital) %>%                       # group data by hospital
  summarise(                                   # create summary columns
    n = n(),                                     # total number of rows in group
    deaths = sum(outcome == "Death", na.rm=T),   # number of deaths in group
    prop_death = deaths/n) %>%                   # proportion deaths in group
  ggplot(                                      # begin plotting
    mapping = aes(
      x = hospital,
      y = prop_death))+ 
  geom_col()+
  theme_minimal()+
  labs(title = "Display y-axis original proportions")



# Display y-axis proportions as percents
########################################
linelist %>%         
  group_by(hospital) %>% 
  summarise(
    n = n(),
    deaths = sum(outcome == "Death", na.rm=T),
    prop_death = deaths/n) %>% 
  ggplot(
    mapping = aes(
      x = hospital,
      y = prop_death))+
  geom_col()+
  theme_minimal()+
  labs(title = "Display y-axis as percents (%)")+
  scale_y_continuous(
    labels = scales::percent                    # display proportions as percents
  )

```

#### Log scale {.unnumbered}  

To transform a continuous axis to log scale, add `trans = "log2"` to the scale command. For purposes of example, we create a data frame of regions with their respective `preparedness_index` and cumulative cases values.  

```{r}
plot_data <- data.frame(
  region = c("A", "B", "C", "D", "E", "F", "G", "H", "I"),
  preparedness_index = c(8.8, 7.5, 3.4, 3.6, 2.1, 7.9, 7.0, 5.6, 1.0),
  cases_cumulative = c(15, 45, 80, 20, 21, 7, 51, 30, 1442)
)

plot_data
```

The cumulative cases for region "I" are dramatically greater than all the other regions. In circumstances like this, you may elect to display the y-axis using a log scale so the reader can see differences between the regions with fewer cumulative cases.  

```{r, warning=F, message=F, out.width=c('50%', '50%'), fig.show='hold'}
# Original y-axis
preparedness_plot <- ggplot(data = plot_data,  
       mapping = aes(
         x = preparedness_index,
         y = cases_cumulative))+
  geom_point(size = 2)+            # points for each region 
  geom_text(
    mapping = aes(label = region),
    vjust = 1.5)+                  # add text labels
  theme_minimal()

preparedness_plot                  # print original plot


# print with y-axis transformed
preparedness_plot+                   # begin with plot saved above
  scale_y_continuous(trans = "log2") # add transformation for y-axis
```



### Gradient scales {.unnumbered}  

Fill gradient scales can involve additional nuance. The defaults are usually quite pleasing, but you may want to adjust the values, cutoffs, etc.  

To demonstrate how to adjust a continuous color scale, we'll use a data set from the [Contact tracing] page that contains the ages of cases and of their source cases.  


```{r, warning=F, message=F}
case_source_relationships <- rio::import(here::here("data", "godata", "relationships_clean.rds")) %>% 
  select(source_age, target_age) 
```

Below, we produce a "raster" heat tile density plot. We won't elaborate how (see the link in paragraph above) but we will focus on how we can adjust the color scale. Read more about the `stat_density2d()` **ggplot2** function [here](https://ggplot2.tidyverse.org/reference/geom_density_2d.html). Note how the `fill` scale is *continuous*.  

```{r, warn=F, message=F}
trans_matrix <- ggplot(
    data = case_source_relationships,
    mapping = aes(x = source_age, y = target_age))+
  stat_density2d(
    geom = "raster",
    mapping = aes(fill = after_stat(density)),
    contour = FALSE)+
  theme_minimal()
```

Now we show some variations on the fill scale: 

```{r, out.width=c('50%', '50%'), fig.show='hold', warning=F, message=F}
trans_matrix
trans_matrix + scale_fill_viridis_c(option = "plasma")
```

Now we show some examples of actually adjusting the break points of the scale:  

* `scale_fill_gradient()` accepts two colors (high/low)  
* `scale_fill_gradientn()` accepts a vector of any length of colors to `values = ` (intermediate values will be interpolated)  
* Use [`scales::rescale()`](https://www.rdocumentation.org/packages/scales/versions/0.4.1/topics/rescale) to adjust how colors are positioned along the gradient; it rescales your vector of positions to be between 0 and 1.  


```{r, out.width=c('50%', '50%'), fig.show='hold', warning=F, message=F}
trans_matrix + 
  scale_fill_gradient(     # 2-sided gradient scale
    low = "aquamarine",    # low value
    high = "purple",       # high value
    na.value = "grey",     # value for NA
    name = "Density")+     # Legend title
  labs(title = "Manually specify high/low colors")

# 3+ colors to scale
trans_matrix + 
  scale_fill_gradientn(    # 3-color scale (low/mid/high)
    colors = c("blue", "yellow","red") # provide colors in vector
  )+
  labs(title = "3-color scale")

# Use of rescale() to adjust placement of colors along scale
trans_matrix + 
  scale_fill_gradientn(    # provide any number of colors
    colors = c("blue", "yellow","red", "black"),
    values = scales::rescale(c(0, 0.05, 0.07, 0.10, 0.15, 0.20, 0.3, 0.5)) # positions for colors are rescaled between 0 and 1
    )+
  labs(title = "Colors not evenly positioned")

# use of limits to cut-off values that get fill color
trans_matrix + 
  scale_fill_gradientn(    
    colors = c("blue", "yellow","red"),
    limits = c(0, 0.0002))+
  labs(title = "Restrict value limits, resulting in grey space")

```


### Palettes {.unnumbered}  

#### Colorbrewer and Viridis {.unnumbered}
More generally, if you want predefined palettes, you can use the `scale_xxx_brewer` or `scale_xxx_viridis_y` functions.  

The 'brewer' functions can draw from [colorbrewer.org](colorbrewer.org) palettes.  

The 'viridis' functions draw from viridis (colourblind friendly!) palettes, which "provide colour maps that are perceptually uniform in both colour and black-and-white. They are also designed to be perceived by viewers with common forms of colour blindness." (read more [here](https://ggplot2.tidyverse.org/reference/scale_viridis.html) and [here](https://bids.github.io/colormap/)). Define if the palette is discrete, continuous, or binned by specifying this at the end of the function (e.g. discrete is `scale_xxx_viridis_d`).

It is advised that you test your plot in this [color blindness simulator](https://www.color-blindness.com/coblis-color-blindness-simulator/). If you have a red/green color scheme, try a "hot-cold" (red-blue) scheme instead as described [here](https://www.visualisingdata.com/2019/08/five-ways-to-design-for-red-green-colour-blindness/#:~:text=The%20pink%2Dred%20through%20to,green%20hues%20used%20by%20default.)  

Here is an example from the [ggplot basics] page, using various color schemes. 

```{r, out.width=c('50%'), fig.show='hold', warning=F, message=F} 
symp_plot <- linelist %>%                                         # begin with linelist
  select(c(case_id, fever, chills, cough, aches, vomit)) %>%     # select columns
  pivot_longer(                                                  # pivot longer
    cols = -case_id,                                  
    names_to = "symptom_name",
    values_to = "symptom_is_present") %>%
  mutate(                                                        # replace missing values
    symptom_is_present = replace_na(symptom_is_present, "unknown")) %>% 
  ggplot(                                                        # begin ggplot!
    mapping = aes(x = symptom_name, fill = symptom_is_present))+
  geom_bar(position = "fill", col = "black") +                    
  theme_classic() +
  theme(legend.position = "bottom")+
  labs(
    x = "Symptom",
    y = "Symptom status (proportion)"
  )

symp_plot  # print with default colors

#################################
# print with manually-specified colors
symp_plot +
  scale_fill_manual(
    values = c("yes" = "black",         # explicitly define colours
               "no" = "white",
               "unknown" = "grey"),
    breaks = c("yes", "no", "unknown"), # order the factors correctly
    name = ""                           # set legend to no title

  ) 

#################################
# print with viridis discrete colors
symp_plot +
  scale_fill_viridis_d(
    breaks = c("yes", "no", "unknown"),
    name = ""
  )


```



<!-- ======================================================= -->
## Change order of discrete variables {}  

Changing the order that discrete variables appear in is often difficult to understand for people who are new to `ggplot2` graphs. It's easy to understand how to do this however once you understand how `ggplot2` handles discrete variables under the hood. Generally speaking, if a discrete varaible is used, it is automatically converted to a `factor` type - which orders factors by alphabetical order by default. To handle this, you simply have to reorder the factor levels to reflect the order you would like them to appear in the chart. For more detailed information on how to reorder `factor` objects, see the factor section of the guide. 

We can look at a common example using age groups - by default the 5-9 age group will be placed in the middle of the age groups (given alphanumeric order), but we can move it behind the 0-4 age group of the chart by releveling the factors.


```{r, , warning=F, message=F}
ggplot(
  data = linelist %>% drop_na(age_cat5),                         # remove rows where age_cat5 is missing
  mapping = aes(x = fct_relevel(age_cat5, "5-9", after = 1))) +  # relevel factor

  geom_bar() +
  
  labs(x = "Age group", y = "Number of hospitalisations",
       title = "Total hospitalisations by age group") +
  
  theme_minimal()


```

#### **ggthemr** {.unnnumbered}  

Also consider using the **ggthemr** package. You can download this package from Github using the instructions [here](https://github.com/Mikata-Project/ggthemr). It offers palettes that are very aesthetically pleasing, but be aware that these typically have a maximum number of values that can be limiting if you want more than 7 or 8 colors.  






## Contour lines  

Contour plots are helpful when you have many points that might cover each other ("overplotting"). The case-source data used above are again plotted, but more simply using `stat_density2d()` and `stat_density2d_filled()` to produce discrete contour levels - like a topographical map. Read more about the statistics [here](https://ggplot2.tidyverse.org/reference/geom_density_2d.html).  


```{r, out.width=c('50%'), fig.show='hold', warning=F, message=F}
case_source_relationships %>% 
  ggplot(aes(x = source_age, y = target_age))+
  stat_density2d()+
  geom_point()+
  theme_minimal()+
  labs(title = "stat_density2d() + geom_point()")


case_source_relationships %>% 
  ggplot(aes(x = source_age, y = target_age))+
  stat_density2d_filled()+
  theme_minimal()+
  labs(title = "stat_density2d_filled()")

```



## Marginal distributions  

To show the distributions on the edges of a `geom_point()` scatterplot, you can use the **ggExtra** package and its function `ggMarginal()`. Save your original ggplot as an object, then pass it to `ggMarginal()` as shown below. Here are the key arguments:  

* You must specify the `type = ` as either "histogram", "density" "boxplot", "violin", or "densigram".  
* By default, marginal plots will appear for both axes. You can set `margins = ` to "x" or "y" if you only want one.  
* Other optional arguments include `fill = ` (bar color), `color = ` (line color), `size = ` (plot size relative to margin size, so larger number makes the marginal plot smaller).  
* You can provide other axis-specific arguments to `xparams = ` and `yparams = `. For example, to have different histogram bin sizes, as shown below.  

You can have the marginal plots reflect groups (columns that have been assigned to `color = ` in your `ggplot()` mapped aesthetics). If this is the case, set the `ggMarginal()` argument `groupColour = ` or `groupFill = ` to `TRUE`, as shown below.  

Read more at [this vignette](https://cran.r-project.org/web/packages/ggExtra/vignettes/ggExtra.html), in the [R Graph Gallery](https://www.r-graph-gallery.com/277-marginal-histogram-for-ggplot2.html) or the function R documentation `?ggMarginal`.  

```{r, message=FALSE, warning=FALSE}
# Install/load ggExtra
pacman::p_load(ggExtra)

# Basic scatter plot of weight and age
scatter_plot <- ggplot(data = linelist)+
  geom_point(mapping = aes(y = wt_kg, x = age)) +
  labs(title = "Scatter plot of weight and age")
```

To add marginal histograms use `type = "histogram"`. You can optionally set `groupFill = TRUE` to get stacked histograms.     

```{r, message=FALSE, warning=FALSE}
# with histograms
ggMarginal(
  scatter_plot,                     # add marginal histograms
  type = "histogram",               # specify histograms
  fill = "lightblue",               # bar fill
  xparams = list(binwidth = 10),    # other parameters for x-axis marginal
  yparams = list(binwidth = 5))     # other parameters for y-axis marginal
```

Marginal density plot with grouped/colored values:  

```{r, message=FALSE, warning=FALSE}

# Scatter plot, colored by outcome
# Outcome column is assigned as color in ggplot. groupFill in ggMarginal set to TRUE
scatter_plot_color <- ggplot(data = linelist %>% drop_na(gender))+
  geom_point(mapping = aes(y = wt_kg, x = age, color = gender)) +
  labs(title = "Scatter plot of weight and age")+
  theme(legend.position = "bottom")

ggMarginal(scatter_plot_color, type = "density", groupFill = TRUE)
```

Set the `size = ` arguemnt to adjust the relative size of the marginal plot. Smaller number makes a larger marginal plot. You also set `color = `. Below are is a marginal boxplot, with demonstration of the `margins = ` argument so it appears on only one axis:  

```{r, message=FALSE, warning=FALSE}
# with boxplot 
ggMarginal(
  scatter_plot,
  margins = "x",      # only show x-axis marginal plot
  type = "boxplot")   
```



<!-- ======================================================= -->
## Smart Labeling {}  

In **ggplot2**, it is also possible to add text to plots. However, this comes with the notable limitation where text labels often clash with data points in a plot, making them look messy or hard to read. There is no ideal way to deal with this in the base package, but there is a **ggplot2** add-on, known as **ggrepel** that makes dealing with this very simple! 

The **ggrepel** package provides two new functions, `geom_label_repel()` and `geom_text_repel()`, which replace `geom_label()` and `geom_text()`. Simply use these functions instead of the base functions to produce neat labels. Within the function, map the aesthetics `aes()` as always, but include the argument `label = ` to which you provide a column name containing the values you want to display (e.g. patient id, or name, etc.). You can make more complex labels by combining columns and newlines (`\n`) within `str_glue()` as shown below.  

A few tips:  

* Use `min.segment.length = 0` to always draw line segments, or `min.segment.length = Inf` to never draw them  
* Use `size = ` outside of `aes()` to set text size  
* Use `force = ` to change the degree of repulsion between labels and their respective points (default is 1)  
* Include `fill = ` within `aes()` to have label colored by value  
  * A letter "a" may appear in the legend - add `guides(fill = guide_legend(override.aes = aes(color = NA)))+` to remove it  

See this is very in-depth [tutorial](https://ggrepel.slowkow.com/articles/examples.html) for more.  

```{r, , warning=F, message=F}
pacman::p_load(ggrepel)

linelist %>%                                               # start with linelist
  group_by(hospital) %>%                                   # group by hospital
  summarise(                                               # create new dataset with summary values per hospital
    n_cases = n(),                                           # number of cases per hospital
    delay_mean = round(mean(days_onset_hosp, na.rm=T),1),    # mean delay per hospital
  ) %>% 
  ggplot(mapping = aes(x = n_cases, y = delay_mean))+      # send data frame to ggplot
  geom_point(size = 2)+                                    # add points
  geom_label_repel(                                        # add point labels
    mapping = aes(
      label = stringr::str_glue(
        "{hospital}\n{n_cases} cases, {delay_mean} days")  # how label displays
      ), 
    size = 3,                                              # text size in labels
    min.segment.length = 0)+                               # show all line segments                
  labs(                                                    # add axes labels
    title = "Mean delay to admission, by hospital",
    x = "Number of cases",
    y = "Mean delay (days)")
```

You can label only a subset of the data points - by using standard `ggplot()` syntax to provide different `data = ` for each `geom` layer of the plot. Below, All cases are plotted, but only a few are labeled.    

```{r, warning=F, message=FALSE}

ggplot()+
  # All points in grey
  geom_point(
    data = linelist,                                   # all data provided to this layer
    mapping = aes(x = ht_cm, y = wt_kg),
    color = "grey",
    alpha = 0.5)+                                              # grey and semi-transparent
  
  # Few points in black
  geom_point(
    data = linelist %>% filter(days_onset_hosp > 15),  # filtered data provided to this layer
    mapping = aes(x = ht_cm, y = wt_kg),
    alpha = 1)+                                                # default black and not transparent
  
  # point labels for few points
  geom_label_repel(
    data = linelist %>% filter(days_onset_hosp > 15),  # filter the data for the labels
    mapping = aes(
      x = ht_cm,
      y = wt_kg,
      fill = outcome,                                          # label color by outcome
      label = stringr::str_glue("Delay: {days_onset_hosp}d")), # label created with str_glue()
    min.segment.length = 0) +                                  # show line segments for all
  
  # remove letter "a" from inside legend boxes
  guides(fill = guide_legend(override.aes = aes(color = NA)))+
  
  # axis labels
  labs(
    title = "Cases with long delay to admission",
    y = "weight (kg)",
    x = "height(cm)")
```





<!-- ======================================================= -->
## Time axes {}

Working with time axes in ggplot can seem daunting, but is made very easy with a few key functions. Remember that when working with time or date that you should ensure that the correct variables are formatted as date or datetime class - see the [Working with dates] page for more information on this, or [Epidemic curves] page (ggplot section) for examples.

The single most useful set of functions for working with dates in `ggplot2` are the scale functions (`scale_x_date()`, `scale_x_datetime()`, and their cognate y-axis functions). These functions let you define how often you have axis labels, and how to format axis labels. To find out how to format dates, see the _working with dates_ section again! You can use the `date_breaks` and `date_labels` arguments to specify how dates should look:

  1. `date_breaks` allows you to specify how often axis breaks occur - you can pass a string here (e.g. `"3 months"`, or "`2 days"`)
  
  2. `date_labels` allows you to define the format dates are shown in. You can pass a date format string to these arguments (e.g. `"%b-%d-%Y"`):


```{r, , warning=F, message=F}
# make epi curve by date of onset when available
ggplot(linelist, aes(x = date_onset)) +
  geom_histogram(binwidth = 7) +
  scale_x_date(
    # 1 break every 1 month
    date_breaks = "1 months",
    # labels should show month then date
    date_labels = "%b %d"
  ) +
  theme_classic()

```



<!-- ======================================================= -->
## Highlighting {}

Highlighting specific elements in a chart is a useful way to draw attention to a specific instance of a variable while also providing information on the dispersion of the full dataset. While this is not easily done in base **ggplot2**, there is an external package that can help to do this known as **gghighlight**. This is easy to use within the ggplot syntax.

The **gghighlight** package uses the `gghighlight()` function to achieve this effect. To use this function, supply a logical statement to the function - this can have quite flexible outcomes, but here we'll show an example of the age distribution of cases in our linelist, highlighting them by outcome.

```{r, , warning=F, message=F}
# load gghighlight
library(gghighlight)

# replace NA values with unknown in the outcome variable
linelist <- linelist %>%
  mutate(outcome = replace_na(outcome, "Unknown"))

# produce a histogram of all cases by age
ggplot(
  data = linelist,
  mapping = aes(x = age_years, fill = outcome)) +
  geom_histogram() + 
  gghighlight::gghighlight(outcome == "Death")     # highlight instances where the patient has died.

```

This also works well with faceting functions - it allows the user to produce facet plots with the background data highlighted that doesn't apply to the facet! Below we count cases by week and plot the epidemic curves by hospital (`color = ` and `facet_wrap()` set to `hospital` column).  

```{r, , warning=F, message=F}

# produce a histogram of all cases by age
linelist %>% 
  count(week = lubridate::floor_date(date_hospitalisation, "week"),
        hospital) %>% 
  ggplot()+
  geom_line(aes(x = week, y = n, color = hospital))+
  theme_minimal()+
  gghighlight::gghighlight() +                      # highlight instances where the patient has died
  facet_wrap(~hospital)                              # make facets by outcome

```





## Plotting multiple datasets  

Note that properly aligning axes to plot from multiple datasets in the same plot can be difficult. Consider one of the following strategies:  

* Merge the data prior to plotting, and convert to "long" format with a column reflecting the dataset  
* Use **cowplot** or a similar package to combine two plots (see below)  






<!-- ======================================================= -->
## Combine plots {}

Two packages that are very useful for combining plots are **cowplot** and **patchwork**. In this page we will mostly focus on **cowplot**, with occassional use of **patchwork**.  

Here is the online [introduction to cowplot](https://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html). You can read the more extensive documentation for each function online [here](https://www.rdocumentation.org/packages/cowplot/versions/1.1.1). We will cover a few of the most common use cases and functions below.  

The **cowplot** package works in tandem with **ggplot2** - essentially, you use it to arrange and combine ggplots and their legends into compound figures. It can also accept **base** R graphics.  

```{r}
pacman::p_load(
  tidyverse,      # data manipulation and visualisation
  cowplot,        # combine plots
  patchwork       # combine plots
)
```


While faceting (described in the [ggplot basics] page) is a convenient approach to plotting, sometimes its not possible to get the results you want from its relatively restrictive approach. Here, you may choose to combine plots by sticking them together into a larger plot. There are three well known packages that are great for this - **cowplot**, **gridExtra**, and **patchwork**. However, these packages largely do the same things, so we'll focus on **cowplot** for this section. 

### `plot_grid()` {.unnumbered}

The **cowplot** package has a fairly wide range of functions, but the easiest use of it can be achieved through the use of `plot_grid()`. This is effectively a way to arrange predefined plots in a grid formation. We can work through another example with the malaria dataset - here we can plot the total cases by district, and also show the epidemic curve over time.


```{r, , warning=F, message=F}
malaria_data <- rio::import(here::here("data", "malaria_facility_count_data.rds")) 

# bar chart of total cases by district
p1 <- ggplot(malaria_data, aes(x = District, y = malaria_tot)) +
  geom_bar(stat = "identity") +
  labs(
    x = "District",
    y = "Total number of cases",
    title = "Total malaria cases by district"
  ) +
  theme_minimal()

# epidemic curve over time
p2 <- ggplot(malaria_data, aes(x = data_date, y = malaria_tot)) +
  geom_col(width = 1) +
  labs(
    x = "Date of data submission",
    y =  "number of cases"
  ) +
  theme_minimal()

cowplot::plot_grid(p1, p2,
                  # 1 column and two rows - stacked on top of each other
                   ncol = 1,
                   nrow = 2,
                   # top plot is 2/3 as tall as second
                   rel_heights = c(2, 3))


```




### Combine legends {.unnumbered}  

If your plots have the same legend, combining them is relatively straight-forward. Simple use the **cowplot** approach above to combine the plots, but remove the legend from one of them (de-duplicate).  

If your plots have different legends, you must use an alternative approach:  

1) Create and save your plots *without legends* using `theme(legend.position = "none")`  
2) Extract the legends from each plot using `get_legend()` as shown below - *but extract legends from the plots modified to actually show the legend*  
3) Combine the legends into a legends panel  
4) Combine the plots and legends panel  


For demonstration we show the two plots separately, and then arranged in a grid with their own legends showing (ugly and inefficient use of space):  

```{r, out.width=c('50%'), fig.show='hold', warning=F, message=F}
p1 <- linelist %>% 
  mutate(hospital = recode(hospital, "St. Mark's Maternity Hospital (SMMH)" = "St. Marks")) %>% 
  count(hospital, outcome) %>% 
  ggplot()+
  geom_col(mapping = aes(x = hospital, y = n, fill = outcome))+
  scale_fill_brewer(type = "qual", palette = 4, na.value = "grey")+
  coord_flip()+
  theme_minimal()+
  labs(title = "Cases by outcome")


p2 <- linelist %>% 
  mutate(hospital = recode(hospital, "St. Mark's Maternity Hospital (SMMH)" = "St. Marks")) %>% 
  count(hospital, age_cat) %>% 
  ggplot()+
  geom_col(mapping = aes(x = hospital, y = n, fill = age_cat))+
  scale_fill_brewer(type = "qual", palette = 1, na.value = "grey")+
  coord_flip()+
  theme_minimal()+
  theme(axis.text.y = element_blank())+
  labs(title = "Cases by age")

```

Here is how the two plots look when combined using `plot_grid()` without combining their legends:  

```{r, warning=F, message=F}
cowplot::plot_grid(p1, p2, rel_widths = c(0.3))
```

And now we show how to combine the legends. Essentially what we do is to define each plot *without* its legend (`theme(legend.position = "none"`), and then we define each plot's legend *separately*, using the `get_legend()` function from **cowplot**. When we extract the legend from the saved plot, we need to add `+` the legend back in, including specifying the placement ("right") and smaller adjustments for alignment of the legends and their titles. Then, we combine the legends together vertically, and then combine the two plots with the newly-combined legends. Voila!  

```{r, warning=F, message=F}

# Define plot 1 without legend
p1 <- linelist %>% 
  mutate(hospital = recode(hospital, "St. Mark's Maternity Hospital (SMMH)" = "St. Marks")) %>% 
  count(hospital, outcome) %>% 
  ggplot()+
  geom_col(mapping = aes(x = hospital, y = n, fill = outcome))+
  scale_fill_brewer(type = "qual", palette = 4, na.value = "grey")+
  coord_flip()+
  theme_minimal()+
  theme(legend.position = "none")+
  labs(title = "Cases by outcome")


# Define plot 2 without legend
p2 <- linelist %>% 
  mutate(hospital = recode(hospital, "St. Mark's Maternity Hospital (SMMH)" = "St. Marks")) %>% 
  count(hospital, age_cat) %>% 
  ggplot()+
  geom_col(mapping = aes(x = hospital, y = n, fill = age_cat))+
  scale_fill_brewer(type = "qual", palette = 1, na.value = "grey")+
  coord_flip()+
  theme_minimal()+
  theme(
    legend.position = "none",
    axis.text.y = element_blank(),
    axis.title.y = element_blank()
  )+
  labs(title = "Cases by age")


# extract legend from p1 (from p1 + legend)
leg_p1 <- cowplot::get_legend(p1 +
                                theme(legend.position = "right",        # extract vertical legend
                                      legend.justification = c(0,0.5))+ # so legends  align
                                labs(fill = "Outcome"))                 # title of legend
# extract legend from p2 (from p2 + legend)
leg_p2 <- cowplot::get_legend(p2 + 
                                theme(legend.position = "right",         # extract vertical legend   
                                      legend.justification = c(0,0.5))+  # so legends align
                                labs(fill = "Age Category"))             # title of legend

# create a blank plot for legend alignment
#blank_p <- patchwork::plot_spacer() + theme_void()

# create legends panel, can be one on top of the other (or use spacer commented above)
legends <- cowplot::plot_grid(leg_p1, leg_p2, nrow = 2, rel_heights = c(.3, .7))

# combine two plots and the combined legends panel
combined <- cowplot::plot_grid(p1, p2, legends, ncol = 3, rel_widths = c(.4, .4, .2))

combined  # print


```

This solution was learned from [this post](https://stackoverflow.com/questions/52060601/ggplot-multiple-legends-arrangement) with a minor fix to align legends from [this post](https://github.com/wilkelab/cowplot/issues/33).  


<span style="color: darkgreen;">**_TIP:_** Fun note - the "cow" in **cowplot** comes from the creator's name - Claus O. Wilke.</span>  


### Inset plots {.unnumbered} 

You can inset one plot in another using **cowplot**. Here are things to be aware of:  

* Define the main plot with `theme_half_open()` from **cowplot**; it may be best to have the legend either on top or bottom  
* Define the inset plot. Best is to have a plot where you do not need a legend. You can remove plot theme elements with `element_blank()` as shown below.  
* Combine them by applying `ggdraw()` to the main plot, then adding `draw_plot()` on the inset plot and specifying the coordinates (x and y of lower left corner), height and width as proportion of the whole main plot.  


```{r, out.width=c('100%'), fig.show='hold', warning=F, message=F}

# Define main plot
main_plot <- ggplot(data = linelist)+
  geom_histogram(aes(x = date_onset, fill = hospital))+
  scale_fill_brewer(type = "qual", palette = 1, na.value = "grey")+ 
  theme_half_open()+
  theme(legend.position = "bottom")+
  labs(title = "Epidemic curve and outcomes by hospital")


# Define inset plot
inset_plot <- linelist %>% 
  mutate(hospital = recode(hospital, "St. Mark's Maternity Hospital (SMMH)" = "St. Marks")) %>% 
  count(hospital, outcome) %>% 
  ggplot()+
    geom_col(mapping = aes(x = hospital, y = n, fill = outcome))+
    scale_fill_brewer(type = "qual", palette = 4, na.value = "grey")+
    coord_flip()+
    theme_minimal()+
    theme(legend.position = "none",
          axis.title.y = element_blank())+
    labs(title = "Cases by outcome") 


# Combine main with inset
cowplot::ggdraw(main_plot)+
     draw_plot(inset_plot,
               x = .6, y = .55,    #x = .07, y = .65,
               width = .4, height = .4)

```


This technique is explained more in these two vignettes:  

[Wilke lab](https://wilkelab.org/cowplot/articles/drawing_with_on_plots.html)  
[draw_plot() documentation](https://www.rdocumentation.org/packages/cowplot/versions/1.1.1/topics/draw_plot)




<!-- ======================================================= -->
## Dual axes {}

A secondary y-axis is often a requested addition to a `ggplot2` graph. While there is a robust debate about the validity of such graphs in the data visualization community, and they are often not recommended, your manager may still want them. Below, we present one method to achieve them: using the **cowplot** package to combine two separate plots.  

This approach involves creating two separate plots - one with a y-axis on the left, and the other with y-axis on the right. Both will use a specific `theme_cowplot()` and must have the same x-axis. Then in a third command the two plots are aligned and overlaid on top of each other. The functionalities of **cowplot**, of which this is only one, are described in depth at this [site](https://wilkelab.org/cowplot/articles/aligning_plots.html).  

To demonstrate this technique we will overlay the epidemic curve with a line of the weekly percent of patients who died. We use this example because the alignment of dates on the x-axis is more complex than say, aligning a bar chart with another plot. Some things to note:  

* The epicurve and the line are aggregated into weeks prior to plotting *and* the `date_breaks` and `date_labels` are identical - we do this so that the x-axes of the two plots are the same when they are overlaid.  
* The y-axis is moved to the right-side for plot 2 with the `position = ` argument of `scale_y_continuous()`.  
* Both plots make use of `theme_cowplot()`  

Note there is another example of this technique in the [Epidemic curves] page - overlaying cumulative incidence on top of the epicurve.  

**Make plot 1**  
This is essentially the epicurve. We use `geom_area()` just to demonstrate its use (area under a line, by default)  

```{r, warning=F, message=F}
pacman::p_load(cowplot)            # load/install cowplot

p1 <- linelist %>%                 # save plot as object
     count(
       epiweek = lubridate::floor_date(date_onset, "week")) %>% 
     ggplot()+
          geom_area(aes(x = epiweek, y = n), fill = "grey")+
          scale_x_date(
               date_breaks = "month",
               date_labels = "%b")+
     theme_cowplot()+
     labs(
       y = "Weekly cases"
     )

p1                                      # view plot 
```

**Make plot 2**  
Create the second plot showing a line of the weekly percent of cases who died.  

```{r, warning=F, message=F}

p2 <- linelist %>%         # save plot as object
     group_by(
       epiweek = lubridate::floor_date(date_onset, "week")) %>% 
     summarise(
       n = n(),
       pct_death = 100*sum(outcome == "Death", na.rm=T) / n) %>% 
     ggplot(aes(x = epiweek, y = pct_death))+
          geom_line()+
          scale_x_date(
               date_breaks = "month",
               date_labels = "%b")+
          scale_y_continuous(
               position = "right")+
          theme_cowplot()+
          labs(
            x = "Epiweek of symptom onset",
            y = "Weekly percent of deaths",
            title = "Weekly case incidence and percent deaths"
          )

p2     # view plot
```

Now we align the plot using the function `align_plots()`, specifying horizontal and vertical alignment ("hv", could also be "h", "v", "none"). We specify alignment of all axes as well (top, bottom, left, and right) with "tblr". The output is of class list (2 elements).    

Then we draw the two plots together using `ggdraw()` (from **cowplot**) and referencing the two parts of the `aligned_plots` object.  

```{r, warning=F, message=F}
aligned_plots <- cowplot::align_plots(p1, p2, align="hv", axis="tblr")         # align the two plots and save them as list
aligned_plotted <- ggdraw(aligned_plots[[1]]) + draw_plot(aligned_plots[[2]])  # overlay them and save the visual plot
aligned_plotted                                                                # print the overlayed plots

```



<!-- ### Statistical transformation {.unnumbered}   -->
<!-- Another way to do this to have the second axis be a direct transformation of the secondary axis.  -->

<!-- Differences in axis values will be purely cosmetic - if you want to show two different variables on one graph, with different y-axis scales for each variable, this will not work without some work behind the scenes. To obtain this effect, you will have to transform one of your variables in the data, and apply the same transformation *in reverse* when specifying the axis labels. Based on this, you can either specify the transformation explicitly (e.g. variable a is around 10x as large as variable b) or calculate it in the code (e.g. what is the ratio between the maximum values of each dataset). -->


<!-- The syntax for adding a secondary axis is very straightforward! When calling a `scale_xxx_xxx()` function (e.g. `scale_y_continuous()`), use the `sec.axis` argument to call the `sec_axis()` function. The `trans` argument in this function allows you to specify the label transformation for the axis - provide this in standard tidyverse syntax.  -->

<!-- For example, if we want to show the number of positive RDTs in the malaria dataset for facility 1, showing 0-4 year olds and all cases on chart: -->


<!-- ```{r, , warning=F, message=F} -->

<!-- # take malaria data from facility 1 -->
<!-- malaria_facility_1 <- malaria_data %>% -->
<!--   filter(location_name == "Facility 1") -->

<!-- # calculate the ratio between malaria_rdt_0-4 and malaria_tot  -->

<!-- tf_ratio <- max(malaria_facility_1$malaria_tot, na.rm = T) / max(malaria_facility_1$`malaria_rdt_0-4`, na.rm = T) -->

<!-- # transform the values in the dataset -->

<!-- malaria_facility_1 <- malaria_facility_1 %>% -->
<!--   mutate(malaria_rdt_0_4_tf = `malaria_rdt_0-4` * tf_ratio) -->


<!-- # plot the graph with dual axes -->

<!-- ggplot(malaria_facility_1, aes(x = data_date)) + -->
<!--   geom_line(aes(y = malaria_tot, col = "Total cases")) + -->
<!--   geom_line(aes(y = malaria_rdt_0_4_tf, col = "Cases: 0-4 years old")) + -->
<!--   scale_y_continuous( -->
<!--     name = "Total cases", -->
<!--     sec.axis = sec_axis(trans = ~ . / tf_ratio, name = "Cases: 0-4 years old") -->
<!--   ) + -->
<!--   labs(x = "date of data collection") + -->
<!--   theme_minimal() + -->
<!--   theme(legend.title = element_blank()) -->



<!-- ``` -->






<!-- ## Sparklines   -->

<!-- UNDER CONSTRUCTION   -->
<!-- (perhaps move to Tables for presentation page) -->




## Packages to help you  


There are some really neat R packages specifically designed to help you navigate **ggplot2**:  


### Point-and-click **ggplot2** with **equisse**  {.unnumbered}

"This addin allows you to interactively explore your data by visualizing it with the ggplot2 package. It allows you to draw bar plots, curves, scatter plots, histograms, boxplot and sf objects, then export the graph or retrieve the code to reproduce the graph."

Install and then launch the addin via the RStudio menu or with `esquisse::esquisser()`.

See the [Github page](https://github.com/dreamRs/esquisse)

[Documentation](https://dreamrs.github.io/esquisse/index.html)









## Miscellaneous  


### Numeric display {.unnumbered}  

You can disable scientific notation by running this command prior to plotting.  

```{r, eval=F}
options(scipen=999)
```

Or apply `number_format()` from the **scales** package to a specific value or column, as shown below.  

Use functions from the package **scales** to easily adjust how numbers are displayed. These can be applied to columns in your data frame, but are shown on individual numbers for purpose of example.  

```{r}
scales::number(6.2e5)
scales::number(1506800.62,  accuracy = 0.1,)
scales::comma(1506800.62, accuracy = 0.01)
scales::comma(1506800.62, accuracy = 0.01,  big.mark = "." , decimal.mark = ",")
scales::percent(0.1)
scales::dollar(56)
scales::scientific(100000)
```

## Resources

Inspiration
[ggplot graph gallery](https://www.tidyverse.org/blog/2018/07/ggplot2-3-0-0/)

Presentation of data
European Centre for Disease Prevention and Control [Guidelines of presentation of surveillance data](https://ecdc.europa.eu/sites/portal/files/documents/Guidelines%20for%20presentation%20of%20surveillance%20data-final-with-cover-for-we....pdf) 


Facets and labellers
[Using labellers for facet strips](http://www.cookbook-r.com/Graphs/Facets_(ggplot2)/#modifying-facet-label-text)
[Labellers](https://ggplot2.tidyverse.org/reference/labellers.html)

Adjusting order with factors
[fct_reorder](https://forcats.tidyverse.org/reference/fct_reorder.html)  
[fct_inorder](https://forcats.tidyverse.org/reference/fct_inorder.html)  
[How to reorder a boxplot](https://cmdlinetips.com/2019/02/how-to-reorder-a-boxplot-in-r/)  
[Reorder a variable in ggplot2](https://www.r-graph-gallery.com/267-reorder-a-variable-in-ggplot2.html)  
[R for Data Science - Factors](https://r4ds.had.co.nz/factors.html)  

Legends  
[Adjust legend order](https://stackoverflow.com/questions/38425908/reverse-stacking-order-without-affecting-legend-order-in-ggplot2-bar-charts)  

Captions
[Caption alignment](https://stackoverflow.com/questions/64701500/left-align-ggplot-caption)  

Labels  
[ggrepel](https://ggrepel.slowkow.com/articles/examples.html)  

Cheatsheets  
[Beautiful plotting with ggplot2](http://zevross.com/blog/2014/08/04/beautiful-plotting-in-r-a-ggplot2-cheatsheet-3/)  




<!-- TO DO - Under construction -->


<!-- * Straight horizontal, vertical, or other line -->

<!-- You can also add straight lines to your plot with `geom_hline()` (horizontal), `geom_vline()` (vertical) or `geom_abline()` (with a specified y intercept and slope) -->


<!-- Using option `label_wrap_gen` in facet_wrap to have multiple strip lines -->
<!-- labels and colors of strips -->

<!-- Axis text vertical adjustment -->
<!-- rotation -->
<!-- Labellers -->

<!-- limit range with limit() and coord_cartesian(), ylim(), or scale_x_continuous() -->
<!-- theme_classic() -->

<!-- expand = c(0,0) -->
<!-- coord_flip() -->
<!-- tick marks -->

<!-- ggrepel -->
<!-- animations -->

<!-- remove -->
<!-- remove title -->
<!-- using fill = or color = in labs() -->
<!-- flip order / don't flip order -->
<!-- move location -->
<!-- color?    theme(legend.title = element_text(colour="chocolate", size=16, face="bold"))+ scale_color_discrete(name="This color is\ncalled chocolate!?") -->
<!-- Color of boxes behind points in legend  -->
<!--      theme(legend.key=element_rect(fill='pink'))   or use fill = NA to remove them. http://zevross.com/blog/2014/08/04/beautiful-plotting-in-r-a-ggplot2-cheatsheet-3/  -->
<!-- Change size of symbols in legend only guides(colour = guide_legend(override.aes = list(size=4))) -->


<!-- Turn off a layer in the legend -->
<!-- geom_text(data=nmmaps, aes(date, temp, label=round(temp)), size=4) -->
<!-- geom_text(data=nmmaps, aes(date, temp, label=round(temp), size=4), show_guide=FALSE) -->

<!-- Force a legend even if there is no aes().  -->
<!-- ggplot(nmmaps, aes(x=date, y=o3))+ -->
<!--      geom_line(aes(color="Important line"))+ -->
<!--      geom_point(aes(color="My points")) -->
<!-- Control the shape in the legend with guides - a list with linetype and shape -->
<!-- ggplot(nmmaps, aes(x=date, y=o3))+geom_line(aes(color="Important line"))+ -->
<!--    geom_point(aes(color="Point values"))+ -->
<!--   scale_colour_manual(name='', values=c('Important line'='grey', 'Point values'='red'), guide='legend') + -->
<!--   guides(colour = guide_legend(override.aes = list(linetype=c(1,0) -->
<!--                                                       , shape=c(NA, 16)))) -->
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/ggplot_tips.Rmd-->


# Đường cong dịch bệnh {#epicurves}  

```{r, out.width=c('75%'), echo=F, message=F}
knitr::include_graphics(here::here("images", "epicurve_top.png"))
```    


An epidemic curve (also known as an "epi curve") is a core epidemiological chart typically used to visualize the temporal pattern of illness onset among a cluster or epidemic of cases.  

Analysis of the epicurve can reveal temporal trends, outliers, the magnitude of the outbreak, the most likely time period of exposure, time intervals between case generations, and can even help identify the mode of transmission of an unidentified disease (e.g. point source, continuous common source, person-to-person propagation). One online lesson on interpretation of epi curves can be found at the website of the [US CDC](https://www.cdc.gov/training/quicklearns/epimode/index.html).    

In this page we demonstrate two approaches to producing epicurves in R:  

* The **incidence2** package, which can produce an epi curve with simple commands  
* The **ggplot2** package, which allows for advanced customizability via more complex commands  

Also addressed are specific use-cases such as:  

* Plotting aggregated count data  
* Faceting or producing small-multiples  
* Applying moving averages  
* Showing which data are "tentative" or subject to reporting delays  
* Overlaying cumulative case incidence using a second axis  

<!-- ======================================================= -->
## Preparation


### Packages {.unnumbered}  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r message=F, warning=F}
pacman::p_load(
  rio,          # file import/export
  here,         # relative filepaths 
  lubridate,    # working with dates/epiweeks
  aweek,        # alternative package for working with dates/epiweeks
  incidence2,   # epicurves of linelist data
  i2extras,     # supplement to incidence2
  stringr,      # search and manipulate character strings
  forcats,      # working with factors
  RColorBrewer, # Color palettes from colorbrewer2.org
  tidyverse     # data management + ggplot2 graphics
) 
```


### Import data {.unnumbered}

Two example datasets are used in this section:  

* Linelist of individual cases from a simulated epidemic  
* Aggregated counts by hospital from the same simulated epidemic  

The datasets are imported using the `import()` function from the **rio** package. See the page on [Import and export] for various ways to import data.  


```{r, echo=F, message=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the counts data into R
count_data <- linelist %>% 
  group_by(hospital, date_hospitalisation) %>% 
  summarize(n_cases = dplyr::n()) %>% 
  filter(date_hospitalisation > as.Date("2013-06-01")) %>% 
  ungroup()
```


**Case linelist**

We import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow step-by-step, see instruction in the [Download handbook and data] page. We assume the file is in the working directory so no sub-folders are specified in this file path.  

```{r, eval=F}
linelist <- import("linelist_cleaned.xlsx")
```

The first 50 rows are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



**Case counts aggregated by hospital**  

For the purposes of the handbook, the dataset of weekly aggregated counts by hospital is created from the `linelist` with the following code. 

```{r, eval=F}
# import the counts data into R
count_data <- linelist %>% 
  group_by(hospital, date_hospitalisation) %>% 
  summarize(n_cases = dplyr::n()) %>% 
  filter(date_hospitalisation > as.Date("2013-06-01")) %>% 
  ungroup()
```

The first 50 rows are displayed below:  

```{r message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(count_data, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```




### Set parameters {.unnumbered}

For production of a report, you may want to set editable parameters such as the date for which the data is current (the "data date"). You can then reference the object `data_date` in your code when applying filters or in dynamic captions.

```{r set_parameters}
## set the report date for the report
## note: can be set to Sys.Date() for the current date
data_date <- as.Date("2015-05-15")
```



### Verify dates {.unnumbered}

Verify that each relevant date column is class Date and has an appropriate range of values. You can do this simply using `hist()` for histograms, or `range()` with `na.rm=TRUE`, or with `ggplot()` as below.  

```{r, out.width = c('50%', '50%', '50%'), fig.show='hold', warning=F, message=F}
# check range of onset dates
ggplot(data = linelist)+
  geom_histogram(aes(x = date_onset))
```



<!-- ======================================================= -->
## Epicurves with **incidence2** package { }

Below we demonstrate how to make epicurves using the **incidence2** package. The authors of this package have tried to allow the user to create and modify epicurves without needing to know **ggplot2** syntax. Much of this page is adapted from the package vignettes, which can be found at the **incidence2** [github page](https://github.com/reconhub/incidence2).   


<!-- ======================================================= -->
### Simple example {.unnumbered}

**2 steps are required to plot an epidemic curve with the *incidence2* package:**  

1) **Create** an *incidence object* (using the function `incidence()`)  
    + Provide the data  
    + Specify the date column to `date_index = `  
    + Specify the `interval = ` into which the cases should be aggregated (daily, weekly, monthly..)  
    + Specify any grouping columns (e.g. gender, hospital, outcome)  
2) **Plot** the incidence object  
    + Specify labels, colors, titles, etc.  


Below, we load the **incidence2** package, create the incidence object from the `linelist` on column `date_onset` and aggregated cases by day. We then print a summary of the incidence object. 

```{r, warning=F, message=F}
# load incidence2 package
pacman::p_load(incidence2)

# create the incidence object, aggregating cases by day
epi_day <- incidence(       # create incidence object
  x = linelist,             # dataset
  date_index = date_onset,  # date column
  interval = "day"          # date grouping interval
  )
```

The **incidence2** object itself looks like a tibble (like a data frame) and can be printed or further manipulated like a data frame.  

```{r}
class(epi_day)
```

Here is what it looks like when printed. It has a `date_index` column and a `count` column.  

```{r}
epi_day
```

You can also print a summary of the object:  

```{r}
# print summary of the incidence object
summary(epi_day)
```

To *plot* the *incidence* object, use `plot()` on the *name of the incidence object*. In the background, the function `plot.incidence2()` is called, so to read the **incidence2**-specific documentation you would run `?plot.incidence2`.  

```{r}
# plot the incidence object
plot(epi_day)
```

If you notice lots of tiny white vertical lines, try to adjust the size of your image. For example, if you export your plot with `ggsave()`, you can provide numbers to `width = ` and `height = `. If you widen the plot those lines may disappear.    



### Change time interval of case aggregation {.unnumbered}  
The `interval = ` argument of `incidence()` defines how the observations are grouped into vertical bars. 

**Specify interval**  

**incidence2** provides flexibility and understandable syntax for specifying how you want to aggregate your cases into epicurve bars. Provide a value like the ones below to the `interval = ` argument. You can write any of the below as plural (e.g. "week**s**"), and you can add numbers before (e.g. "3 months").  

Argument option | Further explanation 
------------------- | ------------------------------------ |
Number (1, 7, 13, 14, etc.) | Number of days per interval  
"week" | note: Monday start day is default
"2 weeks" | or 3, 4, 5...
"Sunday week" | weeks beginning on Sundays (could also use Thursday, etc.)
"2 Sunday weeks" | or 3, 4, 5...
"MMWRweek" | week starts on Sundays - see US CDC
"month" | 1st of month
"quarter" | 1st of month of quarter
"2 months" | or 3, 4, 5...
"year" | 1st day of calendar year


Below are examples of how different intervals look when applied to the linelist. Note how the default format and frequency of the date *labels* on the x-axis change as the date interval changes.  

```{r incidence, out.width=c('50%', '50%', '50%', '50%'), fig.show='hold', warning=F, message=F}
# Create the incidence objects (with different intervals)
##############################
# Weekly (Monday week by default)
epi_wk      <- incidence(linelist, date_onset, interval = "Monday week")

# Sunday week
epi_Sun_wk  <- incidence(linelist, date_onset, interval = "Sunday week")

# Three weeks (Monday weeks by default)
epi_2wk     <- incidence(linelist, date_onset, interval = "2 weeks")

# Monthly
epi_month   <- incidence(linelist, date_onset, interval = "month")

# Quarterly
epi_quarter   <- incidence(linelist, date_onset, interval = "quarter")

# Years
epi_year   <- incidence(linelist, date_onset, interval = "year")


# Plot the incidence objects (+ titles for clarity)
############################
plot(epi_wk)+      labs(title = "Monday weeks")
plot(epi_Sun_wk)+  labs(title = "Sunday weeks")
plot(epi_2wk)+     labs(title = "2 (Monday) weeks")
plot(epi_month)+   labs(title = "Months")
plot(epi_quarter)+ labs(title = "Quarters")
plot(epi_year)+    labs(title = "Years")

```


<!-- **Begin at first case**   -->

<!-- If you want the intervals to begin at the first case, you can add the argument `standard = TRUE` to the `incidence()` command. This only works if the interval is either "week", "month", "quarter" or "year".   -->

**First date**

You can optionally specify a value of class Date (e.g. `as.Date("2016-05-01")`) to `firstdate = ` in the `incidence()` command. If given, the data will be trimmed to this range and the intervals will begin on this date. 



### Groups {.unnumbered}

Groups are specified in the `incidence()` command, and can be used to color the bars or to facet the data. To specify groups in your data provide the column name(s) to the `groups =` argument in the `incidence()` command (no quotes around the column name). If specifying multiple columns, put their names within `c()`.

You can specify that cases with missing values in the grouping columns be listed as a distinct `NA` group by setting `na_as_group = TRUE`. Otherwise, they will be excluded from the plot.   

* To *color the bars by a grouping column*, you must again provide the column name to `fill = ` in the `plot()` command.  

* To *facet based on a grouping column*, see the section below on facets with **incidence2**.  

In the example below, the cases in the whole outbreak are grouped by their age category. Missing values are included as a group. The epicurve interval is weeks.  


```{r, message=F, warning=F}
# Create incidence object, with data grouped by age category
age_outbreak <- incidence(
  linelist,                # dataset
  date_index = date_onset, # date column
  interval = "week",       # Monday weekly aggregation of cases
  groups = age_cat,        # age_cat is set as a group
  na_as_group = TRUE)      # missing values assigned their own group

# plot the grouped incidence object
plot(
  age_outbreak,             # incidence object with age_cat as group
  fill = age_cat)+          # age_cat is used for bar fill color (must have been set as a groups column above)
labs(fill = "Age Category") # change legend title from default "age_cat" (this is a ggplot2 modification)
```
<span style="color: darkgreen;">**_TIP:_** Change the title of the legend by adding `+` the **ggplot2** command `labs(fill = "your title")` to your **incidence2** plot.</span>  

You can also have the grouped bars display side-by-side by setting `stack = FALSE` in `plot()`, as shown below:  

```{r, warning=F, message=F}
# Make incidence object of monthly counts. 
monthly_gender <- incidence(
 linelist,
 date_index = date_onset,
 interval = "month",
 groups = gender            # set gender as grouping column
)

plot(
  monthly_gender,   # incidence object
  fill = gender,    # display bars colored by gender
  stack = FALSE)    # side-by-side (not stacked)
``` 

You can set the `na_as_group = ` argument to FALSE in the `incidence()` command to remove rows with missing values from the plot.  




### Filtered data {.unnumbered}

To plot the epicurve of a subset of data:  

1) Filter the linelist data  
2) Provide the filtered data to the `incidence()` command  
3) Plot the incidence object

The example below uses data filtered to show only cases at Central Hospital.  

```{r, warning=F, message=F}
# filter the linelist
central_data <- linelist %>% 
  filter(hospital == "Central Hospital")

# create incidence object using filtered data
central_outbreak <- incidence(central_data, date_index = date_onset, interval = "week")

# plot the incidence object
plot(central_outbreak, title = "Weekly case incidence at Central Hospital")
```




### Aggregated counts {.unnumbered}

If your original data are aggregated (counts), provide the name of the column that contains the case counts to the `count = ` argument when creating the incidence object with `incidence()`.  

For example, this data frame `count_data` is the linelist aggregated into daily counts by hospital. The first 50 rows look like this:  

```{r message=FALSE, echo=F}
DT::datatable(head(count_data,50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

If you are beginning your analysis with daily count data like the dataset above, your `incidence()` command to convert this to a weekly epicurve by hospital would look like this:  

```{r}
epi_counts <- incidence(              # create weekly incidence object
  count_data,                         # dataset with counts aggregated by day
  date_index = date_hospitalisation,  # column with dates
  count = n_cases,                    # column with counts
  interval = "week",                  # aggregate daily counts up to weeks
  groups = hospital                   # group by hospital
  )

# plot the weekly incidence epi curve, with stacked bars by hospital
plot(epi_counts,                      # incidence object
     fill = hospital)                 # color the bars by hospital
```




### Facets/small multiples {.unnumbered}  

To facet the data by group (i.e. produce "small multiples"):  

1) Specify the faceting column to `groups = ` when you create the incidence object  
2) Use the `facet_plot()` command instead of `plot()`  
3) Specify which grouping columns to use as `fill = ` and which to use as `facets = `  

Below, we set both columns `hospital` and `outcome` as grouping columns in the `incidence()` command. Then, in `facet_plot()` we plot the epicurve, specifying that we want a different epicurve for each hospital and that within each epicurve the bars should be stacked and colored by outcome.  
 

```{r, warning=F, message=F}
epi_wks_hosp_out <- incidence(
  linelist,                      # dataset
  date_index = date_onset,       # date column
  interval = "month",            # monthly bars  
  groups = c(outcome, hospital)  # both outcome and hospital are given as grouping columns
  )

# plot
incidence2::facet_plot(
  epi_wks_hosp_out,      # incidence object
  facets = hospital,     # facet column
  fill = outcome)        # fill column

```

Note that the package **ggtree** (used for displaying phylogenetic trees) also has a function `facet_plot()` - this is why we specified `incidence2::facet_plot()` above.  



### Modifications with `plot()` {.unnumbered} 

An epicurve produced by **incidence2** can be modified via these arguments *within the `plot()` function*.  

**Here are `plot()` arguments that modify the appearance of the bars:**  

Argument | Description | Examples
------------------|---------------------------------------|-------------------
`fill = `|Bar color. Either a color name or a column name previously specified to `groups = ` in the `incidence()` command|`fill = "red"`, or `fill = gender`  
`color = ` |Color around each bar, or around each grouping within a bar|`border = "white"` 
`legend = `|Location of legend|One of "bottom", "top", "left", "right", or "none"  
`alpha = `|Transparency of bars/boxes|1 is fully opaque, 0 is fully transparent
`width = `|Value between 0 and 1 indicating the relative size of the bars to their time interval|`width = .7`  
`show_cases = `|Logical; if TRUE, each case shows as a box. Displays best on smaller outbreaks.|`show_cases = TRUE`  

**Here are `plot()` arguments that modify the date axis:**  

Argument(s)|Description
----------------------|----------------------------------------------------
`centre_dates = `|TRUE/FALSE as to whether date displays appear under center of bars, or at beginning of bars  
`date_format = `|Adjust the date display format using strptime ("%") syntax. Only works if `centre_dates = FALSE` (details below).  
`n.breaks = `|Approximate number of x-axis label breaks desired.  
`angle = `|Angle of x-axis date labels (number of degrees)  
`size = `|Size of text in points  

Note that the `date_breaks = ` argument only works if `centre_dates = FALSE`. Provide a character value in quotation marks using the strptime syntax below, as detailed in the [Working with dates] page. You can use `\n` for a "newline".  

%d = Day number of month (5, 17, 28, etc.)  
%j = Day number of the year (Julian day 001-366)  
%a = Abbreviated weekday (Mon, Tue, Wed, etc.)  
%A = Full weekday (Monday, Tuesday, etc.)  
%w = Weekday number (0-6, Sunday is 0)  
%u = Weekday number (1-7, Monday is 1)  
%W = Week number (00-53, Monday is week start)  
%U = Week number (01-53, Sunday is week start)  
%m = Month number (e.g. 01, 02, 03, 04)  
%b = Abbreviated month (Jan, Feb, etc.)  
%B = Full month (January, February, etc.)  
%y = 2-digit year  (e.g. 89)  
%Y = 4-digit year  (e.g. 1989)  
%h = hours (24-hr clock)  
%m = minutes  
%s = seconds  
%z = offset from GMT  
%Z = Time zone (character)


<!-- <span style="color: darkgreen;">**_TIP:_** For breaks every "nth" interval (e.g. every 4th), use `n.breaks = nrow(i)/n` (where “i” is your incidence object name and “n” is a number). If your data are grouped, you will need to multiply "n" by the number of unique groups.</span>   -->



**Here are `plot()` arguments that modify plot labels:**

Argument(s)|Description
----------------------|----------------------------------------------------
`title = `|Title of plot|`title = "Epidemic curve of Acute Jaundice Syndrome (AJS)"`
`xlab = `|Title of x-axis|`xlab = "Date of onset"`  
`ylab = `|Title of y-axis|`ylab = "Daily case"`  
`size = `|Size of x-axis text in pts (use ggplot's theme() to adjust other sizes)  


An example using many of the above arguments:  

```{r, warning=F, message=F}
# filter the linelist
central_data <- linelist %>% 
  filter(hospital == "Central Hospital")

# create incidence object using filtered data
central_outbreak <- incidence(
  central_data,
  date_index = date_onset,
  interval = "week",
  groups = outcome)

# plot incidence object
plot(
  central_outbreak,
  fill = outcome,                       # box/bar color
  legend = "top",                       # legend on top
  title = "Cases at Central Hospital",  # title
  xlab = "Week of onset",               # x-axis label
  ylab = "Week of onset",               # y-axis label
  show_cases = TRUE,                    # show each case as an individual box
  alpha = 0.7,                          # transparency 
  border = "grey",                      # box border
  angle = 30,                           # angle of date labels
  centre_dates = FALSE,                 # date labels at edge of bar
  date_format = "%a %d %b %Y\n(Week %W)" # adjust how dates are displayed
  )
```

To further adjust plot appearance, see the section below on modifications with `ggplot()`.  






### Modifications with ggplot2 {.unnumbered}

You can further modify an **incidence2** plot by adding **ggplot2** modifications with a `+` after the close of the incidence `plot()` function, as demonstrated below.  

Below, the **incidence2** plot finishes and then **ggplot2** commands are used to modify the axes, add a caption, and adjust the bold font and text size.  

Note that if you add `scale_x_date()`, most date formatting from `plot()` will be overwritten. See the `ggplot()` epicurves section and the Handbook page [ggplot tips] for more options.  

```{r, warning=F, message=F}
# filter the linelist
central_data <- linelist %>% 
  filter(hospital == "Central Hospital")

# create incidence object using filtered data
central_outbreak <- incidence(
  central_data,
  date_index = date_onset,
  interval = "week",
  groups = c(outcome))

# plot incidence object
plot(
  central_outbreak,
  fill = outcome,                       # box/bar color
  legend = "top",                       # legend on top
  title = "Cases at Central Hospital",  # title
  xlab = "Week of onset",               # x-axis label
  ylab = "Week of onset",               # y-axis label
  show_cases = TRUE,                    # show each case as an individual box
  alpha = 0.7,                          # transparency 
  border = "grey",                      # box border
  centre_dates = FALSE,                   
  date_format = "%a %d %b\n%Y (Week %W)", 
  angle = 30                           # angle of date labels
  )+
  
  scale_y_continuous(
    breaks = seq(from = 0, to = 30, by = 5),  # specify y-axis increments by 5
    expand = c(0,0))+                         # remove excess space below 0 on y-axis
  
  # add dynamic caption
  labs(
    fill = "Patient outcome",                               # Legend title
    caption = stringr::str_glue(                            # dynamic caption - see page on characters and strings for details
      "n = {central_cases} from Central Hospital
      Case onsets range from {earliest_date} to {latest_date}. {missing_onset} cases are missing date of onset and not shown",
      central_cases = nrow(central_data),
      earliest_date = format(min(central_data$date_onset, na.rm=T), format = '%a %d %b %Y'),
      latest_date = format(max(central_data$date_onset, na.rm=T), format = '%a %d %b %Y'),      
      missing_onset = nrow(central_data %>% filter(is.na(date_onset)))))+
  
  # adjust bold face, and caption position
  theme(
    axis.title = element_text(size = 12, face = "bold"),    # axis titles larger and bold
    axis.text = element_text(size = 10, face = "bold"),     # axis text size and bold
    plot.caption = element_text(hjust = 0, face = "italic") # move caption to left
  )
  
```




### Change colors  {.unnumbered}  

#### Specify a palette {.unnumbered}  

Provide the name of a pre-defined palette to the `col_pal = ` argument in `plot()`. The **incidence2** package comes with 2 pre-defined paletted: "vibrant" and "muted". In "vibrant" the first 6 colors and distinct and in "muted" the first 9 colors are distinct. After these numbers, the colors are interpolations/intermediaries of other colors. These pre-defined palettes can be found at [this website](https://personal.sron.nl/~pault/#sec:qualitative). The palettes exclude grey, which is reserved for missing data (use `na_color = ` to change this default).  

```{r out.width = c('50%', '50%'), fig.show='hold', warning = F, message = F}
# Create incidence object, with data grouped by age category  
age_outbreak <- incidence(
  linelist,
  date_index = date_onset,   # date of onset for x-axis
  interval = "week",         # weekly aggregation of cases
  groups = age_cat)

# plot the epicurve with default palette
plot(age_outbreak, fill = age_cat, title = "'vibrant' default incidence2 palette")

# plot with different color palette
#plot(age_outbreak, fill = age_cat, col_pal = muted, title = "'muted' incidence2 palette")
```

You can also use one of the **base** R palettes (put the name of the palette *without* quotes).  

```{r out.width = c('50%', '50%'), fig.show='hold', warning = F, message = F}
# plot with base R palette
plot(age_outbreak, fill = age_cat, col_pal = heat.colors, title = "base R heat.colors palette")

# plot with base R palette
plot(age_outbreak, fill = age_cat, col_pal = rainbow, title = "base R rainbow palette")
```

You can also add a color palette from the **viridis** package or **RColorBrewer** package. First those packages must be loaded, then add their respective `scale_fill_*()` functions with a `+`, as shown below.

```{r out.width = c('50%', '50%'), fig.show='hold', warning = F, message = F}
pacman::p_load(RColorBrewer, viridis)

# plot with color palette
plot(age_outbreak, fill = age_cat, title = "Viridis palette")+
  scale_fill_viridis_d(
    option = "inferno",     # color scheme, try also "plasma" or the default
    name = "Age Category",  # legend name
    na.value = "grey")      # for missing values

# plot with color palette
plot(age_outbreak, fill = age_cat, title = "RColorBrewer palette")+
  scale_fill_brewer(
    palette = "Dark2",      # color palette, try also Accent, Dark2, Paired, Pastel1, Pastel2, Set1, Set2, Set3
    name = "Age Category",  # legend name
    na.value = "grey")      # for missing values
```


#### Specify manually {.unnumbered}  

To specify colors manually, add the **ggplot2** function `scale_fill_manual()` to the `plot()` with a `+` and provide the vector of colors names or HEX codes to the argument `values = `. The number of colors listed must equal the number of groups. Be aware of whether missing values are a group - they can be converted to a character value like "Missing" during your data preparation with the function `fct_explicit_na()` as explained in the page on [Factors].  

```{r out.width = c('50%', '50%'), fig.show='hold', warning=F, message=F}
# manual colors
plot(age_outbreak, fill = age_cat, title = "Manually-specified colors")+
  scale_fill_manual(
    values = c("darkgreen", "darkblue", "purple", "grey", "yellow", "orange", "red", "lightblue"),  # colors
    name = "Age Category")      # Name for legend
```

As mentioned in the [ggplot tips] page, you can create your own palettes using `colorRampPalette()` on a vector of colors and specifying the number of colors you want in return. This is a good way to get many colors in a ramp by specifying a few.  

```{r}
my_cols <- c("darkgreen", "darkblue", "purple", "grey", "yellow", "orange")
my_palette <- colorRampPalette(my_cols)(12)  # expand the 6 colors above to 12 colors
my_palette
```
          
          
### Adjust level order {.unnumbered}  

To adjust the order of group appearance (on plot and in legend), the grouping column must be class Factor. See the page on [Factors] for more information.  

First, let's see a weekly epicurve by hospital with the default ordering:  

```{r, message=F, warning=F}
# ORIGINAL - hospital NOT as factor
###################################

# create weekly incidence object, rows grouped by hospital and week
hospital_outbreak <- incidence(
  linelist,
  date_index = date_onset, 
  interval = "week", 
  groups = hospital)

# plot incidence object
plot(hospital_outbreak, fill = hospital, title = "ORIGINAL - hospital not a factor")
```

Now, to adjust the order so that "Missing" and "Other" are at the top of the epicurve we can do the following:  

* Load the package **forcats**, to work with factors  
* Adjust the dataset - in this case we'll define a new dataset (`plot_data`) in which:  
  * the `gender` column is defined as a factor the order of levels are set with `fct_relevel()` so that "Other" and "Missing" are first, so they appear at the top of the bars  
* The incidence object is created and plotted as before  
* We add **ggplot2** modifications  
  * `scale_fill_manual()` to manually assign colors so that "Missing" is grey and "Other" is beige  
 



```{r, message=F, warning=F}
# MODIFIED - hospital as factor
###############################

# load forcats package for working with factors
pacman::p_load(forcats)

# Convert hospital column to factor and adjust levels
plot_data <- linelist %>% 
  mutate(hospital = fct_relevel(hospital, c("Missing", "Other"))) # Set "Missing" and "Other" as top levels


# Create weekly incidence object, grouped by hospital and week
hospital_outbreak_mod <- incidence(
  plot_data,
  date_index = date_onset, 
  interval = "week", 
  groups = hospital)

# plot incidence object
plot(hospital_outbreak_mod, fill = hospital)+
  
  # manual specify colors
  scale_fill_manual(values = c("grey", "beige", "darkgreen", "green2", "orange", "red", "pink"))+                      

  # labels added via ggplot
  labs(
      title = "MODIFIED - hospital as factor",   # plot title
      subtitle = "Other & Missing at top of epicurve",
      y = "Weekly case incidence",               # y axis title  
      x = "Week of symptom onset",               # x axis title
      fill = "Hospital")                         # title of legend     
```

<span style="color: darkgreen;">**_TIP:_** If you want to reverse the order of the legend only, add this **ggplot2** command `guides(fill = guide_legend(reverse = TRUE))`.</span>  



### Vertical gridlines {.unnumbered}  

If you plot with default **incidence2** settings, you may notice that the vertical gridlines appear at each date label and once between each date label. This can result in gridlines intersecting with the top of some bars.  

<!-- [TO DO Note this paragraph is not applicable with version 1.0.0 of incidence2). You can specify the interval for the gridlines by adding **ggplot2**'s `scale_x_date()` command to your **incidence2** plot. Within it, specify the intervals for `date_breaks = ` and `date_minor_breaks = ` (e.g. "weeks" or "3 weeks" or "months"). Note that use of `scale_x_date()` will over-ride any formatting of the date labels in `plot()`, so you will need to specify any string format to `date_labels = ` as below.   -->

You can remove all gridlines by adding the **ggplot2** command `theme_classic()`.  

```{r, warning=F, message=F, out.width = c('50%', '50%', '50%'), fig.show='hold'}
# make incidence object
a <- incidence(
  central_data,
  date_index = date_onset,
  interval = "Monday weeks"
)

# Default gridlines
plot(a, title = "Default lines")

# Specified gridline intervals
# NOT WORKING WITH INCIDENCE2 1.0.0
# plot(a, title = "Weekly lines")+
#   scale_x_date(
#     date_breaks = "4 weeks",      # major vertical lines align on weeks
#     date_minor_breaks = "weeks",  # minor vertical lines every week
#     date_labels = "%a\n%d\n%b")   # format of date labels

# No gridlines
plot(a, title = "No lines")+
  theme_classic()                 # remove all gridlines
```

Note however, that if using weeks, the `date_breaks` and `date_minor_breaks` arguments only work for *Monday* weeks. If your weeks are by another day of the week you will need to manually provide a vector of dates to the `breaks = ` and `minor_breaks = ` arguments instead. See the **ggplot2** section for examples of this using `seq.Date()`.

### Cumulative incidence {.unnumbered}  

You can easily produce a plot of cumulative incidence by passing the incidence object to the **incidence2** command `cumulate()` and then to `plot()`. This also works with `facet_plot()`.  

```{r}
# make weekly incidence object
wkly_inci <- incidence(
  linelist,
  date_index = date_onset,
  interval = "week"
)

# plot cumulative incidence
wkly_inci %>% 
  cumulate() %>% 
  plot()
```


See the section farther down on this page for alternative method to plot cumulative incidence with **ggplot2** - for example to overlay a cumulative incidence line over an epicurve.  

### Rolling average  {.unnumbered}

You can add a rolling average to an **incidence2** plot easily with `add_rolling_average()` from the **i2extras** package. Pass your incidence2 object to this function, and then to `plot()`. Set `before = ` as the number of previous days you want included in the rolling average (default is 2). If your data are grouped, the rolling average will be calculated per group. 

```{r, warning=F, message=F}
rolling_avg <- incidence(                    # make incidence object
  linelist,
  date_index = date_onset,
  interval = "week",
  groups = gender) %>% 
  
  i2extras::add_rolling_average(before = 6)  # add rolling averages (in this case, by gender)

# plot
plot(rolling_avg, n.breaks = 3) # faceted automatically because rolling average on groups
```

To learn how to apply rolling averages more generally on data, see the Handbook page on [Moving averages].  


<!-- ======================================================= -->
## Epicurves with ggplot2 { }

Using `ggplot()` to build your epicurve allows for more flexibility and customization, but requires more effort and understanding of how `ggplot()` works.  

Unlike using the **incidence2** package, you must *manually* control the aggregation of the cases by time (into weeks, months, etc) *and* the intervals of the labels on the date axis. This must be carefully managed.  

These examples use a subset of the `linelist` dataset - only the cases from Central Hospital.  


```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r}
central_data <- linelist %>% 
  filter(hospital == "Central Hospital")
```

```{r, eval=F, echo=F}
detach("package:tidyverse", unload=TRUE)
library(tidyverse)
```


To produce an epicurve with `ggplot()` there are three main elements:  

* A histogram, with linelist cases aggregated into "bins" distinguished by specific "break" points  
* Scales for the axes and their labels  
* Themes for the plot appearance, including titles, labels, captions, etc.


### Specify case bins {.unnumbered}  

Here we show how to specify how cases will be aggregated into histogram bins ("bars"). It is important to recognize that the aggregation of cases into histogram bins is **not** necessarily the same intervals as the dates that will appear on the x-axis. 

Below is perhaps the most simple code to produce daily and weekly epicurves.  

In the over-arching `ggplot()` command the dataset is provided to `data = `. Onto this foundation, the geometry of a histogram is added with a `+`. Within the `geom_histogram()`, we map the aesthetics such that the column `date_onset` is mapped to the x-axis. Also within the `geom_histogram()` but *not* within `aes()` we set the `binwidth =` of the histogram bins, in days. If this **ggplot2** syntax is confusing, review the page on [ggplot basics].  

<span style="color: orange;">**_CAUTION:_** Plotting weekly cases by using `binwidth = 7` starts the first 7-day bin at the first case, which could be any day of the week! To create specific weeks, see section below .</span>


``` {r ggplot_simple,  out.width = c('50%', '50%'), fig.show='hold', warning= F, message = F}
# daily 
ggplot(data = central_data) +          # set data
  geom_histogram(                      # add histogram
    mapping = aes(x = date_onset),     # map date column to x-axis
    binwidth = 1)+                     # cases binned by 1 day 
  labs(title = "Central Hospital - Daily")                # title

# weekly
ggplot(data = central_data) +          # set data 
  geom_histogram(                      # add histogram
      mapping = aes(x = date_onset),   # map date column to x-axis
      binwidth = 7)+                   # cases binned every 7 days, starting from first case (!) 
  labs(title = "Central Hospital - 7-day bins, starting at first case") # title
```

Let us note that the first case in this Central Hospital dataset had symptom onset on:  

```{r}
format(min(central_data$date_onset, na.rm=T), "%A %d %b, %Y")
```

**To manually specify the histogram bin breaks, do *not* use the `binwidth = ` argument, and instead supply a vector of dates to `breaks = `.**  

Create the vector of dates with the **base** R function `seq.Date()`. This function expects arguments `to = `, `from = `, and `by = `. For example, the command below returns monthly dates starting at Jan 15 and ending by June 28.

```{r}
monthly_breaks <- seq.Date(from = as.Date("2014-02-01"),
                           to = as.Date("2015-07-15"),
                           by = "months")

monthly_breaks   # print
```

This vector can be provided to `geom_histogram()` as `breaks = `:  

```{r, warning=F, message=F}
# monthly 
ggplot(data = central_data) +  
  geom_histogram(
    mapping = aes(x = date_onset),
    breaks = monthly_breaks)+         # provide the pre-defined vector of breaks                    
  labs(title = "Monthly case bins")   # title
```

A simple weekly date sequence can be returned by setting `by = "week"`. For example: 

```{r}
weekly_breaks <- seq.Date(from = as.Date("2014-02-01"),
                          to = as.Date("2015-07-15"),
                          by = "week")
```

 
An alternative to supplying specific start and end dates is to write *dynamic* code so that weekly bins begin *the Monday before the first case*. **We will use these date vectors throughout the examples below.**  
     
```{r}
# Sequence of weekly Monday dates for CENTRAL HOSPITAL
weekly_breaks_central <- seq.Date(
  from = floor_date(min(central_data$date_onset, na.rm=T),   "week", week_start = 1), # monday before first case
  to   = ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 1), # monday after last case
  by   = "week")
```  

Let's unpack the rather daunting code above:  

* The "from" value (earliest date of the sequence) is created as follows: the minimum date value (`min()` with `na.rm=TRUE`) in the column `date_onset` is fed to `floor_date()` from the **lubridate** package. `floor_date()` set to "week" returns the start date of that cases's "week", given that the start day of each week is a Monday (`week_start = 1`).  
* Likewise, the "to" value (end date of the sequence) is created using the inverse function `ceiling_date()` to return the Monday *after* the last case.  
* The "by" argument of `seq.Date()` can be set to any number of days, weeks, or months.   
* Use `week_start = 7` for Sunday weeks  

As we will use these date vectors throughout this page, we also define one for the whole outbreak (the above is for Central Hospital only).  

```{r}
# Sequence for the entire outbreak
weekly_breaks_all <- seq.Date(
  from = floor_date(min(linelist$date_onset, na.rm=T),   "week", week_start = 1), # monday before first case
  to   = ceiling_date(max(linelist$date_onset, na.rm=T), "week", week_start = 1), # monday after last case
  by   = "week")
```

These `seq.Date()` outputs can be used to create histogram bin breaks, but also the breaks for the date labels, which may be independent from the bins. Read more about the date labels in later sections.  

<span style="color: darkgreen;">**_TIP:_** For a more simple `ggplot()` command, save the bin breaks and date label breaks as named vectors in advance, and simply provide their names to `breaks = `.</span>  







### Weekly epicurve example {.unnumbered}  

**Below is detailed example code to produce weekly epicurves for Monday weeks, with aligned bars, date labels, and vertical gridlines.** This section is for the user who needs code quickly. To understand each aspect (themes, date labels, etc.) in-depth, continue to the subsequent sections. Of note:  

* The *histogram bin breaks* are defined with `seq.Date()` as explained above to begin the Monday before the earliest case and to end the Monday after the last case  
* The interval of *date labels* is specified by `date_breaks =` within `scale_x_date()`  
* The interval of minor vertical gridlines between date labels is specified to `date_minor_breaks = `  
* `expand = c(0,0)` in the x and y scales removes excess space on each side of the axes, which also ensures the date labels begin from the first bar.  

```{r, warning=F, message=F}
# TOTAL MONDAY WEEK ALIGNMENT
#############################
# Define sequence of weekly breaks
weekly_breaks_central <- seq.Date(
      from = floor_date(min(central_data$date_onset, na.rm=T),   "week", week_start = 1), # Monday before first case
      to   = ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 1), # Monday after last case
      by   = "week")    # bins are 7-days 


ggplot(data = central_data) + 
  
  # make histogram: specify bin break points: starts the Monday before first case, end Monday after last case
  geom_histogram(
    
    # mapping aesthetics
    mapping = aes(x = date_onset),  # date column mapped to x-axis
    
    # histogram bin breaks
    breaks = weekly_breaks_central, # histogram bin breaks defined previously
    
    # bars
    color = "darkblue",     # color of lines around bars
    fill = "lightblue"      # color of fill within bars
  )+ 
    
  # x-axis labels
  scale_x_date(
    expand            = c(0,0),           # remove excess x-axis space before and after case bars
    date_breaks       = "4 weeks",        # date labels and major vertical gridlines appear every 3 Monday weeks
    date_minor_breaks = "week",           # minor vertical lines appear every Monday week
    date_labels       = "%a\n%d %b\n%Y")+ # date labels format
  
  # y-axis
  scale_y_continuous(
    expand = c(0,0))+             # remove excess y-axis space below 0 (align histogram flush with x-axis)
  
  # aesthetic themes
  theme_minimal()+                # simplify plot background
  
  theme(
    plot.caption = element_text(hjust = 0,        # caption on left side
                                face = "italic"), # caption in italics
    axis.title = element_text(face = "bold"))+    # axis titles in bold
  
  # labels including dynamic caption
  labs(
    title    = "Weekly incidence of cases (Monday weeks)",
    subtitle = "Note alignment of bars, vertical gridlines, and axis labels on Monday weeks",
    x        = "Week of symptom onset",
    y        = "Weekly incident cases reported",
    caption  = stringr::str_glue("n = {nrow(central_data)} from Central Hospital; Case onsets range from {format(min(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')} to {format(max(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')}\n{nrow(central_data %>% filter(is.na(date_onset)))} cases missing date of onset and not shown"))
```


#### Sunday weeks {.unnumbered}  

To achieve the above plot for Sunday weeks a few modifications are needed, because the `date_breaks = "weeks"` only work for Monday weeks.  

* The break points of the *histogram bins* must be set to Sundays (`week_start = 7`)  
* Within `scale_x_date()`, the similar date breaks should be provided to `breaks =` and `minor_breaks = ` to ensure the date labels and vertical gridlines align on Sundays.  

For example, the `scale_x_date()` command for Sunday weeks could look like this:  

```{r, eval=F}
scale_x_date(
    expand = c(0,0),
    
    # specify interval of date labels and major vertical gridlines
    breaks = seq.Date(
      from = floor_date(min(central_data$date_onset, na.rm=T),   "week", week_start = 7), # Sunday before first case
      to   = ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 7), # Sunday after last case
      by   = "4 weeks"),
    
    # specify interval of minor vertical gridline 
    minor_breaks = seq.Date(
      from = floor_date(min(central_data$date_onset, na.rm=T),   "week", week_start = 7), # Sunday before first case
      to   = ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 7), # Sunday after last case
      by   = "week"),
   
    # date label format
    date_labels = "%a\n%d %b\n%Y")+         # day, above month abbrev., above 2-digit year

```



### Group/color by value {.unnumbered}

The histogram bars can be colored by group and "stacked". To designate the grouping column, make the following changes. See the [ggplot basics] page for details.  

* Within the histogram aesthetic mapping `aes()`, map the column name to the `group = ` and `fill = ` arguments  
* Remove any `fill = ` argument *outside* of `aes()`, as it will override the one inside  
* Arguments *inside* `aes()` will apply *by group*, whereas any *outside* will apply to all bars (e.g. you may still want `color = ` outside, so each bar has the same border)  

Here is what the `aes()` command would look like to group and color the bars by gender:  

```{r, eval=F}
aes(x = date_onset, group = gender, fill = gender)
```

Here it is applied:  

```{r, warning=F, message=F}
ggplot(data = linelist) +     # begin with linelist (many hospitals)
  
  # make histogram: specify bin break points: starts the Monday before first case, end Monday after last case
  geom_histogram(
    mapping = aes(
      x = date_onset,
      group = hospital,       # set data to be grouped by hospital
      fill = hospital),       # bar fill (inside color) by hospital
    
    # bin breaks are Monday weeks
    breaks = weekly_breaks_all,   # sequence of weekly Monday bin breaks for whole outbreak, defined in previous code       
    
    # Color around bars
    color = "black")
```


### Adjust colors {.unnumbered}  

* To *manually* set the fill for each group, use `scale_fill_manual()` (note: `scale_color_manual()` is different!).
  * Use the `values = ` argument to apply a vector of colors.  
  * Use `na.value = ` to specify a color for `NA` values.  
  * Use the `labels = ` argument to change the text of legend items. To be safe, provide as a named vector like `c("old" = "new", "old" = "new")` or adjust the values in the data itself.  
  * Use `name = ` to give a proper title to the legend  
* For more tips on color scales and palettes, see the page on [ggplot basics].  

```{r, warning=F, message=F}
ggplot(data = linelist)+           # begin with linelist (many hospitals)
  
  # make histogram
  geom_histogram(
    mapping = aes(x = date_onset,
        group = hospital,          # cases grouped by hospital
        fill = hospital),          # bar fill by hospital
    
    # bin breaks
    breaks = weekly_breaks_all,        # sequence of weekly Monday bin breaks, defined in previous code
    
    # Color around bars
    color = "black")+              # border color of each bar
  
  # manual specification of colors
  scale_fill_manual(
    values = c("black", "orange", "grey", "beige", "blue", "brown"),
    labels = c("St. Mark's Maternity Hospital (SMMH)" = "St. Mark's"),
    name = "Hospital") # specify fill colors ("values") - attention to order!
```




### Adjust level order {.unnumbered}  

The order in which grouped bars are stacked is best adjusted by classifying the grouping column as class Factor. You can then designate the factor level order (and their display labels). See the page on [Factors] or [ggplot tips] for details.  

Before making the plot, use the `fct_relevel()` function from **forcats** package to convert the grouping column to class factor and manually adjust the level order, as detailed in the page on [Factors].  

```{r}
# load forcats package for working with factors
pacman::p_load(forcats)

# Define new dataset with hospital as factor
plot_data <- linelist %>% 
  mutate(hospital = fct_relevel(hospital, c("Missing", "Other"))) # Convert to factor and set "Missing" and "Other" as top levels to appear on epicurve top

levels(plot_data$hospital) # print levels in order
```

In the below plot, the only differences from previous is that column `hospital` has been consolidated as above, and we use `guides()` to reverse the legend order, so that "Missing" is on the bottom of the legend.  

```{r, warning=F, message=F}
ggplot(plot_data) +                     # Use NEW dataset with hospital as re-ordered factor
  
  # make histogram
  geom_histogram(
    mapping = aes(x = date_onset,
        group = hospital,               # cases grouped by hospital
        fill = hospital),               # bar fill (color) by hospital
    
    breaks = weekly_breaks_all,         # sequence of weekly Monday bin breaks for whole outbreak, defined at top of ggplot section
    
    color = "black")+                   # border color around each bar
    
  # x-axis labels
  scale_x_date(
    expand            = c(0,0),         # remove excess x-axis space before and after case bars
    date_breaks       = "3 weeks",      # labels appear every 3 Monday weeks
    date_minor_breaks = "week",         # vertical lines appear every Monday week
    date_labels       = "%d\n%b\n'%y")+ # date labels format
  
  # y-axis
  scale_y_continuous(
    expand = c(0,0))+                   # remove excess y-axis space below 0
  
  # manual specification of colors, ! attention to order
  scale_fill_manual(
    values = c("grey", "beige", "black", "orange", "blue", "brown"),
    labels = c("St. Mark's Maternity Hospital (SMMH)" = "St. Mark's"),
    name = "Hospital")+ 
  
  # aesthetic themes
  theme_minimal()+                      # simplify plot background
  
  theme(
    plot.caption = element_text(face = "italic", # caption on left side in italics
                                hjust = 0), 
    axis.title = element_text(face = "bold"))+   # axis titles in bold
  
  # labels
  labs(
    title    = "Weekly incidence of cases by hospital",
    subtitle = "Hospital as re-ordered factor",
    x        = "Week of symptom onset",
    y        = "Weekly cases")
```

<span style="color: darkgreen;">**_TIP:_** To reverse the order of the legend only, add this **ggplot2** command: `guides(fill = guide_legend(reverse = TRUE))`.</span>  





### Adjust legend {.unnumbered}

Read more about legends and scales in the [ggplot tips] page. Here are a few highlights:  

* Edit legend title either in the scale function or with `labs(fill = "Legend title")` (if your are using `color = ` aesthetic, then use `labs(color = "")`)  
* `theme(legend.title = element_blank())` to have no legend title  
* `theme(legend.position = "top")` ("bottom", "left", "right", or "none" to remove the legend)
* `theme(legend.direction = "horizontal")` horizontal legend 
* `guides(fill = guide_legend(reverse = TRUE))` to reverse order of the legend  







### Bars side-by-side {.unnumbered}  

Side-by-side display of group bars (as opposed to stacked) is specified within `geom_histogram()` with `position = "dodge"` placed outside of `aes()`.  

If there are more than two value groups, these can become difficult to read. Consider instead using a faceted plot (small multiples). To improve readability in this example, missing gender values are removed.  

```{r, warning=F, message=F}
ggplot(central_data %>% drop_na(gender))+   # begin with Central Hospital cases dropping missing gender
    geom_histogram(
        mapping = aes(
          x = date_onset,
          group = gender,         # cases grouped by gender
          fill = gender),         # bars filled by gender
        
        # histogram bin breaks
        breaks = weekly_breaks_central,   # sequence of weekly dates for Central outbreak - defined at top of ggplot section
        
        color = "black",          # bar edge color
        
        position = "dodge")+      # SIDE-BY-SIDE bars
                      
  
  # The labels on the x-axis
  scale_x_date(expand            = c(0,0),         # remove excess x-axis space below and after case bars
               date_breaks       = "3 weeks",      # labels appear every 3 Monday weeks
               date_minor_breaks = "week",         # vertical lines appear every Monday week
               date_labels       = "%d\n%b\n'%y")+ # date labels format
  
  # y-axis
  scale_y_continuous(expand = c(0,0))+             # removes excess y-axis space between bottom of bars and the labels
  
  #scale of colors and legend labels
  scale_fill_manual(values = c("brown", "orange"),  # specify fill colors ("values") - attention to order!
                    na.value = "grey" )+     

  # aesthetic themes
  theme_minimal()+                                               # a set of themes to simplify plot
  theme(plot.caption = element_text(face = "italic", hjust = 0), # caption on left side in italics
        axis.title = element_text(face = "bold"))+               # axis titles in bold
  
  # labels
  labs(title    = "Weekly incidence of cases, by gender",
       subtitle = "Subtitle",
       fill     = "Gender",                                      # provide new title for legend
       x        = "Week of symptom onset",
       y        = "Weekly incident cases reported")
```




### Axis limits {.unnumbered}  

There are two ways to limit the extent of axis values.  

Generally the preferred way is to use the command `coord_cartesian()`, which accepts `xlim = c(min, max)` and `ylim = c(min, max)` (where you provide the min and max values). This acts as a "zoom" without actually removing any data, which is important for statistics and summary measures.  

Alternatively, you can set maximum and minimum date values using `limits = c()` within `scale_x_date()`. For example:  

```{r eval=F}
scale_x_date(limits = c(as.Date("2014-04-01"), NA)) # sets a minimum date but leaves the maximum open.  
```

Likewise, if you want to the x-axis to extend to a specific date (e.g. current date), even if no new cases have been reported, you can use:  

```{r eval=F}
scale_x_date(limits = c(NA, Sys.Date()) # ensures date axis will extend until current date  
```

<span style="color: red;">**_DANGER:_** Be cautious setting the y-axis scale breaks or limits (e.g. 0 to 30 by 5: `seq(0, 30, 5)`). Such static numbers can cut-off your plot too short if the data changes to exceed the limit!.</span>



### Date-axis labels/gridlines {.unnumbered} 

<span style="color: darkgreen;">**_TIP:_** Remember that date-axis **labels** are independent from the aggregation of the data into bars, but visually it can be important to align bins, date labels, and vertical grid lines.</span>

To **modify the date labels and grid lines**, use `scale_x_date()` in one of these ways:  

* **If your histogram bins are days, Monday weeks, months, or years**:  
  * Use `date_breaks = ` to specify the interval of labels and major gridlines (e.g. "day", "week", "3 weeks", "month", or "year")
  * Use `date_minor_breaks = ` to specify interval of minor vertical gridlines (between date labels)  
  * Add `expand = c(0,0)` to begin the labels at the first bar  
  * Use `date_labels = ` to specify format of date labels - see the Dates page for tips (use `\n` for a new line)  
* **If your histogram bins are Sunday weeks**:  
  * Use `breaks = ` and `minor_breaks = ` by providing a sequence of date breaks for each
  * You can still use `date_labels = ` and `expand = ` for formatting as described above  

Some notes:  

* See the opening ggplot section for instructions on how to create a sequence of dates using `seq.Date()`.  
* See [this page](https://rdrr.io/r/base/strptime.html) or the [Working with dates] page for tips on creating date labels.  




#### Demonstrations {.unnumbered}

Below is a demonstration of plots where the bins and the plot labels/grid lines are aligned and not aligned:  

```{r fig.show='hold', class.source = 'fold-hide', warning=F, message=F}
# 7-day bins + Monday labels
#############################
ggplot(central_data) +
  geom_histogram(
    mapping = aes(x = date_onset),
    binwidth = 7,                 # 7-day bins with start at first case
    color = "darkblue",
    fill = "lightblue") +
  
  scale_x_date(
    expand = c(0,0),               # remove excess x-axis space below and after case bars
    date_breaks = "3 weeks",       # Monday every 3 weeks
    date_minor_breaks = "week",    # Monday weeks
    date_labels = "%a\n%d\n%b\n'%y")+  # label format
  
  scale_y_continuous(
    expand = c(0,0))+              # remove excess space under x-axis, make flush
  
  labs(
    title = "MISALIGNED",
    subtitle = "! CAUTION: 7-day bars start Thursdays at first case\nDate labels and gridlines on Mondays\nNote how ticks don't align with bars")



# 7-day bins + Months
#####################
ggplot(central_data) +
  geom_histogram(
    mapping = aes(x = date_onset),
    binwidth = 7,
    color = "darkblue",
    fill = "lightblue") +
  
  scale_x_date(
    expand = c(0,0),                  # remove excess x-axis space below and after case bars
    date_breaks = "months",           # 1st of month
    date_minor_breaks = "week",       # Monday weeks
    date_labels = "%a\n%d %b\n%Y")+    # label format
  
  scale_y_continuous(
    expand = c(0,0))+                # remove excess space under x-axis, make flush 
  
  labs(
    title = "MISALIGNED",
    subtitle = "! CAUTION: 7-day bars start Thursdays with first case\nMajor gridlines and date labels at 1st of each month\nMinor gridlines weekly on Mondays\nNote uneven spacing of some gridlines and ticks unaligned with bars")


# TOTAL MONDAY ALIGNMENT: specify manual bin breaks to be mondays
#################################################################
ggplot(central_data) + 
  geom_histogram(
    mapping = aes(x = date_onset),
    
    # histogram breaks set to 7 days beginning Monday before first case
    breaks = weekly_breaks_central,    # defined earlier in this page
    
    color = "darkblue",
    
    fill = "lightblue") + 
  
  scale_x_date(
    expand = c(0,0),                   # remove excess x-axis space below and after case bars
    date_breaks = "4 weeks",           # Monday every 4 weeks
    date_minor_breaks = "week",        # Monday weeks 
    date_labels = "%a\n%d %b\n%Y")+      # label format
  
  scale_y_continuous(
    expand = c(0,0))+                # remove excess space under x-axis, make flush 
  
  labs(
    title = "ALIGNED Mondays",
    subtitle = "7-day bins manually set to begin Monday before first case (28 Apr)\nDate labels and gridlines on Mondays as well")


# TOTAL MONDAY ALIGNMENT WITH MONTHS LABELS:
############################################
ggplot(central_data) + 
  geom_histogram(
    mapping = aes(x = date_onset),
    
    # histogram breaks set to 7 days beginning Monday before first case
    breaks = weekly_breaks_central,            # defined earlier in this page
    
    color = "darkblue",
    
    fill = "lightblue") + 
  
  scale_x_date(
    expand = c(0,0),                   # remove excess x-axis space below and after case bars
    date_breaks = "months",            # Monday every 4 weeks
    date_minor_breaks = "week",        # Monday weeks 
    date_labels = "%b\n%Y")+          # label format
  
  scale_y_continuous(
    expand = c(0,0))+                # remove excess space under x-axis, make flush 
  
  theme(panel.grid.major = element_blank())+  # Remove major gridlines (fall on 1st of month)
          
  labs(
    title = "ALIGNED Mondays with MONTHLY labels",
    subtitle = "7-day bins manually set to begin Monday before first case (28 Apr)\nDate labels on 1st of Month\nMonthly major gridlines removed")


# TOTAL SUNDAY ALIGNMENT: specify manual bin breaks AND labels to be Sundays
############################################################################
ggplot(central_data) + 
  geom_histogram(
    mapping = aes(x = date_onset),
    
    # histogram breaks set to 7 days beginning Sunday before first case
    breaks = seq.Date(from = floor_date(min(central_data$date_onset, na.rm=T),   "week", week_start = 7),
                      to   = ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 7),
                      by   = "7 days"),
    
    color = "darkblue",
    
    fill = "lightblue") + 
  
  scale_x_date(
    expand = c(0,0),
    # date label breaks and major gridlines set to every 3 weeks beginning Sunday before first case
    breaks = seq.Date(from = floor_date(min(central_data$date_onset, na.rm=T),   "week", week_start = 7),
                      to   = ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 7),
                      by   = "3 weeks"),
    
    # minor gridlines set to weekly beginning Sunday before first case
    minor_breaks = seq.Date(from = floor_date(min(central_data$date_onset, na.rm=T),   "week", week_start = 7),
                            to   = ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 7),
                            by   = "7 days"),
    
    date_labels = "%a\n%d\n%b\n'%y")+  # label format
  
  scale_y_continuous(
    expand = c(0,0))+                # remove excess space under x-axis, make flush 
  
  labs(title = "ALIGNED Sundays",
       subtitle = "7-day bins manually set to begin Sunday before first case (27 Apr)\nDate labels and gridlines manually set to Sundays as well")

```





### Aggregated data {.unnumbered} 

Often instead of a linelist, you begin with aggregated counts from facilities, districts, etc. You can make an epicurve with `ggplot()` but the code will be slightly different. This section will utilize the `count_data` dataset that was imported earlier, in the data preparation section. This dataset is the `linelist` aggregated to day-hospital counts. The first 50 rows are displayed below.  

```{r message=FALSE, warning=F, echo=F}
# display the linelist data as a table
DT::datatable(head(count_data, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


#### Plotting daily counts {.unnumbered}  

We can plot a daily epicurve from these *daily counts*. Here are the differences to the code:  

* Within the aesthetic mapping `aes()`, specify `y = ` as the counts column (in this case, the column name is `n_cases`)
* Add the argument `stat = "identity"` within `geom_histogram()`, which specifies that bar height should be the `y = ` value, not the number of rows as is the default  
* Add the argument `width = ` to avoid vertical white lines between the bars. For daily data set to 1. For weekly count data set to 7. For monthly count data, white lines are an issue (each month has different number of days) - consider transforming your x-axis to a categorical ordered factor (months) and using `geom_col()`.


```{r, message=FALSE, warning=F}
ggplot(data = count_data)+
  geom_histogram(
   mapping = aes(x = date_hospitalisation, y = n_cases),
   stat = "identity",
   width = 1)+                # for daily counts, set width = 1 to avoid white space between bars
  labs(
    x = "Date of report", 
    y = "Number of cases",
    title = "Daily case incidence, from daily count data")
```

#### Plotting weekly counts {.unnumbered}

If your data are already case counts by week, they might look like this dataset (called `count_data_weekly`):  

```{r, warning=F, message=F, echo=F}
# Create weekly dataset with epiweek column
count_data_weekly <- count_data %>%
  mutate(epiweek = lubridate::floor_date(date_hospitalisation, "week")) %>% 
  group_by(hospital, epiweek, .drop=F) %>% 
  summarize(n_cases_weekly = sum(n_cases, na.rm=T))   
```

The first 50 rows of `count_data_weekly` are displayed below. You can see that the counts have been aggregated into weeks. Each week is displayed by the first day of the week (Monday by default).  

```{r message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(count_data_weekly, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Now plot so that `x = ` the epiweek column. Remember to add `y = ` the counts column to the aesthetic mapping, and add `stat = "identity"` as explained above.  

```{r, warning=F, message=F}
ggplot(data = count_data_weekly)+
  
  geom_histogram(
    mapping = aes(
      x = epiweek,           # x-axis is epiweek (as class Date)
      y = n_cases_weekly,    # y-axis height in the weekly case counts
      group = hospital,      # we are grouping the bars and coloring by hospital
      fill = hospital),
    stat = "identity")+      # this is also required when plotting count data
     
  # labels for x-axis
  scale_x_date(
    date_breaks = "2 months",      # labels every 2 months 
    date_minor_breaks = "1 month", # gridlines every month
    date_labels = '%b\n%Y')+       #labeled by month with year below
     
  # Choose color palette (uses RColorBrewer package)
  scale_fill_brewer(palette = "Pastel2")+ 
  
  theme_minimal()+
  
  labs(
    x = "Week of onset", 
    y = "Weekly case incidence",
    fill = "Hospital",
    title = "Weekly case incidence, from aggregated count data by hospital")
```




### Moving averages {.unnumbered}

See the page on [Moving averages] for a detailed description and several options. Below is one option for calculating moving averages with the package **slider**. In this approach, *the moving average is calculated in the dataset prior to plotting*:  

1) Aggregate the data into counts as necessary (daily, weekly, etc.) (see [Grouping data] page)  
2) Create a new column to hold the moving average, created with `slide_index()` from **slider** package  
3) Plot the moving average as a `geom_line()` on top of (after) the epicurve histogram  

See the helpful online [vignette for the **slider** package](https://cran.r-project.org/web/packages/slider/vignettes/slider.html)  


```{r, warning=F, message=F}
# load package
pacman::p_load(slider)  # slider used to calculate rolling averages

# make dataset of daily counts and 7-day moving average
#######################################################
ll_counts_7day <- linelist %>%    # begin with linelist
  
  ## count cases by date
  count(date_onset, name = "new_cases") %>%   # name new column with counts as "new_cases"
  drop_na(date_onset) %>%                     # remove cases with missing date_onset
  
  ## calculate the average number of cases in 7-day window
  mutate(
    avg_7day = slider::slide_index(    # create new column
      new_cases,                       # calculate based on value in new_cases column
      .i = date_onset,                 # index is date_onset col, so non-present dates are included in window 
      .f = ~mean(.x, na.rm = TRUE),    # function is mean() with missing values removed
      .before = 6,                     # window is the day and 6-days before
      .complete = FALSE),              # must be FALSE for unlist() to work in next step
    avg_7day = unlist(avg_7day))       # convert class list to class numeric


# plot
######
ggplot(data = ll_counts_7day) +  # begin with new dataset defined above 
    geom_histogram(              # create epicurve histogram
      mapping = aes(
        x = date_onset,          # date column as x-axis
        y = new_cases),          # height is number of daily new cases
        stat = "identity",       # height is y value
        fill="#92a8d1",          # cool color for bars
        colour = "#92a8d1",      # same color for bar border
        )+ 
    geom_line(                   # make line for rolling average
      mapping = aes(
        x = date_onset,          # date column for x-axis
        y = avg_7day,            # y-value set to rolling average column
        lty = "7-day \nrolling avg"), # name of line in legend
      color="red",               # color of line
      size = 1) +                # width of line
    scale_x_date(                # date scale
      date_breaks = "1 month",
      date_labels = '%d/%m',
      expand = c(0,0)) +
    scale_y_continuous(          # y-axis scale
      expand = c(0,0),
      limits = c(0, NA)) +       
    labs(
      x="",
      y ="Number of confirmed cases",
      fill = "Legend")+ 
    theme_minimal()+
    theme(legend.title = element_blank())  # removes title of legend
```




### Faceting/small-multiples {.unnumbered}

As with other ggplots, you can create facetted plots ("small multiples"). As explained in the [ggplot tips] page of this handbook, you can use either `facet_wrap()` or `facet_grid()`. Here we demonstrate with `facet_wrap()`. For epicurves, `facet_wrap()` is typically easier as it is likely that you only need to facet on one column.  

The general syntax is `facet_wrap(rows ~ cols)`, where to the left of the tilde (~) is the name of a column to be spread across the "rows" of the facetted plot, and to the right of the tilde is the name of a column to be spread across the "columns" of the facetted plot. Most simply, just use one column name, to the right of the tilde: `facet_wrap(~age_cat)`.  


**Free axes**  
You will need to decide whether the scales of the axes for each facet are "fixed" to the same dimensions (default), or "free" (meaning they will change based on the data within the facet). Do this with the `scales = ` argument within `facet_wrap()` by specifying "free_x" or "free_y", or "free".  


**Number of cols and rows of facets**  
This can be specified with `ncol = ` and `nrow = ` within `facet_wrap()`. 


**Order of panels**  
To change the order of appearance, change the underlying order of the levels of the factor column used to create the facets.  


**Aesthetics**  
Font size and face, strip color, etc. can be modified through `theme()` with arguments like:  

* `strip.text = element_text()` (size, colour, face, angle...)
* `strip.background = element_rect()` (e.g. element_rect(fill="grey"))  
* `strip.position = ` (position of the strip "bottom", "top", "left", or "right")  


**Strip labels**  
Labels of the facet plots can be modified through the "labels" of the column as a factor, or by the use of a "labeller".  

Make a labeller like this, using the function `as_labeller()` from **ggplot2**. Then provide the labeller to the `labeller = ` argument of `facet_wrap()` as shown below.  

```{r, class.source = 'fold-show'}
my_labels <- as_labeller(c(
     "0-4"   = "Ages 0-4",
     "5-9"   = "Ages 5-9",
     "10-14" = "Ages 10-14",
     "15-19" = "Ages 15-19",
     "20-29" = "Ages 20-29",
     "30-49" = "Ages 30-49",
     "50-69" = "Ages 50-69",
     "70+"   = "Over age 70"))
```

**An example facetted plot** - facetted by column `age_cat`.


```{r, warning=F, message=F}
# make plot
###########
ggplot(central_data) + 
  
  geom_histogram(
    mapping = aes(
      x = date_onset,
      group = age_cat,
      fill = age_cat),    # arguments inside aes() apply by group
      
    color = "black",      # arguments outside aes() apply to all data
        
    # histogram breaks
    breaks = weekly_breaks_central)+  # pre-defined date vector (see earlier in this page)
                      
  # The labels on the x-axis
  scale_x_date(
    expand            = c(0,0),         # remove excess x-axis space below and after case bars
    date_breaks       = "2 months",     # labels appear every 2 months
    date_minor_breaks = "1 month",      # vertical lines appear every 1 month 
    date_labels       = "%b\n'%y")+     # date labels format
  
  # y-axis
  scale_y_continuous(expand = c(0,0))+                       # removes excess y-axis space between bottom of bars and the labels
  
  # aesthetic themes
  theme_minimal()+                                           # a set of themes to simplify plot
  theme(
    plot.caption = element_text(face = "italic", hjust = 0), # caption on left side in italics
    axis.title = element_text(face = "bold"),
    legend.position = "bottom",
    strip.text = element_text(face = "bold", size = 10),
    strip.background = element_rect(fill = "grey"))+         # axis titles in bold
  
  # create facets
  facet_wrap(
    ~age_cat,
    ncol = 4,
    strip.position = "top",
    labeller = my_labels)+             
  
  # labels
  labs(
    title    = "Weekly incidence of cases, by age category",
    subtitle = "Subtitle",
    fill     = "Age category",                                      # provide new title for legend
    x        = "Week of symptom onset",
    y        = "Weekly incident cases reported",
    caption  = stringr::str_glue("n = {nrow(central_data)} from Central Hospital; Case onsets range from {format(min(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')} to {format(max(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')}\n{nrow(central_data %>% filter(is.na(date_onset)))} cases missing date of onset and not shown"))
```

See this [link](https://ggplot2.tidyverse.org/reference/labellers.html) for more information on labellers.  




#### Total epidemic in facet background {.unnumbered}

To show the total epidemic in the background of each facet, add the function `gghighlight()` with empty parentheses to the ggplot. This is from the package **gghighlight**. Note that the y-axis maximum in all facets is now based on the peak of the entire epidemic. There are more examples of this package in the [ggplot tips] page.  

```{r, warning=F, message=F}
ggplot(central_data) + 
  
  # epicurves by group
  geom_histogram(
    mapping = aes(
      x = date_onset,
      group = age_cat,
      fill = age_cat),  # arguments inside aes() apply by group
    
    color = "black",    # arguments outside aes() apply to all data
    
    # histogram breaks
    breaks = weekly_breaks_central)+     # pre-defined date vector (see top of ggplot section)                
  
  # add grey epidemic in background to each facet
  gghighlight::gghighlight()+
  
  # labels on x-axis
  scale_x_date(
    expand            = c(0,0),         # remove excess x-axis space below and after case bars
    date_breaks       = "2 months",     # labels appear every 2 months
    date_minor_breaks = "1 month",      # vertical lines appear every 1 month 
    date_labels       = "%b\n'%y")+     # date labels format
  
  # y-axis
  scale_y_continuous(expand = c(0,0))+  # removes excess y-axis space below 0
  
  # aesthetic themes
  theme_minimal()+                                           # a set of themes to simplify plot
  theme(
    plot.caption = element_text(face = "italic", hjust = 0), # caption on left side in italics
    axis.title = element_text(face = "bold"),
    legend.position = "bottom",
    strip.text = element_text(face = "bold", size = 10),
    strip.background = element_rect(fill = "white"))+        # axis titles in bold
  
  # create facets
  facet_wrap(
    ~age_cat,                          # each plot is one value of age_cat
    ncol = 4,                          # number of columns
    strip.position = "top",            # position of the facet title/strip
    labeller = my_labels)+             # labeller defines above
  
  # labels
  labs(
    title    = "Weekly incidence of cases, by age category",
    subtitle = "Subtitle",
    fill     = "Age category",                                      # provide new title for legend
    x        = "Week of symptom onset",
    y        = "Weekly incident cases reported",
    caption  = stringr::str_glue("n = {nrow(central_data)} from Central Hospital; Case onsets range from {format(min(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')} to {format(max(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')}\n{nrow(central_data %>% filter(is.na(date_onset)))} cases missing date of onset and not shown"))
```


#### One facet with data {.unnumbered}  

If you want to have one facet box that contains all the data, duplicate the entire dataset and treat the duplicates as one faceting value. A "helper" function `CreateAllFacet()` below can assist with this (thanks to this [blog post](https://stackoverflow.com/questions/18933575/easily-add-an-all-facet-to-facet-wrap-in-ggplot2)). When it is run, the number of rows doubles and there will be a new column called `facet` in which the duplicated rows will have the value "all", and the original rows have the their original value of the faceting colum. Now you just have to facet on the `facet` column.   

Here is the helper function. Run it so that it is available to you.  

```{r}
# Define helper function
CreateAllFacet <- function(df, col){
     df$facet <- df[[col]]
     temp <- df
     temp$facet <- "all"
     merged <-rbind(temp, df)
     
     # ensure the facet value is a factor
     merged[[col]] <- as.factor(merged[[col]])
     
     return(merged)
}
```

Now apply the helper function to the dataset, on column `age_cat`:  

```{r}
# Create dataset that is duplicated and with new column "facet" to show "all" age categories as another facet level
central_data2 <- CreateAllFacet(central_data, col = "age_cat") %>%
  
  # set factor levels
  mutate(facet = fct_relevel(facet, "all", "0-4", "5-9",
                             "10-14", "15-19", "20-29",
                             "30-49", "50-69", "70+"))

# check levels
table(central_data2$facet, useNA = "always")
```

Notable changes to the `ggplot()` command are:  

* The data used is now central_data2 (double the rows, with new column "facet")
* Labeller will need to be updated, if used  
* Optional: to achieve vertically stacked facets: the facet column is moved to rows side of equation and on right is replaced by "." (`facet_wrap(facet~.)`), and `ncol = 1`. You may also need to adjust the width and height of the saved png plot image (see `ggsave()` in [ggplot tips]).  

```{r, fig.height=12, fig.width=5, warning=F, message=F}
ggplot(central_data2) + 
  
  # actual epicurves by group
  geom_histogram(
        mapping = aes(
          x = date_onset,
          group = age_cat,
          fill = age_cat),  # arguments inside aes() apply by group
        color = "black",    # arguments outside aes() apply to all data
        
        # histogram breaks
        breaks = weekly_breaks_central)+    # pre-defined date vector (see top of ggplot section)
                     
  # Labels on x-axis
  scale_x_date(
    expand            = c(0,0),         # remove excess x-axis space below and after case bars
    date_breaks       = "2 months",     # labels appear every 2 months
    date_minor_breaks = "1 month",      # vertical lines appear every 1 month 
    date_labels       = "%b\n'%y")+     # date labels format
  
  # y-axis
  scale_y_continuous(expand = c(0,0))+  # removes excess y-axis space between bottom of bars and the labels
  
  # aesthetic themes
  theme_minimal()+                                           # a set of themes to simplify plot
  theme(
    plot.caption = element_text(face = "italic", hjust = 0), # caption on left side in italics
    axis.title = element_text(face = "bold"),
    legend.position = "bottom")+               
  
  # create facets
  facet_wrap(facet~. ,                            # each plot is one value of facet
             ncol = 1)+            

  # labels
  labs(title    = "Weekly incidence of cases, by age category",
       subtitle = "Subtitle",
       fill     = "Age category",                                      # provide new title for legend
       x        = "Week of symptom onset",
       y        = "Weekly incident cases reported",
       caption  = stringr::str_glue("n = {nrow(central_data)} from Central Hospital; Case onsets range from {format(min(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')} to {format(max(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')}\n{nrow(central_data %>% filter(is.na(date_onset)))} cases missing date of onset and not shown"))
```








## Tentative data  


The most recent data shown in epicurves should often be marked as tentative, or subject to reporting delays. This can be done in by adding a vertical line and/or rectangle over a specified number of days. Here are two options:  

1) Use `annotate()`:  
    + For a line use `annotate(geom = "segment")`. Provide `x`, `xend`, `y`, and `yend`. Adjust size, linetype (`lty`), and color.  
    + For a rectangle use `annotate(geom = "rect")`. Provide xmin/xmax/ymin/ymax. Adjust color and alpha.  
2) Group the data by tentative status and color those bars differently  

<span style="color: orange;">**_CAUTION:_** You might try `geom_rect()` to draw a rectangle, but adjusting the transparency does not work in a linelist context. This function overlays one rectangle for each observation/row!. Use either a very low alpha (e.g. 0.01), or another approach. </span>

### Using `annotate()` {.unnumbered}

* Within `annotate(geom = "rect")`, the `xmin` and `xmax` arguments must be given inputs of class Date.  
* Note that because these data are aggregated into weekly bars, and the last bar extends to the Monday after the last data point, the shaded region may appear to cover 4 weeks  
* Here is an `annotate()` [online example](https://ggplot2.tidyverse.org/reference/annotate.html)


```{r, warning=F, message=F}
ggplot(central_data) + 
  
  # histogram
  geom_histogram(
    mapping = aes(x = date_onset),
    
    breaks = weekly_breaks_central,   # pre-defined date vector - see top of ggplot section
    
    color = "darkblue",
    
    fill = "lightblue") +

  # scales
  scale_y_continuous(expand = c(0,0))+
  scale_x_date(
    expand = c(0,0),                   # remove excess x-axis space below and after case bars
    date_breaks = "1 month",           # 1st of month
    date_minor_breaks = "1 month",     # 1st of month
    date_labels = "%b\n'%y")+          # label format
  
  # labels and theme
  labs(
    title = "Using annotate()\nRectangle and line showing that data from last 21-days are tentative",
    x = "Week of symptom onset",
    y = "Weekly case indicence")+ 
  theme_minimal()+
  
  # add semi-transparent red rectangle to tentative data
  annotate(
    "rect",
    xmin  = as.Date(max(central_data$date_onset, na.rm = T) - 21), # note must be wrapped in as.Date()
    xmax  = as.Date(Inf),                                          # note must be wrapped in as.Date()
    ymin  = 0,
    ymax  = Inf,
    alpha = 0.2,          # alpha easy and intuitive to adjust using annotate()
    fill  = "red")+
  
  # add black vertical line on top of other layers
  annotate(
    "segment",
    x     = max(central_data$date_onset, na.rm = T) - 21, # 21 days before last data
    xend  = max(central_data$date_onset, na.rm = T) - 21, 
    y     = 0,         # line begins at y = 0
    yend  = Inf,       # line to top of plot
    size  = 2,         # line size
    color = "black",
    lty   = "solid")+   # linetype e.g. "solid", "dashed"

  # add text in rectangle
  annotate(
    "text",
    x = max(central_data$date_onset, na.rm = T) - 15,
    y = 15,
    label = "Subject to reporting delays",
    angle = 90)
```


The same black vertical line can be achieved with the code below, but using `geom_vline()` you lose the ability to control the height:  

```{r, eval=F}
geom_vline(xintercept = max(central_data$date_onset, na.rm = T) - 21,
           size = 2,
           color = "black")
```



### Bars color {.unnumbered}  

An alternative approach could be to adjust the color or display of the tentative bars of data themselves. You could create a new column in the data preparation stage and use it to group the data, such that the `aes(fill = )` of tentative data can be a different color or alpha than the other bars. 

```{r, message=F, warning=F}
# add column
############
plot_data <- central_data %>% 
  mutate(tentative = case_when(
    date_onset >= max(date_onset, na.rm=T) - 7 ~ "Tentative", # tenative if in last 7 days
    TRUE                                       ~ "Reliable")) # all else reliable

# plot
######
ggplot(plot_data, aes(x = date_onset, fill = tentative)) + 
  
  # histogram
  geom_histogram(
    breaks = weekly_breaks_central,   # pre-defined data vector, see top of ggplot page
    color = "black") +

  # scales
  scale_y_continuous(expand = c(0,0))+
  scale_fill_manual(values = c("lightblue", "grey"))+
  scale_x_date(
    expand = c(0,0),                   # remove excess x-axis space below and after case bars
    date_breaks = "3 weeks",           # Monday every 3 weeks
    date_minor_breaks = "week",        # Monday weeks 
    date_labels = "%d\n%b\n'%y")+      # label format
  
  # labels and theme
  labs(title = "Show days that are tentative reporting",
    subtitle = "")+ 
  theme_minimal()+
  theme(legend.title = element_blank())                 # remove title of legend
  
```


## Multi-level date labels  

If you want multi-level date labels (e.g. month and year) *without duplicating the lower label levels*, consider one of the approaches below:  

Remember - you can can use tools like `\n` *within* the `date_labels` or `labels` arguments to put parts of each label on a new line below. However, the code below helps you take years or months (for example) on a lower line *and only once*. A few notes on the code below:  

* Case counts are aggregated into weeks for aesthetic reasons. See Epicurves page (aggregated data tab) for details.  
* A `geom_area()` line is used instead of a histogram, as the faceting approach below does not work well with histograms.  


**Aggregate to weekly counts**

```{r out.width = c('50%', '50%'), fig.show='hold', warning=F, message=F}

# Create dataset of case counts by week
#######################################
central_weekly <- linelist %>%
  filter(hospital == "Central Hospital") %>%   # filter linelist
  mutate(week = lubridate::floor_date(date_onset, unit = "weeks")) %>%  
  count(week) %>%                              # summarize weekly case counts
  drop_na(week) %>%                            # remove cases with missing onset_date
  complete(                                    # fill-in all weeks with no cases reported
    week = seq.Date(
      from = min(week),   
      to   = max(week),
      by   = "week"),
    fill = list(n = 0))                        # convert new NA values to 0 counts
```

**Make plots**  

```{r, warning=F, message=F}
# plot with box border on year
##############################
ggplot(central_weekly) +
  geom_area(aes(x = week, y = n),    # make line, specify x and y
            stat = "identity") +             # because line height is count number
  scale_x_date(date_labels="%b",             # date label format show month 
               date_breaks="month",          # date labels on 1st of each month
               expand=c(0,0)) +              # remove excess space on each end
  scale_y_continuous(
    expand  = c(0,0))+                       # remove excess space below x-axis
  facet_grid(~lubridate::year(week), # facet on year (of Date class column)
             space="free_x",                
             scales="free_x",                # x-axes adapt to data range (not "fixed")
             switch="x") +                   # facet labels (year) on bottom
  theme_bw() +
  theme(strip.placement = "outside",         # facet labels placement
        strip.background = element_rect(fill = NA, # facet labels no fill grey border
                                        colour = "grey50"),
        panel.spacing = unit(0, "cm"))+      # no space between facet panels
  labs(title = "Nested year labels, grey label border")


# plot with no box border on year
#################################
ggplot(central_weekly,
       aes(x = week, y = n)) +              # establish x and y for entire plot
  geom_line(stat = "identity",              # make line, line height is count number
            color = "#69b3a2") +            # line color
  geom_point(size=1, color="#69b3a2") +     # make points at the weekly data points
  geom_area(fill = "#69b3a2",               # fill area below line
            alpha = 0.4)+                   # fill transparency
  scale_x_date(date_labels="%b",            # date label format show month 
               date_breaks="month",         # date labels on 1st of each month
               expand=c(0,0)) +             # remove excess space
  scale_y_continuous(
    expand  = c(0,0))+                      # remove excess space below x-axis
  facet_grid(~lubridate::year(week),        # facet on year (of Date class column)
             space="free_x",                
             scales="free_x",               # x-axes adapt to data range (not "fixed")
             switch="x") +                  # facet labels (year) on bottom
  theme_bw() +
  theme(strip.placement = "outside",                     # facet label placement
          strip.background = element_blank(),            # no facet lable background
          panel.grid.minor.x = element_blank(),          
          panel.border = element_rect(colour="grey40"),  # grey border to facet PANEL
          panel.spacing=unit(0,"cm"))+                   # No space between facet panels
  labs(title = "Nested year labels - points, shaded, no label border")
```

The above techniques were adapted from [this](https://stackoverflow.com/questions/44616530/axis-labels-on-two-lines-with-nested-x-variables-year-below-months) and [this](https://stackoverflow.com/questions/20571306/multi-row-x-axis-labels-in-ggplot-line-chart) post on stackoverflow.com.  






<!-- ======================================================= -->
## Dual-axis { }  

Although there are fierce discussions about the validity of dual axes within the data visualization community, many epi supervisors still want to see an epicurve or similar chart with a percent overlaid with a second axis. This is discussed more extensively in the [ggplot tips] page, but one example using the **cowplot** method is shown below:  

* Two distinct plots are made, and then combined with **cowplot** package.  
* The plots must have the exact same x-axis (set limits) or else the data and labels will not align  
* Each uses `theme_cowplot()` and one has the y-axis moved to the right side of the plot  

```{r, warning=F, message=F}
#load package
pacman::p_load(cowplot)

# Make first plot of epicurve histogram
#######################################
plot_cases <- linelist %>% 
  
  # plot cases per week
  ggplot()+
  
  # create histogram  
  geom_histogram(
    
    mapping = aes(x = date_onset),
    
    # bin breaks every week beginning monday before first case, going to monday after last case
    breaks = weekly_breaks_all)+  # pre-defined vector of weekly dates (see top of ggplot section)
        
  # specify beginning and end of date axis to align with other plot
  scale_x_date(
    limits = c(min(weekly_breaks_all), max(weekly_breaks_all)))+  # min/max of the pre-defined weekly breaks of histogram
  
  # labels
  labs(
      y = "Daily cases",
      x = "Date of symptom onset"
    )+
  theme_cowplot()


# make second plot of percent died per week
###########################################
plot_deaths <- linelist %>%                        # begin with linelist
  group_by(week = floor_date(date_onset, "week")) %>%  # create week column
  
  # summarise to get weekly percent of cases who died
  summarise(n_cases = n(),
            died = sum(outcome == "Death", na.rm=T),
            pct_died = 100*died/n_cases) %>% 
  
  # begin plot
  ggplot()+
  
  # line of weekly percent who died
  geom_line(                                # create line of percent died
    mapping = aes(x = week, y = pct_died),  # specify y-height as pct_died column
    stat = "identity",                      # set line height to the value in pct_death column, not the number of rows (which is default)
    size = 2,
    color = "black")+
  
  # Same date-axis limits as the other plot - perfect alignment
  scale_x_date(
    limits = c(min(weekly_breaks_all), max(weekly_breaks_all)))+  # min/max of the pre-defined weekly breaks of histogram
  
  
  # y-axis adjustments
  scale_y_continuous(                # adjust y-axis
    breaks = seq(0,100, 10),         # set break intervals of percent axis
    limits = c(0, 100),              # set extent of percent axis
    position = "right")+             # move percent axis to the right
  
  # Y-axis label, no x-axis label
  labs(x = "",
       y = "Percent deceased")+      # percent axis label
  
  theme_cowplot()                   # add this to make the two plots merge together nicely
```

Now use **cowplot** to overlay the two plots. Attention has been paid to the x-axis alignment, side of the y-axis, and use of `theme_cowplot()`.  

```{r, warning=F, message=F}
aligned_plots <- cowplot::align_plots(plot_cases, plot_deaths, align="hv", axis="tblr")
ggdraw(aligned_plots[[1]]) + draw_plot(aligned_plots[[2]])
```




## Cumulative Incidence {}

Note: If using **incidence2**, see the section on how you can produce cumulative incidence with a simple function. This page will address how to calculate cumulative incidence and plot it with `ggplot()`.  

If beginning with a case linelist, create a new column containing the cumulative number of cases per day in an outbreak using `cumsum()` from **base** R:    

```{r}
cumulative_case_counts <- linelist %>% 
  count(date_onset) %>%                # count of rows per day (returned in column "n")   
  mutate(                         
    cumulative_cases = cumsum(n)       # new column of the cumulative number of rows at each date
    )
```

The first 10 rows are shown below:  

```{r message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(cumulative_case_counts, 10), rownames = FALSE, options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )
```



This cumulative column can then be plotted against `date_onset`, using `geom_line()`:

```{r, warning=F, message=F}
plot_cumulative <- ggplot()+
  geom_line(
    data = cumulative_case_counts,
    aes(x = date_onset, y = cumulative_cases),
    size = 2,
    color = "blue")

plot_cumulative
```


It can also be overlaid onto the epicurve, with dual-axis using the **cowplot** method described above and in the [ggplot tips] page:

```{r, warning=F, message=F}
#load package
pacman::p_load(cowplot)

# Make first plot of epicurve histogram
plot_cases <- ggplot()+
  geom_histogram(          
    data = linelist,
    aes(x = date_onset),
    binwidth = 1)+
  labs(
    y = "Daily cases",
    x = "Date of symptom onset"
  )+
  theme_cowplot()

# make second plot of cumulative cases line
plot_cumulative <- ggplot()+
  geom_line(
    data = cumulative_case_counts,
    aes(x = date_onset, y = cumulative_cases),
    size = 2,
    color = "blue")+
  scale_y_continuous(
    position = "right")+
  labs(x = "",
       y = "Cumulative cases")+
  theme_cowplot()+
  theme(
    axis.line.x = element_blank(),
    axis.text.x = element_blank(),
    axis.title.x = element_blank(),
    axis.ticks = element_blank())
```

Now use **cowplot** to overlay the two plots. Attention has been paid to the x-axis alignment, side of the y-axis, and use of `theme_cowplot()`.  

```{r, warning=F, message=F}
aligned_plots <- cowplot::align_plots(plot_cases, plot_cumulative, align="hv", axis="tblr")
ggdraw(aligned_plots[[1]]) + draw_plot(aligned_plots[[2]])
```


<!-- ======================================================= -->
## Resources { }








```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/epicurves.Rmd-->


# Tháp dân số và thang đo Likert {#age-pyramid}  



```{r, out.width = c('50%', '50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "pop_pyramid_baseline.png"))

knitr::include_graphics(here::here("images", "likert.png"))
```


Kim tự tháp nhân khẩu học rất hữu ích khi bạn muốn hiển thị sự phân bố về độ tuổi và giới tính. Code tương tự cũng có thể được sử dụng để trực quan hóa kết quả của các câu hỏi khảo sát kiểu Likert (ví dụ: “Hoàn toàn đồng ý”, “Đồng ý”, “Trung lập”, “Không đồng ý”, “Hoàn toàn không đồng ý”). Trong chương này, chúng tôi đề cập đến những điều sau

* Tạo một tháp dân số nhanh & dễ dàng với package **apyramid**  
* Tùy biến tháp dân số với `ggplot()`  
* Hiển thị dữ liệu nhân khẩu học "nền" trong tháp dân số  
* Sử dụng các biểu đồ kiểu kim tự tháp để hiển thị các loại dữ liệu khác (ví dụ: câu trả lời cho các câu hỏi khảo sát **kiểu Likert**)  





<!-- ======================================================= -->
## Chuẩn bị {}



### Gọi package {.unnumbered}

Đoạn code này hiển thị việc gọi các package cần thiết cho các phân tích. Trong cuốn sách này, chúng tôi nhấn mạnh việc sử dụng hàm `p_load()` từ package **pacman**, giúp cài đặt các package cần thiết *và* gọi chúng ra để sử dụng. Bạn cũng có thể gọi các packages đã cài đặt với hàm `library()` của **base** R. Xem thêm chương [R cơ bản] để có thêm thông tin về các packages trong R.  

```{r}
pacman::p_load(rio,       # to import data
               here,      # to locate files
               tidyverse, # to clean, handle, and plot the data (includes ggplot2 package)
               apyramid,  # a package dedicated to creating age pyramids
               janitor,   # tables and cleaning data
               stringr)   # working with strings for titles, captions, etc.
```




### Nhập dữ liệu {.unnumbered}  

Để bắt đầu, chúng ta nhập bộ dữ liệu có tên linelist đã làm sạch bao gồm các trường hợp từ vụ dịch Ebola mô phỏng. Để tiện theo dõi, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>bấm để tải dữ liệu linelist "đã được làm sạch" </a> (dưới dạng tệp .rds). Nhập dữ liệu bằng hàm `import()` từ package **rio** (nó xử lý nhiều loại tệp như .xlsx, .csv, .rds - xem thêm chương [Nhập xuất dữ liệu] để biết thêm chi tiết.    

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import case linelist 
linelist <- import("linelist_cleaned.rds")
```

50 hàng đầu tiên của bộ dữ liệu được hiển thị như bên dưới.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

### Làm sạch {.unnumbered}  

Để tạo tháp nhân khẩu học theo độ tuổi/giới tính truyền thống, trước tiên dữ liệu phải được làm sạch theo những cách sau:  

* Cột gender phải được làm sạch.  
* Tùy thuộc vào phương pháp của bạn, độ tuổi phải được lưu trữ dưới dạng số hoặc trong cột *age category*.  

Nếu sử dụng nhóm tuổi, các giá trị trong cột phải được sắp xếp thứ tự, hoặc là thứ tự chữ-số mặc định hoặc được đặt có chủ ý bằng cách chuyển đổi thành kiểu factor.  

Sau đây chúng ta sử dụng hàm `tabyl()` từ package **janitor** để khảo sát hai cột `gender` và `age_cat5`.  

```{r}
linelist %>% 
  tabyl(age_cat5, gender)
```


Chúng ta có thể vẽ biểu đồ histogram đối với cột `age` để chắc chắn rằng nó đã được làm sạch và phân loại chính xác:  

```{r}
hist(linelist$age)
```


<!-- ======================================================= -->
## **apyramid** package {}

Package **apyramid** là một sản phẩm của dự án [R4Epis](https://r4epis.netlify.com/). Bạn có thể đọc thêm về package này [tại đây](https://cran.r-project.org/web/packages/apyramid/vignettes/intro.html). Nó cho phép bạn nhanh chóng tạo một tháp tuổi. Để tùy biến đẹp hơn, xem mục [sử dụng `ggplot()`](#demo_pyr_gg). Bạn có thể đọc thêm về package **apyramid** tại trang Help bằng cách nhập `?age_pyramid` vào R console. 

### Dữ liệu Linelist {.unnumbered}  


Sử dụng dữ liệu `linelist` đã làm sạch, chúng ta có thể tạo một tháp tuổi chỉ với một lệnh `age_pyramid()` cơ bản. Trong lệnh này:  

* Đối số `data = ` sử dụng bộ dữ liệu `linelist`  
* Đối số `age_group = ` (trục y) lấy thông tin từ cột nhóm tuổi (trong ngoặc kép)  
* Đối số `split_by = ` (trục x) lấy thông tin từ cột giới  

```{r, warning=F, message=F}
apyramid::age_pyramid(data = linelist,
                      age_group = "age_cat5",
                      split_by = "gender")
```


Tháp có thể hiện thị phần trăm của tất cả các trường hợp trên trục x, thay vì chỉ số lượng, bằng cách thêm `proportional = TRUE`.  

```{r, warning=F, message=F}
apyramid::age_pyramid(data = linelist,
                      age_group = "age_cat5",
                      split_by = "gender",
                      proportional = TRUE)
```


Khi sử dụng package **agepyramid**, nếu cột được phân chia `split_by` là nhị phân (vd. nam/nữ, hoặc có/không), thì kết quả sẽ xuất hiện dưới dạng một kim tự tháp. Tuy nhiên nếu có nhiều hơn hai giá trị trong cột được phân chia `split_by` (không bao gồm `NA`), kim tự tháp sẽ xuất hiện dưới dạng nhiều biểu đồ cột ngang với các thanh màu xám trong "background" cho biết phạm vi của dữ liệu không có mặt cho nhóm tuổi đó. Trong trường hợp này, giá trị của `split_by = ` sẽ xuất hiện dưới dạng nhãn ở đỉnh mỗi biểu đồ. Chẳng hạn, bên dưới là những gì xảy ra nếu `split_by = ` được chỉ định tới cột `hospital`.  

```{r, warning=F, message=F}
apyramid::age_pyramid(data = linelist,
                      age_group = "age_cat5",
                      split_by = "hospital")  
```

#### Giá trị Missing {.unnumbered}  

Các hàng chứa giá trị missing `NA` của các cột `split_by = ` hoặc `age_group = `, nếu được mã hóa là `NA`, sẽ không tự động kích hoạt việc phân chia biểu đồ như được hiển thị ở trên. Mặc định những hàng này không được hiển thị. Tuy nhiên, bạn có thể chỉ định các giá trị missing hiển thị, trong một biểu đồ liền kề và dưới dạng một nhóm tuổi riêng biệt, bằng cách chỉ định `na.rm = FALSE`.  

```{r, warning=F, message=F}
apyramid::age_pyramid(data = linelist,
                      age_group = "age_cat5",
                      split_by = "gender",
                      na.rm = FALSE)         # show patients missing age or gender
```

#### Tỷ lệ, màu sắc, & thẩm mỹ {.unnumbered}  

Theo mặc định, các cột hiển thị số lượng (không phải %), đường gạch ngang giữa cho mỗi nhóm được hiển thị và màu sắc là xanh lục/tím. Các thông số này có thể được điều chỉnh, như được trình bày dưới đây:

Bạn cũng có thể thêm các lệnh `ggplot()` vào biểu đồ bằng cách sử dụng các cú pháp chuẩn của `ggplot()` "+" , chẳng hạn như chủ đề trang trí và điều chỉnh nhãn: 

```{r, warning=F, message=F}
apyramid::age_pyramid(
  data = linelist,
  age_group = "age_cat5",
  split_by = "gender",
  proportional = TRUE,              # show percents, not counts
  show_midpoint = FALSE,            # remove bar mid-point line
  #pal = c("orange", "purple")      # can specify alt. colors here (but not labels)
  )+                 
  
  # additional ggplot commands
  theme_minimal()+                               # simplfy background
  scale_fill_manual(                             # specify colors AND labels
    values = c("orange", "purple"),              
    labels = c("m" = "Male", "f" = "Female"))+
  labs(y = "Percent of all cases",              # note x and y labs are switched
       x = "Age categories",                          
       fill = "Gender", 
       caption = "My data source and caption here",
       title = "Title of my plot",
       subtitle = "Subtitle with \n a second line...")+
  theme(
    legend.position = "bottom",                          # legend to bottom
    axis.text = element_text(size = 10, face = "bold"),  # fonts/sizes
    axis.title = element_text(size = 12, face = "bold"))
```



### Dữ liệu được tổng hợp {.unnumbered}  

Ví dụ bên trên giả định rằng dữ liệu của bạn có định dạng mỗi hàng cho một quan sát. Nếu dữ liệu của bạn đã được tổng hợp thành số lượng theo nhóm tuổi, bạn vẫn có thể sử dụng package **apyramid**, như được trình bày dưới đây.  

Để minh họa, chúng ta sẽ tổng hợp dữ liệu linelist theo số lượng đối với nhóm tuổi và giới, dưới định dạng "ngang". Việc này sẽ mô phỏng như thể dữ liệu ban đầu của bạn đang được trình bày dưới dạng số lượng. Tìm hiểu thêm về [Nhóm dữ liệu] và [Xoay trục dữ liệu] ở các chương tương ứng.  

```{r, warning=F, message=F}
demo_agg <- linelist %>% 
  count(age_cat5, gender, name = "cases") %>% 
  pivot_wider(
    id_cols = age_cat5,
    names_from = gender,
    values_from = cases) %>% 
  rename(`missing_gender` = `NA`)
```

...lệnh trên sẽ khiến bộ dữ liệu trông như thế này: bao gồm các cột nhóm tuổi, số lượng nam, nữ, và missing.  

```{r, echo=F, warning=F, message=F}
# View the aggregated data
DT::datatable(demo_agg, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Để thiết lập chủ đề cho tháp tuổi, chúng ta sẽ xoay trục dữ liệu sang dạng "dọc" bằng hàm `pivot_longer()` trong package **dplyr**. Đó là bởi vì `ggplot()` thường thích dữ liệu được bố trí ở dạng "dọc", và package **apyramid** đang sử dụng `ggplot()`.  

```{r, warning=F, message=F}
# pivot the aggregated data into long format
demo_agg_long <- demo_agg %>% 
  pivot_longer(
    col = c(f, m, missing_gender),            # cols to elongate
    names_to = "gender",                # name for new col of categories
    values_to = "counts") %>%           # name for new col of counts
  mutate(
    gender = na_if(gender, "missing_gender")) # convert "missing_gender" to NA
``` 

```{r, echo=F, warning=F, message=F}
# View the aggregated data
DT::datatable(demo_agg_long, rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Sau đó sử dụng các đối số `split_by = ` và `count = ` của hàm `age_pyramid()` để chỉ định các cột tương ứng trong bộ dữ liệu:  

```{r, warning=F, message=F}
apyramid::age_pyramid(data = demo_agg_long,
                      age_group = "age_cat5",# column name for age category
                      split_by = "gender",   # column name for gender
                      count = "counts")      # column name for case counts
```

Lưu ý rằng ở trên, thứ tự của "m" và "f" là khác nhau (tháp bị đảo ngược). Để điều chỉnh thự tự, bạn phải định nghĩa lại cột giới trong dữ liệu được tổng hợp thành kiểu Factor và sắp xếp thứ tự như mong muốn. Xem chương [Factors].  




<!-- ======================================================= -->
## `ggplot()` {#demo_pyr_gg}


Sử dụng `ggplot()` cho phép bạn xây dựng tháp tuổi một cách linh hoạt hơn, nhưng đòi hỏi nhiều nỗ lực và hiểu biết hơn về cách hoạt động của `ggplot()`. Việc vô tình mắc sai lầm cũng dễ dàng hơn.  

Để sử dụng `ggplot()` tạo tháp nhân khẩu học, bạn tạo hai biểu đồ cột (cho từng giới tính), chuyển đổi các giá trị trong một biểu đồ thành âm và cuối cùng lật các trục x và y để hiển thị các biểu đồ cột theo chiều dọc, xuất phát điểm của chúng gặp nhau ở chính giữa biểu đồ.  


### Chuẩn bị {.unnumbered}

Cách tiếp cận này sử dụng cột tuổi ở dạng *numeric*, không phải cột nhóm tuổi `age_cat5` dạng *categorical*. Vì vậy, chúng ta cần kiểm tra để đảm bảo rằng kiểu của cột này thực sự là dạng số.  

```{r}
class(linelist$age)
```

Bạn có thể sử dụng logic tương tự như dưới đây để xây dựng một kim tự tháp từ dữ liệu dạng danh mục sử dụng `geom_col()` thay vì `geom_histogram()`.  

<!-- ======================================================= -->
### Xây dựng biểu đồ {.unnumbered} 

Trước tiên, hãy hiểu rằng để tạo một kim tự tháp như vậy bằng cách sử dụng `ggplot()`, cách tiếp cận sẽ là như sau:

* Bên trong hàm `ggplot()`, tạo **hai** biểu đồ histograms sử dụng cột tuổi dạng numeric, tương ứng cho hai nhóm (trong trường hợp này là giới nam và nữ). Để thực hiện việc này, dữ liệu cho mỗi biểu đồ được chỉ định trong các lệnh `geom_histogram()` tương ứng của chúng, với các bộ lọc tương ứng được áp dụng cho bộ dữ liệu `linelist`.    

* Một biểu đồ sẽ có các giá trị dương, trong khi biểu đồ kia sẽ có các giá trị được chuyển thành giá trị âm - điều này tạo ra “kim tự tháp” với giá trị `0` ở giữa biểu đồ. Các giá trị âm được tạo bằng cách sử dụng thuật ngữ đặc biệt của  **ggplot2** là `..count..` và nhân với -1.  

* Lệnh `coord_flip()` chuyển trục X và Y, dẫn đến các đồ thị quay dọc và tạo ra hình kim tự tháp.

* Cuối cùng, các nhãn giá trị trục đếm phải được thay đổi để chúng xuất hiện dưới dạng số "dương" trên cả hai mặt của kim tự tháp (mặc dù các giá trị thực tế ở một mặt là âm). 

Một phiên bản **đơn giản** của biểu đồ, sử dụng hàm `geom_histogram()`, như dưới đây:

```{r, warning=F, message=F}
  # begin ggplot
  ggplot(mapping = aes(x = age, fill = gender)) +
  
  # female histogram
  geom_histogram(data = linelist %>% filter(gender == "f"),
                 breaks = seq(0,85,5),
                 colour = "white") +
  
  # male histogram (values converted to negative)
  geom_histogram(data = linelist %>% filter(gender == "m"),
                 breaks = seq(0,85,5),
                 mapping = aes(y = ..count..*(-1)),
                 colour = "white") +
  
  # flip the X and Y axes
  coord_flip() +
  
  # adjust counts-axis scale
  scale_y_continuous(limits = c(-600, 900),
                     breaks = seq(-600,900,100),
                     labels = abs(seq(-600, 900, 100)))
```

<span style="color: red;">**_NGUY HIỂM:_** Nếu như **giới hạn** của trục counts được thiết lập quá nhỏ, và cột số lượng vượt quá giá trị đó, cột sẽ biến mất hoàn toàn hoặc bị rút ngắn một cách không tự nhiên! Hãy chú ý điều này nếu dữ liệu phân tích thường xuyên được cập nhật. Có thể ngăn chặn điều này bằng cách tự động điều chỉnh các giới hạn trục count cho phù hợp với dữ liệu của bạn, như dưới đây.</span>  

Có nhiều thứ bạn có thể thay đổi/thêm vào phiên bản đơn giản này, bao gồm:  

* Tự động điều chỉnh tỷ lệ trục count cho dữ liệu của bạn (tránh các lỗi được thảo luận trong cảnh báo bên dưới)  
* Chỉ định màu sắc và nhãn chú giải một cách thủ công  

**Chuyển đổi số lượng thành tỷ lệ phần trăm**  

Để chuyển đổi số lượng thành phần trăm (của tổng số), hãy thực hiện điều này với dữ liệu của bạn trước khi vẽ biểu đồ. Dưới đây, chúng ta lấy số lượng của age-gender, sau đó `ungroup()`, và tiếp tục `mutate()` để tạo cột phần trăm mới. Nếu bạn muốn phần trăm theo giới tính, hãy bỏ qua bước hủy nhóm.  


```{r, warning=F, message=F}
# create dataset with proportion of total
pyramid_data <- linelist %>%
  count(age_cat5,
        gender,
        name = "counts") %>% 
  ungroup() %>%                 # ungroup so percents are not by group
  mutate(percent = round(100*(counts / sum(counts, na.rm=T)), digits = 1), 
         percent = case_when(
            gender == "f" ~ percent,
            gender == "m" ~ -percent,     # convert male to negative
            TRUE          ~ NA_real_))    # NA val must by numeric as well
```

Quan trọng là, chúng ta lưu các giá trị lớn nhất vầ nhỏ nhất để chúng ta biết giới hạn của thang đo. Chúng sẽ được sử dụng trong lệnh `ggplot()` dưới đây.    

```{r}
max_per <- max(pyramid_data$percent, na.rm=T)
min_per <- min(pyramid_data$percent, na.rm=T)

max_per
min_per
```

Cuối cùng, chúng ta dùng hàm `ggplot()` trên dữ liệu phần trăm. Chúng ta chỉ rõ `scale_y_continuous()` để mở rộng độ dài được xác định trước theo mỗi hướng (dương và "âm"). Chúng ta sử dụng hàm `floor()` vả `ceiling()` để làm tròn số thập phân theo cách thích hợp (làm tròn xuống hoặc lên).  

```{r, warning=F, message=F}
# begin ggplot
  ggplot()+  # default x-axis is age in years;

  # case data graph
  geom_col(data = pyramid_data,
           mapping = aes(
             x = age_cat5,
             y = percent,
             fill = gender),         
           colour = "white")+       # white around each bar
  
  # flip the X and Y axes to make pyramid vertical
  coord_flip()+
  

  # adjust the axes scales
  # scale_x_continuous(breaks = seq(0,100,5), labels = seq(0,100,5)) +
  scale_y_continuous(
    limits = c(min_per, max_per),
    breaks = seq(from = floor(min_per),                # sequence of values, by 2s
                 to = ceiling(max_per),
                 by = 2),
    labels = paste0(abs(seq(from = floor(min_per),     # sequence of absolute values, by 2s, with "%"
                            to = ceiling(max_per),
                            by = 2)),
                    "%"))+  

  # designate colors and legend labels manually
  scale_fill_manual(
    values = c("f" = "orange",
               "m" = "darkgreen"),
    labels = c("Female", "Male")) +
  
  # label values (remember X and Y flipped now)
  labs(
    title = "Age and gender of cases",
    x = "Age group",
    y = "Percent of total",
    fill = NULL,
    caption = stringr::str_glue("Data are from linelist \nn = {nrow(linelist)} (age or sex missing for {sum(is.na(linelist$gender) | is.na(linelist$age_years))} cases) \nData as of: {format(Sys.Date(), '%d %b %Y')}")) +
  
  # display themes
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.background = element_blank(),
    axis.line = element_line(colour = "black"),
    plot.title = element_text(hjust = 0.5), 
    plot.caption = element_text(hjust=0, size=11, face = "italic")
    )

```



<!-- ======================================================= -->
### So sánh với đường cơ sở  {.unnumbered} 

With the flexibility of `ggplot()`, you can have a second layer of bars in the background that represent the "true" or "baseline" population pyramid. This can provide a nice visualization to compare the observed with the baseline.  

Với sự linh hoạt của `ggplot()`, bạn có thể có lớp thanh thứ hai trong nền đại diện cho tháp dân số “chuẩn” hoặc “đường cơ sở”. Điều này có thể cung cấp khả năng trực quan hóa tốt để so sánh những gì quan sát được với đường cơ sở.

Nhập và xem dữ liệu dân số (xem chương [Tải sách và dữ liệu]):

```{r echo=F}
# import the population demographics data
pop <- rio::import(here::here("data", "standardization", "country_demographics.csv"))
```

```{r eval=F}
# import the population demographics data
pop <- rio::import("country_demographics.csv")
```

```{r, echo=F, warning=F, message=F}
# display the linelist data as a table
DT::datatable(pop, rownames = FALSE, options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )
```


Đầu tiên là một số bước quản lý dữ liệu:  

Ở đây chúng ta sắp xếp lại thứ tự của các danh mục tuổi mà chúng ta muốn chúng xuất hiện. Do một số điểm khác biệt trong cách thực thi `ggplot()`, trong trường hợp cụ thể này, dễ dàng nhất là lưu trữ chúng dưới dạng vectơ ký tự và sử dụng chúng sau này trong hàm vẽ đồ thị.  

```{r}
# record correct age cat levels
age_levels <- c("0-4","5-9", "10-14", "15-19", "20-24",
                "25-29","30-34", "35-39", "40-44", "45-49",
                "50-54", "55-59", "60-64", "65-69", "70-74",
                "75-79", "80-84", "85+")
```

Kết hợp dữ liệu quần thể và dữ liệu trường hợp thông qua hàm `bind_rows()` của package  **dplyr**:  

* Trước tiên, hãy đảm bảo hai bộ dữ liệu có tên cột *giống nhau*, các giá trị nhóm tuổi và giá trị giới tính  
* Làm cho chúng có cấu trúc dữ liệu giống nhau: cột nhóm tuổi, giới tính, số lượng và phần trăm tổng số 
* Gắn chúng lại với nhau, một bộ dữ liệu này ở trên bộ dữ liệu kia (`bind_rows()`)  



```{r, warning=F, message=F}
# create/transform populaton data, with percent of total
########################################################
pop_data <- pop %>% 
  pivot_longer(      # pivot gender columns longer
    cols = c(m, f),
    names_to = "gender",
    values_to = "counts") %>% 
  
  mutate(
    percent  = round(100*(counts / sum(counts, na.rm=T)),1),  # % of total
    percent  = case_when(                                                        
     gender == "f" ~ percent,
     gender == "m" ~ -percent,               # if male, convert % to negative
     TRUE          ~ NA_real_))
```

Xem lại bộ dữ liệu dân số đã thay đổi

```{r, echo=F, warning=F, message=F}
# display the linelist data as a table
DT::datatable(pop_data, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Bây giờ thực hiện tương tự cho bộ linelist. Sẽ hơi khác một chút bởi vì nó bắt đầu với các trường hợp theo hàng, không phải số lượng.  

```{r, warning=F, message=F}
# create case data by age/gender, with percent of total
#######################################################
case_data <- linelist %>%
  count(age_cat5, gender, name = "counts") %>%  # counts by age-gender groups
  ungroup() %>% 
  mutate(
    percent = round(100*(counts / sum(counts, na.rm=T)),1),  # calculate % of total for age-gender groups
    percent = case_when(                                     # convert % to negative if male
      gender == "f" ~ percent,
      gender == "m" ~ -percent,
      TRUE          ~ NA_real_))
```

Xem lại bộ dữ liệu trường hợp đã thay đổi  

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(case_data, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Bây giờ hai data frame sẽ được kết hợp, cái này ở trên cái kia (chúng có cùng tên cột). Chúng ta có thể "đặt tên" cho từng data frame, và sử dụng đối số `.id = ` để tạo một cột mới "data_source" sẽ cho biết dữ liệu có nguồn gốc từ data frame nào. Chúng tôi có thể sử dụng cột này để lọc với hàm `ggplot()`.  



```{r, warning=F, message=F}
# combine case and population data (same column names, age_cat values, and gender values)
pyramid_data <- bind_rows("cases" = case_data, "population" = pop_data, .id = "data_source")
```

Lưu trữ các giá trị phần trăm tối đa và tối thiểu, được sử dụng trong hàm vẽ biểu đồ để xác định phạm vi của biểu đồ (và không cắt ngắn bất kỳ cột nào!)

```{r}
# Define extent of percent axis, used for plot limits
max_per <- max(pyramid_data$percent, na.rm=T)
min_per <- min(pyramid_data$percent, na.rm=T)
```

Bây giờ biểu đồ được tạo bởi `ggplot()` có: 

* Một biểu đồ cột ngang của dữ liệu dan số (rộng hơn, trong suốt)
* Một biểu đồ cột ngang của dữ liệu các trường hợp (nhỏ hơn, đậm hơn)  


```{r, warning=F, message=F}

# begin ggplot
##############
ggplot()+  # default x-axis is age in years;

  # population data graph
  geom_col(
    data = pyramid_data %>% filter(data_source == "population"),
    mapping = aes(
      x = age_cat5,
      y = percent,
      fill = gender),
    colour = "black",                               # black color around bars
    alpha = 0.2,                                    # more transparent
    width = 1)+                                     # full width
  
  # case data graph
  geom_col(
    data = pyramid_data %>% filter(data_source == "cases"), 
    mapping = aes(
      x = age_cat5,                               # age categories as original X axis
      y = percent,                                # % as original Y-axis
      fill = gender),                             # fill of bars by gender
    colour = "black",                               # black color around bars
    alpha = 1,                                      # not transparent 
    width = 0.3)+                                   # half width
  
  # flip the X and Y axes to make pyramid vertical
  coord_flip()+
  
  # manually ensure that age-axis is ordered correctly
  scale_x_discrete(limits = age_levels)+     # defined in chunk above
  
  # set percent-axis 
  scale_y_continuous(
    limits = c(min_per, max_per),                                          # min and max defined above
    breaks = seq(floor(min_per), ceiling(max_per), by = 2),                # from min% to max% by 2 
    labels = paste0(                                                       # for the labels, paste together... 
              abs(seq(floor(min_per), ceiling(max_per), by = 2)), "%"))+                                                  

  # designate colors and legend labels manually
  scale_fill_manual(
    values = c("f" = "orange",         # assign colors to values in the data
               "m" = "darkgreen"),
    labels = c("f" = "Female",
               "m"= "Male"),      # change labels that appear in legend, note order
  ) +

  # plot labels, titles, caption    
  labs(
    title = "Case age and gender distribution,\nas compared to baseline population",
    subtitle = "",
    x = "Age category",
    y = "Percent of total",
    fill = NULL,
    caption = stringr::str_glue("Cases shown on top of country demographic baseline\nCase data are from linelist, n = {nrow(linelist)}\nAge or gender missing for {sum(is.na(linelist$gender) | is.na(linelist$age_years))} cases\nCase data as of: {format(max(linelist$date_onset, na.rm=T), '%d %b %Y')}")) +
  
  # optional aesthetic themes
  theme(
    legend.position = "bottom",                             # move legend to bottom
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.background = element_blank(),
    axis.line = element_line(colour = "black"),
    plot.title = element_text(hjust = 0), 
    plot.caption = element_text(hjust=0, size=11, face = "italic"))

```


<!-- ======================================================= -->
## Thang đo Likert {}

Các kỹ thuật được sử dụng để tạo tháp dân số với `ggplot()` cũng có thể được sử dụng để lập các biểu đồ của dữ liệu khảo sát từ thang đo Likert.  

```{r, eval=F, echo=F}
data_raw <- import("P:/Shared/equateur_mve_2020/lessons learned/Ebola After-Action Survey - HQ epi team (form responses).csv")


likert_data <- data_raw %>% 
  select(2, 4:11) %>% 
  rename(status = 1,
         Q1 = 2,
         Q2 = 3,
            Q3 = 4,
            Q4 = 5,
            Q5 = 6,
            Q6 = 7,
            Q7 = 8,
            Q8 = 9) %>% 
  mutate(status = case_when(
           stringr::str_detect(status, "Mar") ~ "Senior",
           stringr::str_detect(status, "Jan") ~ "Intermediate",
           stringr::str_detect(status, "Feb") ~ "Junior",
           TRUE ~ "Senior")) %>% 
  mutate(Q4 = recode(Q4, "Not applicable" = "Very Poor"))

table(likert_data$status)

rio::export(likert_data, here::here("data", "likert_data.csv"))
```

Nhập dữ liệu (xem chương [Tải sách và dữ liệu] nếu cần).  

```{r echo=F}
# import the likert survey response data
likert_data <- rio::import(here::here("data", "likert_data.csv"))
```

```{r, eval=F}
# import the likert survey response data
likert_data <- rio::import("likert_data.csv")
```

Bắt đầu với dữ liệu giống như sau, với một biến phân loại từng người trả lời (`status`) và câu trả lời của họ cho 8 câu hỏi trên thang điểm Likert 4 mức độ ("Rất kém", "Kém", "Tốt", "Rất tốt").  

```{r, echo=F, message=FALSE}
# display the linelist data as a table
DT::datatable(likert_data, rownames = FALSE, filter="top", options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )
```

Đầu tiên là một vài bước quản lý số liệu:  

* Xoay trục dữ liệu dài hơn
* Tạo cột mới `direction` tùy thuộc vào việc phản hồi "tích cực" hay "tiêu cực"
* Thiết lập thứ bậc kiểu factor cho cột `status` và cột `Response`  
* Lưu trữ giá trị đếm tối đa để các giới hạn của biểu đồ là phù hợp  


```{r, warning=F, message=F}
melted <- likert_data %>% 
  pivot_longer(
    cols = Q1:Q8,
    names_to = "Question",
    values_to = "Response") %>% 
  mutate(
    
    direction = case_when(
      Response %in% c("Poor","Very Poor")  ~ "Negative",
      Response %in% c("Good", "Very Good") ~ "Positive",
      TRUE                                 ~ "Unknown"),
    
    status = fct_relevel(status, "Junior", "Intermediate", "Senior"),
    
    # must reverse 'Very Poor' and 'Poor' for ordering to work
    Response = fct_relevel(Response, "Very Good", "Good", "Very Poor", "Poor")) 

# get largest value for scale limits
melted_max <- melted %>% 
  count(status, Question) %>% # get counts
  pull(n) %>%                 # column 'n'
  max(na.rm=T)                # get max
```


Bây giờ hãy cùng vẽ biểu đồ. Tương tự các tháp tuổi ở trên, chúng ta đang tạo hai biểu đồ thanh và đảo các giá trị của một trong số chúng thành âm. 

Chúng ta sử dụng hàm `geom_bar()` bởi vì dữ liệu của chúng ta mỗi quan sát nằm trên một hàng, không phải là số lượng tổng hợp. Chúng ta sử dụng thuật ngữ đặc biệt của **ggplot2** là `..count..` ở một biểu đồ thanh để đảo ngược các giá trị thành âm(*-1), sau đó chúng ta thiết lập `position = "stack"` để các giá trị xếp chồng lên nhau.  

```{r, warning=F, message=F}
# make plot
ggplot()+
     
  # bar graph of the "negative" responses 
     geom_bar(
       data = melted %>% filter(direction == "Negative"),
       mapping = aes(
         x = status,
         y = ..count..*(-1),    # counts inverted to negative
         fill = Response),
       color = "black",
       closed = "left",
       position = "stack")+
     
     # bar graph of the "positive responses
     geom_bar(
       data = melted %>% filter(direction == "Positive"),
       mapping = aes(
         x = status,
         fill = Response),
       colour = "black",
       closed = "left",
       position = "stack")+
     
     # flip the X and Y axes
     coord_flip()+
  
     # Black vertical line at 0
     geom_hline(yintercept = 0, color = "black", size=1)+
     
    # convert labels to all positive numbers
    scale_y_continuous(
      
      # limits of the x-axis scale
      limits = c(-ceiling(melted_max/10)*11,    # seq from neg to pos by 10, edges rounded outward to nearest 5
                 ceiling(melted_max/10)*10),   
      
      # values of the x-axis scale
      breaks = seq(from = -ceiling(melted_max/10)*10,
                   to = ceiling(melted_max/10)*10,
                   by = 10),
      
      # labels of the x-axis scale
      labels = abs(unique(c(seq(-ceiling(melted_max/10)*10, 0, 10),
                            seq(0, ceiling(melted_max/10)*10, 10))))) +
     
    # color scales manually assigned 
    scale_fill_manual(
      values = c("Very Good"  = "green4", # assigns colors
                "Good"      = "green3",
                "Poor"      = "yellow",
                "Very Poor" = "red3"),
      breaks = c("Very Good", "Good", "Poor", "Very Poor"))+ # orders the legend
     
    
     
    # facet the entire plot so each question is a sub-plot
    facet_wrap( ~ Question, ncol = 3)+
     
    # labels, titles, caption
    labs(
      title = str_glue("Likert-style responses\nn = {nrow(likert_data)}"),
      x = "Respondent status",
      y = "Number of responses",
      fill = "")+

     # display adjustments 
     theme_minimal()+
     theme(axis.text = element_text(size = 12),
           axis.title = element_text(size = 14, face = "bold"),
           strip.text = element_text(size = 14, face = "bold"),  # facet sub-titles
           plot.title = element_text(size = 20, face = "bold"),
           panel.background = element_rect(fill = NA, color = "black")) # black box around each facet
```


<!-- ======================================================= -->
## Nguồn {}

[apyramid documentation](https://cran.r-project.org/web/packages/apyramid/vignettes/intro.html)



```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/age_pyramid.Rmd-->


# Biểu đồ nhiệt {#heatmaps}  


Heat plots, also known as "heat maps" or "heat tiles", can be useful visualizations when trying to display 3 variables (x-axis, y-axis, and fill). Below we demonstrate two examples:  

* A visual matrix of transmission events by age ("who infected whom")  
* Tracking reporting metrics across many facilities/jurisdictions over time  


```{r, out.width = c('50%', '50%'), fig.show='hold', warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "transmission_matrix.png"))

knitr::include_graphics(here::here("images", "heat_tile.png"))

```





<!-- ======================================================= -->
## Preparation { }

### Load packages {.unnumbered}  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  tidyverse,       # data manipulation and visualization
  rio,             # importing data 
  lubridate        # working with dates
  )
```

**Datasets**  

This page utilizes the case linelist of a simulated outbreak for the transmission matrix section, and a separate dataset of daily malaria case counts by facility for the metrics tracking section. They are loaded and cleaned in their individual sections.  







## Transmission matrix  

Heat tiles can be useful to visualize matrices. One example is to display "who-infected-whom" in an outbreak. This assumes that you have information on transmission events.  

Note that the [Contact tracing] page contains another example of making a heat tile contact matrix, using a different (perhaps more simple) dataset where the ages of cases and their sources are neatly aligned in the same row of the data frame. This same data is used to make a *density* map in the [ggplot tips] page. This example below begins from a case linelist and so involves considerable data manipulation prior to achieving a plotable data frame. So there are many scenarios to chose from...  


We begin from the case linelist of a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Import your data with the `import()` function from the **rio** package (it accepts many file types like .xlsx, .rds, .csv - see the [Import and export] page for details).  


The first 50 rows of the linelist are shown below for demonstration:  


```{r, echo=F}
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```


```{r, eval=F}
linelist <- import("linelist_cleaned.rds")
```


In this linelist:  

* There is one row per case, as identified by `case_id`  
* There is a later column `infector` that contains the `case_id` of the *infector*, who is also a case in the linelist  


```{r message=FALSE, echo=F}
# display the population as a table
DT::datatable(head(linelist, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



### Data preparation {.unnumbered}  

**Objective**: We need to achieve a "long"-style data frame that contains one row per possible age-to-age transmission route, with a numeric column containing that row's proportion of all observed transmission events in the linelist.  

This will take several data manuipulation steps to achieve:  


#### Make cases data frame {.unnumbered} 

To begin, we create a data frame of the cases, their ages, and their infectors - we call the data frame `case_ages`. The first 50 rows are displayed below.  

```{r}
case_ages <- linelist %>% 
  select(case_id, infector, age_cat) %>% 
  rename("case_age_cat" = "age_cat")
```

```{r message=FALSE, echo=F}
# display the shapefile as a table
DT::datatable(head(case_ages, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

#### Make infectors data frame {.unnumbered}  

Next, we create a data frame of the infectors - at the moment it consists of a single column. These are the infector IDs from the linelist. Not every case has a known infector, so we remove missing values. The first 50 rows are displayed below.  


```{r}
infectors <- linelist %>% 
  select(infector) %>% 
  drop_na(infector)
```

```{r message=FALSE, echo=F}
# display the shapefile as a table
DT::datatable(head(infectors, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Next, we use joins to procure the ages of the infectors. This is not simple, because in the `linelist`, the infector's ages are not listed as such. We achieve this result by joining the case `linelist` to the infectors. We begin with the infectors, and `left_join()` (add) the case `linelist` such that the `infector` id column left-side "baseline" data frame joins to the `case_id` column in the right-side `linelist` data frame.  

Thus, the data from the infector's case record in the linelist (including age) is added to the infector row. The 50 first rows are displayed below.  

```{r}
infector_ages <- infectors %>%             # begin with infectors
  left_join(                               # add the linelist data to each infector  
    linelist,
    by = c("infector" = "case_id")) %>%    # match infector to their information as a case
  select(infector, age_cat) %>%            # keep only columns of interest
  rename("infector_age_cat" = "age_cat")   # rename for clarity
```

```{r message=FALSE, echo=F}
# display the shapefile as a table
DT::datatable(head(infector_ages, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Then, we combine the cases and their ages with the infectors and their ages. Each of these data frame has the column `infector`, so it is used for the join. The first rows are displayed below:    

```{r}
ages_complete <- case_ages %>%  
  left_join(
    infector_ages,
    by = "infector") %>%        # each has the column infector
  drop_na()                     # drop rows with any missing data
```


```{r message=FALSE, echo=F}
# display the shapefile as a table
DT::datatable(head(ages_complete, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Below, a simple cross-tabulation of counts between the case and infector age groups. Labels added for clarity.  

```{r}
table(cases = ages_complete$case_age_cat,
      infectors = ages_complete$infector_age_cat)
```


We can convert this table to a data frame with `data.frame()` from **base** R, which also automatically converts it to "long" format, which is desired for the `ggplot()`. The first rows are shown below.  

```{r}
long_counts <- data.frame(table(
    cases     = ages_complete$case_age_cat,
    infectors = ages_complete$infector_age_cat))
```

```{r message=FALSE, echo=F}
# display the shapefile as a table
DT::datatable(head(long_counts, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


Now we do the same, but apply `prop.table()` from **base** R to the table so instead of counts we get proportions of the total. The first 50 rows are shown below.    

```{r}
long_prop <- data.frame(prop.table(table(
    cases = ages_complete$case_age_cat,
    infectors = ages_complete$infector_age_cat)))
```

```{r message=FALSE, echo=F}
# display the shapefile as a table
DT::datatable(head(long_prop, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```




### Create heat plot {.unnumbered}  

Now finally we can create the heat plot with **ggplot2** package, using the `geom_tile()` function. See the [ggplot tips] page to learn more extensively about color/fill scales, especially the `scale_fill_gradient()` function.  

* In the aesthetics `aes()` of `geom_tile()` set the x and y as the case age and infector age  
* Also in `aes()` set the argument `fill = ` to the `Freq` column - this is the value that will be converted to a tile color  
* Set a scale color with `scale_fill_gradient()` - you can specify the high/low colors  
  * Note that `scale_color_gradient()` is different! In this case you want the fill  
* Because the color is made via "fill", you can use the `fill = ` argument in `labs()` to change the legend title  

```{r}
ggplot(data = long_prop)+       # use long data, with proportions as Freq
  geom_tile(                    # visualize it in tiles
    aes(
      x = cases,         # x-axis is case age
      y = infectors,     # y-axis is infector age
      fill = Freq))+            # color of the tile is the Freq column in the data
  scale_fill_gradient(          # adjust the fill color of the tiles
    low = "blue",
    high = "orange")+
  labs(                         # labels
    x = "Case age",
    y = "Infector age",
    title = "Who infected whom",
    subtitle = "Frequency matrix of transmission events",
    fill = "Proportion of all\ntranmsission events"     # legend title
  )
  
```



<!-- ======================================================= -->
## Reporting metrics over time { }

Often in public health, one objective is to assess trends over time for many entities (facilities, jurisdictions, etc.). One way to visualize such trends over time is a heat plot where the x-axis is time and on the y-axis are the many entities.  



### Data preparation {.unnumbered}

We begin by importing a dataset of daily malaria reports from many facilities. The reports contain a date, province, district, and malaria counts. See the page on [Download handbook and data] for information on how to download these data. Below are the first 30 rows:  

```{r, echo=F}
facility_count_data <- rio::import(here::here("data", "malaria_facility_count_data.rds")) %>% 
  select(location_name, data_date, District, malaria_tot)
```

```{r, eval=F}
facility_count_data <- import("malaria_facility_count_data.rds")
```


```{r, echo=F}
DT::datatable(head(facility_count_data,30), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```


#### Aggregate and summarize {.unnumbered}

**The objective in this example** is to transform the daily facility *total* malaria case counts (seen in previous tab) into *weekly* summary statistics of facility reporting performance - in this case *the proportion of days per week that the facility reported any data*. For this example we will show data only for **Spring District**.  

To achieve this we will do the following data management steps:  

1) Filter the data as appropriate (by place, date)  
2) Create a week column using `floor_date()` from package **lubridate**  
    + This function returns the start-date of a given date's week, using a specified start date of each week (e.g. "Mondays")  
3) The data are grouped by columns "location" and "week" to create analysis units of "facility-week"  
4) The function `summarise()` creates new columns to reflecting summary statistics per facility-week group:  
    + Number of days per week (7 - a static value)  
    + Number of reports received from the facility-week (could be more than 7!)  
    + Sum of malaria cases reported by the facility-week (just for interest)  
    + Number of *unique* days in the facility-week for which there is data reported  
    + **Percent of the 7 days per facility-week for which data was reported**  
5) The data frame is joined with `right_join()` to a comprehensive list of all possible facility-week combinations, to make the dataset complete. The matrix of all possible combinations is created by applying `expand()` to those two columns of the data frame as it is at that moment in the pipe chain (represented by `.`). Because a `right_join()` is used, all rows in the `expand()` data frame are kept, and added to `agg_weeks` if necessary. These new rows appear with `NA` (missing) summarized values.  


Below we demonstrate step-by-step:  

```{r, message=FALSE, warning=FALSE}
# Create weekly summary dataset
agg_weeks <- facility_count_data %>% 
  
  # filter the data as appropriate
  filter(
    District == "Spring",
    data_date < as.Date("2020-08-01")) 
```

Now the dataset has ` nrow(agg_weeks)` rows, when it previously had ` nrow(facility_count_data)`.  

Next we create a `week` column reflecting the start date of the week for each record. This is achieved with the **lubridate** package and the function `floor_date()`, which is set to "week" and for the weeks to begin on Mondays (day 1 of the week - Sundays would be 7). The top rows are shown below.  

```{r}
agg_weeks <- agg_weeks %>% 
  # Create week column from data_date
  mutate(
    week = lubridate::floor_date(                     # create new column of weeks
      data_date,                                      # date column
      unit = "week",                                  # give start of the week
      week_start = 1))                                # weeks to start on Mondays 
```

The new week column can be seen on the far right of the data frame  

```{r, echo=F}
DT::datatable(head(agg_weeks,30), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```

Now we group the data into facility-weeks and summarise them to produce statistics per facility-week. See the page on [Descriptive tables] for tips. The grouping itself doesn't change the data frame, but it impacts how the subsequent summary statistics are calculated.  

The top rows are shown below. Note how the columns have completely changed to reflect the desired summary statistics. Each row reflects one facility-week. 

```{r, warning=F, message=F}
agg_weeks <- agg_weeks %>%   

  # Group into facility-weeks
  group_by(location_name, week) %>%
  
  # Create summary statistics columns on the grouped data
  summarize(
    n_days          = 7,                                          # 7 days per week           
    n_reports       = dplyr::n(),                                 # number of reports received per week (could be >7)
    malaria_tot     = sum(malaria_tot, na.rm = T),                # total malaria cases reported
    n_days_reported = length(unique(data_date)),                  # number of unique days reporting per week
    p_days_reported = round(100*(n_days_reported / n_days)))      # percent of days reporting
```

```{r, echo=F}
DT::datatable(head(agg_weeks,30), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```

Finally, we run the command below to ensure that ALL possible facility-weeks are present in the data, even if they were missing before.  

We are using a `right_join()` on itself (the dataset is represented by ".") but having been expanded to include all possible combinations of the columns `week` and `location_name`. See documentation on the `expand()` function in the page on [Pivoting]. Before running this code the dataset contains ` nrow(agg_weeks)` rows.   

```{r, message=F, warning=F}
# Create data frame of every possible facility-week
expanded_weeks <- agg_weeks %>% 
  mutate(week = as.factor(week)) %>%         # convert date to a factor so expand() works correctly
  tidyr::expand(., week, location_name) %>%  # expand data frame to include all possible facility-week combinations
                                             # note: "." represents the dataset at that moment in the pipe chain
  mutate(week = as.Date(week))               # re-convert week to class Date so the subsequent right_join works
```

Here is `expanded_weeks`:  

```{r, echo=F}
DT::datatable(expanded_weeks, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```

Before running this code, `agg_weeks` contains ` nrow(agg_weeks)` rows.   

```{r}
# Use a right-join with the expanded facility-week list to fill-in the missing gaps in the data
agg_weeks <- agg_weeks %>%      
  right_join(expanded_weeks) %>%                            # Ensure every possible facility-week combination appears in the data
  mutate(p_days_reported = replace_na(p_days_reported, 0))  # convert missing values to 0                           
```

After running this code, `agg_weeks` contains ` nrow(agg_weeks)` rows.   


<!-- ======================================================= -->
### Create heat plot {.unnumbered}


The `ggplot()` is made using `geom_tile()` from the **ggplot2** package:  

* Weeks on the x-axis is transformed to dates, allowing use of `scale_x_date()`  
* `location_name` on the y-axis will show all facility names  
* The `fill` is `p_days_reported`, the performance for that facility-week (numeric)  
* `scale_fill_gradient()` is used on the numeric fill, specifying colors for high, low, and `NA`  
* `scale_x_date()` is used on the x-axis specifying labels every 2 weeks and their format  
* Display themes and labels can be adjusted as necessary




<!-- ======================================================= -->
### Basic {.unnumbered}  

A basic heat plot is produced below, using the default colors, scales, etc. As explained above, within the `aes()` for `geom_tile()` you must provide an x-axis column, y-axis column, **and** a column for the the `fill = `. The fill is the numeric value that presents as tile color.  

```{r}
ggplot(data = agg_weeks)+
  geom_tile(
    aes(x = week,
        y = location_name,
        fill = p_days_reported))
```

### Cleaned plot {.unnumbered}

We can make this plot look better by adding additional **ggplot2** functions, as shown below. See the page on [ggplot tips] for details.  

```{r, message=FALSE, warning=FALSE}
ggplot(data = agg_weeks)+ 
  
  # show data as tiles
  geom_tile(
    aes(x = week,
        y = location_name,
        fill = p_days_reported),      
    color = "white")+                 # white gridlines
  
  scale_fill_gradient(
    low = "orange",
    high = "darkgreen",
    na.value = "grey80")+
  
  # date axis
  scale_x_date(
    expand = c(0,0),             # remove extra space on sides
    date_breaks = "2 weeks",     # labels every 2 weeks
    date_labels = "%d\n%b")+     # format is day over month (\n in newline)
  
  # aesthetic themes
  theme_minimal()+                                  # simplify background
  
  theme(
    legend.title = element_text(size=12, face="bold"),
    legend.text  = element_text(size=10, face="bold"),
    legend.key.height = grid::unit(1,"cm"),           # height of legend key
    legend.key.width  = grid::unit(0.6,"cm"),         # width of legend key
    
    axis.text.x = element_text(size=12),              # axis text size
    axis.text.y = element_text(vjust=0.2),            # axis text alignment
    axis.ticks = element_line(size=0.4),               
    axis.title = element_text(size=12, face="bold"),  # axis title size and bold
    
    plot.title = element_text(hjust=0,size=14,face="bold"),  # title right-aligned, large, bold
    plot.caption = element_text(hjust = 0, face = "italic")  # caption right-aligned and italic
    )+
  
  # plot labels
  labs(x = "Week",
       y = "Facility name",
       fill = "Reporting\nperformance (%)",           # legend title, because legend shows fill
       title = "Percent of days per week that facility reported data",
       subtitle = "District health facilities, May-July 2020",
       caption = "7-day weeks beginning on Mondays.")
```





<!-- ======================================================= -->
### Ordered y-axis {.unnumbered}  

Currently, the facilities are ordered "alpha-numerically" from the bottom to the top. If you want to adjust the order the y-axis facilities, convert them to class factor and provide the order. See the page on [Factors] for tips.  

Since there are many facilities and we don't want to write them all out, we will try another approach - ordering the facilities in a data frame and using the resulting column of names as the factor level order. Below, the column `location_name` is converted to a factor, and the order of its levels is set based on the total number of reporting days filed by the facility across the whole time-span.  

To do this, we create a data frame which represents the total number of reports per facility, arranged in ascending order. We can use this vector to order the factor levels in the plot.   

```{r}
facility_order <- agg_weeks %>% 
  group_by(location_name) %>% 
  summarize(tot_reports = sum(n_days_reported, na.rm=T)) %>% 
  arrange(tot_reports) # ascending order
```

See the data frame below:  

```{r, echo=F}
DT::datatable(facility_order, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```




Now use a column from the above data frame (`facility_order$location_name`) to be the order of the factor levels of `location_name` in the data frame `agg_weeks`:  

```{r, warning=F, message=F}
# load package 
pacman::p_load(forcats)

# create factor and define levels manually
agg_weeks <- agg_weeks %>% 
  mutate(location_name = fct_relevel(
    location_name, facility_order$location_name)
    )
```

And now the data are re-plotted, with location_name being an ordered factor:  

```{r, message=FALSE, warning=FALSE}
ggplot(data = agg_weeks)+ 
  
  # show data as tiles
  geom_tile(
    aes(x = week,
        y = location_name,
        fill = p_days_reported),      
    color = "white")+                 # white gridlines
  
  scale_fill_gradient(
    low = "orange",
    high = "darkgreen",
    na.value = "grey80")+
  
  # date axis
  scale_x_date(
    expand = c(0,0),             # remove extra space on sides
    date_breaks = "2 weeks",     # labels every 2 weeks
    date_labels = "%d\n%b")+     # format is day over month (\n in newline)
  
  # aesthetic themes
  theme_minimal()+                                  # simplify background
  
  theme(
    legend.title = element_text(size=12, face="bold"),
    legend.text  = element_text(size=10, face="bold"),
    legend.key.height = grid::unit(1,"cm"),           # height of legend key
    legend.key.width  = grid::unit(0.6,"cm"),         # width of legend key
    
    axis.text.x = element_text(size=12),              # axis text size
    axis.text.y = element_text(vjust=0.2),            # axis text alignment
    axis.ticks = element_line(size=0.4),               
    axis.title = element_text(size=12, face="bold"),  # axis title size and bold
    
    plot.title = element_text(hjust=0,size=14,face="bold"),  # title right-aligned, large, bold
    plot.caption = element_text(hjust = 0, face = "italic")  # caption right-aligned and italic
    )+
  
  # plot labels
  labs(x = "Week",
       y = "Facility name",
       fill = "Reporting\nperformance (%)",           # legend title, because legend shows fill
       title = "Percent of days per week that facility reported data",
       subtitle = "District health facilities, May-July 2020",
       caption = "7-day weeks beginning on Mondays.")
```





<!-- ======================================================= -->
### Display values {.unnumbered}  


You can add a `geom_text()` layer on top of the tiles, to display the actual numbers of each tile. Be aware this may not look pretty if you have many small tiles!  

The following code has been added: `geom_text(aes(label = p_days_reported))`. This adds text onto every tile. The text displayed is the value assigned to the argument `label = `, which in this case has been set to the same numeric column `p_days_reported` that is also used to create the color gradient.  



  
```{r, message=FALSE, warning=FALSE}
ggplot(data = agg_weeks)+ 
  
  # show data as tiles
  geom_tile(
    aes(x = week,
        y = location_name,
        fill = p_days_reported),      
    color = "white")+                 # white gridlines
  
  # text
  geom_text(
    aes(
      x = week,
      y = location_name,
      label = p_days_reported))+      # add text on top of tile
  
  # fill scale
  scale_fill_gradient(
    low = "orange",
    high = "darkgreen",
    na.value = "grey80")+
  
  # date axis
  scale_x_date(
    expand = c(0,0),             # remove extra space on sides
    date_breaks = "2 weeks",     # labels every 2 weeks
    date_labels = "%d\n%b")+     # format is day over month (\n in newline)
  
  # aesthetic themes
  theme_minimal()+                                    # simplify background
  
  theme(
    legend.title = element_text(size=12, face="bold"),
    legend.text  = element_text(size=10, face="bold"),
    legend.key.height = grid::unit(1,"cm"),           # height of legend key
    legend.key.width  = grid::unit(0.6,"cm"),         # width of legend key
    
    axis.text.x = element_text(size=12),              # axis text size
    axis.text.y = element_text(vjust=0.2),            # axis text alignment
    axis.ticks = element_line(size=0.4),               
    axis.title = element_text(size=12, face="bold"),  # axis title size and bold
    
    plot.title = element_text(hjust=0,size=14,face="bold"),  # title right-aligned, large, bold
    plot.caption = element_text(hjust = 0, face = "italic")  # caption right-aligned and italic
    )+
  
  # plot labels
  labs(x = "Week",
       y = "Facility name",
       fill = "Reporting\nperformance (%)",           # legend title, because legend shows fill
       title = "Percent of days per week that facility reported data",
       subtitle = "District health facilities, May-July 2020",
       caption = "7-day weeks beginning on Mondays.")
```




<!-- ======================================================= -->
## Resources { }

[scale_fill_gradient()](https://ggplot2.tidyverse.org/reference/scale_gradient.html)  

[R graph gallery - heatmap](https://ggplot2.tidyverse.org/reference/scale_gradient.html)  




```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/heatmaps.Rmd-->


# Sơ đồ và biểu đồ {#diagrams}  



```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "flow_chart.png"))
knitr::include_graphics(here::here("images", "sankey_diagram.png"))
```


This page covers code to produce:  

* Flow diagrams using **DiagrammeR** and the DOT language  
* Alluvial/Sankey diagrams  
* Event timelines  

<!-- * DAGs (Directed Acyclic Graphs)   -->
<!-- * GANTT charts   -->


<!-- ======================================================= -->
## Preparation { }

### Load packages {.unnumbered}  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  DiagrammeR,     # for flow diagrams
  networkD3,      # For alluvial/Sankey diagrams
  tidyverse)      # data management and visualization
```

### Import data {.unnumbered}  

Most of the content in this page does not require a dataset. However, in the Sankey diagram section, we will use the case linelist from a simulated Ebola epidemic. If you want to follow along for this part, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Import data with the `import()` function from the **rio** package (it handles many file types like .xlsx, .csv, .rds - see the [Import and export] page for details).  

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.rds")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



<!-- ======================================================= -->
## Flow diagrams { }

One can use the R package **DiagrammeR** to create charts/flow charts. They can be static, or they can adjust somewhat dynamically based on changes in a dataset.  

**Tools**  

The function `grViz()` is used to create a "Graphviz" diagram. This function accepts a *character string input containing instructions* for making the diagram. Within that string, the instructions are written in a different language, called [DOT](https://graphviz.org/doc/info/lang.html) - it is quite easy to learn the basics.  

**Basic structure**  

1) Open the instructions `grViz("`  
2) Specify directionality and name of the graph, and open brackets, e.g. `digraph my_flow_chart {`
3) Graph statement (layout, rank direction)  
4) Nodes statements (create nodes)
5) Edges statements (gives links between nodes)  
6) Close the instructions `}")`  

### Simple examples {.unnumbered} 

Below are two simple examples  

A very minimal example:  

```{r out.width='50%'}
# A minimal plot
DiagrammeR::grViz("digraph {
  
graph[layout = dot, rankdir = LR]

a
b
c

a -> b -> c
}")
```

An example with perhaps a bit more applied public health context:  

```{r out.width='50%'}
grViz("                           # All instructions are within a large character string
digraph surveillance_diagram {    # 'digraph' means 'directional graph', then the graph name 
  
  # graph statement
  #################
  graph [layout = dot,
         rankdir = TB,
         overlap = true,
         fontsize = 10]
  
  # nodes
  #######
  node [shape = circle,           # shape = circle
       fixedsize = true
       width = 1.3]               # width of circles
  
  Primary                         # names of nodes
  Secondary
  Tertiary

  # edges
  #######
  Primary   -> Secondary [label = ' case transfer']
  Secondary -> Tertiary [label = ' case transfer']
}
")
```

### Syntax  {.unnumbered}

**Basic syntax**  

Node names, or edge statements, can be separated with spaces, semicolons, or newlines.  

**Rank direction**  

A plot can be re-oriented to move left-to-right by adjusting the `rankdir` argument within the graph statement. The default is TB (top-to-bottom), but it can be LR (left-to-right), RL, or BT.  

**Node names**  

Node names can be single words, as in the simple example above. To use multi-word names or special characters (e.g. parentheses, dashes), put the node name within single quotes (' '). It may be easier to have a short node name, and assign a *label*, as shown below within brackets [ ]. If you want to have a newline within the node's name, you must do it via a label - use `\n` in the node label within single quotes, as shown below.  

**Subgroups**  
Within edge statements, subgroups can be created on either side of the edge with curly brackets ({ }). The edge then applies to all nodes in the bracket - it is a shorthand.  


**Layouts**  

* dot (set `rankdir` to either TB, LR, RL, BT, )
* neato  
* twopi  
* circo  


**Nodes - editable attributes**  

* `label` (text, in single quotes if multi-word)  
* `fillcolor` (many possible colors)  
* `fontcolor`  
* `alpha` (transparency 0-1)  
* `shape` (ellipse, oval, diamond, egg, plaintext, point, square, triangle)  
* `style`  
* `sides`  
* `peripheries`  
* `fixedsize` (h x w)  
* `height`  
* `width`  
* `distortion`  
* `penwidth` (width of shape border)  
* `x` (displacement left/right)  
* `y` (displacement up/down)  
* `fontname`  
* `fontsize`  
* `icon`  


**Edges - editable attributes**  

* `arrowsize`  
* `arrowhead` (normal, box, crow, curve, diamond, dot, inv, none, tee, vee)  
* `arrowtail`  
* `dir` (direction, )  
* `style` (dashed, ...)  
* `color`  
* `alpha`  
* `headport` (text in front of arrowhead)  
* `tailport` (text in behind arrowtail)  
* `fontname`  
* `fontsize`  
* `fontcolor`  
* `penwidth` (width of arrow)  
* `minlen` (minimum length)

**Color names**: hexadecimal values or 'X11' color names, see [here for X11 details](http://rich-iannone.github.io/DiagrammeR/graphviz_and_mermaid.html) 


### Complex examples  {.unnumbered}

The example below expands on the surveillance_diagram, adding complex node names, grouped edges, colors and styling


```
DiagrammeR::grViz("               # All instructions are within a large character string
digraph surveillance_diagram {    # 'digraph' means 'directional graph', then the graph name 
  
  # graph statement
  #################
  graph [layout = dot,
         rankdir = TB,            # layout top-to-bottom
         fontsize = 10]
  

  # nodes (circles)
  #################
  node [shape = circle,           # shape = circle
       fixedsize = true
       width = 1.3]                      
  
  Primary   [label = 'Primary\nFacility'] 
  Secondary [label = 'Secondary\nFacility'] 
  Tertiary  [label = 'Tertiary\nFacility'] 
  SC        [label = 'Surveillance\nCoordination',
             fontcolor = darkgreen] 
  
  # edges
  #######
  Primary   -> Secondary [label = ' case transfer',
                          fontcolor = red,
                          color = red]
  Secondary -> Tertiary [label = ' case transfer',
                          fontcolor = red,
                          color = red]
  
  # grouped edge
  {Primary Secondary Tertiary} -> SC [label = 'case reporting',
                                      fontcolor = darkgreen,
                                      color = darkgreen,
                                      style = dashed]
}
")
```


```{r out.width='50%', echo=F}
DiagrammeR::grViz("               # All instructions are within a large character string
digraph surveillance_diagram {    # 'digraph' means 'directional graph', then the graph name 
  
  # graph statement
  #################
  graph [layout = dot,
         rankdir = TB,            # layout top-to-bottom
         fontsize = 10]
  

  # nodes (circles)
  #################
  node [shape = circle,           # shape = circle
       fixedsize = true
       width = 1.3]                      
  
  Primary   [label = 'Primary\nFacility'] 
  Secondary [label = 'Secondary\nFacility'] 
  Tertiary  [label = 'Tertiary\nFacility'] 
  SC        [label = 'Surveillance\nCoordination',
             fontcolor = darkgreen] 
  
  # edges
  #######
  Primary   -> Secondary [label = 'case transfer',
                          fontcolor = red,
                          color = red]
  Secondary -> Tertiary [label = 'case transfer',
                          fontcolor = red,
                          color = red]
  
  # grouped edge
  {Primary Secondary Tertiary} -> SC [label = 'case reporting',
                                      fontcolor = darkgreen,
                                      color = darkgreen,
                                      style = dashed]
}
")
```

**Sub-graph clusters**  

To group nodes into boxed clusters, put them within the same named subgraph (`subgraph name {}`). To have each subgraph identified within a bounding box, begin the name of the subgraph with "cluster", as shown with the 4 boxes below.  

```
DiagrammeR::grViz("             # All instructions are within a large character string
digraph surveillance_diagram {  # 'digraph' means 'directional graph', then the graph name 
  
  # graph statement
  #################
  graph [layout = dot,
         rankdir = TB,            
         overlap = true,
         fontsize = 10]
  

  # nodes (circles)
  #################
  node [shape = circle,                  # shape = circle
       fixedsize = true
       width = 1.3]                      # width of circles
  
  subgraph cluster_passive {
    Primary   [label = 'Primary\nFacility'] 
    Secondary [label = 'Secondary\nFacility'] 
    Tertiary  [label = 'Tertiary\nFacility'] 
    SC        [label = 'Surveillance\nCoordination',
               fontcolor = darkgreen] 
  }
  
  # nodes (boxes)
  ###############
  node [shape = box,                     # node shape
        fontname = Helvetica]            # text font in node
  
  subgraph cluster_active {
    Active [label = 'Active\nSurveillance'] 
    HCF_active [label = 'HCF\nActive Search']
  }
  
  subgraph cluster_EBD {
    EBS [label = 'Event-Based\nSurveillance (EBS)'] 
    'Social Media'
    Radio
  }
  
  subgraph cluster_CBS {
    CBS [label = 'Community-Based\nSurveillance (CBS)']
    RECOs
  }

  
  # edges
  #######
  {Primary Secondary Tertiary} -> SC [label = 'case reporting']

  Primary   -> Secondary [label = 'case transfer',
                          fontcolor = red]
  Secondary -> Tertiary [label = 'case transfer',
                          fontcolor = red]
  
  HCF_active -> Active
  
  {'Social Media' Radio} -> EBS
  
  RECOs -> CBS
}
")

```


```{r out.width='120%', echo=F}
DiagrammeR::grViz("             # All instructions are within a large character string
digraph surveillance_diagram {  # 'digraph' means 'directional graph', then the graph name 
  
  # graph statement
  #################
  graph [layout = dot,
         rankdir = TB,            
         overlap = true,
         fontsize = 10]
  

  # nodes (circles)
  #################
  node [shape = circle,                  # shape = circle
       fixedsize = true
       width = 1.3]                      # width of circles
  
  subgraph cluster_passive {
    Primary   [label = 'Primary\nFacility'] 
    Secondary [label = 'Secondary\nFacility'] 
    Tertiary  [label = 'Tertiary\nFacility'] 
    SC        [label = 'Surveillance\nCoordination',
               fontcolor = darkgreen] 
  }
  
  # nodes (boxes)
  ###############
  node [shape = box,                     # node shape
        fontname = Helvetica]            # text font in node
  
  subgraph cluster_active {
    Active [label = 'Active\nSurveillance'] 
    HCF_active [label = 'HCF\nActive Search']
  }
  
  subgraph cluster_EBD {
    EBS [label = 'Event-Based\nSurveillance (EBS)'] 
    'Social Media'
    Radio
  }
  
  subgraph cluster_CBS {
    CBS [label = 'Community-Based\nSurveillance (CBS)']
    RECOs
  }

  
  # edges
  #######
  {Primary Secondary Tertiary} -> SC [label = 'case reporting']

  Primary   -> Secondary [label = 'case transfer',
                          fontcolor = red]
  Secondary -> Tertiary [label = 'case transfer',
                          fontcolor = red]
  
  HCF_active -> Active
  
  {'Social Media' Radio} -> EBS
  
  RECOs -> CBS
}
")

```


**Node shapes**  

The example below, borrowed from [this tutorial](http://rich-iannone.github.io/DiagrammeR/), shows applied node shapes and a shorthand for serial edge connections  

```{r out.width='75%'}
DiagrammeR::grViz("digraph {

graph [layout = dot, rankdir = LR]

# define the global styles of the nodes. We can override these in box if we wish
node [shape = rectangle, style = filled, fillcolor = Linen]

data1 [label = 'Dataset 1', shape = folder, fillcolor = Beige]
data2 [label = 'Dataset 2', shape = folder, fillcolor = Beige]
process [label =  'Process \n Data']
statistical [label = 'Statistical \n Analysis']
results [label= 'Results']

# edge definitions with the node IDs
{data1 data2}  -> process -> statistical -> results
}")
```


### Outputs  {.unnumbered}

How to handle and save outputs  

* Outputs will appear in RStudio's Viewer pane, by default in the lower-right alongside Files, Plots, Packages, and Help.  
* To export you can "Save as image" or "Copy to clipboard" from the Viewer. The graphic will adjust to the specified size.  




### Parameterized figures {.unnumbered} 

Here is a quote from this tutorial: https://mikeyharper.uk/flowcharts-in-r-using-diagrammer/  

"Parameterized figures: A great benefit of designing figures within R is that we are able to connect the figures directly with our analysis by reading R values directly into our flowcharts. For example, suppose you have created a filtering process which removes values after each stage of a process, you can have a figure show the number of values left in the dataset after each stage of your process. To do this we, you can use the @@X symbol directly within the figure, then refer to this in the footer of the plot using [X]:, where X is the a unique numeric index."  

We encourage you to review this tutorial if parameterization is something you are interested in.  


<!-- And below is some example code from this tutorial. -->

<!-- ```{r, eval=F} -->
<!-- # Define some sample data -->
<!-- data <- list(a=1000, b=800, c=600, d=400) -->


<!-- DiagrammeR::grViz(" -->
<!-- digraph graph2 { -->

<!-- graph [layout = dot] -->

<!-- # node definitions with substituted label text -->
<!-- node [shape = rectangle, width = 4, fillcolor = Biege] -->
<!-- a [label = '@@1'] -->
<!-- b [label = '@@2'] -->
<!-- c [label = '@@3'] -->
<!-- d [label = '@@4'] -->

<!-- a -> b -> c -> d -->

<!-- } -->

<!-- [1]:  paste0('Raw Data (n = ', data$a, ')') -->
<!-- [2]: paste0('Remove Errors (n = ', data$b, ')') -->
<!-- [3]: paste0('Identify Potential Customers (n = ', data$c, ')') -->
<!-- [4]: paste0('Select Top Priorities (n = ', data$d, ')') -->
<!-- ") -->

<!-- ``` -->



<!-- ### CONSORT diagram  {.unnumbered} -->

<!-- THIS SECTION IS UNDER CONSTRUCTION   -->

<!-- https://scriptsandstatistics.wordpress.com/2017/12/22/how-to-draw-a-consort-flow-diagram-using-r-and-graphviz/ -->

<!-- Note above is out of date via DiagrammeR -->




<!-- ======================================================= -->
## Alluvial/Sankey Diagrams { }

### Load packages {.unnumbered}  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

We load the **networkD3** package to produce the diagram, and also **tidyverse** for the data preparation steps.  

```{r}
pacman::p_load(
  networkD3,
  tidyverse)
```

### Plotting from dataset {.unnumbered} 

Plotting the connections in a dataset. Below we demonstrate using this package on the case `linelist`. Here is an [online tutorial](https://www.r-graph-gallery.com/321-introduction-to-interactive-sankey-diagram-2.html).    

We begin by getting the case counts for each unique age category and hospital combination. We've removed values with missing age category for clarity. We also re-label the `hospital` and `age_cat` columns as `source` and `target` respectively. These will be the two sides of the alluvial diagram.  

```{r}
# counts by hospital and age category
links <- linelist %>% 
  drop_na(age_cat) %>% 
  select(hospital, age_cat) %>%
  count(hospital, age_cat) %>% 
  rename(source = hospital,
         target = age_cat)
```

The dataset now look like this:  

```{r message=FALSE, echo=F}
DT::datatable(links, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```


Now we create a data frame of all the diagram nodes, under the column `name`. This consists of all the values for `hospital` and `age_cat`. Note that we ensure they are all class Character before combining them. and adjust the ID columns to be numbers instead of labels:  

```{r}
# The unique node names
nodes <- data.frame(
  name=c(as.character(links$source), as.character(links$target)) %>% 
    unique()
  )

nodes  # print
```
The we edit the `links` data frame, which we created above with `count()`. We add two numeric columns `IDsource` and `IDtarget` which will actually reflect/create the links between the nodes. These columns will hold the rownumbers (position) of the source and target nodes. 1 is subtracted so that these position numbers begin at 0 (not 1).  

```{r}
# match to numbers, not names
links$IDsource <- match(links$source, nodes$name)-1 
links$IDtarget <- match(links$target, nodes$name)-1
```

The links dataset now looks like this:  

```{r message=FALSE, echo=F}
DT::datatable(links, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```

Now plot the Sankey diagram with `sankeyNetwork()`. You can read more about each argument by running `?sankeyNetwork` in the console. Note that unless you set `iterations = 0` the order of your nodes may not be as expected. 


```{r}

# plot
######
p <- sankeyNetwork(
  Links = links,
  Nodes = nodes,
  Source = "IDsource",
  Target = "IDtarget",
  Value = "n",
  NodeID = "name",
  units = "TWh",
  fontSize = 12,
  nodeWidth = 30,
  iterations = 0)        # ensure node order is as in data
p
```



Here is an example where the patient Outcome is included as well. Note in the data preparation step we have to calculate the counts of cases between age and hospital, and separately between hospital and outcome - and then bind all these counts together with `bind_rows()`.  

```{r}
# counts by hospital and age category
age_hosp_links <- linelist %>% 
  drop_na(age_cat) %>% 
  select(hospital, age_cat) %>%
  count(hospital, age_cat) %>% 
  rename(source = age_cat,          # re-name
         target = hospital)

hosp_out_links <- linelist %>% 
    drop_na(age_cat) %>% 
    select(hospital, outcome) %>% 
    count(hospital, outcome) %>% 
    rename(source = hospital,       # re-name
           target = outcome)

# combine links
links <- bind_rows(age_hosp_links, hosp_out_links)

# The unique node names
nodes <- data.frame(
  name=c(as.character(links$source), as.character(links$target)) %>% 
    unique()
  )

# Create id numbers
links$IDsource <- match(links$source, nodes$name)-1 
links$IDtarget <- match(links$target, nodes$name)-1

# plot
######
p <- sankeyNetwork(Links = links,
                   Nodes = nodes,
                   Source = "IDsource",
                   Target = "IDtarget",
                   Value = "n",
                   NodeID = "name",
                   units = "TWh",
                   fontSize = 12,
                   nodeWidth = 30,
                   iterations = 0)
p

```


https://www.displayr.com/sankey-diagrams-r/



<!-- ======================================================= -->
## Event timelines { }

To make a timeline showing specific events, you can use the `vistime` package.

See this [vignette](https://cran.r-project.org/web/packages/vistime/vignettes/vistime-vignette.html#ex.-2-project-planning)

```{r}
# load package
pacman::p_load(vistime,  # make the timeline
               plotly    # for interactive visualization
               )
```

```{r, echo=F}
# reference: https://cran.r-project.org/web/packages/vistime/vignettes/vistime-vignette.html#ex.-2-project-planning

data <- read.csv(text="event, group, start, end, color
                       Event 1, Group A,2020-01-22,2020-01-22, #90caf9
                       Event 1, Group B,2020-01-23,2020-01-23, #90caf9
                       Event 1, Group C,2020-01-23,2020-01-23, #1565c0
                       Event 1, Group D,2020-01-25,2020-01-25, #f44336
                       Event 1, Group E,2020-01-25,2020-01-25, #90caf9
                       Event 1, Group F,2020-01-26,2020-01-26, #8d6e63
                       Event 1, Group G,2020-01-27,2020-01-27, #1565c0
                       Event 1, Group H,2020-01-27,2020-01-27, #90caf9
                       Event 1, Group I,2020-01-27,2020-01-27,#90a4ae
                       Event 2, Group A,2020-01-28,2020-01-28,#fc8d62
                       Event 2, Group C,2020-01-28,2020-01-28, #6a3d9a
                       Event 2, Group J,2020-01-28,2020-01-28, #90caf9
                       Event 2, Group J,2020-01-28,2020-01-28, #fc8d62
                       Event 2, Group J,2020-01-28,2020-01-28, #1565c0
")
```

Here is the events dataset we begin with:  

```{r message=FALSE, echo=F}
DT::datatable(data, rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```



```{r}
p <- vistime(data)    # apply vistime

library(plotly)

# step 1: transform into a list
pp <- plotly_build(p)

# step 2: Marker size
for(i in 1:length(pp$x$data)){
  if(pp$x$data[[i]]$mode == "markers") pp$x$data[[i]]$marker$size <- 10
}

# step 3: text size
for(i in 1:length(pp$x$data)){
  if(pp$x$data[[i]]$mode == "text") pp$x$data[[i]]$textfont$size <- 10
}


# step 4: text position
for(i in 1:length(pp$x$data)){
  if(pp$x$data[[i]]$mode == "text") pp$x$data[[i]]$textposition <- "right"
}

#print
pp

```



<!-- ======================================================= -->
## DAGs { }

You can build a DAG manually using the **DiagammeR** package and DOT language as described above.  

Alternatively, there are packages like **ggdag** and **daggity**

[Introduction to DAGs ggdag vignette](https://cran.r-project.org/web/packages/ggdag/vignettes/intro-to-dags.html)   

[Causal inference with dags in R](https://www.r-bloggers.com/2019/08/causal-inference-with-dags-in-r/#:~:text=In%20a%20DAG%20all%20the,for%20drawing%20and%20analyzing%20DAGs.)  





<!-- ======================================================= -->
## Resources { }



Much of the above regarding the DOT language is adapted from the tutorial [at this site](https://mikeyharper.uk/flowcharts-in-r-using-diagrammer/)  

Another more in-depth [tutorial on DiagammeR](http://rich-iannone.github.io/DiagrammeR/)

This page on [Sankey diagrams](https://www.displayr.com/sankey-diagrams-r/
)  




```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/diagrams.Rmd-->


# Biểu đồ kết hợp {#combination-analysis}  

```{r echo=F, out.width= "75%", warning=F, message=F}
pacman::p_load(tidyverse,
               UpSetR,
               ggupset)

# Adds new symptom variables to the linelist, with random "yes" or "no" values 
linelist_sym <- linelist %>% 
  mutate(fever  = sample(c("yes", "no"), nrow(linelist), replace = T, prob = c(0.80, 0.20)),
         chills = sample(c("yes", "no"), nrow(linelist), replace = T, prob = c(0.20, 0.80)),
         cough  = sample(c("yes", "no"), nrow(linelist), replace = T, prob = c(0.9, 0.15)),
         aches  = sample(c("yes", "no"), nrow(linelist), replace = T, prob = c(0.10, 0.90)),
         vomit = sample(c("yes", "no"), nrow(linelist), replace = T))

linelist_sym_2 <- linelist_sym %>% 
  
  # convert the "yes" and "no" values into the symptom name itself
  mutate(fever = case_when(fever == "yes" ~ 1,          # if old value is "yes", new value is "fever"
                           TRUE           ~ 0),   # if old value is anything other than "yes", the new value is NA
         
         chills = case_when(chills == "yes" ~ 1,
                           TRUE           ~ 0),
         
         cough = case_when(cough == "yes" ~ 1,
                           TRUE           ~ 0),
         
         aches = case_when(aches == "yes" ~ 1,
                           TRUE           ~ 0),
         
         vomit = case_when(vomit == "yes" ~ 1,
                           TRUE           ~ 0))

# Make the plot
UpSetR::upset(
  select(linelist_sym_2, fever, chills, cough, aches, vomit),
  sets = c("fever", "chills", "cough", "aches", "vomit"),
  order.by = "freq",
  sets.bar.color = c("blue", "red", "yellow", "darkgreen", "orange"), # optional colors
  empty.intersections = "on",
  # nsets = 3,
  number.angles = 0,
  point.size = 3.5,
  line.size = 2, 
  mainbar.y.label = "Symptoms Combinations",
  sets.x.label = "Patients with Symptom")

```



Phân tích này vẽ biểu đồ tần suất của các **kết hợp** giá trị/phản hồi khác nhau. Trong ví dụ này, chúng ta sẽ vẽ biểu đồ tần suất các trường hợp có biểu hiện kết hợp nhiều triệu chứng khác nhau.  

Phân tích này còn thường được gọi bằng những tên khác như:  

* **"Phân tích nhiều lựa chọn"**  
* **"Phân tích các bộ (sets)"**  
* **"Phân tích kết hợp"**  

Trong biểu đồ minh họa bên trên, năm triệu chứng được trình bày. Bên dưới mỗi thanh dọc là một đường và dấu chấm biểu thị sự kết hợp của các triệu chứng được phản ánh bởi thanh ở trên. Ở bên trái, các thanh ngang phản ánh tần suất của từng triệu chứng riêng lẻ.  

Phương pháp đầu tiên chúng tôi sẽ trình bày sử dụng package **ggupset**, phương pháp thứ hai sử dụng package **UpSetR**. 




  



<!-- ======================================================= -->
## Chuẩn bị {  }

### Gọi package {.unnumbered}

Đoạn code này hiển thị việc gọi các gói cần thiết cho các phân tích. Trong cuốn sách này, chúng tôi nhấn mạnh việc sử dụng hàm `p_load()` từ package **pacman**, giúp cài đặt các package nếu cần thiết *và* gọi chúng ra để sử dụng. Bạn cũng có thể gọi các packages đã cài đặt với hàm `library()` của **base** R. Xem thêm chương [R cơ bản] để có thêm thông tin về các packages trong R.  

```{r, warning=F, message=F}
pacman::p_load(
  tidyverse,     # data management and visualization
  UpSetR,        # special package for combination plots
  ggupset)       # special package for combination plots
```

<!-- ======================================================= -->
### Nhập dữ liệu {.unnumbered}  


Để bắt đầu, chúng ta nhập bộ dữ liệu có tên linelist đã làm sạch bao gồm các trường hợp từ vụ dịch Ebola mô phỏng. Để tiện theo dõi, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>bấm để tải dữ liệu linelist "đã được làm sạch" </a> (dưới dạng tệp .rds). Nhập dữ liệu bằng hàm `import()` từ package **rio** (nó xử lý nhiều loại tệp như .xlsx, .csv, .rds - xem thêm chương [Nhập xuất dữ liệu] để biết thêm chi tiết.  



```{r, echo=F}
# import the linelist into R
linelist_sym <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import case linelist 
linelist_sym <- import("linelist_cleaned.rds")
```


Bộ số liệu linelist bao gồm năm biến "có/không" về các triệu chứng được ghi nhận. Chúng ta sẽ cần phải biến đổi các biến số này một chút trước khi sử dụng package **ggupset** để tạo biểu đồ. Xem dữ liệu (cuộn sang phải để xem các biến triệu chứng).  

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist_sym, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



<!-- ======================================================= -->
### Định dạng lại giá trị {.unnumbered}  

Để tương đồng với định dạng của package **ggupset**, chúng ta cần đổi giá trị "yes" và "no" thành tên các triệu chứng thực tế, sử dụng hàm `case_when()` từ package **dplyr**. Nếu giá trị là "no", chúng ta sẽ bỏ trống, nghĩa là biến mới sẽ có giá trị hoặc là `NA` hoặc là triệu chứng.  
 

```{r, warning=F, message=F}
# create column with the symptoms named, separated by semicolons
linelist_sym_1 <- linelist_sym %>% 
  
  # convert the "yes" and "no" values into the symptom name itself
  mutate(
    fever = case_when(
      fever == "yes" ~ "fever",          # if old value is "yes", new value is "fever"
      TRUE           ~ NA_character_),   # if old value is anything other than "yes", the new value is NA
         
    chills = case_when(
       chills == "yes" ~ "chills",
       TRUE           ~ NA_character_),
    
    cough = case_when(
      cough == "yes" ~ "cough",
      TRUE           ~ NA_character_),
         
    aches = case_when(
      aches == "yes" ~ "aches",
      TRUE           ~ NA_character_),
         
    vomit = case_when(
      vomit == "yes" ~ "vomit",
      TRUE           ~ NA_character_)
    )
```

Bây giờ chúng ta tạo hai cột cuối cùng:  

1. Kết hợp (ghép lại với nhau) tất cả các triệu chứng của bệnh nhân (thành một cột ký tự)  
2. Chuyển đổi định dạng cột bên trên thành kiểu *danh sách* để được chấp nhận bởi package **ggupset** khi vẽ biểu đồ  

Xem thêm chương [Ký tự và chuỗi] để biết thêm về hàm `unite()` trong package **stringr**

```{r, warning=F, message=F}
linelist_sym_1 <- linelist_sym_1 %>% 
  unite(col = "all_symptoms",
        c(fever, chills, cough, aches, vomit), 
        sep = "; ",
        remove = TRUE,
        na.rm = TRUE) %>% 
  mutate(
    # make a copy of all_symptoms column, but of class "list" (which is required to use ggupset() in next step)
    all_symptoms_list = as.list(strsplit(all_symptoms, "; "))
    )
```

Bây giờ chúng ta cùng xem dữ liệu mới. Lưu ý hai cột ở cuối bên phải - các giá trị kết hợp được ghép và danh sách

```{r, echo=F, , warning=F, message=F}
DT::datatable(head(linelist_sym_1,50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```


<!-- ======================================================= -->
## **ggupset** {  }

Gọi package

```{r}
pacman::p_load(ggupset)
```


Vẽ biểu đồ. Chúng ta bắt đầu bằng hàm `ggplot()` và `geom_bar()`, nhưng sau đó chúng ta thêm hàm đặc biệt `scale_x_upset()` từ package **ggupset**.  

```{r, warning=F, message=F}
ggplot(
  data = linelist_sym_1,
  mapping = aes(x = all_symptoms_list)) +
geom_bar() +
scale_x_upset(
  reverse = FALSE,
  n_intersections = 10,
  sets = c("fever", "chills", "cough", "aches", "vomit"))+
labs(
  title = "Signs & symptoms",
  subtitle = "10 most frequent combinations of signs and symptoms",
  caption = "Caption here.",
  x = "Symptom combination",
  y = "Frequency in dataset")

```
  
Bạn có thể đọc thêm về package **ggupset** [ở tài liệu online này](https://rdrr.io/cran/ggupset/man/scale_x_upset.html) hoặc trong tài liệu trợ giúp của package bằng cách gõ vào cửa RStudio Help lệnh `?ggupset`.  


<!-- ======================================================= -->
## `UpSetR` {  }

Package **UpSetR** cho phép tùy chỉnh biểu đồ sâu hơn, nhưng nó cũng khó thực hiện hơn:


**Gọi package**  

```{r}
pacman::p_load(UpSetR)
```

**Làm sạch dữ liệu**  

Chúng ta phải chuyển đổi các triệu chứng trong bộ dữ liệu `linelist` thành các giá trị 1 / 0. 

```{r}
# Make using upSetR

linelist_sym_2 <- linelist_sym %>% 
  
  # convert the "yes" and "no" values into the symptom name itself
  mutate(
    fever = case_when(
      fever == "yes" ~ 1,    # if old value is "yes", new value is 1
      TRUE           ~ 0),   # if old value is anything other than "yes", the new value is 0
         
    chills = case_when(
      chills == "yes" ~ 1,
      TRUE           ~ 0),
         
    cough = case_when(
      cough == "yes" ~ 1,
      TRUE           ~ 0),
         
    aches = case_when(
      aches == "yes" ~ 1,
      TRUE           ~ 0),
         
    vomit = case_when(
      vomit == "yes" ~ 1,
      TRUE           ~ 0)
    )
```

Bây giờ chúng ta hãy vẽ biểu đồ bằng hàm tùy chỉnh `upset()` - chỉ sử dụng các cột triệu chứng. Bạn phải chỉ định “bộ” nào để so sánh (tên của các cột triệu chứng). Một cách khác, sử dụng `nsets = ` và `order.by = "freq"` để chỉ hiện thị X các sự kết hợp nhiều nhất.  

```{r, warning=F, message=F}

# Make the plot
UpSetR::upset(
  select(linelist_sym_2, fever, chills, cough, aches, vomit),
  sets = c("fever", "chills", "cough", "aches", "vomit"),
  order.by = "freq",
  sets.bar.color = c("blue", "red", "yellow", "darkgreen", "orange"), # optional colors
  empty.intersections = "on",
  # nsets = 3,
  number.angles = 0,
  point.size = 3.5,
  line.size = 2, 
  mainbar.y.label = "Symptoms Combinations",
  sets.x.label = "Patients with Symptom")

```


<!-- ======================================================= -->
## Nguồn {  }

[The github page on UpSetR](https://github.com/hms-dbmi/UpSetR)  

[A Shiny App version - you can upload your own data](https://gehlenborglab.shinyapps.io/upsetr/)  

[*documentation - difficult to interpret](https://cran.r-project.org/web/packages/UpSetR/UpSetR.pdf)  


```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/combination_analysis.Rmd-->


# Chuỗi lây nhiễm {#transmission-chains}


<!-- ======================================================= -->
## Overview {  }

The primary tool to handle, analyse and visualise transmission chains and contact
tracing data is the package **epicontacts**, developed by the folks at
RECON. Try out the interactive plot below by hovering over the nodes for more
information, dragging them to move them and clicking on them to highlight downstream cases.

```{r out.width=c('25%', '25%'), fig.show='hold', echo=F}

## install development version of epicontacts
if(
  !"epicontacts" %in% rownames(installed.packages()) |
  packageVersion("epicontacts") != "1.2.0"
) remotes::install_github("reconhub/epicontacts@timeline")

## install and load packages
pacman::p_load(tidyverse, epicontacts, magrittr, here, webshot, visNetwork)

## load linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds")) %>%
  filter(!duplicated(case_id))

## generate contacts
contacts <- linelist %>%
  transmute(
    from = infector,
    to = case_id,
    location = sample(c("Community", "Nosocomial"), n(), TRUE),
    duration = sample.int(10, n(), TRUE)
  ) %>%
  drop_na(from)

## generate epicontacts
epic <- epicontacts::make_epicontacts(
  linelist = linelist,
  contacts = contacts, 
  directed = TRUE
)

## subset object
epic %<>% subset(
  node_attribute = list(date_onset = c(as.Date(c("2014-06-01", "2014-07-01"))))
) %>%
  thin("contacts")

## plot with date of onset as x-axis
plot(
  epic,
  x_axis = "date_onset",
  label = FALSE,
  node_color = "outcome",
  col_pal = c(Death = "firebrick", Recover = "green"),
  node_shape = "gender",
  shapes = c(f = "female", m = "male"),
  unlinked_pos = "bottom",
  date_labels = "%b %d %Y",
  node_size = 35,
  font_size = 20,
  arrow_size = 0.5,
  height = 800,
  width = 700,
  edge_linetype = "location",
  legend_width = 0.15,
  highlight_downstream = TRUE,
  selector = FALSE
)

```

<!-- ======================================================= -->
## Preparation {  }

### Load packages {.unnumbered}  

First load the standard packages required for data import and manipulation. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  
 
	
```{r transmission_chains_packages, eval = FALSE}
pacman::p_load(
   rio,          # File import
   here,         # File locator
   tidyverse,    # Data management + ggplot2 graphics
   remotes       # Package installation from github
)
```
	
You will require the development version of **epicontacts**, which can be
installed from github using the `p_install_github()` function from **pacman**. You only need to run this command
below once, not every time you use the package (thereafter, you can use `p_load()` as usual).

```{r transmission_chains_epicontacts_install, eval = FALSE}
pacman::p_install_gh("reconhub/epicontacts@timeline")
```


### Import data {.unnumbered}

We import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow step-by-step, see instructions in the [Download handbook and data] page. The dataset is imported using the `import()` function from the **rio** package. See the page on [Import and export] for various ways to import data.

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.xlsx")
```

The first 50 rows of the linelist are displayed below. Of particular interest are the columns `case_id`, `generation`, `infector`, and `source`.  

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


### Creating an epicontacts object {.unnumbered}

We then need to create an **epicontacts** object, which requires two types of
data:

* a linelist documenting cases where columns are variables and rows correspond to unique cases
* a list of edges defining links between cases on the basis of their unique IDs (these can be contacts,
  transmission events, etc.)

As we already have a linelist, we just need to create a list of edges between
cases, more specifically between their IDs. We can extract transmission links from the
linelist by linking the `infector` column with the `case_id` column. At this point we can also add "edge
properties", by which we mean any variable describing the link between the two
cases, not the cases themselves. For illustration, we will add a `location`
variable describing the location of the transmission event, and a duration
variable describing the duration of the contact in days.

In the code below, the **dplyr** function `transmute` is similar to `mutate`, except it only keeps
the columns we have specified within the function. The `drop_na` function will
filter out any rows where the specified columns have an `NA` value; in this
case, we only want to keep the rows where the infector is known.

```{r transmission_chains_create_contacts,}
## generate contacts
contacts <- linelist %>%
  transmute(
    infector = infector,
    case_id = case_id,
    location = sample(c("Community", "Nosocomial"), n(), TRUE),
    duration = sample.int(10, n(), TRUE)
  ) %>%
  drop_na(infector)
```

We can now create the **epicontacts** object using the `make_epicontacts`
function. We need to specify which column in the linelist points to the unique case
identifier, as well as which columns in the contacts point to the unique
identifiers of the cases involved in each link. These links are directional in
that infection is going _from_ the infector _to_ the case, so we need to specify
the `from` and `to` arguments accordingly. We therefore also set the `directed`
argument to `TRUE`, which will affect future operations.

```{r transmission_chains_create_epicontacts,}
## generate epicontacts object
epic <- make_epicontacts(
  linelist = linelist,
  contacts = contacts,
  id = "case_id",
  from = "infector",
  to = "case_id",
  directed = TRUE
)
```
Upon examining the **epicontacts** objects, we can see that the `case_id` column
in the linelist has been renamed to `id` and the `case_id` and `infector`
columns in the contacts have been renamed to `from` and `to`. This ensures
consistency in subsequent handling, visualisation and analysis operations.

```{r transmission_chains_view_epicontacts,}
## view epicontacts object
epic
```

<!-- ======================================================= -->
## Handling {  }

### Subsetting {.unnumbered}

The `subset()` method for `epicontacts` objects allows for, among other things,
filtering of networks based on properties of the linelist ("node attributes") and the contacts
database ("edge attributes"). These values must be passed as named lists to the
respective argument. For example, in the code below we are keeping only the
male cases in the linelist that have an infection date between April and
July 2014 (dates are specified as ranges), and transmission links that occured
in the hospital.

```{r transmission_chains_subset_nodes,}
sub_attributes <- subset(
  epic,
  node_attribute = list(
    gender = "m",
    date_infection = as.Date(c("2014-04-01", "2014-07-01"))
  ), 
  edge_attribute = list(location = "Nosocomial")
)
sub_attributes
```

We can use the `thin` function to either filter the linelist to include cases
that are found in the contacts by setting the argument `what = "linelist"`, or
filter the contacts to include cases that are found in the linelist by setting
the  argument `what = "contacts"`. In the code below, we are further filtering the
epicontacts object to keep only the transmission links involving the male cases
infected between April and July which we had filtered for above. We can see that
only two known transmission links fit that specification.

```{r transmission_chains_thin,}
sub_attributes <- thin(sub_attributes, what = "contacts")
nrow(sub_attributes$contacts)
```

In addition to subsetting by node and edge attributes, networks can be pruned to
only include components that are connected to certain nodes. The `cluster_id`
argument takes a vector of case IDs and returns the linelist of individuals that
are linked, directly or indirectly, to those IDs. In the code below, we can see
that a total of 13 linelist cases are involved in the clusters containing
`2ae019` and `71577a`.

```{r}
sub_id <- subset(epic, cluster_id = c("2ae019","71577a"))
nrow(sub_id$linelist)
```

The `subset()` method for `epicontacts` objects also allows filtering by cluster
size using the `cs`, `cs_min` and `cs_max` arguments. In the code below, we are
keeping only the cases linked to clusters of 10 cases or larger, and can see that
271 linelist cases are involved in such clusters.
    
```{r}   
sub_cs <- subset(epic, cs_min = 10)
nrow(sub_cs$linelist)
```

### Accessing IDs {.unnumbered}

The `get_id()` function retrieves information on case IDs in the
dataset, and can be parameterized as follows:

- **linelist**: IDs in the line list data
- **contacts**: IDs in the contact dataset ("from" and "to" combined)
- **from**: IDs in the "from" column of contact datset
- **to** IDs in the "to" column of contact dataset
- **all**: IDs that appear anywhere in either dataset
- **common**: IDs that appear in both contacts dataset and line list
    
For example, what are the first ten IDs in the contacts dataset?
```{r transmission_chains_get_ids,}
contacts_ids <- get_id(epic, "contacts")
head(contacts_ids, n = 10)
```

How many IDs are found in both the linelist and the contacts?
```{r transmission_chains_get_both,}
length(get_id(epic, "common"))
```

<!-- ======================================================= -->
## Visualization {  }

### Basic plotting {.unnumbered}

All visualisations of **epicontacts** objects are handled by the `plot`
function. We will first filter the **epicontacts** object to include only the
cases with onset dates in June 2014 using the `subset` function, and only
include the contacts linked to those cases using the `thin` function.
	
```{r transmission_chains_basic_plot_sub,}
## subset epicontacts object
sub <- epic %>%
  subset(
    node_attribute = list(date_onset = c(as.Date(c("2014-06-30", "2014-06-01"))))
  ) %>%
 thin("contacts")
```

We can then create the basic, interactive plot very simply as follows:

```{r transmission_chains_basic_plot,}
## plot epicontacts object
plot(
  sub,
  width = 700,
  height = 700
)
```

You can move the nodes around by dragging them, hover over them for more
information and click on them to highlight connected cases.

There are a large number of arguments to further modify this plot. We will cover
the main ones here, but check out the documentation via `?vis_epicontacts` (the
function called when using `plot` on an **epicontacts** object) to get a full
description of the function arguments.

#### Visualising node attributes {.unnumbered}

Node color, node shape and node size can be mapped to a given column in the linelist 
using the `node_color`, `node_shape` and `node_size` arguments. This is similar
to the `aes` syntax you may recognise from **ggplot2**. 

The specific colors, shapes and sizes of nodes can be specified as follows:

* **Colors** via the `col_pal` argument, either by providing a name list for manual
specification of each color as done below, or by providing a color palette
function such as `colorRampPalette(c("black", "red", "orange"))`, which would
provide a gradient of colours between the ones specified.

* **Shapes** by passing a named list to the `shapes` argument, specifying one shape
  for each unique element in the linelist column specified by the `node_shape`
  argument. See `codeawesome` for available shapes.

* **Size** by passing a size range of the nodes to the `size_range` argument.

Here an example, where color represents the outcome, shape the gender and size
the age:

```{r transmission_chains_node_attribute,}
plot(
  sub, 
  node_color = "outcome",
  node_shape = "gender",
  node_size = 'age',
  col_pal = c(Death = "firebrick", Recover = "green"),
  shapes = c(f = "female", m = "male"),
  size_range = c(40, 60),
  height = 700,
  width = 700
)
```

#### Visualising edge attributes {.unnumbered}

Edge color, width and linetype can be mapped to a given column in the contacts
dataframe using the `edge_color`, `edge_width` and `edge_linetype`
arguments. The specific colors and widths of the edges can be specified as follows:

* **Colors** via the `edge_col_pal` argument, in the same manner used for `col_pal`.

* **Widths** by passing a size range of the nodes to the `width_range` argument.

Here an example:

```{r transmission_chains_edge_attribute,}

plot(
  sub, 
  node_color = "outcome",
  node_shape = "gender",
  node_size = 'age',
  col_pal = c(Death = "firebrick", Recover = "green"),
  shapes = c(f = "female", m = "male"),
  size_range = c(40, 60),
  edge_color = 'location',
  edge_linetype = 'location',
  edge_width = 'duration',
  edge_col_pal = c(Community = "orange", Nosocomial = "purple"),
  width_range = c(1, 3),
  height = 700,
  width = 700
)

```

### Temporal axis {.unnumbered}

We can also visualise the network along a temporal axis by mapping the `x_axis`
argument to a column in the linelist. In the example below, the x-axis
represents the date of symptom onset. We have also specified the `arrow_size`
argument to ensure the arrows are not too large, and set `label = FALSE` to make
 the figure less cluttered.

```{r transmission_chains_x_axis,}
plot(
  sub,
  x_axis = "date_onset",
  node_color = "outcome",
  col_pal = c(Death = "firebrick", Recover = "green"),
  arrow_size = 0.5,
  node_size = 13,
  label = FALSE,
  height = 700,
  width = 700
)
```

There are a large number of additional arguments to futher specify how this
network is visualised along a temporal axis, which you can check out
via `?vis_temporal_interactive` (the function called when using `plot` on
an **epicontacts** object with `x_axis` specified). We'll go through some
below.

#### Specifying transmission tree shape {.unnumbered}

There are two main shapes that the transmission tree can assume, specified using
the `network_shape` argument. The first is a `branching` shape as shown above,
where a straight edge connects any two nodes. This is the most intuitive
representation, however can result in overlapping edges in a densely connected
network. The second shape is `rectangle`, which produces a tree resembling a
phylogeny. For example:

```{r transmission_chains_rectangle,}
plot(
  sub,
  x_axis = "date_onset",
  network_shape = "rectangle",
  node_color = "outcome",
  col_pal = c(Death = "firebrick", Recover = "green"),
  arrow_size = 0.5,
  node_size = 13,
  label = FALSE,
  height = 700,
  width = 700
)
```

Each case node can be assigned a unique vertical position by toggling the
`position_dodge` argument. The position of unconnected cases (i.e. with no
reported contacts) is specified using the `unlinked_pos` argument.

```{r transmission_chains_dodge,}
plot(
  sub,
  x_axis = "date_onset",
  network_shape = "rectangle",
  node_color = "outcome",
  col_pal = c(Death = "firebrick", Recover = "green"),
  position_dodge = TRUE,
  unlinked_pos = "bottom",
  arrow_size = 0.5,
  node_size = 13,
  label = FALSE,
  height = 700,
  width = 700
)
```

The position of the parent node relative to the children nodes can be
specified using the `parent_pos` argument. The default option is to place the
parent node in the middle, however it can be placed at the bottom (`parent_pos =
'bottom'`) or at the top (`parent_pos = 'top'`).

```{r transmission_chains_parent_pos,}
plot(
  sub,
  x_axis = "date_onset",
  network_shape = "rectangle",
  node_color = "outcome",
  col_pal = c(Death = "firebrick", Recover = "green"),
  parent_pos = "top",
  arrow_size = 0.5,
  node_size = 13,
  label = FALSE,
  height = 700,
  width = 700
)
```

#### Saving plots and figures {.unnumbered}

You can save a plot as an interactive, self-contained html file with the
`visSave` function from the **VisNetwork** package:

```{r transmission_chains_save, eval=F}

plot(
  sub,
  x_axis = "date_onset",
  network_shape = "rectangle",
  node_color = "outcome",
  col_pal = c(Death = "firebrick", Recover = "green"),
  parent_pos = "top",
  arrow_size = 0.5,
  node_size = 13,
  label = FALSE,
  height = 700,
  width = 700
) %>%
  visNetwork::visSave("network.html")

```

Saving these network outputs as an image is unfortunately less easy and requires
you to save the file as an html and then take a screenshot of this file using
the `webshot` package. In the code below, we are converting the html file saved
above into a PNG:

```{r transmission_chains_webshot, eval=F}
webshot(url = "network.html", file = "network.png")
```

### Timelines {.unnumbered}

You can also case timelines to the network, which are represented on the x-axis
of each case. This can be used to visualise case locations, for example, or time
to outcome. To generate a timeline, we need to create a data.frame of at least
three columns indicating the case ID, the start date of the "event" and the end
of date of the "event". You can also add any number of other columns which can
then be mapped to node and edge properties of the timeline. In the code below,
we generate a timeline going from the date of symptom onset to the date of
outcome, and keep the outcome and hospital variables which we use to define the
node shape and colour. Note that you can have more than one timeline row/event
per case, for example if a case is transferred between multiple hospitals.

```{r transmission_chains_create_timeline,}

## generate timeline
timeline <- linelist %>%
  transmute(
    id = case_id,
    start = date_onset,
    end = date_outcome,
    outcome = outcome,
    hospital = hospital
  )

```

We then pass the timeline element to the `timeline` argument. We can map
timeline attributes to timeline node colours, shapes and sizes in the same way
defined in previous sections, except that we have _two_ nodes: the start and end
node of each timeline, which have seperate arguments. For example,
`tl_start_node_color` defines which timeline column is mapped to the colour of
the start node, while `tl_end_node_shape` defines which timeline column is
mapped to the shape of the end node. We can also map colour, width, linetype and
labels to the timeline _edge_ via the `tl_edge_*` arguments. 

See `?vis_temporal_interactive` (the function called when plotting an
epicontacts object) for detailed documentation on the arguments. Each argument
is annotated in the code below too:

```{r transmission_chains_vis_timeline,}

## define shapes
shapes <- c(
  f = "female",
  m = "male",
  Death = "user-times",
  Recover = "heartbeat",
  "NA" = "question-circle"
)

## define colours
colours <- c(
  Death = "firebrick",
  Recover = "green",
  "NA" = "grey"
)

## make plot
plot(
  sub,
  ## max x coordinate to date of onset
  x_axis = "date_onset",
  ## use rectangular network shape
  network_shape = "rectangle",
  ## mape case node shapes to gender column
  node_shape = "gender",
  ## we don't want to map node colour to any columns - this is important as the
  ## default value is to map to node id, which will mess up the colour scheme
  node_color = NULL,
  ## set case node size to 30 (as this is not a character, node_size is not
  ## mapped to a column but instead interpreted as the actual node size)
  node_size = 30,
  ## set transmission link width to 4 (as this is not a character, edge_width is
  ## not mapped to a column but instead interpreted as the actual edge width)
  edge_width = 4,
  ## provide the timeline object
  timeline = timeline,
  ## map the shape of the end node to the outcome column in the timeline object
  tl_end_node_shape = "outcome",
  ## set the size of the end node to 15 (as this is not a character, this
  ## argument is not mapped to a column but instead interpreted as the actual
  ## node size)
  tl_end_node_size = 15,
  ## map the colour of the timeline edge to the hospital column
  tl_edge_color = "hospital",
  ## set the width of the timeline edge to 2 (as this is not a character, this
  ## argument is not mapped to a column but instead interpreted as the actual
  ## edge width)
  tl_edge_width = 2,
  ## map edge labels to the hospital variable
  tl_edge_label = "hospital",
  ## specify the shape for everyone node attribute (defined above)
  shapes = shapes,
  ## specify the colour palette (defined above)
  col_pal = colours,
  ## set the size of the arrow to 0.5
  arrow_size = 0.5,
  ## use two columns in the legend
  legend_ncol = 2,
  ## set font size
  font_size = 15,
  ## define formatting for dates
  date_labels = c("%d %b %Y"),
  ## don't plot the ID labels below nodes
  label = FALSE,
  ## specify height
  height = 1000,
  ## specify width
  width = 1200,
  ## ensure each case node has a unique y-coordinate - this is very important
  ## when using timelines, otherwise you will have overlapping timelines from
  ## different cases
  position_dodge = TRUE
)

```

<!-- ======================================================= -->
## Analysis {  }

### Summarising {.unnumbered}

We can get an overview of some of the network properties using the
`summary` function.

```{r transmission_chains_summarise_epicontacts,}
## summarise epicontacts object
summary(epic)
```

For example, we can see that only 57% of contacts have both cases in the
linelist; this means that the we do not have linelist data on a significant
number of cases involved in these transmission chains.

### Pairwise characteristics {.unnumbered}

The `get_pairwise()` function allows processing of variable(s) in the line list
according to each pair in the contact dataset. For the following example, date
of onset of disease is extracted from the line list in order to compute the
difference between disease date of onset for each pair. The value that is
produced from this comparison represents the **serial interval (si)**.

```{r transmission_chains_pairwise,}
si <- get_pairwise(epic, "date_onset")   
summary(si)
tibble(si = si) %>%
  ggplot(aes(si)) +
  geom_histogram() +
  labs(
    x = "Serial interval",
    y = "Frequency"
  )
```

The `get_pairwise()` will interpret the class of the column being used for
comparison, and will adjust its method of comparing the values accordingly. For
numbers and dates (like the **si** example above), the function will subtract
the values. When applied to columns that are characters or categorical,
`get_pairwise()` will paste values together. Because the function also allows
for arbitrary processing (see "f" argument), these discrete combinations can be
easily tabulated and analyzed.
    
```{r transmission_chains_pairwise_2,}
head(get_pairwise(epic, "gender"), n = 10)
get_pairwise(epic, "gender", f = table)
fisher.test(get_pairwise(epic, "gender", f = table))
```

Here, we see a significant association between transmission links and gender.

### Identifying clusters {.unnumbered}

The `get_clusters()` function can be used for to identify connected components
in an `epicontacts` object. First, we use it to retrieve a `data.frame`
containing the cluster information:

```{r transmission_chains_cluster,}
clust <- get_clusters(epic, output = "data.frame")
table(clust$cluster_size)
ggplot(clust, aes(cluster_size)) +
  geom_bar() +
  labs(
    x = "Cluster size",
    y = "Frequency"
  )
```

Let us look at the largest clusters. For this, we add cluster information to the
`epicontacts` object and then subset it to keep only the largest clusters:

```{r transmission_chains_cluster_2,}
epic <- get_clusters(epic)
max_size <- max(epic$linelist$cluster_size)
plot(subset(epic, cs = max_size))
```

### Calculating degrees {.unnumbered}

The degree of a node corresponds to its number of edges or connections to other
nodes. `get_degree()` provides an easy method for calculating this value for
`epicontacts` networks. A high degree in this context indicates an individual
who was in contact with many others. The `type` argument indicates that we want
to count both the in-degree and out-degree, the `only_linelist` argument
indicates that we only want to calculate the degree for cases in the linelist.

```{r transmission_chains_degree,}
deg_both <- get_degree(epic, type = "both", only_linelist = TRUE)
```

Which individuals have the ten most contacts?

```{r}
head(sort(deg_both, decreasing = TRUE), 10)
```

What is the mean number of contacts?

```{r}
mean(deg_both)
```

<!-- ======================================================= -->
## Resources {  }

The
[epicontacts page](https://www.repidemicsconsortium.org/epicontacts/index.html)
provides an overview of the package functions and includes some more in-depth
vignettes.

The [github page](http://github.com/reconhub/epicontacts) can be used to raise
issues and request features.
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/transmission_chains.Rmd-->


# Cây phả hệ {#phylogenetic-trees}  


<!-- ======================================================= -->

## Overview {}


**Phylogenetic trees** are used to visualize and describe the relatedness and evolution of organisms based on the sequence of their genetic code.  

They can be constructed from genetic sequences using distance-based methods (such as neighbor-joining method) or character-based methods (such as maximum likelihood and Bayesian Markov Chain Monte Carlo method). Next-generation sequencing (NGS) has become more affordable and is becoming more widely used in public health to describe pathogens causing infectious diseases. Portable sequencing devices decrease the turn around time and hold promises to make data available for the support of outbreak investigation in real-time. NGS data can be used to identify the origin or source of an outbreak strain and its propagation, as well as determine presence of antimicrobial resistance genes. To visualize the genetic relatedness between samples a phylogenetic tree is constructed.  

In this page we will learn how to use the **ggtree** package, which allows for combined visualization of phylogenetic trees with additional sample data in form of a dataframe. This will enable us to observe patterns and improve understanding of the outbreak dynamic.

```{r, phylogenetic_trees_overview_graph, out.width=c('80%'), fig.align='center', fig.show='hold', echo = FALSE}

pacman::p_load(here, ggplot2, dplyr, ape, ggtree, treeio, ggnewscale)

tree <- ape::read.tree(here::here("data", "phylo", "Shigella_tree.txt"))

sample_data <- read.csv(here::here("data","phylo", "sample_data_Shigella_tree.csv"),sep=",", na.strings=c("NA"), head = TRUE, stringsAsFactors=F)


ggtree(tree, layout="circular", branch.length='none') %<+% sample_data + # the %<+% is used to add your dataframe with sample data to the tree
  aes(color=I(Belgium))+ # color the branches according to a variable in your dataframe
  scale_color_manual(name = "Sample Origin", # name of your color scheme (will show up in the legend like this)
                    breaks = c("Yes", "No"), # the different options in your variable
                   labels = c("NRCSS Belgium", "Other"), # how you want the different options named in your legend, allows for formatting
                 values= c("blue", "black"), # the color you want to assign to the variable 
                 na.value = "black") + # color NA values in black as well
  new_scale_color()+ # allows to add an additional color scheme for another variable
     geom_tippoint(aes(color=Continent), size=1.5)+ # color the tip point by continent, you may change shape adding "shape = "
scale_color_brewer(name = "Continent",  # name of your color scheme (will show up in the legend like this)
                       palette="Set1", # we choose a set of colors coming with the brewer package
                   na.value="grey")+ # for the NA values we choose the color grey
  theme(legend.position= "bottom")

```

<!-- ======================================================= -->

## Preparation {}

### Load packages {.unnumbered}  

This code chunk shows the loading of required packages. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r, phylogenetic_trees_loading_packages}
pacman::p_load(
  rio,             # import/export
  here,            # relative file paths
  tidyverse,       # general data management and visualization
  ape,             # to import and export phylogenetic files
  ggtree,          # to visualize phylogenetic files
  treeio,          # to visualize phylogenetic files
  ggnewscale)      # to add additional layers of color schemes

```

### Import data {.unnumbered}  

The data for this page can be downloaded with the instructions on the [Download handbook and data] page.  

There are several different formats in which a phylogenetic tree can be stored (eg. Newick, NEXUS, Phylip). A common one is the Newick file format (.nwk), which is the standard for representing trees in computer-readable form. This means an entire tree can be expressed in a string format such as  "((t2:0.04,t1:0.34):0.89,(t5:0.37,(t4:0.03,t3:0.67):0.9):0.59); ", listing all nodes and tips and their relationship (branch length) to each other.  

Note: It is important to understand that the phylogenetic tree file in itself does not contain sequencing data, but is merely the result of the genetic distances between the sequences. We therefore cannot extract sequencing data from a tree file.

First, we use the `read.tree()` function from **ape** package to import a Newick phylogenetic tree file in .txt format, and store it in a list object of class "phylo". If necessary, use the  `here()` function from the **here** package to specify the relative file path.

Note: In this case the newick tree is saved as a .txt file for easier handling and downloading from Github.

```{r, echo=F}
tree <- ape::read.tree(here::here("data", "phylo", "Shigella_tree.txt"))
```


```{r, echo=T, eval=F}
tree <- ape::read.tree("Shigella_tree.txt")
```

We inspect our tree object and see it contains 299 tips (or samples) and 236 nodes.  

```{r}
tree
```

Second, we import a table stored as a .csv file with additional information for each sequenced sample, such as gender, country of origin and attributes for antimicrobial resistance, using the `import()` function from the **rio** package:

```{r, echo=F}
sample_data <- import(here("data", "phylo", "sample_data_Shigella_tree.csv"))
```

```{r, echo=T, eval=F}
sample_data <- import("sample_data_Shigella_tree.csv")
```

Below are the first 50 rows of the data:  

```{r message=FALSE, echo=F}
DT::datatable(head(sample_data,50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

### Clean and inspect {.unnumbered}  

We clean and inspect our data: In order to assign the correct sample data to the phylogenetic tree, the values in the column `Sample_ID` in the `sample_data` data frame need to match the `tip.labels` values in the `tree` file: 

We check the formatting of the `tip.labels` in the `tree` file by looking at the first 6 entries using with `head()` from **base** R.
```{r, phylogenetic_trees_inspect_sampledata}
head(tree$tip.label) 
```

We also make sure the first column in our `sample_data` data frame is `Sample_ID`. We look at the column names of our dataframe using  `colnames()` from **base** R.

```{r}
colnames(sample_data)   
```

We look at the `Sample_IDs` in the data frame to make sure the formatting is the same than in the `tip.label` (eg. letters are all capitals, no extra underscores `_` between letters and numbers, etc.)

```{r}
head(sample_data$Sample_ID) # we again inspect only the first 6 using head()
```

We can also compare if all samples are present in the `tree` file and vice versa by generating a logical vector of TRUE or FALSE where they do or do not match. These are not printed here, for simplicity.  

```{r, eval=F}
sample_data$Sample_ID %in% tree$tip.label

tree$tip.label %in% sample_data$Sample_ID
```

We can use these vectors to show any sample IDs that are not on the tree (there are none).  

```{r}
sample_data$Sample_ID[!tree$tip.label %in% sample_data$Sample_ID]
```

Upon inspection we can see that the format of `Sample_ID` in the dataframe corresponds to the format of sample names at the `tip.labels`. These do not have to be sorted in the same order to be matched.

We are ready to go!




<!-- ======================================================= -->

## Simple tree visualization {}


### Different tree layouts {.unnumbered}  

**ggtree** offers many different layout formats and some may be more suitable for your specific purpose than others. Below are a few demonstrations. For other options see this [online book](http://yulab-smu.top/treedata-book/chapter4.html).  

Here are some example tree layouts:
```{r, phylogenetic_trees_example_formats, out.width=c('50%'), fig.show='hold'}

ggtree(tree)                                            # simple linear tree
ggtree(tree,  branch.length = "none")                   # simple linear tree with all tips aligned
ggtree(tree, layout="circular")                         # simple circular tree
ggtree(tree, layout="circular", branch.length = "none") # simple circular tree with all tips aligned

```

### Simple tree plus sample data {.unnumbered}  

The **%<+%** operator is used to connect the `sample_data` data frame to the `tree` file.
The most easy annotation of your tree is the addition of the sample names at the tips, as well as coloring of tip points and if desired the branches:

Here is an example of a circular tree: 
```{r, phylogenetic_trees_adding_sampledata, fig.align='center', warning=F, message=F}

ggtree(tree, layout = "circular", branch.length = 'none') %<+% sample_data + # %<+% adds dataframe with sample data to tree
  aes(color = I(Belgium))+                       # color the branches according to a variable in your dataframe
  scale_color_manual(
    name = "Sample Origin",                      # name of your color scheme (will show up in the legend like this)
    breaks = c("Yes", "No"),                     # the different options in your variable
    labels = c("NRCSS Belgium", "Other"),        # how you want the different options named in your legend, allows for formatting
    values = c("blue", "black"),                  # the color you want to assign to the variable 
    na.value = "black") +                        # color NA values in black as well
  new_scale_color()+                             # allows to add an additional color scheme for another variable
    geom_tippoint(
      mapping = aes(color = Continent),          # tip color by continent. You may change shape adding "shape = "
      size = 1.5)+                               # define the size of the point at the tip
  scale_color_brewer(
    name = "Continent",                    # name of your color scheme (will show up in the legend like this)
    palette = "Set1",                      # we choose a set of colors coming with the brewer package
    na.value = "grey") +                    # for the NA values we choose the color grey
  geom_tiplab(                             # adds name of sample to tip of its branch 
    color = 'black',                       # (add as many text lines as you wish with + , but you may need to adjust offset value to place them next to each other)
    offset = 1,
    size = 1,
    geom = "text",
    align = TRUE)+    
  ggtitle("Phylogenetic tree of Shigella sonnei")+       # title of your graph
  theme(
    axis.title.x = element_blank(), # removes x-axis title
    axis.title.y = element_blank(), # removes y-axis title
    legend.title = element_text(    # defines font size and format of the legend title
      face = "bold",
      size = 12),   
    legend.text=element_text(       # defines font size and format of the legend text
      face = "bold",
      size = 10),  
    plot.title = element_text(      # defines font size and format of the plot title
      size = 12,
      face = "bold"),  
    legend.position = "bottom",     # defines placement of the legend
    legend.box = "vertical",        # defines placement of the legend
    legend.margin = margin())   
```

You can export your tree plot with `ggsave()` as you would any other ggplot object. Written this way, `ggsave()` saves the last image produced to the file path you specify. Remember that you can use `here()` and relative file paths to easily save in subfolders, etc.  

```{r, eval=F}
ggsave("example_tree_circular_1.png", width = 12, height = 14)

```


<!-- ======================================================= -->

## Tree manipulation {}

Sometimes you may have a very large phylogenetic tree and you are only interested in one part of the tree. For example, if you produced a tree including historical or international samples to get a large overview of where your dataset might fit in the bigger picture. But then to look closer at your data you want to inspect only that portion of the bigger tree.

Since the phylogenetic tree file is just the output of sequencing data analysis, we can not manipulate the order of the nodes and branches in the file itself. These have already been determined in previous analysis from the raw NGS data. We are able though to zoom into parts, hide parts and even subset part of the tree. 

### Zoom in {.unnumbered}  

If you don't want to "cut" your tree, but only inspect part of it more closely you can zoom in to view a specific part.

First, we plot the entire tree in linear format and add numeric labels to each node in the tree.
```{r, phylogenetic_trees_zoom_in, out.width=c('50%'), fig.show='hold', fig.align='center'}

p <- ggtree(tree,) %<+% sample_data +
  geom_tiplab(size = 1.5) +                # labels the tips of all branches with the sample name in the tree file
  geom_text2(
    mapping = aes(subset = !isTip,
                  label = node),
    size = 5,
    color = "darkred",
    hjust = 1,
    vjust = 1)                            # labels all the nodes in the tree

p  # print

```

To zoom in to one particular branch (sticking out to the right), use `viewClade()` on the ggtree object `p` and provide the node number to get a closer look:
```{r phylogenetic_trees_zoom_in_452, out.width=c('50%'), fig.show='hold', fig.align='center'}

viewClade(p, node = 452)

```

### Collapsing branches {.unnumbered} 

However, we may want to ignore this branch and can collapse it at that same node (node nr. 452) using `collapse()`. This tree is defined as `p_collapsed`. 

```{r phylogenetic_trees_collapse_452, out.width=c('50%'), fig.show='hold', fig.align='center'}

p_collapsed <- collapse(p, node = 452)
p_collapsed
```

For clarity, when we print `p_collapsed`, we add a `geom_point2()` (a blue diamond) at the node of the collapsed branch.  
```{r}
p_collapsed + 
geom_point2(aes(subset = (node == 452)),  # we assign a symbol to the collapsed node
            size = 5,                     # define the size of the symbol
            shape = 23,                   # define the shape of the symbol
            fill = "steelblue")           # define the color of the symbol
```

### Subsetting a tree {.unnumbered} 

If we want to make a more permanent change and create a new, reduced tree to work with we can subset part of it with `tree_subset()`. Then you can save it as new newick tree file or .txt file. 

First, we inspect the tree nodes and tip labels in order to decide what to subset.  

```{r, phylogenetic_trees_subsetting, out.width=c('50%'), fig.show='hold', fig.align='center'}
ggtree(
  tree,
  branch.length = 'none',
  layout = 'circular') %<+% sample_data +               # we add the asmple data using the %<+% operator
  geom_tiplab(size = 1)+                                # label tips of all branches with sample name in tree file
  geom_text2(
    mapping = aes(subset = !isTip, label = node),
    size = 3,
    color = "darkred") +                                # labels all the nodes in the tree
 theme(
   legend.position = "none",                            # removes the legend all together
   axis.title.x = element_blank(),
   axis.title.y = element_blank(),
   plot.title = element_text(size = 12, face="bold"))
```

Now, say we have decided to subset the tree at node 528 (keep only tips within this branch after node 528) and we save it as a new `sub_tree1` object:

```{r}
sub_tree1 <- tree_subset(
  tree,
  node = 528)                                            # we subset the tree at node 528
```

Lets have a look at the subset tree 1:

```{r}
ggtree(sub_tree1) +
  geom_tiplab(size = 3) +
  ggtitle("Subset tree 1")
```

You can also subset based on one particular sample, specifying how many nodes "backwards" you want to include. Let's subset the same part of the tree based on a sample, in this case S17BD07692, going back 9 nodes and we save it as a new `sub_tree2` object:

```{r}
sub_tree2 <- tree_subset(
  tree,
  "S17BD07692",
  levels_back = 9) # levels back defines how many nodes backwards from the sample tip you want to go
```

Lets have a look at the subset tree 2:

```{r}
ggtree(sub_tree2) +
  geom_tiplab(size =3)  +
  ggtitle("Subset tree 2")

```

You can also save your new tree either as a Newick type or even a text file using the `write.tree()` function from **ape** package:

```{r, eval=F, phylogenetic_trees_write_tree}
# to save in .nwk format
ape::write.tree(sub_tree2, file='data/phylo/Shigella_subtree_2.nwk')

# to save in .txt format
ape::write.tree(sub_tree2, file='data/phylo/Shigella_subtree_2.txt')

```

### Rotating nodes in a tree {.unnumbered} 


As mentioned before we cannot change the order of tips or nodes in the tree, as this is based on their genetic relatedness and is not subject to visual manipulation. But we can rote branches around nodes if that eases our visualization.

First, we plot our new subset tree 2 with node labels to choose the node we want to manipulate and store it an a ggtree plot object `p`.

```{r, phylogenetic_trees_rotating_1, out.width=c('50%'), fig.show='hold', fig.align='center'}

p <- ggtree(sub_tree2) +  
  geom_tiplab(size = 4) +
  geom_text2(aes(subset=!isTip, label=node), # labels all the nodes in the tree
             size = 5,
             color = "darkred", 
             hjust = 1, 
             vjust = 1) 
p
```

We can then manipulate nodes by applying **ggtree::rotate()** or **ggtree::flip()**: 
Note: to illustrate which nodes we are manipulating we first apply the **geom_hilight()** function from **ggtree** to highlight the samples in the nodes we are interested in and store that ggtree plot object in a new object `p1`.

```{r, phylogenetic_trees_rotating_2, out.width=c('50%'), fig.show='hold', fig.align='center'}

p1 <- p + geom_hilight(  # highlights node 39 in blue, "extend =" allows us to define the length of the color block
  node = 39,
  fill = "steelblue",
  extend = 0.0017) +  
geom_hilight(            # highlights the node 37 in yellow
  node = 37,
  fill = "yellow",
  extend = 0.0017) +               
ggtitle("Original tree")


p1 # print
```

Now we can rotate node 37 in object `p1` so that the samples on node 38 move to the top. We store the rotated tree in a new object `p2`.
```{r}
p2 <- rotate(p1, 37) + 
      ggtitle("Rotated Node 37")


p2   # print
```

Or we can use the flip command to rotate node 36 in object `p1` and switch node 37 to the top and node 39 to the bottom. We store the flipped tree in a new object `p3`.
```{r}

p3 <-  flip(p1, 39, 37) +
      ggtitle("Rotated Node 36")


p3   # print
```

### Example subtree with sample data annotation {.unnumbered} 

Lets say we are investigating the cluster of cases with clonal expansion which occurred in 2017 and 2018 at node 39 in our sub-tree. We add the year of strain isolation as well as travel history and color by country to see origin of other closely related strains:

```{r, phylogenetic_trees_inspect_subset_example, out.width=c('80%'), fig.show='hold', fig.align='center', warning=F, message=F}

ggtree(sub_tree2) %<+% sample_data +     # we use th %<+% operator to link to the sample_data
  geom_tiplab(                          # labels the tips of all branches with the sample name in the tree file
    size = 2.5,
    offset = 0.001,
    align = TRUE) + 
  theme_tree2()+
  xlim(0, 0.015)+                       # set the x-axis limits of our tree
  geom_tippoint(aes(color=Country),     # color the tip point by continent
                size = 1.5)+ 
  scale_color_brewer(
    name = "Country", 
    palette = "Set1", 
    na.value = "grey")+
  geom_tiplab(                          # add isolation year as a text label at the tips
    aes(label = Year),
    color = 'blue',
    offset = 0.0045,
    size = 3,
    linetype = "blank" ,
    geom = "text",
    align = TRUE)+ 
  geom_tiplab(                          # add travel history as a text label at the tips, in red color
    aes(label = Travel_history),
    color = 'red',
    offset = 0.006,
    size = 3,
    linetype = "blank",
    geom = "text",
    align = TRUE)+ 
  ggtitle("Phylogenetic tree of Belgian S. sonnei strains with travel history")+  # add plot title
  xlab("genetic distance (0.001 = 4 nucleotides difference)")+                    # add a label to the x-axis 
  theme(
    axis.title.x = element_text(size = 10),
    axis.title.y = element_blank(),
    legend.title = element_text(face = "bold", size = 12),
    legend.text = element_text(face = "bold", size = 10),
    plot.title = element_text(size = 12, face = "bold"))

```

Our observation points towards an import event of strains from Asia, which then circulated in Belgium over the years and seem to have caused our latest outbreak.

<!-- ======================================================= -->

## More complex trees: adding heatmaps of sample data {.unnumbered}


We can add more complex information, such as categorical presence of antimicrobial resistance genes and numeric values for actually measured resistance to antimicrobials in form of a heatmap using the **ggtree::gheatmap()** function.

First we need to plot our tree (this can be either linear or circular) and store it in a new ggtree plot object `p`: We will use the sub_tree from part 3.)
```{r, phylogenetic_trees_sampledata_heatmap, out.width=c('60%'), fig.align='center', fig.show='hold'}

p <- ggtree(sub_tree2, branch.length='none', layout='circular') %<+% sample_data +
  geom_tiplab(size =3) + 
 theme(
   legend.position = "none",
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    plot.title = element_text(
      size = 12,
      face = "bold",
      hjust = 0.5,
      vjust = -15))
p

```

Second, we prepare our data. To visualize different variables with new color schemes, we subset our dataframe to the desired variable. It is important to add the `Sample_ID` as rownames otherwise it cannot match the data to the tree `tip.labels`:

In our example we want to look at gender and mutations that could confer resistance to Ciprofloxacin, an important first line antibiotic used to treat Shigella infections.

We create a dataframe for gender: 
```{r, phylogenetic_trees_sampledata_heatmap_data}
gender <- data.frame("gender" = sample_data[,c("Gender")])
rownames(gender) <- sample_data$Sample_ID
```

We create a dataframe for mutations in the gyrA gene, which confer Ciprofloxacin resistance:
```{r}
cipR <- data.frame("cipR" = sample_data[,c("gyrA_mutations")])
rownames(cipR) <- sample_data$Sample_ID

```
We create a dataframe for the measured minimum inhibitory concentration (MIC) for Ciprofloxacin from the laboratory:
```{r}
MIC_Cip <- data.frame("mic_cip" = sample_data[,c("MIC_CIP")])
rownames(MIC_Cip) <- sample_data$Sample_ID
```

We create a first plot adding a binary heatmap for gender to the phylogenetic tree and storing it in a new ggtree plot object `h1`:
```{r, phylogenetic_trees_sampledata_heatmap_gender, out.width=c('70%'), fig.show='hold', fig.align='center'}

h1 <-  gheatmap(p, gender,                                 # we add a heatmap layer of the gender dataframe to our tree plot
                offset = 10,                               # offset shifts the heatmap to the right,
                width = 0.10,                              # width defines the width of the heatmap column,
                color = NULL,                              # color defines the boarder of the heatmap columns
         colnames = FALSE) +                               # hides column names for the heatmap
  scale_fill_manual(name = "Gender",                       # define the coloring scheme and legend for gender
                    values = c("#00d1b1", "purple"),
                    breaks = c("Male", "Female"),
                    labels = c("Male", "Female")) +
   theme(legend.position = "bottom",
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 10),
        legend.box = "vertical", legend.margin = margin())
h1

```

Then we add information on mutations in the gyrA gene, which confer resistance to Ciprofloxacin:

Note: The presence of chromosomal point mutations in WGS data was prior determined using the PointFinder tool developed by Zankari et al. (see reference in the additional references section)

First, we assign a new color scheme to our existing plot object `h1` and store it in a now object `h2`. This enables us to define and change the colors for our second variable in the heatmap.
```{r}
h2 <- h1 + new_scale_fill() 
```

Then we add the second heatmap layer to `h2` and store the combined plots in a new object `h3`:

```{r, phylogenetic_trees_sampledata_heatmap_cip_genes, out.width=c('80%'), fig.show='hold', fig.align='center'}

h3 <- gheatmap(h2, cipR,         # adds the second row of heatmap describing Ciprofloxacin resistance mutations
               offset = 12, 
               width = 0.10, 
               colnames = FALSE) +
  scale_fill_manual(name = "Ciprofloxacin resistance \n conferring mutation",
                    values = c("#fe9698","#ea0c92"),
                    breaks = c( "gyrA D87Y", "gyrA S83L"),
                    labels = c( "gyrA d87y", "gyrA s83l")) +
   theme(legend.position = "bottom",
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 10),
        legend.box = "vertical", legend.margin = margin())+
  guides(fill = guide_legend(nrow = 2,byrow = TRUE))
h3
```

We repeat the above process, by first adding a new color scale layer to our existing object `h3`, and then adding the continuous data on the minimum inhibitory concentration (MIC) of Ciprofloxacin for each strain to the resulting object `h4` to produce the final object `h5`:
```{r, phylogenetic_trees_sampledata_heatmap_cip_MIC, out.width=c('90%'), fig.show='hold', fig.align='center'}
# First we add the new coloring scheme:
h4 <- h3 + new_scale_fill()

# then we combine the two into a new plot:
h5 <- gheatmap(h4, MIC_Cip,  
               offset = 14, 
               width = 0.10,
                colnames = FALSE)+
  scale_fill_continuous(name = "MIC for Ciprofloxacin",  # here we define a gradient color scheme for the continuous variable of MIC
                      low = "yellow", high = "red",
                      breaks = c(0, 0.50, 1.00),
                      na.value = "white") +
   guides(fill = guide_colourbar(barwidth = 5, barheight = 1))+
   theme(legend.position = "bottom",
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 10),
        legend.box = "vertical", legend.margin = margin())
h5

```

We can do the same exercise for a linear tree:
```{r, phylogenetic_trees_sampledata_heatmap_linear_1, out.width=c('80%'), fig.show='hold', fig.align='center'}

p <- ggtree(sub_tree2) %<+% sample_data +
  geom_tiplab(size = 3) + # labels the tips
  theme_tree2()+
  xlab("genetic distance (0.001 = 4 nucleotides difference)")+
  xlim(0, 0.015)+
 theme(legend.position = "none",
      axis.title.y = element_blank(),
      plot.title = element_text(size = 12, 
                                face = "bold",
                                hjust = 0.5,
                                vjust = -15))
p
```

First we add gender:  

```{r, phylogenetic_trees_sampledata_heatmap_linear_2, out.width=c('80%'), fig.show='hold', fig.align='center'}

h1 <-  gheatmap(p, gender, 
                offset = 0.003,
                width = 0.1, 
                color="black", 
         colnames = FALSE)+
  scale_fill_manual(name = "Gender",
                    values = c("#00d1b1", "purple"),
                    breaks = c("Male", "Female"),
                    labels = c("Male", "Female"))+
   theme(legend.position = "bottom",
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 10),
        legend.box = "vertical", legend.margin = margin())
h1
```


Then we add Ciprofloxacin resistance mutations after adding another color scheme layer:


```{r, phylogenetic_trees_sampledata_heatmap_linear_3, out.width=c('80%'), fig.show='hold', fig.align='center'}

h2 <- h1 + new_scale_fill()
h3 <- gheatmap(h2, cipR,   
               offset = 0.004, 
               width = 0.1,
               color = "black",
                colnames = FALSE)+
  scale_fill_manual(name = "Ciprofloxacin resistance \n conferring mutation",
                    values = c("#fe9698","#ea0c92"),
                    breaks = c( "gyrA D87Y", "gyrA S83L"),
                    labels = c( "gyrA d87y", "gyrA s83l"))+
   theme(legend.position = "bottom",
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 10),
        legend.box = "vertical", legend.margin = margin())+
  guides(fill = guide_legend(nrow = 2,byrow = TRUE))
 h3
```

Then we add the minimum inhibitory concentration determined by the laboratory (MIC):

```{r, phylogenetic_trees_sampledata_heatmap_linear_4, out.width=c('80%'), fig.show='hold', fig.align='center'}

h4 <- h3 + new_scale_fill()
h5 <- gheatmap(h4, MIC_Cip, 
               offset = 0.005,  
               width = 0.1,
               color = "black", 
                colnames = FALSE)+
  scale_fill_continuous(name = "MIC for Ciprofloxacin",
                      low = "yellow", high = "red",
                      breaks = c(0,0.50,1.00),
                      na.value = "white")+
   guides(fill = guide_colourbar(barwidth = 5, barheight = 1))+
   theme(legend.position = "bottom",
        legend.title = element_text(size = 10),
        legend.text = element_text(size = 8),
        legend.box = "horizontal", legend.margin = margin())+
  guides(shape = guide_legend(override.aes = list(size = 2)))
h5

```


<!-- ======================================================= -->
## Resources {}

http://hydrodictyon.eeb.uconn.edu/eebedia/index.php/Ggtree# Clade_Colors
https://bioconductor.riken.jp/packages/3.2/bioc/vignettes/ggtree/inst/doc/treeManipulation.html
https://guangchuangyu.github.io/ggtree-book/chapter-ggtree.html
https://bioconductor.riken.jp/packages/3.8/bioc/vignettes/ggtree/inst/doc/treeManipulation.html

Ea Zankari, Rosa Allesøe, Katrine G Joensen, Lina M Cavaco, Ole Lund, Frank M Aarestrup, PointFinder: a novel web tool for WGS-based detection of antimicrobial resistance associated with chromosomal point mutations in bacterial pathogens, Journal of Antimicrobial Chemotherapy, Volume 72, Issue 10, October 2017, Pages 2764–2768, https://doi.org/10.1093/jac/dkx217


```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/phylogenetic_trees.Rmd-->


# Biểu đồ tương tác {#interactive-plots}  

Data visualisation is increasingly required to be interrogable by the audience. Consequently, is is becoming common to create interactive plots. There are several ways to include these but the two most common are **plotly** and **shiny**. 

In this page we will focus on converting an existing `ggplot()` plot into an interactive plot with **plotly**. You can read more about **shiny** in the [Dashboards with Shiny] page. What is worth mentioning is that interactive plots are only useable in HTML format R markdown documents, not PDF or Word documents.

Below is a basic epicurve that has been transformed to be interactive using the integration of **ggplot2** and **plotly** (hover your mouse over the plot, zoom in, or click items in the legend). 

```{r plotly_demo, out.width=c('75%'), out.height=c('500px'), echo=F, warning=F, message=F}
pacman::p_load(plotly, rio, here, ggplot2, dplyr, lubridate)
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

## these buttons are superfluous/distracting
plotly_buttons_remove <- list('zoom2d','pan2d','lasso2d', 'select2d','zoomIn2d',
                              'zoomOut2d','autoScale2d','hoverClosestCartesian',
                              'toggleSpikelines','hoverCompareCartesian')

p <- linelist %>% 
  mutate(outcome = if_else(is.na(outcome), "Unknown", outcome),
         date_earliest = if_else(is.na(date_infection), date_onset, date_infection),
         week_earliest = floor_date(date_earliest, unit = "week",week_start = 1))%>% 
  count(week_earliest, outcome) %>% 
  ggplot()+
  geom_col(aes(week_earliest, n, fill = outcome))+
  xlab("Week of infection/onset") + ylab("Cases per week")+
  theme_minimal()

p %>% 
  ggplotly() %>% 
  partial_bundle() %>% 
  config(displaylogo = FALSE, modeBarButtonsToRemove = plotly_buttons_remove)

```

<!-- ======================================================= -->
## Preparation {  }

### Load packages {.unnumbered}  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  


```{r}
pacman::p_load(
  rio,       # import/export
  here,      # filepaths
  lubridate, # working with dates
  plotly,    # interactive plots
  scales,    # quick percents
  tidyverse  # data management and visualization
  ) 
```

### Start with a `ggplot()` {.unnumbered}  

In this page we assume that you are beginning with a `ggplot()` plot that you want to convert to be interactive. We will build several of these plots in this page, using the case `linelist` used in many pages of this handbook.  


### Import data {.unnumbered}

To begin, we import the cleaned linelist of cases from a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Import data with the `import()` function from the **rio** package (it handles many file types like .xlsx, .csv, .rds - see the [Import and export] page for details).  

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import case linelist 
linelist <- import("linelist_cleaned.rds")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```






  
<!-- ======================================================= -->
## Plot with `ggplotly()` {  }

The function `ggplotly()` from the **plotly** package makes it easy to convert a `ggplot()` to be interactive. Simply save your `ggplot()` and then pipe it to the `ggplotly()` function.  


Below, we plot a simple line representing the proportion of cases who died in a given week:  

We begin by creating a summary dataset of each epidemiological week, and the percent of cases with a known outcome that died.  

```{r}
weekly_deaths <- linelist %>%
  group_by(epiweek = floor_date(date_onset, "week")) %>%  # create and group data by epiweek column
  summarise(                                              # create new summary data frame:
    n_known_outcome = sum(!is.na(outcome), na.rm=T),      # number of cases per group with known outcome
    n_death  = sum(outcome == "Death", na.rm=T),          # number of cases per group who died
    pct_death = 100*(n_death / n_known_outcome)           # percent of cases with known outcome who died
  )
```
Here is the first 50 rows of the `weekly_deaths` dataset.  

```{r message=FALSE, echo=F}
DT::datatable(head(weekly_deaths, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```
Then we create the plot with **ggplot2**, using `geom_line()`.  

```{r, warning=F, message=F}
deaths_plot <- ggplot(data = weekly_deaths)+            # begin with weekly deaths data
  geom_line(mapping = aes(x = epiweek, y = pct_death))  # make line 

deaths_plot   # print
```


We can make this interactive by simply passing this plot to `ggplotly()`, as below. Hover your mouse over the line to show the x and y values. You can zoom in on the plot, and drag it around. You can also see icons in the upper-right of the plot. In order, they allow you to:  

* Download the current view as a PNG image  
* Zoom in with a select box  
* "Pan", or move across the plot by clicking and dragging the plot  
* Zoom in, zoom out, or return to default zoom  
* Reset axes to defaults  
* Toggle on/off "spike lines" which are dotted lines from the interactive point extending to the x and y axes  
* Adjustments to whether data show when you are not hovering on the line  


```{r}
deaths_plot %>% plotly::ggplotly()
```

Grouped data work with `ggplotly()` as well. Below, a weekly epicurve is made, grouped by outcome. The stacked bars are interactive. Try clicking on the different items in the legend (they will appear/disappear).  


```{r plot_show, eval=F}
# Make epidemic curve with incidence2 pacakge
p <- incidence2::incidence(
  linelist,
  date_index = date_onset,
  interval = "weeks",
  groups = outcome) %>% plot(fill = outcome)
```

```{r, echo=T, eval=F}
# Plot interactively  
p %>% plotly::ggplotly()
```
  
```{r, warning = F, message = F, , out.width=c('95%'), out.height=c('500px'), echo=FALSE}
p %>% 
  ggplotly() %>% 
  partial_bundle() 
```
  
<!-- ======================================================= -->
## Modifications {  }

### File size {.unnumbered}  

When exporting in an R Markdown generated HTML (like this book!) you want to make the plot as small data size as possible (with no negative side effects in most cases). For this, just pipe the interactive plot to `partial_bundle()`, also from **plotly**.  

```{r plot_tidyshow, eval=F}
p <- p %>% 
  plotly::ggplotly() %>%
  plotly::partial_bundle()
```

### Buttons {.unnumbered}  

Some of the buttons on a standard plotly are superfluous and can be distracting, so you can remove them. You can do this simply by piping the output into `config()` from **plotly** and specifying which buttons to remove. In the below example we specify in advance the names of the buttons to remove, and provide them to the argument `modeBarButtonsToRemove = `. We also set `displaylogo = FALSE` to remove the plotly logo.  

```{r plot_tidyshow2, eval=F}
## these buttons are distracting and we want to remove them
plotly_buttons_remove <- list('zoom2d','pan2d','lasso2d', 'select2d','zoomIn2d',
                              'zoomOut2d','autoScale2d','hoverClosestCartesian',
                              'toggleSpikelines','hoverCompareCartesian')

p <- p %>%          # re-define interactive plot without these buttons
  plotly::config(displaylogo = FALSE, modeBarButtonsToRemove = plotly_buttons_remove)
```



<!-- ======================================================= -->
## Heat tiles {  }

You can make almost any `ggplot()` plot interactive, including heat tiles. In the page on [Heat plots] you can read about how to make the below plot, which displays the proportion of days per week that certain facilities reported data to their province.  

Here is the code, although we will not describe it in depth here.  

```{r  message=F, warning=F}
# import data
facility_count_data <- rio::import(here::here("data", "malaria_facility_count_data.rds"))

# aggregate data into Weeks for Spring district
agg_weeks <- facility_count_data %>% 
  filter(District == "Spring",
         data_date < as.Date("2020-08-01")) %>% 
  mutate(week = aweek::date2week(
    data_date,
    start_date = "Monday",
    floor_day = TRUE,
    factor = TRUE)) %>% 
  group_by(location_name, week, .drop = F) %>%
  summarise(
    n_days          = 7,
    n_reports       = n(),
    malaria_tot     = sum(malaria_tot, na.rm = T),
    n_days_reported = length(unique(data_date)),
    p_days_reported = round(100*(n_days_reported / n_days))) %>% 
  right_join(tidyr::expand(., week, location_name)) %>% 
  mutate(week = aweek::week2date(week))

# create plot
metrics_plot <- ggplot(agg_weeks,
       aes(x = week,
           y = location_name,
           fill = p_days_reported))+
  geom_tile(colour="white")+
  scale_fill_gradient(low = "orange", high = "darkgreen", na.value = "grey80")+
  scale_x_date(expand = c(0,0),
               date_breaks = "2 weeks",
               date_labels = "%d\n%b")+
  theme_minimal()+ 
  theme(
    legend.title = element_text(size=12, face="bold"),
    legend.text  = element_text(size=10, face="bold"),
    legend.key.height = grid::unit(1,"cm"),
    legend.key.width  = grid::unit(0.6,"cm"),
    axis.text.x = element_text(size=12),
    axis.text.y = element_text(vjust=0.2),
    axis.ticks = element_line(size=0.4),
    axis.title = element_text(size=12, face="bold"),
    plot.title = element_text(hjust=0,size=14,face="bold"),
    plot.caption = element_text(hjust = 0, face = "italic")
    )+
  labs(x = "Week",
       y = "Facility name",
       fill = "Reporting\nperformance (%)",
       title = "Percent of days per week that facility reported data",
       subtitle = "District health facilities, April-May 2019",
       caption = "7-day weeks beginning on Mondays.")

metrics_plot # print
```

Below, we make it interactive and modify it for simple buttons and file size.  

```{r,  out.width=c('95%'), out.height=c('500px')}
metrics_plot %>% 
  plotly::ggplotly() %>% 
  plotly::partial_bundle() %>% 
  plotly::config(displaylogo = FALSE, modeBarButtonsToRemove = plotly_buttons_remove)
```

<!-- ## Maps {.unnumbered}   -->

<!-- You can also make `ggplot()` GIS maps interactive, although it makes a bit more care.  -->

<!-- THIS SECTION IS UNDER CONSTRUCTION  -->

<!-- Although **plotly** works well with `ggplot2::geom_sf` in RStudio, when you try to include its outputs in R Markdown HTML files (like this book), it doesn't work well.   -->

<!-- So instead you can use {**plotly**}'s own mapping tools which can be tricky but are easy when you know how. Read on...   -->

<!-- We're going to use Covid-19 incidence across African countries for this example. The data used can be found on the [World Health Organisation website](https://covid19.who.int/table).   -->

<!-- You'll also need a new type of file, a GeoJSON, which is sort of similar to a shp file for those familiar with GIS. For this book, we used one from [here](https://geojson-maps.ash.ms).   -->

<!-- GeoJSON files are stored in R as complex lists and you'll need to maipulate them a little. -->

<!-- ```{r, echo=T,} -->
<!-- ## You need two new packages: {rjson} and {purrr} -->
<!-- pacman::p_load(plotly, rjson, purrr) -->

<!-- ## This is a simplified version of the WHO data -->
<!-- df <- rio::import(here::here("data", "gis", "covid_incidence.csv")) -->

<!-- ## Load your geojson file -->
<!-- geoJSON <- rjson::fromJSON(file=here::here("data", "gis", "africa_countries.geo.json")) -->

<!-- ## Here are some of the properties for each element of the object -->
<!-- head(geoJSON$features[[1]]$properties) -->

<!-- ``` -->


<!-- This is the tricky part. For {**plotly**} to match your incidence data to GeoJSON, the countries in the geoJSON need an id in a specific place in the list of lists. For this we need to build a basic function: -->
<!-- ```{r} -->
<!-- ## The property column we need to choose here is "sovereignt" as it is the names for each country -->
<!-- give_id <- function(x){ -->

<!--   x$id <- x$properties$sovereignt  ## Take sovereignt from properties and set it as the id -->

<!--   return(x) -->
<!-- } -->

<!-- ## Use {purrr} to apply this function to every element of the features list of the geoJSON object -->
<!-- geoJSON$features <- purrr::map(.x = geoJSON$features, give_id) -->
<!-- ``` -->

<!-- <!-- ======================================================= --> -->
<!-- ### Maps - plot {  } -->

<!-- UNDER CONSTRUCTION -->

<!-- ```{r, echo=FALSE, eval=FALSE, out.width=c('95%'), out.height=c('500px'),warning=F} -->
<!-- plotly::plot_ly() %>%  -->
<!--   plotly::add_trace(                    #The main plot mapping functionn -->
<!--     type="choropleth", -->
<!--     geojson=geoJSON, -->
<!--     locations=df$Name,          #The column with the names (must match id) -->
<!--     z=df$Cumulative_incidence,  #The column with the incidence values -->
<!--     zmin=0, -->
<!--     zmax=57008, -->
<!--     colorscale="Viridis", -->
<!--     marker=list(line=list(width=0)) -->
<!--   ) %>% -->
<!--   colorbar(title = "Cases per million") %>% -->
<!--   layout(title = "Covid-19 cumulative incidence", -->
<!--                  geo = list(scope = 'africa')) %>%  -->
<!--   config(displaylogo = FALSE, modeBarButtonsToRemove = plotly_buttons_remove) -->
<!-- ``` -->

<!-- ======================================================= -->
## Resources {  }

Plotly is not just for R, but also works well with Python (and really any data science language as it's built in JavaScript). You can read more about it on the [plotly website](https://plotly.com/r/)


```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/interactive_plots.Rmd-->

# (PART) Báo cáo và dashboards {.unnumbered}
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/cat_reports_dashboards.Rmd-->


# Báo cáo với R Markdown {#rmarkdown}  

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/rmarkdown_overview.png"))
```

R Markdown là một công cụ được sử dụng rộng rãi nhằm tạo ra tài liệu tự động, tái lập và chia sẻ được, như các báo cáo. Nó có thể tạo ra kết quả thống kê hoặc tương tác, ở định dạng Word, pdf, html, powerpoint, và các định dạng khác. 

Một R Markdown script chứa xen kẽ các code R và văn bản khiến script này thật sự *trở thành một tài liệu đầu ra của bạn*. Bạn có thể tạo ra một tài liệu đã được định dạng toàn bộ, gồm có văn bản thuần túy (có thể thay đổi dựa trên số liệu của bạn), bảng, biểu đồ, đầu mục/số, thư mục, v.v.   

Những tài liệu như vậy có thể được cập nhật thường xuyên (ví dụ: báo cáo giám sát hàng ngày) và/hoặc chạy một tập dữ liệu (ví dụ: báo cáo cho mỗi *jurisdiction*-quyền xét xử).    

Những chương khác trong cuốn sách này mở rộng các chủ đề khác:  

* Chương [Tổ chức báo cáo định kỳ] trình bày cách tự động hóa các báo cáo sản phẩm với các tập tin tự tạo kèm theo thời gian.  
* Chương [Dashboards với R Markdown] giải thích cách định dạng một báo cáo R Markdown dưới dạng một dashboard.  


Một lưu ý là dự án [R4Epis](https://r4epis.netlify.app/) đã tạo ra các mẫu R Markdown script cho những đợt dịch và trường hợp khảo sát phổ biến ở vị trí dự án MSF.  


<!-- ======================================================= -->
## Chuẩn bị {  }

**Nền tảng về R Markdown**

Nhằm giải thích một số khái niệm và packages cần thiết:

* **Markdown** là một "ngôn ngữ" cho phép bạn soạn tài liệu bằng chữ thuần túy, sau đó có thể chuyển đổi sang html và các định dạng khác. Nó không dành riêng cho R. Các tệp được viết trong Markdown có đuôi '.md'.  
* **R Markdown**: là một biến thể trên markdown _chỉ dành riêng cho R_ - nó cho phép bạn soạn một tài liệu sử dụng markdown để tạo chữ cũng như *nhúng code R và hiển thị kết quả đầu ra*. Các tệp R Markdown có đuôi '.Rmd'.   
* **Package rmarkdown**: Package được R sử dụng để chuyển tệp .Rmd thành đầu ra mong muốn. Nó tập trung vào việc chuyển cú pháp markdown (chữ), vì vậy chúng ta cũng cần tới... 
* **knitr**: Package R này sẽ đọc các đoạn code, thực thi chúng, và 'knit' (kết hợp) chúng vào lại tài liệu. Đây là cách bảng, biểu đồ được thêm vào văn bản.  
* **Pandoc**: Cuối cùng, pandoc thật sự chuyển đổi kết quả đầu ra thành word/pdf/powerpoint, v.v. Nó là một phần mềm tách biệt khỏi R nhưng được cài đặt tự động cùng với RStudio.  

Tổng kết lại, quá trình được tiến hành *trong nền* (bạn không cần biết tới tất cả những bước này!), gồm chuyển tệp .Rmd tới **knitr** để thực thi các đoạn code R và tạo một tệp .md (markdown) mới bao gồm cả code R lẫn kết quả đầu ra đã được chuyển đổi. Các tệp .md này sau đó được pandoc chạy để tạo ra sản phẩm hoàn thiện như là một tài liệu Microsoft Word, tệp HTML, tài liệu powerpoint, pdf, v.v.  

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/0_rmd.png"))
```

(Nguồn: https://rmarkdown.rstudio.com/authoring_quick_tour.html):

**Cài đặt**

Để tạo một kết quả đầu ra của R Markdown, bạn cần phải cài đặt:  

* Package **rmarkdown** (**knitr** cũng sẽ được cài đặt tự động)  
* Pandoc sẽ được cài đặt cùng với RStudio. Nếu bạn không dùng RStudio, bạn có thể tải Pandoc tại đây:  http://pandoc.org.  
* Nếu bạn muốn tạo đầu ra là tệp PDF (phức tạp hơn một chút), bạn sẽ cần cài đặt LaTex. Với những người dùng R Markdown chưa cài đặt LaTex trước đó, các bạn có thể cài đặt TinyTeX (https://yihui.name/tinytex/). Bạn có thể sử dụng lệnh sau để cài đặt:

```{r, eval=F}
pacman::p_load(tinytex)     # install tinytex package
tinytex::install_tinytex()  # R command to install TinyTeX software 
```

<!-- ======================================================= -->
## Bắt đầu {  }

### Cài đặt package rmarkdown  {.unnumbered}

Cài đặt R package **rmarkdown**. Trong quyển sổ tay này chúng tôi nhấn mạnh việc sử dụng hàm `p_load()` từ package **pacman**, giúp cài đặt package nếu cần *và* gọi ra để sử dụng. Bạn cũng có thể cài đặt package với `library()` từ **base** R. Xem thêm ở chương [R cơ bản] để biết thêm thông tin về R packages.  

```{r, eval=F}
pacman::p_load(rmarkdown)
```

### Bắt đầu một tệp Rmd mới {.unnumbered}

Trong RStudio, để mở một tệp R markdown mới, bắt đầu với 'File', rồi 'New file', rồi 'R markdown...'.  

```{r out.width = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/1_gettingstarted.png"))
```

R Studio sẽ đưa ra cho bạn một số lựa chọn kết quả đầu ra. Trong ví dụ bên dưới, chúng tôi chọn "HTML" bởi chúng tôi muốn tạo ra một văn bản html. Tiêu đề và tên các tác giả không quan trọng. Nếu loại tài liệu đầu ra bạn muốn không có, đừng lo - bạn có thể chọn bất kỳ loại nào và thay đổi trong script sau đó.  

```{r out.width = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/1_gettingstartedB.png"))
```

Như vậy sẽ mở ra một script .Rmd mới.  

### Điều quan trọng cần phải biết {.unnumbered}
 
**Thư mục làm việc**

Thư mục làm việc của một tệp markdown là ở bất kỳ vị trí nào tệp được lưu. Ví dụ, nếu R project nằm trong `~/Documents/projectX ` và tệp Rmd được lưu ở thư mục con `~/Documents/projectX/markdownfiles/markdown.Rmd`, thì lệnh `read.csv(“data.csv”)` trong markdown sẽ tìm tệp csv trong thư mục `markdownfiles`, mà không phải là ở thư mục gốc nơi mà scripts trong projects sẽ thường tự động tìm kiếm.  

Để tìm kiếm các tệp này ở chỗ khác, bạn sẽ vừa cần sử dụng cả đường dẫn tệp đầy đủ và sử dụng package **here**. Package **here** cài đặt thư mục làm việc tới thư mục gốc của R project và sẽ được giải thích chi tiết trong các chương [Dự án R] và [Nhập xuất dữ liệu] của sổ tay này. Ví dụ, để nhập một tệp tên "data.csv" từ trong thư mục `projectX`, code sẽ là `import(here(“data.csv”))`.  

Lưu ý rằng không nên sử dụng `setwd()` trong R Markdown scripts - nó chỉ dùng cho các đoạn code được viết trong đó. 

**Làm việc với ổ cứng mạng so với trên máy tính của bạn**

Vì R Markdown có thể gặp các vấn đề với pandoc khi chạy trên một ổ cứng mạng được chia sẻ, thư mục của bạn nên nằm ở ở cứng vật lý, ví dụ trong dự án thuộc 'My Documents'. Nếu bạn dùng Git (rất khuyến khích!), nó sẽ thân thuộc hơn. Để biết thêm chi tiết, xem thêm chương [R trên ổ cứng mạng] và [Các lỗi thường gặp].  


<!-- ======================================================= -->
## Các cấu phần của R Markdown {  }

Một tài liệu R Markdown có thể chỉnh sửa trong RStudio giống như một R script tiêu chuẩn. Khi bạn tạo một R Markdown script mới, RStudio vô cùng hữu ích bằng cách đưa ra một mẫu có các phần khác nhau của một tệp R Markdown script.  

Dưới đây là khung hiển thị khi bắt đầu một Rmd script mới có kết quả đầu ra là html (như phần trước)  

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/2_defaultRMD.png"))
```

Như bạn có thể thấy, một tệp Rmd gồm có 3 cấu phần: YAML, chữ Markdown, và đoạn code R.    

Chúng sẽ *tạo ra và trở thành tài liệu đầu ra của bạn*. Xem biểu đồ bên dưới:  

```{r out.width = "100%", out.height="150%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/rmarkdown_translation.png"))
```



### YAML metadata {.unnumbered}

'YAML metadata' hoặc chỉ 'YAML' nằm ở trên cùng trong tài liệu R Markdown. Phần này của script sẽ cho tệp Rmd của bạn biết cần tạo loại kết quả đầu ra nào, định dạng mong muốn, và metadata khác như tiêu đề tài liệu, tác giả, và ngày. Có những công dụng khác không được nhắc ở đây (nhưng được nhắc tới trong 'Tạo kết quả đầu ra'). Lưu ý rằng có sự thụt đầu dòng, nhưng chỉ chấp nhận spaces chứ không nhận tabs.  

Phần này phải được bắt đầu với với một dòng gồm có 3 dấu gạch ngang `---` và phải kết thúc với một dòng chỉ có 3 dấu gạch ngang `---`. Tham số của YAML ở dạng cặp `key:value`. Dấu hai chấm trong YAML có vị trí quan trọng - cặp `key:value` được tách ra bởi dấu hai chấm (không phải dấu bằng!)  

YAML nên bắt đầu với metadata cho tài liệu. Thứ tự của những tham số YAML chính (không thụt lề) không quan trọng. Ví dụ:  

```yaml
title: "My document"
author: "Me"
date: "`r Sys.Date()`"
```

Bạn có thể sử dụng code R trong giá trị YAML bằng cách viết nó như code tại dòng (mở đầu bằng `r` trong dấu back-ticks/nháy đơn ngược) nhưng nằm trong trích dẫn (xem tại ví dụ phía trên với `date: `).  

Trong hình ảnh phía trên, vì chúng ta ấn vào kết quả đầu ra mặc định là tệp html, chúng ta có thể thấy rằng YAML hiện `output: html_document`. Tuy nhiên, chúng ta cũng có thể thay đổi thành `powerpoint_presentation` hoặc `word_document` hoặc cả `pdf_document`.


### Văn bản {.unnumbered}

Đây là phần diễn giải trong tài liệu của bạn, gồm các tiêu đề và đề mục. Nó được viết bằng ngôn ngữ "markdown", được sử dụng trên nhiều phần mềm khác nhau.  

Dưới đây là những cách cốt lõi để viết văn bản định dạng này. Xem thêm tài liệu mở rộng có sẵn trên R Markdown "cheatsheet" tại [Trang web RStudio](https://rstudio.com/resources/cheatsheets/).  

#### Dòng mới {.unnumbered}  

Duy nhất trong R Markdown, để bắt đầu một dòng mới, nhập *hai dấu cách** vào cuối dòng trước đó và ấn Enter/Return.  


#### Định dạng {.unnumbered}  

Đặt các chữ bình thường bên trong các kí tự sau để thay đổi định dạng của chúng ở đầu ra.  


* Gạch dưới (`_chữ_`) hoặc dấu hoa thị đơn (`*chữ*`) để _in nghiêng_
* Dấu hoa thị kép (`**chữ**`) để **in đậm**
* Dấu nháy đơn ngược (````chữ````) để hiển thị chữ như code  

Phông chữ hiển thị thực tế có thể được đặt bằng cách sử dụng các mẫu cụ thể (được chỉ định trong YAML metadata; xem các tab ví dụ).  

#### Màu sắc {.unnumbered}  

Không có cơ chế đơn giản nào để thay đổi màu chữ trong R Markdown. Một giải pháp khác, *NẾU đầu ra của bạn là tệp HTML*, là thêm một dòng HTML vào văn bản markdown. Đoạn mã HTML dưới đây sẽ in ra một dòng văn bản có màu đỏ đậm.  

```md
<span style="color: red;">**_DANGER:_** This is a warning.</span>  
```

<span style="color: red;">**_DANGER:_** This is a warning.</span>  


#### Tiêu đề và đầu mục {.unnumbered}  

Dấu thăng trong phần văn bản của một script R Markdown tạo ra đầu mục. Điều này khác với một đoạn code R trong script, trong đó ký hiệu thăng là cơ chế nhận xét/chú thích/hủy kích hoạt, như trong R script bình thường.  

Các mức tiêu đề khác nhau được thiết lập với số lượng ký hiệu thăng khác nhau khi bắt đầu một dòng mới. Một ký hiệu thăng là tiêu đề hoặc đầu mục chính. Hai ký hiệu thăng là một đầu mục cấp hai. Các đầu mục cấp ba và cấp bốn có thể được tạo bằng các ký hiệu thăng liên tiếp.  

```md
# Đầu mục cấp một / Tiêu đề

## Đầu mục cấp hai

### Đầu mục cấp ba
```


#### Dấu đầu dòng và đánh số {.unnumbered}  

Sử dụng dấu hoa thị (`*`) để tạo danh sách gạch đầu dòng. Kết thúc câu trước, nhập hai dấu cách, Enter/Return *hai lần*, và bắt đầu tạo gạch đầu dòng. Thêm một dấu cách vào giữa dấu hoa thị và chữ đầu dòng. Sau mỗi đầu dòng nhập hai dấu cách và Enter/Return. Gạch đầu dòng phụ được tạo tương tự nhưng được thụt vào. Các số cũng hoạt động tương tự nhưng thay vì dấu hoa thị, viết 1), 2), v.v. Dưới đây là cách văn bản R Markdown script của bạn trông như thế nào.  


```md
Đây là gạch đầu dòng của tôi (có hai dấu cách sau dấu hai chấm này):  

* Gạch đầu dòng 1 (theo sau là 2 dấu cách và Enter/Return)  
* Gạch đầu dòng 2 (theo sau là 2 dấu cách và Enter/Return)  
  * Gạch đầu dòng phụ 1 (theo sau là 2 dấu cách và Enter/Return)  
  * Gạch đầu dòng phụ 2 (theo sau là 2 dấu cách và Enter/Return)  
  
```


#### Chữ chú giải {.unnumbered}

Bạn có thể "chú giải" văn bản R Markdown giống như bạn sử dụng "#" để chú giải một dòng code R trong một đoạn code R. Chỉ cần đánh dấu văn bản và nhấn Ctrl+Shift+c (Cmd+Shift+c cho Mac). Văn bản sẽ được bao quanh bởi các mũi tên và chuyển sang màu xanh lục. Nó sẽ không xuất hiện trong kết quả đầu ra của bạn.  


```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/rmarkdown_hide_text.png"))
```


### Đoạn Code {.unnumbered}

Các phần của script dành riêng để chạy code R được gọi là "chunk - đoạn". Đây là nơi bạn có thể tải các package, nhập dữ liệu và thực hiện quản lý và trực quan hóa dữ liệu thực tế. Có thể có nhiều đoạn code, vì vậy chúng có thể giúp bạn tổ chức code R của mình thành các phần, có thể xen kẽ với văn bản. Cần lưu ý:  
Những 'đoạn' này sẽ có màu nền hơi khác so với phần diễn giải của tài liệu.  

Mỗi đoạn code được mở với một dòng bắt đầu với 3 dấu nháy đơn ngược, và ngoặc nhọn chứa các tham số cho đoạn (`{ }`). Đoạn code kết thúc với 3 dấu nháy đơn ngược.    

Bạn có thể tạo một đoạn code mới bằng cách tự gõ nó ra, hoặc bằng cách sử dụng phím tắt "Ctrl + Alt + i" (hoặc Cmd + Shift + r trong Mac), hoặc bằng cách nhấp vào biểu tượng 'insert a new code chunk' màu xanh lục ở đầu trình chỉnh sửa script của bạn.  

Một số lưu ý về nội dung bên trong dấu ngoặc nhọn `{ }`:

*	Chúng bắt đầu bằng ‘r’ để chỉ ra rằng tên ngôn ngữ trong đoạn này là R
*	Sau chữ r, bạn có thể tùy ý viết "tên" đoạn - việc này không cần thiết nhưng có thể giúp bạn sắp xếp công việc của mình tốt hơn. Lưu ý rằng nếu bạn đặt tên cho các phần của mình, bạn phải LUÔN sử dụng các tên độc nhất, nếu không R sẽ báo lỗi khi bạn cố gắng kết xuất.  
*	Dấu ngoặc nhọn cũng có thể bao gồm các tùy chọn khác, được viết dưới dạng `tag = value`, chẳng hạn như:  
  * `eval = FALSE` để không chạy code R   
  * `echo = FALSE` để không hiển thị mã nguồn code R của đoạn trong kết quả đầu ra  
  * `warning = FALSE` để không hiển thị cảnh báo được tạo bởi code R  
  * `message = FALSE` để không hiển thị bất kỳ thông báo nào được tạo bởi code R  
  * `include =` TRUE/FALSE có bao gồm đầu ra đoạn code (ví dụ: các đồ thị) trong tài liệu hay không  
  * `out.width = ` và `out.height =` - cung cấp theo kiểu `out.width = "75%"`  
  * `fig.align = "center"` điều chỉnh cách một hình được căn trên trang  
  * `fig.show='hold'` nếu đoạn code của bạn hiển thị nhiều biểu đồ và bạn muốn chúng hiển thị cạnh nhau  (cùng với `out.width = c("33%", "67%")`. Có thể đặt là `fig.show='asis'` để hiển thị chúng dưới code tạo chúng, `'hide'` để ẩn, hoặc `'animate'` để nối nhiều cái thành một ảnh động.  
* Tiêu đề đoạn phải được viết trên *một dòng*   
* Cố gắng tránh dấu chấm, dấu gạch dưới và dấu cách. Sử dụng dấu gạch ngang ( - ) thay thế nếu bạn cần dấu ngăn cách.  

Đọc kĩ hơn về các tùy chọn **knitr** [tại đây](https://yihui.org/knitr/options/).  

Một số tùy chọn ở trên có thể được định cấu hình với trỏ và nhấp bằng cách sử dụng các nút cài đặt ở trên cùng bên phải của đoạn. Tại đây, bạn có thể chỉ định những phần nào của đoạn mà bạn muốn tài liệu được kết xuất bao gồm cụ thể là code, kết quả đầu ra và cảnh báo. Điều này sẽ xuất hiện dưới dạng tùy chọn được viết trong dấu ngoặc nhọn, ví dụ `echo = FALSE` nếu bạn muốn 'Chỉ hiển thị đầu ra'.  

```{r out.width = "80%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/3_chunk.png"))
```

Ngoài ra còn có hai mũi tên ở trên cùng bên phải của mỗi đoạn code. Chúng rất hữu ích để chạy code trong một đoạn hoặc tất cả code trong các đoạn trước. Di chuột lên chúng để xem chúng làm gì.  


Để các tùy chọn toàn cục được áp dụng cho tất cả các đoạn trong tập lệnh, bạn có thể thiết lập điều này trong đoạn code R đầu tiên của mình trong tập lệnh. Ví dụ, để chỉ các kết quả đầu ra được hiển thị cho mỗi đoạn code chứ không phải bản thân code, bạn có thể đưa lệnh này vào đoạn code R:

```{r, eval=F}
knitr::opts_chunk$set(echo = FALSE) 
```



#### Code R trong văn bản {.unnumbered}  

Bạn cũng có thể bao gồm code R đơn giản trong dấu nháy đơn ngược. Trong dấu nháy đơn ngược, hãy bắt đầu mã bằng "r" và một dấu cách, để RStudio biết đánh giá code đó là code R. Xem ví dụ bên dưới.  

Ví dụ bên dưới hiển thị nhiều cấp tiêu đề, dấu đầu dòng và sử dụng code R cho ngày hiện tại (`Sys.Date ()`) để chuyển thành ngày in.  

```{r out.width = "80%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/2_text.png"))
```



  

Phía trên là một ví dụ đơn giản (hiển thị ngày hiện tại), nhưng sử dụng cùng một cú pháp, bạn có thể hiển thị các giá trị được tạo bởi code R phức tạp hơn (ví dụ: để tính toán giá trị nhỏ nhất, trung bình, giá trị lớn nhất của một cột). Bạn cũng có thể tích hợp các đối tượng hoặc giá trị R đã được tạo trong các đoạn code R trước đó trong script.  

Ví dụ, script dưới đây tính toán tỷ lệ các trường hợp dưới 18 tuổi, sử dụng các hàm **tidyverse** và tạo các đối tượng `less18`,` total` và `less18prop`. Giá trị động này được chèn vào văn bản tiếp theo. Chúng ta thấy nó trông như thế nào khi được knit vào một tài liệu word.  

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/3_chunktext.png"))
```


### Ảnh {.unnumbered}  

Bạn có thể đưa hình ảnh vào R Markdown của mình theo một trong hai cách:  

```{r, eval=F}
![]("path/to/image.png")  
```

Nếu cách trên không được, thử dùng `knitr::include_graphics()`  

```{r, eval=F}
knitr::include_graphics("path/to/image.png")
```

(hãy nhớ rằng, đường dẫn tệp của bạn có thể được viết bằng cách sử dụng package **here**)

```{r, eval=F}
knitr::include_graphics(here::here("path", "to", "image.png"))
```


### Bảng {.unnumbered}  

Tạo bảng bằng dấu gạch ngang ( - ) và dấu thanh ( | ). Số lượng dấu gạch nối trước/giữa các thanh cho phép số lượng khoảng trắng trong ô trước khi văn bản bắt đầu được bao lại.  


```md
Cột 1    |Cột  2    |Cột 3
---------|----------|--------
Ô A      |Ô B       |Ô C
Ô D      |Ô E       |Ô F
```

Code trên tạo ra bảng ở dưới:  

Cột 1    |Cột  2    |Cột 3
---------|----------|--------
Ô A      |Ô B       |Ô C
Ô D      |Ô E       |Ô F


### Các phần được chia thẻ {.unnumbered}  


Đối với đầu ra là tệp HTML, bạn có thể sắp xếp các phần thành các "tab - thẻ". Chỉ cần thêm `.tabset` vào trong dấu ngoặc nhọn` {} `được đặt *sau đề mục*. Bất kỳ đề mục phụ nào bên dưới tiêu đề đó (cho đến khi tiêu đề khác cùng cấp) sẽ xuất hiện dưới dạng các tab mà người dùng có thể nhấp qua. Đọc thêm [tại đây](https://bookdown.org/yihui/rmarkdown-cookbook/html-tabs.html)  


```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/tabbed_script.png"))
knitr::include_graphics(here::here("images", "markdown/tabbed_view.gif"))

```


Bạn có thể thêm một tùy chọn bổ sung `.tabset-pills` sau `.tabset` để tạo cho các thẻ có giao diện "được tô màu". Lưu ý rằng khi xem đầu ra HTML theo thẻ, chức năng tìm kiếm Ctrl+f sẽ chỉ tìm kiếm các tab "đang hoạt động" chứ không phải các thẻ ẩn.  





<!-- ======================================================= -->
## Cấu trúc tệp {}


Có nhiều cách để tổ chức R Markdown của bạn và bất kỳ script R nào liên quan. Mỗi cách đều có cả ưu điểm và nhược điểm:  

* R Markdown khép kín - mọi thứ cần thiết cho báo cáo đều được nhập hoặc tạo trong R Markdown  
  * Nguồn từ các tệp khác - Bạn có thể chạy các script R bên ngoài bằng lệnh `source ()` và sử dụng đầu ra của chúng trong Rmd  
  * Scipt con - một cơ chế thay thế cho `source()`  
* Sử dụng "runfile" - Chạy các lệnh trong script R *trước khi* kết xuất tệp R Markdown  


### Rmd khép kín {.unnumbered}  

Đối với một báo cáo tương đối đơn giản, bạn có thể chọn tổ chức tập lệnh R Markdown của mình sao cho nó "khép kín" và không liên quan đến bất kỳ tập lệnh bên ngoài nào.    


Mọi thứ bạn cần để chạy R markdown đều được nhập hoặc tạo trong tệp Rmd, bao gồm tất cả các đoạn code và package. Cách tiếp cận "khép kín" này phù hợp khi bạn không cần xử lý nhiều dữ liệu (ví dụ: nó mang đến một tệp dữ liệu sạch hoặc gần sạch) và việc kết xuất R Markdown sẽ không mất quá nhiều thời gian.  

Trong trường hợp này, một cấu trúc hợp lý của script R Markdown có thể là:  

1) Thiết lập các tùy chọn **knitr** chung  
2) Tải packages  
3) Nhập dữ liệu  
4) Xử lý dữ liệu  
5) Tạo kết quả đầu ra (bảng, đồ thị, etc.)  
6) Lưu kết quả đầu ra nếu cần (.csv, .png, v.v.)  

#### Nguồn từ các tệp khác {.unnumbered}

Một biến thể của cách tiếp cận "khép kín" là có các đoạn code R Markdown "nguồn" (chạy) chạy các script R khác. Điều này có thể làm cho tập lệnh R Markdown của bạn ít lộn xộn hơn, đơn giản hơn và dễ tổ chức hơn. Nó cũng có thể hữu ích nếu bạn muốn hiển thị số liệu cuối cùng ở đầu báo cáo. Theo cách tiếp cận này, script R Markdown cuối cùng chỉ đơn giản là kết hợp các đầu ra được xử lý trước thành một tài liệu.  

Một cách để làm điều này là cung cấp script R (đường dẫn tệp và tên có phần mở rộng) cho lệnh R **base** `source()`.  

```{r, eval=F}
source("your-script.R", local = knitr::knit_global())
# or sys.source("your-script.R", envir = knitr::knit_global())
```

Lưu ý rằng khi sử dụng `source ()` *trong* R Markdown, các tệp bên ngoài sẽ vẫn được chạy *trong quá trình kết xuất tệp Rmd* của bạn. Do đó, mỗi tập lệnh được chạy mỗi lần bạn kết xuất báo cáo. Do đó, việc có các lệnh `source ()` này *trong* R Markdown không tăng tốc thời gian chạy của bạn, cũng như không hỗ trợ nhiều cho việc gỡ lỗi, vì lỗi vẫn được hiển thị khi tạo R Markdown.  

Một giải pháp thay thế là sử dụng tùy chọn `child = ` của **knitr**. GIẢI THÍCH THÊM ĐỂ LÀM

Bạn phải để ý các *môi trường* R khác nhau. Các đối tượng được tạo trong một môi trường sẽ không nhất thiết phải có sẵn cho môi trường được R Markdown sử dụng.


### Runfile {.unnumbered}  

Cách tiếp cận này liên quan đến việc sử dụng R script có chứa (các) lệnh `render ()` để xử lý trước các đối tượng đưa vào R markdown.  

Ví dụ, bạn có thể tải các package, tải và làm sạch dữ liệu, và thậm chí tạo các biểu đồ trước khi `render()`. Các bước này có thể xảy ra trong R script hoặc trong các script khác được lưu nguồn. Miễn là các lệnh này diễn ra trong cùng một phiên RStudio và các đối tượng được lưu vào môi trường, các đối tượng sau đó có thể được gọi trong nội dung Rmd. Sau đó, bản thân R markdown sẽ chỉ được sử dụng cho bước cuối cùng - để tạo ra kết quả với tất cả các đối tượng được xử lý trước. Điều này dễ dàng hơn để gỡ lỗi nếu có sự cố.  


Cách tiếp cận này hữu ích vì những lý do sau:  


* Thông báo lỗi mang nhiều thông tin hơn - những thông báo này sẽ được tạo từ R script, không phải R Markdown. Lỗi R Markdown có xu hướng cho bạn biết đoạn nào có vấn đề, nhưng sẽ không cho bạn biết dòng nào.

* Nếu có thể, bạn có thể chạy các bước xử lý dài trước lệnh `render ()` - chúng sẽ chỉ chạy một lần.  



Trong ví dụ bên dưới, chúng ta có một R script riêng biệt, trong đó chúng tôi xử lý trước một đối tượng `data` vào Môi trường R và sau đó kết xuất "create_output.Rmd" bằng cách sử dụng `render()`.  

```{r, eval=F}
data <- import("datafile.csv") %>%       # Load data and save to environment
  select(age, hospital, weight)          # Select limited columns

rmarkdown::render(input = "create_output.Rmd")   # Create Rmd file
```





### Cấu trúc thư mục {.unnumbered}  

Quy trình làm việc cũng liên quan đến cấu trúc thư mục tổng thể, chẳng hạn như có thư mục 'đầu ra' cho các tài liệu và số liệu đã tạo và thư mục 'dữ liệu' hoặc 'đầu vào' cho dữ liệu đã được làm sạch. Chúng tôi không đi sâu vào chi tiết ở đây, nhưng hãy xem chương [Tổ chức báo cáo định kỳ].  






## Tạo tài liệu  

Bạn có thể tạo tài liệu theo những cách sau:  

* Thủ công bằng cách nhấn nút "Knit" ở đầu trình chỉnh sửa script RStudio (nhanh chóng và dễ dàng)  
* Chạy lệnh `render()` (được thực thi bên ngoài R Markdown script)  


### Cách 1: Nút "Knit" {.unnumbered}  

Khi bạn mở tệp Rmd, hãy nhấn vào biểu tượng/nút 'Knit' ở đầu tệp.  

R Studio sẽ cho bạn thấy tiến trình trong tab ‘R Markdown’ gần R console của bạn. Tài liệu sẽ tự động mở khi hoàn tất.  

Tài liệu sẽ được lưu trong cùng một thư mục với R markdown script của bạn và có cùng tên tệp (ngoại trừ phần mở rộng). Điều này rõ ràng không phải là lý tưởng cho việc kiểm soát phiên bản (nó sẽ bị ghi đè lên mỗi lần bạn knit, trừ khi được di chuyển theo cách thủ công), như sau đó bạn có thể cần phải tự đổi tên tệp (ví dụ: thêm ngày).  

Đây là nút tắt của RStudio cho hàm `render ()` từ **rmarkdown**. Cách tiếp cận này chỉ tương thích với R markdown khép kín, nơi tất cả các thành phần cần thiết tồn tại hoặc có nguồn trong tệp.  

```{r out.width = "90%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/4_progress.png"))
```



### Cách 2: Lệnh `render()` {.unnumbered}

Một cách khác để tạo đầu ra R Markdown của bạn là chạy hàm `render ()` (từ package **rmarkdown**). Bạn phải thực hiện lệnh này *bên ngoài* R Markdown script - vì vậy hoặc trong một tập lệnh R riêng biệt (thường được gọi là "run file") hoặc dưới dạng lệnh độc lập trong R Console.  

```{r, eval=F}
rmarkdown::render(input = "my_report.Rmd")
```

Như với "knit", cài đặt mặc định sẽ lưu đầu ra Rmd vào cùng thư mục với Rmd script, với cùng tên tệp (ngoài phần mở rộng tệp). Ví dụ: “my_report.Rmd” khi được knit sẽ tạo ra “my_report.docx” nếu bạn đang knit vào một tài liệu word. Tuy nhiên, bằng cách sử dụng `render ()`, bạn có tùy chọn sử dụng các cài đặt khác nhau. `render ()` có thể chấp nhận các đối số bao gồm:  

* `output_format = ` Đây là định dạng đầu ra để chuyển đổi (e.g. `"html_document"`, `"pdf_document"`, `"word_document"`, hoặc `"all"`). Bạn cũng có thể chỉ định điều này trong YAML bên trong R Markdown script.  
* `output_file = ` Đây là tên của tệp đầu ra (và đường dẫn tệp). Điều này có thể được tạo thông qua các hàm R như `here()` hoặc `str_glue()` như minh họa bên dưới.  
* `output_dir = ` Đây là thư mục đầu ra (thư mục) để lưu tệp. Điều này cho phép bạn chọn một thư mục khác với thư mục mà tệp Rmd được lưu vào.  
* `output_options = ` Bạn có thể cung cấp một danh sách các tùy chọn sẽ ghi đè các tùy chọn đó trong script YAML (ví dụ, )
* `output_yaml = `  Bạn có thể cung cấp đường dẫn đến tệp .yml chứa thông số kỹ thuật YAML  
* `params = ` Xem phần thông số bên dưới  
* Xem danh sách đầy đủ [tại đây](https://pkgs.rstudio.com/rmarkdown/reference/render.html)  

Ví dụ, để cải thiện kiểm soát phiên bản, lệnh sau sẽ lưu tệp đầu ra trong thư mục con 'đầu ra', với ngày hiện tại trong tên tệp. Để tạo tên tệp, hàm `str_glue ()` từ package **stringr** được sử dụng để 'dán' các chuỗi tĩnh lại với nhau (được viết đơn giản) với code R động (được viết trong dấu ngoặc nhọn). Ví dụ: nếu đó là ngày 10 tháng 4 năm 2021, tên tệp từ bên dưới sẽ là “Report_2021-04-10.docx”. Xem chương [Ký tự và chuỗi] để biết thêm chi tiết về `str_glue ()`.  


```{r, eval=F}
rmarkdown::render(
  input = "create_output.Rmd",
  output_file = stringr::str_glue("outputs/Report_{Sys.Date()}.docx")) 
```

Khi file đang kết xuất, RStudio Console sẽ cho bạn thấy quá trình kết xuất tới 100%, và một thông báo cuối để báo rằng quá trình kết xuất đã hoàn thành.  


###  Cách 3: **reportfactory**  package {.unnumbered}  


R package **reportfactory** cung cấp một phương pháp thay thế để tổ chức và soạn báo cáo R Markdown *phù hợp với các tình huống mà bạn chạy báo cáo thường xuyên (ví dụ hàng ngày, hàng tuần ...).* Nó giúp giảm bớt việc soạn nhiều tệp R Markdown và tổ chức đầu ra của chúng. Về bản chất, nó cung cấp một "nhà máy" mà từ đó bạn có thể chạy báo cáo R Markdown, nhận các thư mục được đánh dấu ngày tháng và thời gian tự động cho kết quả đầu ra và có cách kiểm soát phiên bản "nhẹ nhàng".  

Đọc thêm về quy trình công việc này trong chương về [Tổ chức báo cáo định kỳ].  



<!-- ======================================================= -->
## Báo cáo được tham số hóa {  }

Bạn có thể sử dụng tham số hóa để tạo báo cáo động, sao cho báo cáo có thể được chạy với cài đặt cụ thể (ví dụ: ngày hoặc địa điểm cụ thể hoặc với các tùy chọn knit nhất định). Dưới đây, chúng tôi tập trung vào những điều cơ bản, nhưng có thêm các tài liệu [chi tiết trực tuyến](https://bookdown.org/yihui/rmarkdown/parameterized-reports.html) về các báo cáo được tham số hóa.  

Sử dụng bộ dữ liệu Ebola có tên linelist làm ví dụ, giả sử chúng ta muốn chạy một báo cáo giám sát tiêu chuẩn cho từng bệnh viện mỗi ngày. Chúng ta chỉ ra cách người ta có thể làm điều này bằng cách sử dụng các tham số.  

*Quan trọng: các báo cáo động cũng có thể thực hiện được mà không có cấu trúc tham số chính thức (không có `params:`), bằng cách sử dụng các đối tượng R đơn giản trong R script liền kề. Điều này được giải thích ở cuối phần này.*



### Cài đặt tham số {.unnumbered}

Bạn có nhiều cách để chỉ định giá trị tham số cho đầu ra R Markdown của mình.  

#### Cách 1: Đặt tham số trong YAML {.unnumbered}

Chỉnh sửa YAML để bao gồm tùy chọn `params:`, với các biểu thức được thụt lề cho mỗi tham số bạn muốn xác định. Trong ví dụ này, chúng tôi tạo các tham số `date` và` hospital`, với các giá trị mà chúng tôi chỉ định. Các giá trị này có thể thay đổi mỗi khi chạy báo cáo. Nếu bạn sử dụng nút "Knit" để tạo ra đầu ra, các tham số sẽ có các giá trị mặc định này. Tương tự như vậy, nếu bạn sử dụng `render ()`, các tham số sẽ có các giá trị mặc định này trừ khi được chỉ định khác trong lệnh `render ()`.  


```yaml
---
title: Surveillance report
output: html_document
params:
 date: 2021-04-10
 hospital: Central Hospital
---
```

Ở chế độ nền, các giá trị tham số này được chứa trong danh sách chỉ đọc được gọi là `params`. Do đó, bạn có thể chèn các giá trị tham số trong code R như cách bạn làm với một đối tượng/giá trị R khác trong môi trường của bạn. Chỉ cần gõ `params $` theo sau là tên tham số. Ví dụ: `params $ Hospital` để đại diện cho tên bệnh viện ("Central Hospital" theo mặc định).  

Lưu ý rằng các tham số cũng có thể giữ các giá trị `true` hoặc `false`, và vì vậy chúng có thể được đưa vào các tùy chọn **knitr** của bạn trong một đoạn code R. Ví dụ: bạn có thể đặt `{r, eval=params$run}` thay vì `{r, eval=FALSE}`, và bây giờ việc đoạn code chạy hay không phụ thuộc vào giá trị của tham số `run:`.  

Lưu ý rằng đối với các tham số là ngày tháng, chúng sẽ được nhập dưới dạng một chuỗi. Vì vậy, để `params$date` được diễn giải trong code R, nó có thể sẽ cần được bao bọc bằng `as.Date()` hoặc một hàm tương tự để chuyển đổi thành lớp Date.




#### Cách 2: Đặt tham số trong `render()` {.unnumbered}  

Như đã đề cập ở trên, thay thế cho việc nhấn nút "Knit" để tạo đầu ra là thực thi hàm `render()` từ một script riêng biệt. Trong trường hợp sau này, bạn có thể chỉ định các tham số được sử dụng trong việc hiển thị đó cho đối số `params = ` của `render ()`.  

Lưu ý rằng bất kỳ giá trị tham số nào được cung cấp ở đây sẽ *ghi đè* các giá trị mặc định của chúng nếu được viết trong YAML. Chúng tôi viết các giá trị trong dấu ngoặc kép vì trong trường hợp này chúng phải được định nghĩa là giá trị ký tự/chuỗi.  

Lệnh dưới đây kết xuất tệp "surveillance_report.Rmd", chỉ định tên và thư mục tệp đầu ra động, đồng thời cung cấp một `list()` gồm hai tham số và giá trị của chúng cho đối số `params = `.  

```{r, eval=F}
rmarkdown::render(
  input = "surveillance_report.Rmd",  
  output_file = stringr::str_glue("outputs/Report_{Sys.Date()}.docx"),
  params = list(date = "2021-04-10", hospital  = "Central Hospital"))
```


#### Cách 3:  Đặt tham số sử Giao diện Người dùng (GUI) {.unnumbered}  

Để có cảm giác tương tác hơn, bạn cũng có thể sử dụng Giao diện Người dùng (Graphical User Interface - GUI) để chọn thủ công các giá trị cho các tham số. Để thực hiện việc này, chúng ta có thể nhấp vào menu thả xuống bên cạnh nút ‘Knit’ và chọn ‘Knit with parameter’.  

Một cửa sổ bật lên sẽ xuất hiện cho phép bạn nhập các giá trị cho các tham số được thiết lập trong YAML của tài liệu.  

```{r out.width = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/5_parametersGUI.png"))
```

Bạn có thể đạt được điều tương tự thông qua lệnh `render()` bằng cách chỉ định `params = "ask"`, như được minh họa bên dưới.  

```{r, eval=F}
rmarkdown::render(
  input = "surveillance_report.Rmd",  
  output_file = stringr::str_glue("outputs/Report_{Sys.Date()}.docx"),
  params = “ask”)
```


Tuy nhiên, việc nhập giá trị vào cửa sổ bật lên này có thể xảy ra lỗi và lỗi chính tả. Bạn có thể chọn thêm các hạn chế cho các giá trị có thể được nhập thông qua menu thả xuống. Bạn có thể làm điều này bằng cách thêm vào YAML một số thông số kỹ thuật cho mỗi mục nhập `params:`.  

* `label: ` là cách tiêu đề cho menu thả xuống cụ thể đó  
* `value: ` là giá trị mặc định (bắt đầu)  
* `input: ` đặt thành `select` cho menu thả xuống  
* `choices: ` cung cấp các giá trị đủ điều kiện trong menu thả xuống  

Dưới đây, các thông số kỹ thuật này được viết cho tham số `hospital`.  

```yaml
---
title: Surveillance report
output: html_document
params:
 date: 2021-04-10
 hospital: 
  label: “Town:”
  value: Central Hospital
  input: select
  choices: [Central Hospital, Military Hospital, Port Hospital, St. Mark's Maternity Hospital (SMMH)]
---
```

Khi knit (thông qua nút 'knit with parameters' hoặc bằng cách `render()`), cửa sổ bật lên sẽ có các tùy chọn thả xuống để bạn lựa chọn.  

```{r out.width = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/5_parametersGUIB.png"))
```




### Ví dụ tham số hóa {.unnumbered} 

Đoạn mã sau đây tạo các tham số cho `date` và `hospital`, được sử dụng trong R Markdown tương ứng là `params$date` và `params$hospital`.

Trong kết quả đầu ra của báo cáo, hãy xem cách dữ liệu được lọc cho bệnh viện cụ thể và tiêu đề biểu đồ đề cập đến bệnh viện và ngày chính xác. Chúng tôi sử dụng tệp "linelist_cleaned.rds" ở đây, nhưng nó sẽ đặc biệt thích hợp nếu bản thân tệp linelist cũng có một dấu ngày tháng bên trong nó để căn chỉnh với ngày được tham số hóa.  

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/6_Rmdexample.png"))
```

Knit sẽ tạo ra kết quả cuối cùng với phông chữ và bố cục mặc định.  

```{r out.width = "80%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/6_RmdexampleB.png"))
```


### Tham số hóa không có `params` {.unnumbered}

Nếu bạn đang kết xuất tệp R Markdown với `render ()` từ một script riêng biệt, bạn thực sự có thể tạo ra tác động của tham số hóa mà không cần sử dụng chức năng `params:`.  

Ví dụ, trong *R script* có chứa lệnh `render ()`, bạn có thể chỉ cần xác định `hospital` và `date` là hai đối tượng R (giá trị) trước lệnh `render ()`. Trong R Markdown, bạn sẽ không cần phải có phần `params:` trong YAML, và chúng tôi sẽ đề cập đến đối tượng `date` hơn là `params$date` và `hospital` hơn là `params$hospital`.  


```{r, eval=F}
# This is a R script that is separate from the R Markdown

# define R objects
hospital <- "Central Hospital"
date <- "2021-04-10"

# Render the R markdown
rmarkdown::render(input = "create_output.Rmd") 
```

Làm theo cách này có nghĩa là bạn không thể “knit với các tham số”, sử dụng GUI hoặc bao gồm các tùy chọn knit trong các tham số. Tuy nhiên, điều này có thể có ích vì nó cho phép code đơn giản hơn.


<!-- ======================================================= -->

## Tạo vòng lặp nhiều báo cáo   {  }

Chúng ta có thể muốn chạy một báo cáo nhiều lần, thay đổi các thông số đầu vào, để tạo ra một báo cáo cho từng khu vực pháp lý/đơn vị. Điều này có thể được thực hiện bằng cách sử dụng các công cụ cho việc *lặp lại*, được giải thích chi tiết trong chương về [Lặp, vòng lặp và danh sách]. Các tùy chọn bao gồm package **purrr** hoặc sử dụng *vòng lặp for* như được giải thích bên dưới.  

Dưới đây, chúng tôi sử dụng  *vòng lặp for* đơn giản để tạo báo cáo giám sát cho tất cả các bệnh viện được chọn. Điều này được thực hiện bằng một lệnh (thay vì thay đổi từng thông số bệnh viện theo cách thủ công). Lệnh kết xuất báo cáo phải tồn tại trong một script riêng biệt *bên ngoài* báo cáo Rmd. Script này cũng sẽ chứa các đối tượng được xác định để "lặp qua" - ngày hôm nay và một vectơ tên bệnh viện để lặp qua.


```{r, eval=F}
hospitals <- c("Central Hospital",
                "Military Hospital", 
                "Port Hospital",
                "St. Mark's Maternity Hospital (SMMH)") 
```

Sau đó, chúng tôi cung cấp từng giá trị này vào lệnh `render()` bằng cách sử dụng một vòng lặp, lệnh này chạy lệnh một lần cho mỗi giá trị trong vectơ `hospitals`. Chữ cái `i` đại diện cho vị trí chỉ mục (từ 1 đến 4) của bệnh viện hiện đang được sử dụng trong lần lặp đó, như vậy `hospital_list[1]` sẽ là “Central Hospital”. Thông tin này được cung cấp ở hai nơi trong lệnh `render()`:  

1) Đối với tên tệp, sao cho tên tệp của lần lặp đầu tiên nếu được tạo vào ngày 10 tháng 4 năm 2021 sẽ là “Report_Central Hospital_2021-04-10.docx”, được lưu trong thư mục con ‘output’ của thư mục làm việc.  
2) Với `params =` sao cho Rmd sử dụng tên bệnh viện trong nội bộ bất cứ khi nào giá trị `params$hospital` được gọi (ví dụ: chỉ để lọc tập dữ liệu cho bệnh viện cụ thể). Trong ví dụ này, bốn tệp sẽ được tạo - mỗi tệp cho một bệnh viện.  

```{r, eval=F}
for(i in 1:length(hospitals)){
  rmarkdown::render(
    input = "surveillance_report.Rmd",
    output_file = str_glue("output/Report_{hospitals[i]}_{Sys.Date()}.docx"),
    params = list(hospital  = hospitals[i]))
}       
```



<!-- Trong trường hợp bạn không sử dụng hình thức tham số nghiêm ngặt này nhưng lưu các đối tượng vào môi trường, như đã thảo luận ở cuối phần tham số, hàm kết xuất sẽ giống như sau:  -->

<!-- ```md -->
<!-- for(i in 1:length(hospital_list)){ -->
<!-- rmarkdown::render("surveillance_report.Rmd", -->
<!--                   output_file = paste0("output/Report_", hospital_list[i], refdate, ".docx") -->
<!-- }        -->
<!-- ``` -->
<!-- Chữ trong phần đánh dấu sau đó sẽ cần tham chiếu đến `hospital_list[i]` và `refdate`.  -->






<!-- ======================================================= -->
## Mẫu  

Bằng cách sử dụng tài liệu mẫu có chứa bất kỳ định dạng mong muốn nào, bạn có thể điều chỉnh tính thẩm mỹ của đầu ra Rmd sẽ trông như thế nào. Ví dụ, bạn có thể tạo tệp MS Word hoặc Powerpoint chứa các trang/trang trình bày với kích thước, hình đóng dấu, hình nền và phông chữ mong muốn.  

### Tài liệu Word {.unnumbered}

Để tạo một mẫu, hãy bắt đầu một tài liệu word mới (hoặc sử dụng đầu ra hiện có với định dạng phù hợp với bạn) và chỉnh sửa phông chữ bằng cách xác định Kiểu. Trong Kiểu, Đầu mục 1, 2 và 3 đề cập đến các cấp đề mục markdown khác nhau (tương ứng `#Đề mục 1`, `##Đề mục 2` và `### Đề mục 3`). Nhấp chuột phải vào kiểu và nhấp vào 'sửa đổi' để thay đổi định dạng phông chữ cũng như đoạn văn (ví dụ: bạn có thể giới thiệu các ngắt trang trước các kiểu nhất định có thể giúp giãn cách). Các khía cạnh khác của tài liệu word như lề, kích thước trang, đề mục, v.v., có thể được thay đổi giống như một tài liệu word thông thường mà bạn đang làm việc trực tiếp bên trong.  

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/7_template.png"))
```

### Tài liệu Powerpoint {.unnumbered}

Như trên, hãy tạo một slide mới hoặc sử dụng một file powerpoint hiện có với định dạng mong muốn. Để chỉnh sửa thêm, hãy nhấp vào 'View' và 'Slide Master'. Từ đây, bạn có thể thay đổi giao diện trang chiếu 'master' bằng cách chỉnh sửa định dạng văn bản trong các hộp văn bản, cũng như kích thước nền/trang cho trang tổng thể.  

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/8_ppttemplate.png"))
```

Thật không may, việc chỉnh sửa tệp powerpoint hơi kém linh hoạt:  

* Đề mục cấp đầu tiên (`# Đề mục 1`) sẽ tự động trở thành tiêu đề của trang trình bày mới,  
* Chữ `## Đề mục 2` sẽ không xuất hiện dưới dạng phụ đề mà là chữ trong hộp văn bản chính của trang chiếu (trừ khi bạn tìm được cách để làm rộng chế độ xem Master).
* Các ô và bảng đã xuất sẽ tự động chuyển sang các trang trình bày mới. Bạn sẽ cần kết hợp chúng, chẳng hạn như hàm **patchwork** để kết hợp các ggplots, để chúng hiển thị trên cùng một trang. Xem [bài đăng trên blog](https://mattherman.info/blog/ppt-patchwork/) về cách sử dụng package **patchwork** để đặt nhiều hình ảnh trên một trang chiếu.  

Xem [**officer** package](https://davidgohel.github.io/officer/) để biết công cụ làm việc chuyên sâu hơn với các bài thuyết trình powerpoint.    




### Tích hợp các mẫu vào YAML {.unnumbered}

Khi một mẫu được chuẩn bị, chi tiết của mẫu này có thể được thêm vào YAML của Rmd bên dưới dòng 'đầu ra' và bên dưới nơi loại tài liệu được chỉ định (chính nó sẽ đi đến một dòng riêng). Lưu ý `reference_doc` có thể được sử dụng cho các mẫu slide powerpoint.  

Dễ dàng nhất là lưu mẫu trong cùng một thư mục với nơi chứa tệp Rmd (như trong ví dụ bên dưới) hoặc trong một thư mục con bên trong.  

```yaml
---
title: Surveillance report
output: 
 word_document:
  reference_docx: "template.docx"
params:
 date: 2021-04-10
 hospital: Central Hospital
template:
 
---
```

### Định dạng tệp HTML {.unnumbered}

Các tệp HTML không sử dụng các mẫu, nhưng có thể có các kiểu được định cấu hình trong YAML. HTML là tài liệu tương tác và đặc biệt linh hoạt. Chúng tôi đề cập đến một số tùy chọn cơ bản ở đây.  

* Mục lục: Chúng ta có thể thêm mục lục với `toc: true` bên dưới, và cũng chỉ định rằng nó vẫn có thể xem được ("float") khi bạn cuộn, với` toc_float: true`.  

* Chủ đề: Chúng ta có thể tham khảo một số chủ đề được tạo sẵn, lấy từ thư viện chủ đề của Bootswatch. Trong ví dụ dưới đây, chúng tôi sử dụng cerulean. Các tùy chọn khác bao gồm: journal, flatly, darkly, readable, spacelab, united, cosmo, lumen, paper, sandstone, simplex, và yeti.  

* Đánh dấu: Định cấu hình này sẽ thay đổi giao diện của chữ được đánh dấu (ví dụ: code trong các đoạn được hiển thị). Các kiểu được hỗ trợ bao gồm mặc định, tango, pygments, kate, monochrome, espresso, zenburn, hasdock, breezedark và textmate.  

Đây là một ví dụ về cách tích hợp các tùy chọn trên vào YAML.

```yaml
---
title: "HTML example"
output:
  html_document:
    toc: true
    toc_float: true
    theme: cerulean
    highlight: kate
    
---
```

Dưới đây là hai ví dụ về kết quả đầu ra HTML, cả hai đều có mục lục nổi, nhưng chủ đề và kiểu đánh dấu khác nhau được chọn:  


```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/9_html.png"))
```


## Nội dung động 

Trong đầu ra HTML, nội dung báo cáo của bạn có thể là động. Dưới đây là một số ví dụ:  

### Bảng {.unnumbered}  

Trong báo cáo HTML, bạn có thể in khung/ô dữ liệu sao cho nội dung là động, với các bộ lọc và thanh cuộn. Có một số packages cung cấp khả năng này.  

Để thực hiện việc này với package **DT**, như được sử dụng trong cuốn sổ tay này, bạn có thể chèn một đoạn code như sau:  

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/10_dynamictable.png"))
```

Hàm `datatable()` sẽ in khung dữ liệu đã cung cấp dưới dạng bảng động cho trình đọc. Bạn có thể đặt `rownames = FALSE` để đơn giản hóa phần ngoài cùng bên trái của bảng. `filter = "top"` cung cấp một bộ lọc trên mỗi cột. Trong đối số `option()` cung cấp danh sách các thông số kỹ thuật khác. Dưới đây, chúng tôi bao gồm hai đối số: `pageLength = 5` đặt số hàng xuất hiện là 5 (các hàng còn lại có thể được xem bằng cách phân trang thông qua các mũi tên) và` scrollX = TRUE` bật thanh cuộn ở cuối bảng (đối với cột mở rộng quá xa sang bên phải).  

Nếu tập dữ liệu của bạn rất lớn, hãy xem xét chỉ hiển thị X hàng trên cùng bằng cách gói khung dữ liệu trong `head()`.  


### Tiện ích HTML {.unnumbered}

[Tiện ích HTML cho R](http://www.htmlwidgets.org/) là một lớp packages R đặc biệt cho phép tăng tính tương tác bằng cách sử dụng các thư viện JavaScript. Bạn có thể nhúng chúng vào các đầu ra HTML R Markdown.  

Một số ví dụ phổ biến về các tiện ích này bao gồm:  

* Plotly (được sử dụng trong chương sổ tay này và trong chương [Biểu đồ tương tác])
* visNetwork (được sử dụng trong chương [Chuỗi lây nhiễm] của sổ tay này)  
* Leaflet (được sử dụng trong chương [GIS Cơ bản] của sổ tay này)  
* dygraphs ( hữu ích để hiển thị dữ liệu chuỗi thời gian tương tác)  
* DT (`datatable()`) (được sử dụng để hiển thị các bảng động với bộ lọc, sắp xếp, v.v.)  

Hàm `ggplotly()` từ **plotly** đặc biệt dễ sử dụng. Xem chương [Biểu đồ tương tác].  

## Tài nguyên {  }

Tìm thêm thông tin tại:  

* https://bookdown.org/yihui/rmarkdown/
* https://rmarkdown.rstudio.com/articles_intro.html

Một giải thích tốt về so sánh giữa markdown, knitr và Rmarkdown ở đây: https://stackoverflow.com/questions/40563479/relationship-between-r-markdown-knitr-pandoc-and-bookdown


```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/rmarkdown.Rmd-->


# Tổ chức báo cáo định kỳ {#reportfactory}  

This page covers the **reportfactory** package, which is an *accompaniment to using R Markdown for reports*. 

In scenarios where you run reports routinely (daily, weekly, etc.), it eases the compilation of multiple R Markdown files and the organization of their outputs. In essence, it provides a "factory" from which you can run the R Markdown reports, get automatically date- and time-stamped folders for the outputs, and have "light" version control.  

**reportfactory** is one of the packages developed by RECON (R Epidemics Consortium). Here is their [website](https://www.repidemicsconsortium.org/) and [Github](https://github.com/reconverse).  


## Preparation

### Load packages {.unnumbered}  

From within RStudio, install the latest version of the **reportfactory** package from Github.  

You can do this via the **pacman** package with `p_load_current_gh()` which will force intall of the latest version from Github. Provide the character string "reconverse/reportfactory", which specifies the Github organization (reconverse) and repository (reportfactory). You can also use `install_github()` from the **remotes** package, as an alternative.

```{r, eval=FALSE}
# Install and load the latest version of the package from Github
pacman::p_load_current_gh("reconverse/reportfactory")
#remotes::install_github("reconverse/reportfactory") # alternative
```


## New factory  

To create a new factory, run the function `new_factory()`. This will create a new self-contained R project folder. By default:  

* The factory will be added to your working directory
* The name of the factory R project will be called "new_factory.Rproj"  
* Your RStudio session will "move in" to this R project  

```{r, eval=F}
# This will create the factory in the working directory
new_factory()
```

Looking inside the factory, you can see that sub-folders and some files were created automatically.  


```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_new2.png"))
```

* The *report_sources* folder will hold your R Markdown scripts, which generate your reports  
* The *outputs* folder will hold the report outputs (e.g. HTML, Word, PDF, etc.)  
* The *scripts* folder can be used to store other R scripts (e.g. that are sourced by your Rmd scripts)  
* The *data* folder can be used to hold your data ("raw" and "clean" subfolders are included)  
* A *.here* file, so you can use the **here** package to call files in sub-folders by their relation to this root folder (see [R projects] page for details)  
* A *gitignore* file was created in case you link this R project to a Github repository (see [Version control and collaboration with Github])  
* An empty README file, for if you use a Github repository  


<span style="color: orange;">**_CAUTION:_** depending on your computer's setting, files such as ".here" may exist but be invisible.</span>  

Of the default settings, below are several that you might want to adjust within the `new_factory()` command:  

* `factory = ` - Provide a name for the factory folder (default is "new_factory")  
* `path = ` - Designate a file path for the new factory (default is the working directory)  
* `report_sources = ` Provide an alternate name for the subfolder which holds the R Markdown scripts (default is "report_sources")  
* `outputs = ` Provide an alternate name for the folder which holds the report outputs (default is "outputs")  

See `?new_factory` for a complete list of the arguments.  


When you create the new factory, your R session is transferred to the new R project, so you should again load the **reportfactory** package.  

```{r, eval=FALSE}
pacman::p_load(reportfactory)
```

Now you can run a the `factory_overview()` command to see the internal structure (all folders and files) in the factory.  

```{r, eval=F}
factory_overview()            # print overview of the factory to console
```

The following "tree" of the factory's folders and files is printed to the R console. Note that in the "data" folder there are sub-folders for "raw" and "clean" data, and example CSV data. There is also "example_report.Rmd" in the "report_sources" folder.    

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_overview.png"))
```


## Create a report  

From within the factory R project, create a R Markdown report just as you would normally, and save it into the "report_sources" folder. See the [R Markdown][Reports with R Markdown] page for instructions. For purposes of example, we have added the following to the factory:  

* A new R markdown script entitled "daily_sitrep.Rmd", saved within the "report_sources" folder  
* Data for the report ("linelist_cleaned.rds"), saved to the "clean" sub-folder within the "data" folder  

We can see using `factory_overview()` our R Markdown in the "report_sources" folder and the data file in the "clean" data folder (highlighted):

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_overview2.png"))
```

Below is a screenshot of the beginning of the R Markdown "daily_sitrep.Rmd". You can see that the output format is set to be HTML, via the YAML header `output: html_document`. 

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_new_rmd.png"))
```

In this simple script, there are commands to:  

* Load necessary packages  
* Import the linelist data using a filepath from the **here** package (read more in the page on [Import and export])  

```{r, eval=F}
linelist <- import(here("data", "clean", "linelist_cleaned.rds"))
```

* Print a summary table of cases, and export it with `export()` as a .csv file  
* Print an epicurve, and export it with `ggsave()` as a .png file  


You can review just the list of R Markdown reports in the "report_sources" folder with this command:  

```{r, eval=F}
list_reports()
```



## Compile  

In a report factory, to "compile" a R Markdown report means that the .Rmd script will be run and the output will be produced (as specified in the script YAML e.g. as HTML, Word, PDF, etc).  

*The factory will automatically create a date- and time-stamped folder for the outputs in the "outputs" folder.*  

The report itself and any exported files produced by the script (e.g. csv, png, xlsx) will be saved into this folder. In addition, the Rmd script itself will be saved in this folder, so you have a record of that version of the script.  

This contrasts with the normal behavior of a "knitted" R Markdown, which saves outputs to the location of the Rmd script. This default behavior can result in crowded, messy folders. The factory aims to improve organization when one needs to run reports frequently.  

### Compile by name {.unnumbered}  

You can compile a specific report by running `compile_reports()` and providing the Rmd script name (without .Rmd extension) to `reports = `. For simplicity, you can skip the `reports = ` and just write the R Markdown name in quotes, as below.  

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_compile1.png"))
```


This command would compile only the "daily_sitrep.Rmd" report, saving the HTML report, and the .csv table and .png epicurve exports into a date- and time-stamped sub-folder specific to the report, within the "outputs" folder.  

Note that if you choose to provide the .Rmd extension, you must correctly type the extension as it is saved in the file name (.rmd vs. .Rmd).  

Also note that when you compile, you may see several files temporarily appear in the "report_sources" folder - but they will soon disappear as they are transferred to the correct "outputs" folder. 

### Compile by number {.unnumbered}

You can also specify the Rmd script to compile by providing a number or vector of numbers to `reports = `. The numbers must align with the order the reports appear when you run `list_reports()`.  

```{r, eval=F}
# Compile the second and fourth Rmds in the "report_sources" folder
compile_reports(reports = c(2, 4))
```



### Compile all {.unnumbered}

You can compile *all* the R Markdown reports in the "report_sources" folder by setting the `reports = ` argument to TRUE.  

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_compile_all.png"))
```


### Compile from sub-folder {.unnumbered}  

You can add sub-folders to the "report_sources" folder. To run an R Markdown report from a subfolder, simply provide the name of the folder to `subfolder = `. Below is an example of code to compile a Rmd report that lives in a sub_folder of "report_sources".  

```{r, eval=F}
compile_reports(
     reports = "summary_for_partners.Rmd",
     subfolder = "for_partners")
```

You can compile all Rmd reports within a subfolder by providing the subfolder name to `reports = `, with a slash on the end, as below.  

```{r, eval=F}
compile_reports(reports = "for_partners/")
```


### Parameterization {.unnumbered}

As noted in the page on [Reports with R Markdown], you can run reports with specified parameters. You can pass these parameters as a list to `compile_reports()` via the `params = ` argument. For example, in this fictional report there are three parameters provided to the R Markdown reports.  

```{r, eval=F}
compile_reports(
  reports = "daily_sitrep.Rmd",
  params = list(most_recent_data = TRUE,
                region = "NORTHERN",
                rates_denominator = 10000),
  subfolder = "regional"
)
```


### Using a "run-file" {.unnumbered}  

If you have multiple reports to run, consider creating a R script that contains all the `compile_reports()` commands. A user can simply run all the commands in this R script and all the reports will compile. You can save this "run-file" to the "scripts" folder.  



## Outputs  

After we have compiled the reports a few times, the "outputs" folder might look like this (highlights added for clarity):  


```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_overview_all.png"))
```


* Within "outputs", sub-folders have been created for each Rmd report  
* Within those, further sub-folders have been created for each unique compiling  
  * These are date- and time-stamped ("2021-04-23_T11-07-36" means 23rd April 2021 at 11:07:36)  
  * You can edit the date/time-stamp format. See `?compile_reports`
* Within each date/time compiled folder, the report output is stored (e.g. HTML, PDF, Word) along with the Rmd script (version control!) and any other exported files (e.g. table.csv, epidemic_curve.png)  

Here is a view inside one of the date/time-stamped folders, for the "daily_sitrep" report. The file path is highlighted in yellow for emphasis.  

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_compile_folder.png"))
```


Finally, below is a screenshot of the HTML report output.  


```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_html.png"))
```

You can use `list_outputs()` to review a list of the outputs.  




## Miscellaneous  

### Knit {.unnumbered} 

You can still "knit" one of your R Markdown reports by pressing the "Knit" button, if you want. If you do this, as by default, the outputs will appear in the folder where the Rmd is saved - the "report_sources" folder. In prior versions of **reportfactory**, having any non-Rmd files in "report_sources" would prevent compiling, but this is no longer the case. You can run `compile_reports()` and no error will occur.  

### Scripts {.unnumbered}  

We encourage you to utilize the "scripts" folder to store "runfiles" or .R scripts that are sourced by your .Rmd scripts. See the page on [R Markdown][Reports with R Markdown] for tips on how to structure your code across several files.  


### Extras {.unnumbered} 

* With **reportfactory**, you can use the function `list_deps()` to list all packages required across all the reports in the entire factory.  

* There is an accompanying package in development called **rfextras** that offers more helper functions to assist you in building reports, such as:  
  * `load_scripts()` - sources/loads all .R scripts in a given folder (the "scripts" folder by default)  
  * `find_latest()` - finds the latest version of a file (e.g. the latest dataset)




<!-- ======================================================= -->
## Resources {  }

See the **reportfactory** package's [Github page](https://github.com/reconverse/reportfactory)

See the **rfextras** package's [Github page](https://github.com/reconhub/rfextras)  

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/reportfactory.Rmd-->


# Dashboards với R Markdown {#flexdashboard}

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_output.png"))
```

This page will cover the basic use of the **flexdashboard** package. This package allows you to easily format R Markdown output as a dashboard with panels and pages. The dashboard content can be text, static figures/tables or interactive graphics.  

Advantages of **flexdashboard**:  

* It requires minimal non-standard R coding - with very little practice you can quickly create a dashboard  
* The dashboard can usually be emailed to colleagues as a self-contained HTML file - no server required  
* You can combine **flexdashboard** with **shiny**, **ggplotly**, and other *"html widgets"* to add interactivity  

Disadvantages of **flexdashboard**:  

* Less customization as compared to using **shiny** alone to create a dashboard  


Very comprehensive tutorials on using **flexdashboard** that informed this page can be found in the Resources section. Below we describe the core features and give an example of building a dashboard to explore an outbreak, using the case `linelist` data.  


## Preparation

### Load packages {.unnumbered}  

In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  rio,             # data import/export     
  here,            # locate files
  tidyverse,       # data management and visualization
  flexdashboard,   # dashboard versions of R Markdown reports
  shiny,           # interactive figures
  plotly           # interactive figures
)
```

### Import data {.unnumbered}  

We import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Import data with the `import()` function from the **rio** package (it handles many file types like .xlsx, .csv, .rds - see the [Import and export] page for details). 

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.rds")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


## Create new R Markdown  

After you have installed the package, create a new R Markdown file by clicking through to *File > New file > R Markdown*. 

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_new1.png"))
```


In the window that opens, select "From Template" and select the "Flex Dashboard" template. You will then be prompted to name the document. In this page's example, we will name our R Markdown as "outbreak_dashboard.Rmd".  
  

```{r out.width = "100%", out.height="75%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_new2.png"))
```




## The script  

The script is an R Markdown script, and so has the same components and organization as described in the page on [Reports with R Markdown]. We briefly re-visit these and highlight differences from other R Markdown output formats.  

### YAML {.unnumbered}  

At the top of the script is the "YAML" header. This must begin with three dashes `---` and must close with three dashes `---`. YAML parameters comes in `key:value` pairs. **The indentation and placement of colons in YAML is important** - the `key:value` pairs are separated by colons (not equals signs!). 

The YAML should begin with metadata for the document. The order of these primary YAML parameters (not indented) does not matter. For example:  

```{r, eval=F}
title: "My document"
author: "Me"
date: "`r Sys.Date()`"
```

You can use R code in YAML values by putting it like in-line code (preceeded by `r` within backticks) but also within quotes (see above for Date).  

A required YAML parameter is `output: `, which specifies the type of file to be produced (e.g. `html_document`, `pdf_document`, `word_document`, or `powerpoint_presentation`). For **flexdashboard** this parameter value is a bit confusing - it must be set as `output:flexdashboard::flex_dashboard`. Note the single and double colons, and the underscore. This YAML output parameter is often followed by *an additional colon* and indented sub-parameters (see `orientation: ` and `vertical_layout: ` parameters below).  

```{r, eval=F}
title: "My dashboard"
author: "Me"
date: "`r Sys.Date()`"
output:
  flexdashboard::flex_dashboard:
    orientation: rows
    vertical_layout: scroll
```

As shown above, indentations (2 spaces) are used for sub-parameters. In this case, do not forget to put an additional colon after the primary, like `key:value:`.  

If appropriate, logical values should be given in YAML in lowercase (`true`, `false`, `null`). If a colon is part of your value (e.g. in the title) put the value in quotes. See the examples in sections below.  



### Code chunks {.unnumbered}  

An R Markdown script can contain multiple code "chunks" - these are areas of the script where you can write multiple-line R code and they function just like mini R scripts.  

Code chunks are created with three back-ticks and curly brackets with a lowercase "r" within. The chunk is closed with three backticks. You can create a new chunk by typing it out yourself, by using the keyboard shortcut "Ctrl + Alt + i" (or Cmd + Shift + r in Mac), or by clicking the green 'insert a new code chunk' icon at the top of your script editor. Many examples are given below.  


### Narrative text {.unnumbered}  

Outside of an R code "chunk", you can write narrative text. As described in the page on [Reports with R Markdown], you can italicize text by surrounding it with one asterisk (*), or bold by surrounding it with two asterisks (**). Recall that bullets and numbering schemes are sensitive to newlines, indentation, and finishing a line with two spaces.  

You can also insert in-line R code into text as described in the [Reports with R Markdown] page, by surrounding the code with backticks and starting the command with "r": `` ` 1+1` ``(see example with date above).  



### Headings {.unnumbered}  

Different heading levels are established with different numbers of hash symbols, as described in the [Reports with R Markdown] page.  

In **flexdashboard**, a primary heading (#) creates a "page" of the dashboard. Second-level headings (##) create a column or a row depending on your `orientation:` parameter (see details below). Third-level headings (###) create panels for plots, charts, tables, text, etc.   

```md
# First-level heading (page)

## Second level heading (row or column)  

### Third-level heading (pane for plot, chart, etc.)
```





## Section attributes  

As in a normal R markdown, you can specify attributes to apply to parts of your dashboard by including `key=value` options after a heading, within curly brackets `{ }`. For example, in a typical HTML R Markdown report you might organize sub-headings into tabs with `## My heading {.tabset}`.  

Note that these attributes are written after a *heading* in a text portion of the script. These are different than the **knitr** options inserted within at the top of R code chunks, such as `out.height = `.  

Section attributes specific to **flexdashboard** include:  

* `{data-orientation=}` Set to either `rows` or `columns`. If your dashboard has multiple pages, add this attribute to each page to indicate orientation (further explained in [layout section](#layout)).  
* `{data-width=}` and `{data-height=}` set relative size of charts, columns, rows laid out in the same dimension (horizontal or vertical). Absolute sizes are adjusted to best fill the space on any display device thanks to the [flexbox](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Flexible_Box_Layout/Using_CSS_flexible_boxes) engine.  
     * Height of charts also depends on whether you set the YAML parameter `vertical_layout: fill` or `vertical_layout: scroll`. If set to scroll, figure height will reflect the traditional `fig.height = ` option in the R code chunk.  
     * See complete size documentation at the [flexdashboard website](https://rmarkdown.rstudio.com/flexdashboard/using.html#sizing)  
* `{.hidden}` Use this to exclude a specific page from the navigation bar  
* `{data-navbar=}` Use this in a page-level heading to nest it within a navigation bar drop-down menu. Provide the name (in quotes) of the drop-down menu. See example below.  


## Layout {#layout}  

Adjust the layout of your dashboard in the following ways:  

* Add pages, columns/rows, and charts with R Markdown headings (e.g. #, ##, or ###)  
* Adjust the YAML parameter `orientation:` to either `rows` or `columns`  
* Specify whether the layout fills the browser or allows scrolling  
* Add tabs to a particular section heading  


### Pages {.unnumbered}  

First-level headings (#) in the R Markdown will represent "pages" of the dashboard. By default, pages will appear in a navigation bar along the top of the dashboard.  

```{r, out.height = c('100%'), out.width = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_pages_top_script.png"))
```


```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_pages_top_view.png"))
```



You can group pages into a "menu" within the top navigation bar by adding the attribute `{data-navmenu=}` to the page heading. Be careful - do not include spaces around the equals sign otherwise it will not work!  

```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_navmenu_script.png"))
```


Here is what the script produces:  


```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_navmenu_view.png"))
```

You can also convert a page or a column into a "sidebar" on the left side of the dashboard by adding the `{.sidebar}` attribute. It can hold text (viewable from any page), or if you have integrated **shiny** interactivity it can be useful to hold user-input controls such as sliders or drop-down menus.  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_sidebar_script.png"))
```

Here is what the script produces:  

```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_sidebar_view.png"))
```




### Orientation {.unnumbered}  

Set the `orientation:` yaml parameter to indicate how your second-level (##) R Markdown headings should be interpreted - as either `orientation: columns` or `orientation: rows`. 

Second-level headings (##) will be interpreted as new columns or rows based on this `orientation` setting.  

If you set `orientation: columns`, second-level headers will create new columns in the dashboard. The below dashboard has one page, containing two columns, with a total of three panels. You can adjust the relative width of the columns with `{data-width=}` as shown below.  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_columns_script.png"))
```

Here is what the script produces:  

```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_columns_view.png"))
```

If you set `orientation: rows`, second-level headers will create new rows instead of columns. Below is the same script as above, but `orientation: rows` so that second-level headings produce rows instead of columns. You can adjust the relative *height* of the rows with `{data-height=}` as shown below.  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_rows_script.png"))
```

Here is what the script produces:  

```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_rows_view.png"))
```

If your dashboard has multiple pages, you can designate the orientation for each specific page by adding the `{data-orientation=}` attribute the header of each page (specify either `rows` or `columns` without quotes).  

### Tabs {.unnumbered} 

You can divide content into tabs with the `{.tabset}` attribute, as in other HTML R Markdown outputs.  

Simply add this attribute after the desired heading. Sub-headings under that heading will be displayed as tabs. For example, in the example script below column 2 on the right (##) is modified so that the epidemic curve and table panes (###) are displayed in tabs.  

You can do the same with rows if your orientation is rows.  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_tabs_script.png"))
```

Here is what the script produces:  

```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_tabs_view.png"))
```


## Adding content  

Let's begin to build a dashboard. Our simple dashboard will have 1 page, 2 columns, and 4 panels. We will build the panels piece-by-piece for demonstration.  

You can easily include standard R outputs such as text, ggplots, and tables (see [Tables for presentation] page). Simply code them within an R code chunk as you would for any other R Markdown script.  

Note: you can download the finished Rmd script and HTML dashboard output - see the [Download handbook and data] page.  


### Text {.unnumbered}  

You can type in Markdown text and include *in-line* code as for any other R Markdown output. See the [Reports with R Markdown] page for details. 

In this dashboard we include a summary text panel that includes dynamic text showing the latest hospitalisation date and number of cases reported in the outbreak. 

### Tables {.unnumbered}  

You can include R code chunks that print outputs such as tables. But the output will look best and respond to the window size if you use the `kable()` function from **knitr** to display your tables. The **flextable** functions may produce tables that are shortened / cut-off.  

For example, below we feed the `linelist()` through a `count()` command to produce a summary table of cases by hospital. Ultimately, the table is piped to `knitr::kable()` and the result has a scroll bar on the right. You can read more about customizing your table with `kable()` and **kableExtra** [here](https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html).  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_tables_script.png"))
```

Here is what the script produces:  

```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_tables_view.png"))
```


If you want to show a dynamic table that allows the user to filter, sort, and/or click through "pages" of the data frame, use the package **DT** and it's function `datatable()`, as in the code below.  

The example code below, the data frame `linelist` is printed. You can set `rownames = FALSE` to conserve horizontal space, and `filter = "top"` to have filters on top of every column. A list of other specifications can be provided to `options = `. Below, we set `pageLength = ` so that 5 rows appear and `scrollX = ` so the user can use a scroll bar on the bottom to scroll horizontally. The argument `class = 'white-space: nowrap'` ensures that each row is only one line (not multiple lines). You can read about other possible arguments and values [here](https://rstudio.github.io/DT/?_ga=2.2810736.1321860763.1619286819-369061888.1601594705) or by entering `?datatable`

```{r, eval=F}
DT::datatable(linelist, 
              rownames = FALSE, 
              options = list(pageLength = 5, scrollX = TRUE), 
              class = 'white-space: nowrap' )
```

### Plots {.unnumbered}  

You can print plots to a dashboard pane as you would in an R script. In our example, we use the **incidence2** package to create an "epicurve" by age group with two simple commands (see [Epidemic curves] page). However, you could use `ggplot()` and print a plot in the same manner.  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_plots_script.png"))
```

Here is what the script produces:  

```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_plots_view.png"))
```


### Interactive plots {.unnumbered}  

You can also pass a standard ggplot or other plot object to `ggplotly()` from the **plotly** package (see the [Interactive plots] page). This will make your plot interactive, allow the reader to "zoom in", and show-on-hover the value of every data point (in this scenario the number of cases per week and age group in the curve).  

```{r, eval=F}
age_outbreak <- incidence(linelist, date_onset, "week", groups = age_cat)
plot(age_outbreak, fill = age_cat, col_pal = muted, title = "") %>% 
  plotly::ggplotly()
```

Here is what this looks like in the dashboard (gif). This interactive functionality will still work even if you email the dashboard as a static file (not online on a server).  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_ggplotly.gif"))
```

### HTML widgets {.unnumbered}

[HTML widgets for R](http://www.htmlwidgets.org/) are a special class of R packages that increases interactivity by utilizing JavaScript libraries. You can embed them in R Markdown outputs (such as a flexdashboard) and in Shiny dashboards.  

Some common examples of these widgets include:  

* Plotly (used in this handbook page and in the [Interative plots] page)
* visNetwork (used in the [Transmission Chains] page of this handbook)  
* Leaflet (used in the [GIS Basics] page of this handbook)  
* dygraphs (useful for interactively showing time series data)  
* DT (`datatable()`) (used to show dynamic tables with filter, sort, etc.)  

Below we demonstrate adding an epidemic transmission chain which uses visNetwork to the dashboard. The script shows only the new code added to the "Column 2" section of the R Markdown script. You can find the code in the [Transmission chains] page of this handbook.  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_chain_script.png"))
```

Here is what the script produces:  

```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_chain.gif"))
```



## Code organization

You may elect to have all code within the R Markdown **flexdashboard** script. Alternatively, to have a more clean and concise dashboard script you may choose to call upon code/figures that are hosted or created in external R scripts. This is described in greater detail in the [Reports with R Markdown] page. 


## Shiny  

Integrating the R package **shiny** can make your dashboards even more reactive to user input. For example, you could have the user select a jurisdiction, or a date range, and have panels react to their choice (e.g. filter the data displayed). To embed **shiny** reactivity into **flexdashboard**, you need only make a few changes to your **flexdashboard** R Markdown script.  

You can use **shiny** to produce apps/dashboards *without* flexdashboard too. The handbook page on [Dashboards with Shiny] gives an overview of this approach, including primers on **shiny** syntax, app file structure, and options for sharing/publishing (including free server options). These syntax and general tips translate into the **flexdashboard** context as well.  

Embedding **shiny** in **flexdashboard** is however, a fundamental change to your flexdashboard. It will no longer produce an HTML output that you can send by email and anyone could open and view. Instead, it will be an "app". The "Knit" button at the top of the script will be replaced by a "Run document" icon, which will open an instance of the interactive the dashboard locally on your computer.  

Sharing your dashboard will now require that you either:  

* Send the Rmd script to the viewer, they open it in R on their computer, and run the app, or  
* The app/dashboard is hosted on a server accessible to the viewer  

Thus, there are benefits to integrating **shiny**, but also complications. If easy sharing by email is a priority and you don't need **shiny** reactive capabilities, consider the reduced interactivity offered by `ggplotly()` as demonstrated above.    

Below we give a very simple example using the same "outbreak_dashboard.Rmd" as above. Extensive documentation on integrating Shiny into **flexdashboard** is available online [here](https://rmarkdown.rstudio.com/flexdashboard/shiny.html).  



### Settings {.unnumbered}  

Enable **shiny** in a **flexdashboard** by adding the YAML parameter `runtime: shiny` at the same indentation level as `output: `, as below:  

```md
---
title: "Outbreak dashboard (Shiny demo)"
output: 
  flexdashboard::flex_dashboard:
    orientation: columns
    vertical_layout: fill
runtime: shiny
---

```

It is also convenient to enable a "side bar" to hold the shiny input widgets that will collect information from the user. As explained above, create a column and indicate the `{.sidebar}` option to create a side bar on the left side. You can add text and R chunks containing the **shiny** `input` commands within this column.  

If your app/dashboard is hosted on a server and may have multiple simultaneous users, name the first R code chunk as `global`. Include the commands to import/load your data in this chunk. This special named chunk is treated differently, and the data imported within it are only imported once (not continuously) and are available for all users. This improves the start-up speed of the app.  

### Worked example {.unnumbered}  

Here we adapt the flexdashboard script "outbreak_dashboard.Rmd" to include **shiny**. We will add the capability for the user to select a hospital from a drop-down menu, and have the epidemic curve reflect only cases from that hospital, with a dynamic plot title. We do the following:  

* Add `runtime: shiny` to the YAML  
* Re-name the setup chunk as `global`  
* Create a sidebar containing:  
  * Code to create a vector of unique hospital names  
  * A `selectInput()` command (**shiny** drop-down menu) with the choice of hospital names. The selection is saved as `hospital_choice`, which can be referenced in later code as `input$hospital_choice`  
* The epidemic curve code (column 2) is wrapped within `renderPlot({ })`, including:  
  * A filter on the dataset restricting the column `hospital` to the current value of `input$hospital_choice`  
  * A dynamic plot title that incorporates `input$hospital_choice`  
  
Note that any code referencing an `input$` value must be within a `render({})` function (to be reactive).  

Here is the top of the script, including YAML, global chunk, and sidebar:  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_shiny_script1.png"))
```
  
Here is the Column 2, with the reactive epicurve plot:  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_shiny_script2.png"))
```

And here is the dashboard:  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_shiny_view.gif"))
```




### Other examples {.unnumbered}  

To read a health-related example of a Shiny-**flexdashboard** using the **shiny** interactivity and the **leaflet** mapping widget, see this chapter of the online book [Geospatial Health Data: Modeling and Visualization with R-INLA and Shiny](https://www.paulamoraga.com/book-geospatial/sec-dashboardswithshiny.html).  




## Sharing  

Dashboards that do not contain Shiny elements will output an HTML file (.html), which can be emailed (if size permits). This is useful, as you can send the "dashboard" report and not have to set up a server to host it as a website.  

If you have embedded **shiny**, you will not be able to send an output by email, but you can send the script itself to an R user, or host the dashboard on a server as explained above.  


## Resources  

Excellent tutorials that informed this page can be found below. If you review these, most likely within an hour you can have your own dashboard.  

https://bookdown.org/yihui/rmarkdown/dashboards.html

https://rmarkdown.rstudio.com/flexdashboard/

https://rmarkdown.rstudio.com/flexdashboard/using.html

https://rmarkdown.rstudio.com/flexdashboard/examples.html
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/flexdashboard.Rmd-->


# Dashboards với Shiny {#shiny-basics}  

Dashboards are often a great way to share results from analyses with others. Producing a dashboard with **shiny** requires a relatively advanced knowledge of the R language, but offers incredible customization and possibilities.  

<!-- One of the largest drawbacks of `R` is its usability for people who are new to or have no experience with programming languages. While these skills are very valuable, most people will find that this represents a barrier to sharing analyses, especially in multidisciplinary environments. It requires some work to maintain an `R` installation, and not everyone will be comfortable running shared code, even if it's well documented and easy to read. This is *especially* true when users have to change parameters of code!  -->

<!-- R based dashboards are also advantageous in that they centralise how code is run - when the same code is run on different machines, often people will have to deal with differing file paths, different R versions, and different package installations. For this reason, dashboards are a great way to share code with others in a user friendly way! -->

It is recommended that someone learning dashboards with **shiny** has good knowledge of data transformation and visualisation, and is comfortable debugging code, and writing functions. Working with dashboards is not intuitive when you're starting, and is difficult to understand at times, but is a great skill to learn and gets much easier with practice!

This page will give a short overview of how to make dashboards with **shiny** and its extensions. 
For an alternative method of making dashboards that is faster, easier, but perhaps less customizeable, see the page on **flextable** ([Dashboards with R Markdown]).  



## Preparation  


### Load packages {.unnumbered}  

In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

We begin by installing the **shiny** R package:  

```{r, eval = FALSE}
pacman::p_load("shiny")
```


### Import data {.unnumbered}  

If you would like to follow-along with this page, see this section of the [Download handbook and data](#data_shiny). There are links to download the R scripts and data files that produce the final Shiny app.  

If you try to re-construct the app using these files, please be aware of the R project folder structure that is created over the course of the demonstration (e.g. folders for "data" and for "funcs").  



<!-- ======================================================= -->
## The structure of a shiny app {  }

### Basic file structures {.unnumbered}  

To understand `shiny`, we first need to understand how the file structure of an app works! We should make a brand new directory before we start. This can actually be made easier by choosing _New project_ in _Rstudio_, and choosing _Shiny Web Application_. This will create the basic structure of a shiny app for you.

When opening this project, you'll notice there is a `.R` file already present called _app.R_. It is *essential* that we have one of two basic file structures:

1. One file called _app.R_, *or*  
2. Two files, one called _ui.R_ and the other _server.R_  

In this page, we will use the first approach of having one file called *app.R*. Here is an example script:  

```{r, eval = FALSE}
# an example of app.R

library(shiny)

ui <- fluidPage(

    # Application title
    titlePanel("My app"),

    # Sidebar with a slider input widget
    sidebarLayout(
        sidebarPanel(
            sliderInput("input_1")
        ),

        # Show a plot 
        mainPanel(
           plotOutput("my_plot")
        )
    )
)

# Define server logic required to draw a histogram
server <- function(input, output) {
     
     plot_1 <- reactive({
          plot_func(param = input_1)
     })
     
    output$my_plot <- renderPlot({
       plot_1()
    })
}


# Run the application 
shinyApp(ui = ui, server = server)


```


If you open this file, you'll notice that two objects are defined - one called `ui` and another called `server`. These objects *must* be defined in *every* shiny app and are central to the structure of the app itself! In fact, the only difference between the two file structures described above is that in structure 1, both `ui` and `server` are defined in one file, whereas in structure 2 they are defined in separate files. Note: we can also (and we should if we have a larger app) have other .R files in our structure that we can `source()` into our app.



### The server and the ui {.unnumbered}

We next need to understand what the `server` and `ui` objects actually _do_. *Put simply, these are two objects that are interacting with each other whenever the user interacts with the shiny app.*

The UI element of a shiny app is, on a basic level, R code that creates an HTML interface. This means everything that is *displayed* in the UI of an app. This generally includes:

* "Widgets" - dropdown menus, check boxes, sliders, etc that can be interacted with by the user
* Plots, tables, etc - outputs that are generated with R code
* Navigation aspects of an app - tabs, panes, etc. 
* Generic text, hyperlinks, etc
* HTML and CSS elements (addressed later)

The most important thing to understand about the UI is that it *receives inputs* from the user and *displays outputs* from the server. There is no *active* code running in the ui *at any time* - all changes seen in the UI are passed through the server (more or less). So we have to make our plots, downloads, etc in the server

The server of the shiny app is where all code is being run once the app starts up. The way this works is a little confusing. The server function will effectively _react_ to the user interfacing with the UI, and run chunks of code in response. If things change in the server, these will be passed back up to the ui, where the changes can be seen. Importantly, the code in the server will be executed *non-consecutively* (or it's best to think of it this way). Basically, whenever a ui input affects a chunk of code in the server, it will run automatically, and that output will be produced and displayed.

This all probably sounds very abstract for now, so we'll have to dive into some examples to get a clear idea of how this actually works. 


### Before you start to build an app {.unnumbered}

Before you begin to build an app, its immensely helpful to know *what* you want to build. Since your UI will be written in code, you can't really visualise what you're building unless you are aiming for something specific. For this reason, it is immensely helpful to look at lots of examples of shiny apps to get an idea of what you can make - even better if you can look at the source code behind these apps! Some great resources for this are:

* The [Rstudio app gallery](https://shiny.rstudio.com/gallery/)  

Once you get an idea for what is possible, it's also helpful to map out what you want yours to look like - you can do this on paper or in any drawing software (PowerPoint, MS paint, etc.). It's helpful to start simple for your first app! There's also no shame in using code you find online of a nice app as a template for your work - its much easier than building something from scratch!



## Building a UI 

When building our app, its easier to work on the UI first so we can see what we're making, and not risk the app failing because of any server errors. As mentioned previously, its often good to use a template when working on the UI. There are a number of standard layouts that can be used with shiny that are available from the base shiny package, but it's worth noting that there are also a number of package extensions such as `shinydashboard`. We'll use an example from base shiny to start with. 

A shiny UI is generally defined as a series of nested functions, in the following order

1. A function defining the general layout (the most basic is `fluidPage()`, but more are available)
2. Panels within the layout such as:
     - a sidebar (`sidebarPanel()`)
     - a "main" panel (`mainPanel()`)
     - a tab (`tabPanel()`)
     - a generic "column" (`column()`)
3. Widgets and outputs - these can confer inputs to the server (widgets) or outputs from the server (outputs)
     - Widgets generally are styled as `xxxInput()` e.g. `selectInput()`
     - Outputs are generally styled as `xxxOutput()` e.g. `plotOutput()`

It's worth stating again that these can't be visualised easily in an abstract way, so it's best to look at an example! Lets consider making a basic app that visualises our malaria facility count data by district. This data has a lot of differnet parameters, so it would be great if the end user could apply some filters to see the data by age group/district as they see fit! We can use a very simple shiny layout to start - the sidebar layout. This is a layout where widgets are placed in a sidebar on the left, and the plot is placed on the right.

Lets plan our app - we can start with a selector that lets us choose the district where we want to visualise data, and another to let us visualise the age group we are interested in. We'll aim to use these filters to show an epicurve that reflects these parameters. So for this we need:

1. Two dropdown menus that let us choose the district we want, and the age group we're interested in. 
2. An area where we can show our resulting epicurve.

This might look something like this:

```{r, eval = FALSE}

library(shiny)

ui <- fluidPage(

  titlePanel("Malaria facility visualisation app"),

  sidebarLayout(

    sidebarPanel(
         # selector for district
         selectInput(
              inputId = "select_district",
              label = "Select district",
              choices = c(
                   "All",
                   "Spring",
                   "Bolo",
                   "Dingo",
                   "Barnard"
              ),
              selected = "All",
              multiple = TRUE
         ),
         # selector for age group
         selectInput(
              inputId = "select_agegroup",
              label = "Select age group",
              choices = c(
                   "All ages" = "malaria_tot",
                   "0-4 yrs" = "malaria_rdt_0-4",
                   "5-14 yrs" = "malaria_rdt_5-14",
                   "15+ yrs" = "malaria_rdt_15"
              ), 
              selected = "All",
              multiple = FALSE
         )

    ),

    mainPanel(
      # epicurve goes here
      plotOutput("malaria_epicurve")
    )
    
  )
)


```


When app.R is run with the above UI code (with no active code in the `server` portion of app.R) the layout appears looking like this - note that there will be no plot if there is no server to render it, but our inputs are working!

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "shiny", "simple_UI_view.png"))
```

This is a good opportunity to discuss how widgets work - note that each widget is accepting an `inputId`, a `label`, and a series of other options that are specific to the widget type. This `inputId` is extremely important - these are the IDs that are used to pass information from the UI to the server. For this reason, they *must be unique*. You should make an effort to name them something sensible, and specific to what they are interacting with in cases of larger apps.

You should read documentation carefully for full details on what each of these widgets do. Widgets will pass specific types of data to the server depending on the widget type, and this needs to be fully understood. For example, `selectInput()` will pass a character type to the server:

- If we select _Spring_ for the first widget here, it will pass the character object `"Spring"` to the server. 
- If we select two items from the dropdown menu, they will come through as a character vector (e.g. `c("Spring", "Bolo")`).

Other widgets will pass different types of object to the server! For example:

- `numericInput()` will pass a numeric type object to the server
- `checkboxInput()` will pass a logical type object to the server (`TRUE` or `FALSE`)

It's also worth noting the *named vector* we used for the age data here. For many widgets, using a named vector as the choices will display the *names* of the vector as the display choices, but pass the selected *value* from the vector to the server. I.e. here someone can select "15+" from the drop-down menu, and the UI will pass `"malaria_rdt_15"` to the server - which happens to be the name of the column we're interested in!


There are loads of widgets that you can use to do lots of things with your app. Widgets also allow you to upload files into your app, and download outputs. There are also some excellent shiny extensions that give you access to more widgets than base shiny - the **shinyWidgets** package is a great example of this. To look at some examples you can look at the following links:

- [base shiny widget gallery](https://shiny.rstudio.com/gallery/widget-gallery.html)
- [shinyWidgets gallery](https://github.com/dreamRs/shinyWidgets)



## Loading data into our app

The next step in our app development is getting the server up and running. To do this however, we need to get some data into our app, and figure out all the calculations we're going to do. A shiny app is not straightforward to debug, as it's often not clear where errors are coming from, so it's ideal to get all our data processing and visualisation code working before we start making the server itself.

So given we want to make an app that shows epi curves that change based on user input, we should think about what code we would need to run this in a normal R script. We'll need to:

1. Load our packages
2. Load our data
3. Transform our data
4. Develop a _function_ to visualise our data based on user inputs

This list is pretty straightforward, and shouldn't be too hard to do. It's now important to think about which parts of this process need to *be done only once* and which parts need to *run in response to user inputs*. This is because shiny apps generally run some code before running, which is only performed once. It will help our app's performance if as much of our code can be moved to this section. For this example, we only need to load our data/packages and do basic transformations once, so we can put that code *outside the server*. This means the only thing we'll need in the server is the code to visualise our data. Lets develop all of these componenets in a script first. However, since we're visualising our data with a function, we can also put the code _for the function_ outside the server so our function is in the environment when the app runs!

First lets load our data. Since we're working with a new project, and we want to make it clean, we can create a new directory called data, and add our malaria data in there. We can run this code below in a testing script we will eventually delete when we clean up the structure of our app.

```{r, echo = TRUE}
pacman::p_load("tidyverse", "lubridate")

# read data
malaria_data <- rio::import(here::here("data", "malaria_facility_count_data.rds")) %>% 
  as_tibble()

print(malaria_data)


```


It will be easier to work with this data if we use tidy data standards, so we should also transform into a longer data format, where age group is a column, and cases is another column. We can do this easily using what we've learned in the [Pivoting data] page.  


```{r, echo = TRUE}

malaria_data <- malaria_data %>%
  select(-newid) %>%
  pivot_longer(cols = starts_with("malaria_"), names_to = "age_group", values_to = "cases_reported")

print(malaria_data)

```

And with that we've finished preparing our data! This crosses items 1, 2, and 3 off our list of things to develop for our "testing R script". The last, and most difficult task will be building a function to produce an epicurve based on user defined parameters. As mentioned previously, it's *highly recommended* that anyone learning shiny first look at the section on functional programming ([Writing functions]) to understand how this works!

When defining our function, it might be hard to think about what parameters we want to include. For functional programming with shiny, every relevent parameter will generally have a widget associated with it, so thinking about this is usually quite easy! For example in our current app, we want to be able to filter by district, and have a widget for this, so we can add a district parameter to reflect this. We *don't* have any app functionality to filter by facility (for now), so we don't need to add this as a parameter. Lets start by making a function with three parameters:

1. The core dataset
2. The district of choice
3. The age group of choice

```{r}

plot_epicurve <- function(data, district = "All", agegroup = "malaria_tot") {
  
  if (!("All" %in% district)) {
    data <- data %>%
      filter(District %in% district)
    
    plot_title_district <- stringr::str_glue("{paste0(district, collapse = ', ')} districts")
    
  } else {
    
    plot_title_district <- "all districts"
    
  }
  
  # if no remaining data, return NULL
  if (nrow(data) == 0) {
    
    return(NULL)
  }
  
  data <- data %>%
    filter(age_group == agegroup)
  
  
  # if no remaining data, return NULL
  if (nrow(data) == 0) {
    
    return(NULL)
  }
  
  if (agegroup == "malaria_tot") {
      agegroup_title <- "All ages"
  } else {
    agegroup_title <- stringr::str_glue("{str_remove(agegroup, 'malaria_rdt')} years")
  }
  
  
  ggplot(data, aes(x = data_date, y = cases_reported)) +
    geom_col(width = 1, fill = "darkred") +
    theme_minimal() +
    labs(
      x = "date",
      y = "number of cases",
      title = stringr::str_glue("Malaria cases - {plot_title_district}"),
      subtitle = agegroup_title
    )
  
  
  
}

```


We won't go into great detail about this function, as it's relatively simple in how it works. One thing to note however, is we handle errors by returning `NULL` when it would otherwise give an error. This is because when a shiny server produces a `NULL` object instead of a plot object, nothing will be shown in the ui! This is important, as otherwise errors will often cause your app to stop working.  

Another thing to note is the use of the `%in%` operator when evaluating the `district` input. As mentioned above, this could arrive as a character vector with multiple values, so using `%in%` is more flexible than say, `==`.  

Let's test our function!

```{r, echo = TRUE, warning = FALSE}

plot_epicurve(malaria_data, district = "Bolo", agegroup = "malaria_rdt_0-4")

```

With our function working, we now have to understand how this all is going to fit into our shiny app. We mentioned the concept of _startup code_ before, but lets look at how we can actually incorporate this into the structure of our app. There are two ways we can do this!

1. Put this code in your _app.R_ file at the start of the script (above the UI), or  
2. Create a new file in your app's directory called _global.R_, and put the startup code in this file.

It's worth noting at this point that it's generally easier, especially with bigger apps, to use the second file structure, as it lets you separate your file structure in a simple way. Lets fully develop a this global.R script now. Here is what it could look like:


```{r, eval = F}
# global.R script

pacman::p_load("tidyverse", "lubridate", "shiny")

# read data
malaria_data <- rio::import(here::here("data", "malaria_facility_count_data.rds")) %>% 
  as_tibble()

# clean data and pivot longer
malaria_data <- malaria_data %>%
  select(-newid) %>%
  pivot_longer(cols = starts_with("malaria_"), names_to = "age_group", values_to = "cases_reported")


# define plotting function
plot_epicurve <- function(data, district = "All", agegroup = "malaria_tot") {
  
  # create plot title
  if (!("All" %in% district)) {            
    data <- data %>%
      filter(District %in% district)
    
    plot_title_district <- stringr::str_glue("{paste0(district, collapse = ', ')} districts")
    
  } else {
    
    plot_title_district <- "all districts"
    
  }
  
  # if no remaining data, return NULL
  if (nrow(data) == 0) {
    
    return(NULL)
  }
  
  # filter to age group
  data <- data %>%
    filter(age_group == agegroup)
  
  
  # if no remaining data, return NULL
  if (nrow(data) == 0) {
    
    return(NULL)
  }
  
  if (agegroup == "malaria_tot") {
      agegroup_title <- "All ages"
  } else {
    agegroup_title <- stringr::str_glue("{str_remove(agegroup, 'malaria_rdt')} years")
  }
  
  
  ggplot(data, aes(x = data_date, y = cases_reported)) +
    geom_col(width = 1, fill = "darkred") +
    theme_minimal() +
    labs(
      x = "date",
      y = "number of cases",
      title = stringr::str_glue("Malaria cases - {plot_title_district}"),
      subtitle = agegroup_title
    )
  
  
  
}



```


Easy! One great feature of shiny is that it will understand what files named _app.R_, _server.R_, _ui.R_, and _global.R_ are for, so there is no need to connect them to each other via any code. So just by having this code in _global.R_ in the directory it will run before we start our app!.  

We should also note that it would improve our app's organisation if we moved the plotting function to its own file - this will be especially helpful as apps become larger. To do this, we could make another directory called _funcs_, and put this function in as a file called _plot_epicurve.R_. We could then read this function in via the following command in _global.R_

```{r, eval = F}

source(here("funcs", "plot_epicurve.R"), local = TRUE)

```

Note that you should *always* specify `local = TRUE` in shiny apps, since it will affect sourcing when/if the app is published on a server. 

## Developing an app server

Now that we have most of our code, we just have to develop our server. This is the final piece of our app, and is probably the hardest to understand. The server is a large R function, but its helpful to think of it as a series of smaller functions, or tasks that the app can perform. It's important to understand that these functions are not executed in a linear order. There is an order to them, but it's not fully necessary to understand when starting out with shiny. At a very basic level, these tasks or functions will activate when there is a change in user inputs that affects them, *unless the developer has set them up so they behave differently*. Again, this is all quite abstract, but lets first go through the three basic types of shiny _objects_

1. Reactive sources - this is another term for user inputs. The shiny server has access to the outputs from the UI through the widgets we've programmed. Every time the values for these are changed, this is passed down to the server.

2. Reactive conductors - these are objects that exist *only* inside the shiny server. We don't actually need these for simple apps, but they produce objects that can only be seen inside the server, and used in other operations. They generally depend on reactive sources.

3. Endpoints - these are outputs that are passed from the server to the UI. In our example, this would be the epi curve we are producing. 

With this in mind lets construct our server step-by-step. We'll show our UI code again here just for reference:

```{r, eval = FALSE}

ui <- fluidPage(

  titlePanel("Malaria facility visualisation app"),

  sidebarLayout(

    sidebarPanel(
         # selector for district
         selectInput(
              inputId = "select_district",
              label = "Select district",
              choices = c(
                   "All",
                   "Spring",
                   "Bolo",
                   "Dingo",
                   "Barnard"
              ),
              selected = "All",
              multiple = TRUE
         ),
         # selector for age group
         selectInput(
              inputId = "select_agegroup",
              label = "Select age group",
              choices = c(
                   "All ages" = "malaria_tot",
                   "0-4 yrs" = "malaria_rdt_0-4",
                   "5-14 yrs" = "malaria_rdt_5-14",
                   "15+ yrs" = "malaria_rdt_15"
              ), 
              selected = "All",
              multiple = FALSE
         )

    ),

    mainPanel(
      # epicurve goes here
      plotOutput("malaria_epicurve")
    )
    
  )
)


```

From this code UI we have:

- Two inputs:
  - District selector (with an inputId of `select_district`)
  - Age group selector (with an inputId of `select_agegroup`)
- One output:
  - The epicurve (with an outputId of `malaria_epicurve`)

As stated previously, these unique names we have assigned to our inputs and outputs are crucial. They *must be unique* and are used to pass information between the ui and server. In our server, we access our inputs via the syntax `input$inputID` and outputs and passed to the ui through the syntax `output$output_name` Lets have a look at an example, because again this is hard to understand otherwise!

```{r, eval = FALSE}

server <- function(input, output, session) {
  
  output$malaria_epicurve <- renderPlot(
    plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup)
  )
  
}


```


The server for a simple app like this is actually quite straightforward! You'll notice that the server is a function with three parameters - `input`, `output`, and `session` - this isn't that important to understand for now, but its important to stick to this setup! In our server we only have one task - this renders a plot based on our function we made earlier, and the inputs from the server. Notice how the names of the input and output objects correspond exactly to those in the ui.

To understand the basics of how the server reacts to user inputs, you should note that the output will know (through the underlying package) when inputs change, and rerun this function to create a plot every time they change. Note that we also use the `renderPlot()` function here - this is one of a family of class-specific functions that pass those objects to a ui output. There are a number of functions that behave similarly, but you need to ensure the function used matches the class of object you're passing to the ui! For example:

- `renderText()` - send text to the ui
- `renderDataTable` - send an interactive table to the ui.

Remember that these also need to match the output *function* used in the ui - so `renderPlot()` is paired with `plotOutput()`, and `renderText()` is matched with `textOutput()`. 

So we've finally made a functioning app! We can run this by pressing the Run App button on the top right of the script window in Rstudio. You should note that you can choose to run your app in your default browser (rather than Rstudio) which will more accurately reflect what the app will look like for other users.  


```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "shiny", "app_simple_view.gif"))
```


It is fun to note that in the R console, the app is "listening"! Talk about reactivity!  

```{r, echo=F}
knitr::include_graphics(here::here("images", "shiny", "listening.png"))
```


<!-- TO DO: *ADD SOMETHING ON DOWNLOADING A ZIP FILE OF THE APP?*  -->



## Adding more functionality

At this point we've finally got a running app, but we have very little functionality. We also haven't really scratched the surface of what shiny can do, so there's a lot more to learn about! Lets continue to build our existing app by adding some extra features. Some things that could be nice to add could be: 

1. Some explanatory text 
2. A download button for our plot - this would provide the user with a high quality version of the image that they're generating in the app
3. A selector for specific facilities
4. Another dashboard page - this could show a table of our data.

This is a lot to add, but we can use it to learn about a bunch of different shiny featues on the way. There is so much to learn about shiny (it can get *very* advanced, but its hopefully the case that once users have a better idea of how to use it they can become more comfortable using external learning sources as well).



### Adding static text {.unnumbered}  

Lets first discuss adding static text to our shiny app. Adding text to our app is extremely easy, once you have a basic grasp of it. Since static text doesn't change in the shiny app (If you'd like it to change, you can use *text rendering* functions in the server!), all of shiny's static text is generally added in the ui of the app. We wont go through this in great detail, but you can add a number of different elements to your ui (and even custom ones) by interfacing R with *HTML* and *css*.

HTML and css are languages that are explicitly involved in user interface design. We don't need to understand these too well, but *HTML* creates objects in UI (like a text box, or a table), and *css* is generally used to change the style and aesthetics of those objects. Shiny has access to a large array of _HTML tags_ - these are present for objects that behave in a specific way, such as headers, paragraphs of text, line breaks, tables, etc. We can use some of these examples like this:

- `h1()` - this a a *header* tag, which will make enclosed text automatically larger, and change defaults as they pertain to the font face, colour etc (depending on the overall theme of your app). You can access _smaller and smaller_ sub-heading with `h2()` down to `h6()` as well. Usage looks like:
  * `h1("my header - section 1")`

- `p()` - this is a *paragraph* tag, which will make enclosed text similar to text in a body of text. This text will automatically wrap, and be of a relatively small size (footers could be smaller for example.) Think of it as the text body of a word document. Usage looks like:  

  * `p("This is a larger body of text where I am explaining the function of my app")`
  
- `tags$b()` and `tags$i()` - these are used to create bold `tags$b()` and italicised `tags$i()` with whichever text is enclosed!

- `tags$ul()`, `tags$ol()` and `tags$li()` - these are tags used in creating *lists*. These are all used within the syntax below, and allow the user to create either an ordered list (`tags$ol()`; i.e. numbered) or unordered list (`tags$ul()`, i.e. bullet points). `tags$li()` is used to denote items in the list, regardless of which type of list is used. e.g.:

```{r, eval = F}

tags$ol(
  
  tags$li("Item 1"),
  
  tags$li("Item 2"),
  
  tags$li("Item 3")
  
)

```

- `br()` and `hr()` - these tags create *linebreaks* and *horizontal lines* (with a linebreak) respectively. Use them to separate out the sections of your app and text! There is no need to pass any items to these tags (parentheses can remain empty).


- `div()` - this is a *generic* tag that can *contain anything*, and can be *named anything*. Once you progress with ui design, you can use these to compartmentalize your ui, give specific sections specific styles, and create interactions between the server and UI elements. We won't go into these in detail, but they're worth being aware of!

Note that every one of these objects can be accessed through `tags$...` or for some, just the function. These are effectively synonymous, but it may help to use the `tags$...` style if you'd rather be more explicit and not overwrite the functions accidentally. This is also by no means an exhaustive list of tags available. There is a full list of all tags available in shiny  [here](https://shiny.rstudio.com/articles/tag-glossary.html) and even more can be used by inserting HTML directly into your ui!


If you're feeling confident, you can also add any *css styling elements* to your HTML tags with the `style` argument in any of them. We won't go into how this works in detail, but one tip for testing aesthetic changes to a UI is using the HTML inspector mode in chrome (of your shiny app you are running in browser), and editing the style of objects yourself!

Lets add some text to our app

```{r, eval = F}

ui <- fluidPage(

  titlePanel("Malaria facility visualisation app"),

  sidebarLayout(

    sidebarPanel(
         h4("Options"),
         # selector for district
         selectInput(
              inputId = "select_district",
              label = "Select district",
              choices = c(
                   "All",
                   "Spring",
                   "Bolo",
                   "Dingo",
                   "Barnard"
              ),
              selected = "All",
              multiple = TRUE
         ),
         # selector for age group
         selectInput(
              inputId = "select_agegroup",
              label = "Select age group",
              choices = c(
                   "All ages" = "malaria_tot",
                   "0-4 yrs" = "malaria_rdt_0-4",
                   "5-14 yrs" = "malaria_rdt_5-14",
                   "15+ yrs" = "malaria_rdt_15"
              ), 
              selected = "All",
              multiple = FALSE
         ),
    ),

    mainPanel(
      # epicurve goes here
      plotOutput("malaria_epicurve"),
      br(),
      hr(),
      p("Welcome to the malaria facility visualisation app! To use this app, manipulate the widgets on the side to change the epidemic curve according to your preferences! To download a high quality image of the plot you've created, you can also download it with the download button. To see the raw data, use the raw data tab for an interactive form of the table. The data dictionary is as follows:"),
    tags$ul(
      tags$li(tags$b("location_name"), " - the facility that the data were collected at"),
      tags$li(tags$b("data_date"), " - the date the data were collected at"),
      tags$li(tags$b("submitted_daate"), " - the date the data were submitted at"),
      tags$li(tags$b("Province"), " - the province the data were collected at (all 'North' for this dataset)"),
      tags$li(tags$b("District"), " - the district the data were collected at"),
      tags$li(tags$b("age_group"), " - the age group the data were collected for (0-5, 5-14, 15+, and all ages)"),
      tags$li(tags$b("cases_reported"), " - the number of cases reported for the facility/age group on the given date")
    )
    
  )
)
)



```

```{r, echo=F}
knitr::include_graphics(here::here("images", "shiny", "app_text_view.png"))
```


### Adding a link {.unnumbered}

To add a link to a website, use `tags$a()` with the link and display text as shown below. To have as a standalone paragraph, put it within `p()`. To have only a few words of a sentence linked, break the sentence into parts and use `tags$a()` for the hyperlinked part. To ensure the link opens in a *new* browser window, add `target = "_blank"` as an argument.  

```{r, eval=F}
tags$a(href = "www.epiRhandbook.com", "Visit our website!")
```



### Adding a download button {.unnumbered}

Lets move on to the second of the three features. A download button is a fairly common thing to add to an app and is fairly easy to make. We need to add another Widget to our ui, and we need to add another output to our server to attach to it. We can also introduce *reactive conductors* in this example!


Lets update our ui first - this is easy as shiny comes with a widget called `downloadButton()` - lets give it an inputId and a label.

```{r, eval = FALSE}

ui <- fluidPage(

  titlePanel("Malaria facility visualisation app"),

  sidebarLayout(

    sidebarPanel(
         # selector for district
         selectInput(
              inputId = "select_district",
              label = "Select district",
              choices = c(
                   "All",
                   "Spring",
                   "Bolo",
                   "Dingo",
                   "Barnard"
              ),
              selected = "All",
              multiple = FALSE
         ),
         # selector for age group
         selectInput(
              inputId = "select_agegroup",
              label = "Select age group",
              choices = c(
                   "All ages" = "malaria_tot",
                   "0-4 yrs" = "malaria_rdt_0-4",
                   "5-14 yrs" = "malaria_rdt_5-14",
                   "15+ yrs" = "malaria_rdt_15"
              ), 
              selected = "All",
              multiple = FALSE
         ),
         # horizontal line
         hr(),
         downloadButton(
           outputId = "download_epicurve",
           label = "Download plot"
         )

    ),

    mainPanel(
      # epicurve goes here
      plotOutput("malaria_epicurve"),
      br(),
      hr(),
      p("Welcome to the malaria facility visualisation app! To use this app, manipulate the widgets on the side to change the epidemic curve according to your preferences! To download a high quality image of the plot you've created, you can also download it with the download button. To see the raw data, use the raw data tab for an interactive form of the table. The data dictionary is as follows:"),
      tags$ul(
        tags$li(tags$b("location_name"), " - the facility that the data were collected at"),
        tags$li(tags$b("data_date"), " - the date the data were collected at"),
        tags$li(tags$b("submitted_daate"), " - the date the data were submitted at"),
        tags$li(tags$b("Province"), " - the province the data were collected at (all 'North' for this dataset)"),
        tags$li(tags$b("District"), " - the district the data were collected at"),
        tags$li(tags$b("age_group"), " - the age group the data were collected for (0-5, 5-14, 15+, and all ages)"),
        tags$li(tags$b("cases_reported"), " - the number of cases reported for the facility/age group on the given date")
      )
      
    )
    
  )
)


```
 
Note that we've also added in a `hr()` tag - this adds a horizontal line separating our control widgets from our download widgets. This is another one of the HTML tags that we discussed previously.

Now that we have our ui ready, we need to add the server component. Downloads are done in the server with the `downloadHandler()` function. Similar to our plot, we need to attach it to an output that has the same inputId as the download button. This function takes two arguments - `filename` and `content` - these are both functions. As you might be able to guess, `filename` is used to specify the name of the downloaded file, and `content` is used to specify what should be downloaded. `content` contain a function that you would use to save data locally - so if you were downloading a csv file you could use `rio::export()`. Since we're downloading a plot, we'll use `ggplot2::ggsave()`. Lets look at how we would program this (we won't add it to the server yet). 

```{r, eval = FALSE}

server <- function(input, output, session) {
  
  output$malaria_epicurve <- renderPlot(
    plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup)
  )
  
  output$download_epicurve <- downloadHandler(
    filename = function() {
      stringr::str_glue("malaria_epicurve_{input$select_district}.png")
    },
    
    content = function(file) {
      ggsave(file, 
             plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup),
             width = 8, height = 5, dpi = 300)
    }
    
  )
  
}


```


Note that the `content` function always takes a `file` argument, which we put where the output file name is specified. You might also notice that we're repeating code here - we are using our `plot_epicurve()` function twice in this server, once for the download and once for the image displayed in the app. While this wont massively affect performance, this means that the code to generate this plot will have to be run when the user changes the widgets specifying the district and age group, *and* again when you want to download the plot. In larger apps, suboptimal decisions like this one will slow things down more and more, so it's good to learn how to make our app more efficient in this sense. What would make more sense is if we had a way to run the epicurve code when the districts/age groups are changes, *and let that be used by* the renderPlot() and downloadHandler() functions. This is where reactive conductors come in! 

Reactive conductors are objects that are created in the shiny server in a *reactive* way, but are not outputted - they can just be used by other parts of the server. There are a number of different kinds of *reactive conductors*, but we'll go through the basic two.

1.`reactive()` - this is the most basic reactive conductor - it will react whenever any inputs used inside of it change (so our district/age group widgets)  
2. `eventReactive()`- this rective conductor works the same as `reactive()`, except that the user can specify which inputs cause it to rerun. This is useful if your reactive conductor takes a long time to process, but this will be explained more later.  

Lets look at the two examples:

```{r, eval = FALSE}

malaria_plot_r <- reactive({
  
  plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup)
  
})


# only runs when the district selector changes!
malaria_plot_er <- eventReactive(input$select_district, {
  
  plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup)
  
})



```

When we use the `eventReactive()` setup, we can specify which inputs cause this chunk of code to run - this isn't very useful to us at the moment, so we can leave it for now. Note that you can include multiple inputs with `c()`

Lets look at how we can integrate this into our server code:


```{r, eval = FALSE}

server <- function(input, output, session) {
  
  malaria_plot <- reactive({
    plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup)
  })
  
  
  
  output$malaria_epicurve <- renderPlot(
    malaria_plot()
  )
  
  output$download_epicurve <- downloadHandler(
    
    filename = function() {
      stringr::str_glue("malaria_epicurve_{input$select_district}.png")
    },
    
    content = function(file) {
      ggsave(file, 
             malaria_plot(),
             width = 8, height = 5, dpi = 300)
    }
    
  )
  
}


```

You can see we're just calling on the output of our reactive we've defined in both our download and plot rendering functions. One thing to note that often trips people up is you have to use the outputs of reactives as if they were functions - so you *must add empty brackets at the end of them* (i.e. `malaria_plot()` is correct, and `malaria_plot` is not). Now that we've added this solution our app is a little tidyer, faster, and easier to change since all our code that runs the epicurve function is in one place.


```{r, echo=F}
knitr::include_graphics(here::here("images", "shiny", "download_button_view.png"))
```


### Adding a facility selector {.unnumbered}  

Lets move on to our next feature - a selector for specific facilities. We'll implement another parameter into our function so we can pass this as an argument from our code. Lets look at doing this first - it just operates off the same principles as the other parameters we've set up. Lets update and test our function.


```{r, echo = TRUE}

plot_epicurve <- function(data, district = "All", agegroup = "malaria_tot", facility = "All") {
  
  if (!("All" %in% district)) {
    data <- data %>%
      filter(District %in% district)
    
    plot_title_district <- stringr::str_glue("{paste0(district, collapse = ', ')} districts")
    
  } else {
    
    plot_title_district <- "all districts"
    
  }
  
  # if no remaining data, return NULL
  if (nrow(data) == 0) {
    
    return(NULL)
  }
  
  data <- data %>%
    filter(age_group == agegroup)
  
  
  # if no remaining data, return NULL
  if (nrow(data) == 0) {
    
    return(NULL)
  }
  
  if (agegroup == "malaria_tot") {
      agegroup_title <- "All ages"
  } else {
    agegroup_title <- stringr::str_glue("{str_remove(agegroup, 'malaria_rdt')} years")
  }
  
    if (!("All" %in% facility)) {
    data <- data %>%
      filter(location_name == facility)
    
    plot_title_facility <- facility
    
  } else {
    
    plot_title_facility <- "all facilities"
    
  }
  
  # if no remaining data, return NULL
  if (nrow(data) == 0) {
    
    return(NULL)
  }

  
  
  ggplot(data, aes(x = data_date, y = cases_reported)) +
    geom_col(width = 1, fill = "darkred") +
    theme_minimal() +
    labs(
      x = "date",
      y = "number of cases",
      title = stringr::str_glue("Malaria cases - {plot_title_district}; {plot_title_facility}"),
      subtitle = agegroup_title
    )
  
  
  
}
```

Let's test it:  

```{r, warning=F, message=F}

plot_epicurve(malaria_data, district = "Spring", agegroup = "malaria_rdt_0-4", facility = "Facility 1")

```


With all the facilites in our data, it isn't very clear which facilities correspond to which districts - and the end user won't know either. This might make using the app quite unintuitive. For this reason, we should make the facility options in the UI change dynamically as the user changes the district - so one filters the other! Since we have so many variables that we're using in the options, we might also want to generate some of our options for the ui in our _global.R_ file _from the data_. For example, we can add this code chunk to _global.R_ after we've read our data in:



```{r, , message =  FALSE}

all_districts <- c("All", unique(malaria_data$District))

# data frame of location names by district
facility_list <- malaria_data %>%
  group_by(location_name, District) %>%
  summarise() %>% 
  ungroup()

```

Let's look at them:  

```{r}
all_districts
```


```{r}
facility_list
```


We can pass these new variables to the ui without any issue, since they are globally visible by both the server and the ui! Lets update our UI:


```{r, eval = FALSE}


ui <- fluidPage(

  titlePanel("Malaria facility visualisation app"),

  sidebarLayout(

    sidebarPanel(
         # selector for district
         selectInput(
              inputId = "select_district",
              label = "Select district",
              choices = all_districts,
              selected = "All",
              multiple = FALSE
         ),
         # selector for age group
         selectInput(
              inputId = "select_agegroup",
              label = "Select age group",
              choices = c(
                   "All ages" = "malaria_tot",
                   "0-4 yrs" = "malaria_rdt_0-4",
                   "5-14 yrs" = "malaria_rdt_5-14",
                   "15+ yrs" = "malaria_rdt_15"
              ), 
              selected = "All",
              multiple = FALSE
         ),
         # selector for facility
         selectInput(
           inputId = "select_facility",
           label = "Select Facility",
           choices = c("All", facility_list$location_name),
           selected = "All"
         ),
         
         # horizontal line
         hr(),
         downloadButton(
           outputId = "download_epicurve",
           label = "Download plot"
         )

    ),

    mainPanel(
      # epicurve goes here
      plotOutput("malaria_epicurve"),
      br(),
      hr(),
      p("Welcome to the malaria facility visualisation app! To use this app, manipulate the widgets on the side to change the epidemic curve according to your preferences! To download a high quality image of the plot you've created, you can also download it with the download button. To see the raw data, use the raw data tab for an interactive form of the table. The data dictionary is as follows:"),
      tags$ul(
        tags$li(tags$b("location_name"), " - the facility that the data were collected at"),
        tags$li(tags$b("data_date"), " - the date the data were collected at"),
        tags$li(tags$b("submitted_daate"), " - the date the data were submitted at"),
        tags$li(tags$b("Province"), " - the province the data were collected at (all 'North' for this dataset)"),
        tags$li(tags$b("District"), " - the district the data were collected at"),
        tags$li(tags$b("age_group"), " - the age group the data were collected for (0-5, 5-14, 15+, and all ages)"),
        tags$li(tags$b("cases_reported"), " - the number of cases reported for the facility/age group on the given date")
      )
      
    )
    
  )
)


```


Notice how we're now passing variables for our choices instead of hard coding them in the ui! This might make our code more compact as well! Lastly, we'll have to update the server. It will be easy to update our function to incorporate our new input (we just have to pass it as an argument to our new parameter), but we should remember we also want the ui to update dynamically when the user changes the selected district. It is important to understand here that we *can change the parameters and behaviour of widgets* while the app is running, but this needs to be done *in the server*. We need to understand a new way to output to the server to learn how to do this.

The functions we need to understand how to do this are known as *observer* functions, and are similar to *reactive* functions in how they behave. They have one key difference though:

- Reactive functions do not directly affect outputs, and produce objects that can be seen in other locations in the server
- Observer functions *can* affect server outputs, but do so via side effects of other functions. (They can also do other things, but this is their main function in practice)

Similar to reactive functions, there are two flavours of observer functions, and they are divided by the same logic that divides reactive functions:

1. `observe()` - this function runs whenever any inputs used inside of it change
2. `observeEvent()` - this function runs when a *user-specified* input changes

We also need to understand the shiny-provided functions that update widgets. These are fairly straightforward to run - they first take the `session` object from the server function (this doesn't need to be understood for now), and then the `inputId` of the function to be changed. We then pass new versions of all parameters that are already taken by `selectInput()` - these will be automatically updated in the widget. 

Lets look at an isolated example of how we could use this in our server. When the user changes the district, we want to filter our tibble of facilities by district, and update the choices to *only reflect those that are available in that district* (and an option for all facilities)

```{r, eval = FALSE}

observe({
  
  if (input$select_district == "All") {
    new_choices <- facility_list$location_name
  } else {
    new_choices <- facility_list %>%
      filter(District == input$select_district) %>%
      pull(location_name)
  }
  
  new_choices <- c("All", new_choices)
  
  updateSelectInput(session, inputId = "select_facility",
                    choices = new_choices)
  
})


```

And that's it! we can add it into our server, and that behaviour will now work. Here's what our new server should look like:

```{r, eval = FALSE}
server <- function(input, output, session) {
  
  malaria_plot <- reactive({
    plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup, facility = input$select_facility)
  })
  
  
  
  observe({
    
    if (input$select_district == "All") {
      new_choices <- facility_list$location_name
    } else {
      new_choices <- facility_list %>%
        filter(District == input$select_district) %>%
        pull(location_name)
    }
    
    new_choices <- c("All", new_choices)
    
    updateSelectInput(session, inputId = "select_facility",
                      choices = new_choices)
    
  })
  
  
  output$malaria_epicurve <- renderPlot(
    malaria_plot()
  )
  
  output$download_epicurve <- downloadHandler(
    
    filename = function() {
      stringr::str_glue("malaria_epicurve_{input$select_district}.png")
    },
    
    content = function(file) {
      ggsave(file, 
             malaria_plot(),
             width = 8, height = 5, dpi = 300)
    }
    
  )
  
  
  
}

```


```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "shiny", "app_menu_view.gif"))
```







### Adding another tab with a table {.unnumbered}

Now we'll move on to the last component we want to add to our app. We'll want to separate our ui into two tabs, one of which will have an interactive table where the user can see the data they are making the epidemic curve with. To do this, we can use the packaged ui elements that come with shiny relevant to tabs. On a basic level, we can enclose most of our main panel in this general structure:

```{r, eval = FALSE}


# ... the rest of ui

mainPanel(
  
  tabsetPanel(
    type = "tabs",
    tabPanel(
      "Epidemic Curves",
      ...
    ),
    tabPanel(
      "Data",
      ...
    )
  )
)


```

Lets apply this to our ui. We also will want to use the **DT** package here - this is a great package for making interactive tables from pre-existing data. We can see it being used for `DT::datatableOutput()` in this example.

```{r, echo = FALSE}
library(DT)
```

```{r, eval = FALSE}
ui <- fluidPage(
     
     titlePanel("Malaria facility visualisation app"),
     
     sidebarLayout(
          
          sidebarPanel(
               # selector for district
               selectInput(
                    inputId = "select_district",
                    label = "Select district",
                    choices = all_districts,
                    selected = "All",
                    multiple = FALSE
               ),
               # selector for age group
               selectInput(
                    inputId = "select_agegroup",
                    label = "Select age group",
                    choices = c(
                         "All ages" = "malaria_tot",
                         "0-4 yrs" = "malaria_rdt_0-4",
                         "5-14 yrs" = "malaria_rdt_5-14",
                         "15+ yrs" = "malaria_rdt_15"
                    ), 
                    selected = "All",
                    multiple = FALSE
               ),
               # selector for facility
               selectInput(
                    inputId = "select_facility",
                    label = "Select Facility",
                    choices = c("All", facility_list$location_name),
                    selected = "All"
               ),
               
               # horizontal line
               hr(),
               downloadButton(
                    outputId = "download_epicurve",
                    label = "Download plot"
               )
               
          ),
          
          mainPanel(
               tabsetPanel(
                    type = "tabs",
                    tabPanel(
                         "Epidemic Curves",
                         plotOutput("malaria_epicurve")
                    ),
                    tabPanel(
                         "Data",
                         DT::dataTableOutput("raw_data")
                    )
               ),
               br(),
               hr(),
               p("Welcome to the malaria facility visualisation app! To use this app, manipulate the widgets on the side to change the epidemic curve according to your preferences! To download a high quality image of the plot you've created, you can also download it with the download button. To see the raw data, use the raw data tab for an interactive form of the table. The data dictionary is as follows:"),
               tags$ul(
                    tags$li(tags$b("location_name"), " - the facility that the data were collected at"),
                    tags$li(tags$b("data_date"), " - the date the data were collected at"),
                    tags$li(tags$b("submitted_daate"), " - the date the data were submitted at"),
                    tags$li(tags$b("Province"), " - the province the data were collected at (all 'North' for this dataset)"),
                    tags$li(tags$b("District"), " - the district the data were collected at"),
                    tags$li(tags$b("age_group"), " - the age group the data were collected for (0-5, 5-14, 15+, and all ages)"),
                    tags$li(tags$b("cases_reported"), " - the number of cases reported for the facility/age group on the given date")
               )
               
               
          )
     )
)


```


Now our app is arranged into tabs! Lets make the necessary edits to the server as well. Since we dont need to manipulate our dataset at all before we render it this is actually very simple - we just render the malaria_data dataset via DT::renderDT() to the ui!


```{r, eval = FALSE}
server <- function(input, output, session) {
  
  malaria_plot <- reactive({
    plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup, facility = input$select_facility)
  })
  
  
  
  observe({
    
    if (input$select_district == "All") {
      new_choices <- facility_list$location_name
    } else {
      new_choices <- facility_list %>%
        filter(District == input$select_district) %>%
        pull(location_name)
    }
    
    new_choices <- c("All", new_choices)
    
    updateSelectInput(session, inputId = "select_facility",
                      choices = new_choices)
    
  })
  
  
  output$malaria_epicurve <- renderPlot(
    malaria_plot()
  )
  
  output$download_epicurve <- downloadHandler(
    
    filename = function() {
      stringr::str_glue("malaria_epicurve_{input$select_district}.png")
    },
    
    content = function(file) {
      ggsave(file, 
             malaria_plot(),
             width = 8, height = 5, dpi = 300)
    }
    
  )
  
  # render data table to ui
  output$raw_data <- DT::renderDT(
    malaria_data
  )
  
  
}


```


```{r, out.width=c('100%', '100%'), fig.show='hold', echo = F, fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "shiny", "app_table_view.gif"))
```


## Sharing shiny apps

Now that you've developed your app, you probably want to share it with others - this is the main advantage of shiny after all! We can do this by sharing the code directly, or we could publish on a server. If we share the code, others will be able to see what you've done and build on it, but this will negate one of the main advantages of shiny - *it can eliminate the need for end-users to maintain an R installation*. For this reason, if you're sharing your app with users who are not comfortable with R, it is much easier to share an app that has been published on a server. 

If you'd rather share the code, you could make a .zip file of the app, or better yet, *publish your app on github and add collaborators.* You can refer to the section on github for further information here.

However, if we're publishing the app online, we need to do a little more work. Ultimately, we want your app to be able to be accessed via a web URL so others can get quick and easy access to it. Unfortunately, to publish you app on a server, you need to have access to a server to publish it on! There are a number of hosting options when it comes to this:

- _shinyapps.io_: this is the easiest place to publish shiny apps, as it has the smallest amount of configuration work needed, and has some free, but limited licenses.

- _RStudio Connect_: this is a far more powerful version of an R server, that can perform many operations, including publishing shiny apps. It is however, harder to use, and less recommended for first-time users.

For the purposes of this document, we will use _shinyapps.io_, since it is easier for first time users. You can make a free account here to start - there are also different price plans for server licesnses if needed. The more users you expect to have, the more expensive your price plan may have to be, so keep this under consideration. If you're looking to create something for a small set of individuals to use, a free license may be perfectly suitable, but a public facing app may need more licenses.

First we should make sure our app is suitable for publishing on a server. In your app, you should restart your R session, and ensure that it runs without running any extra code. This is important, as an app that requires package loading, or data reading not defined in your app code won't run on a server. Also note that you can't have any *explicit* file paths in your app - these will be invalid in the server setting - using the `here` package solves this issue very well. Finally, if you're reading data from a source that requires user-authentication, such as your organisation's servers, this will not generally work on a server. You will need to liase with your IT department to figure out how to whitelist the shiny server here.

*signing up for account*

Once you have your account, you can navigate to the tokens page under _Accounts_. Here you will want to add a new token - this will be used to deploy your app. 

From here, you should note that the url of your account will reflect the name of your app - so if your app is called _my_app_, the url will be appended as _xxx.io/my_app/_. Choose your app name wisely! Now that you are all ready, click deploy - if successful this will run your app on the web url you chose!

*something on making apps in documents?*

## Further reading

So far, we've covered a lot of aspects of shiny, and have barely scratched the surface of what is on offer for shiny. While this guide serves as an introduction, there is loads more to learn to fully understand shiny. You should start making apps and gradually add more and more functionality


## Recommended extension packages

The following represents a selection of high quality shiny extensions that can help you get a lot more out of shiny. In no particular order:

- **shinyWidgets** - this package gives you many many more widgets that can be used in your app. Run `shinyWidgets::shinyWidgetsGallery()` to see a selection of available widgets with this package. See examples [here](https://github.com/dreamRs/shinyWidgets)  

- **shinyjs** - this is an excellent package that gives the user the ability to greatly extend shiny's utility via a series of javascript. The applications of this package range from very simple to highly advanced, but you might want to first use it to manipulate the ui in simple ways, like hiding/showing elements, or enabling/disabling buttons. Find out more [here](https://deanattali.com/shinyjs/basic)

- **shinydashboard** - this package massively expands the available ui that can be used in shiny, specifically letting the user create a complex dashboard with a variety of complex layouts. See more [here](https://rstudio.github.io/shinydashboard/)

- **shinydashboardPlus** - get even more features out of the **shinydashboard** framework! See more [here](https://rinterface.github.io/shinydashboardPlus/articles/shinydashboardPlus.html)

- **shinythemes** - change the default css theme for your shiny app with a wide range of preset templates! See more [here](https://rstudio.github.io/shinythemes/)


There are also a number of packages that can be used to create interactive outputs that are shiny compatible. 

- **DT** is semi-incorporated into base-shiny, but provides a great set of functions to create interactive tables.

- **plotly** is a package for creating interactive plots that the user can manipulate in app. You can also convert your plot to interactive versions via `plotly::ggplotly()`! As alternatives, **dygraphs** and **highcharter** are also excellent.


## Recommended resources



```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/shiny_basics.Rmd-->

# (PART) Tổng hợp {.unnumbered}
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/cat_misc.Rmd-->

# Viết hàm {#writing-functions} 


<!-- ======================================================= -->
## Preparation {  }


### Load packages {-}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r, echo=F, warning=F, message=F}
pacman::p_load(
  rio,          # File import
  here,         # File locator
  skimr,        # get overview of data
  tidyverse,    # data management + ggplot2 graphics, 
  gtsummary,    # summary statistics and tests
  janitor,      # adding totals and percents to tables
  scales,       # easily convert proportions to percents  
  flextable,     # converting tables to HTML
  purrr,          #makes functional programming easier
  readr,          #to read csv files
  highcharter     #to create highchart object and draw particular plot

  )
```

### Import data {-}

We import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow step-by-step, see instructions in the [Download book and data] page. The dataset is imported using the `import()` function from the **rio** package. See the page on [Import and export] for various ways to import data.

We will also use in the last part of this page some data on H7N9 flu from 2013.

```{r, echo=F}
# import the linelists into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

flu_china <- rio::import(here::here("data", "case_linelists", "fluH7N9_China_2013.csv"))

```


## Functions  

Functions are helpful in programming since they allow to make codes easier to understand, somehow shorter and less prone to errors (given there were no errors in the function itself).

If you have come so far to this handbook, it means you have came across endless functions since in R, every operation is a function call
`+, for, if, [, $, { …`. For example `x + y` is the same as`'+'(x, y)`

R is one the languages that offers the most possibility to work with functions and give enough tools to the user to easily write them. We should not think about functions as fixed at the top or at the end of the programming chain, R offers the possibility to use them as if they were vectors and even to use them inside other functions, lists...

Lot of very advanced resources on functional programming exist and we will only give here an insight to help you start with functional programming with short practical examples. You are then encouraged to visit the links on references to read more about it.





## Why would you use a function? 

Before answering this question, it is important to note that you have already had tips to get to write your very first R functions in the page on [Iteration, loops, and lists] of this handbook. In fact, use of "if/else" and loops is often a core part of many of our functions since they easily help to either broaden the application of our code allowing multiple conditions or to iterate codes for repeating tasks.

- I am repeating multiple times the same block of code to apply it to a different variable or data?

- Getting rid of it will it substantially shorten my overall code and make it run quicker?

- Is it possible that the code I have written is used again but with a different value at many places of the code?

If the answer to one of the previous questions is "YES", then you probably need to write a function

## How does R  build functions?

Functions in R have three main components:

- the `formals()` which is the list of arguments which controls how we can call the function

- the `body()` that is the code inside the function i.e. within the brackets or following the parenthesis depending on how we write it

and,

- the `environment()` which will help locate the function's variables and determines how the function finds value.
 
Once you have created your function, you can verify each of these components by calling the function associated.
 

## Basic syntax and structure

- A function will need to be named properly so that its job is easily understandable as soon as we read its name. Actually this is already the case with majority of the base R architecture. Functions like  `mean()`, `print()`, `summary()` have names that are very straightforward 

- A function will need arguments, such as the data to work on and other objects that can be static values among other options  

- And finally a function will give an output based on its core task and the arguments it has been given. Usually we will use the built-in functions as `print()`, `return()`... to produce the output. The output can be a logical value, a number, a character, a data frame...in short any kind of R object.

Basically this is the composition of a function:

```{r, eval=FALSE}

function_name <- function(argument_1, argument_2, argument_3){
  
           function_task
  
           return(output)
}


```

We can create our first function that will be called `contain_covid19()`. 

```{r}

contain_covid19 <- function(barrier_gest, wear_mask, get_vaccine){
  
                            if(barrier_gest == "yes" & wear_mask == "yes" & get_vaccine == "yes" ) 
       
                            return("success")
  
  else("please make sure all are yes, this pandemic has to end!")
}


```

We can then verify the components of our newly created function.

```{r}

formals(contain_covid19)
body(contain_covid19)
environment(contain_covid19)

```


Now we will test our function. To call our written function, you use it as you use all R functions i.e by writing the function name and adding the required arguments.

```{r}

contain_covid19(barrier_gest = "yes", wear_mask = "yes", get_vaccine = "yes")

```

We can write again the name of each argument for precautionary reasons. But without specifying them, the code should work since R has in memory the positioning of each argument. So as long as you put the values of the arguments in the correct order, you can skip writing the arguments names when calling the functions.

```{r}

contain_covid19("yes", "yes", "yes")

```

Then let's look what happens if one of the values is `"no"` or **not** `"yes"`.

```{r}

contain_covid19(barrier_gest = "yes", wear_mask = "yes", get_vaccine = "no")
```

If we provide an argument that is not recognized, we get an error: 

```{r, eval=F}
contain_covid19(barrier_gest = "sometimes", wear_mask = "yes", get_vaccine = "no")
```

`Error in contain_covid19(barrier_gest = "sometimes", wear_mask = "yes",  : 
  could not find function "contain_covid19"`


<span style="color: black;">**_NOTE:_** Some functions  (most of time very short and straightforward) may not need a name and can be used directly on a line of code or inside another function to do quick task. They are called **anonymous functions** .</span>

For instance below is a first anonymous function that   keeps only character variables the dataset.

```{r, eval=F}
linelist %>% 
  dplyr::slice_head(n=10) %>%  #equivalent to R base "head" function and that return first n observation of the  dataset
  select(function(x) is.character(x)) 
```
  
```{r, echo=F}
linelist %>% 
  dplyr::slice_head(n=10) %>%  #equivalent to R base "head" function and that return first n observation of the  dataset
  select(function(x) is.character(x)) %>%  
DT::datatable(rownames = FALSE, filter="top", options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )
```


Then another function that selects every second observation of our dataset (may be relevant when we have longitudinal data with many records per patient for instance after having ordered by date or visit).
In this case, the proper function writing outside dplyr would be `function (x) (x%%2 == 0)` to apply to the vector containing all row numbers.


```{r, eval=F}
linelist %>%   
   slice_head(n=20) %>% 
   tibble::rownames_to_column() %>% # add indices of each obs as rownames to clearly see the final selection
   filter(row_number() %%2 == 0)
```

```{r, echo=F}
linelist %>%   
   slice_head(n=20) %>% 
   tibble::rownames_to_column() %>%    # add indices of each obs as rownames to clearly see the final selection
   filter(row_number() %%2 == 0) %>% 
DT::datatable(rownames = FALSE, filter="top", options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )

```


A possible base R code for the same task would be:

```{r, eval = F}

linelist_firstobs <- head(linelist, 20)

linelist_firstobs[base::Filter(function(x) (x%%2 == 0), seq(nrow(linelist_firstobs))),]
```

```{r, echo=F}

linelist_firstobs <- head(linelist, 20)

linelist_firstobs[base::Filter(function(x) (x%%2 == 0), seq(nrow(linelist_firstobs))),] %>% 
DT::datatable(rownames = FALSE, filter="top", options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )

```


<span style="color: orange;">**_CAUTION:_** Though it is true that using functions can help us with our code, it can nevertheless be  time consuming to write some functions or to fix one if it has not been thought thoroughly, written adequately and is returning errors as a result. For this reason it is often recommended to first write the R code, make sure it does what we intend it to do, and then transform it into a function with its three main components as listed above. </span>

## Examples  

### Return proportion tables for several columns {.unnumbered}  

Yes, we already have nice functions in many packages allowing to summarize information in a very easy and nice way. But we will still try to make our own, in our first steps to getting used to writing functions.

In this example we want to show how writing a simple function would avoid you copy-pasting the same code multiple times.

```{r}

proptab_multiple <- function(my_data, var_to_tab){
  
  #print the name of each variable of interest before doing the tabulation
  print(var_to_tab)

  with(my_data,
       rbind( #bind the results of the two following function by row
        #tabulate the variable of interest: gives only numbers
          table(my_data[[var_to_tab]], useNA = "no"),
          #calculate the proportions for each variable of interest and round the value to 2 decimals
         round(prop.table(table(my_data[[var_to_tab]]))*100,2)
         )
       )
}


proptab_multiple(linelist, "gender")

proptab_multiple(linelist, "age_cat")

proptab_multiple(linelist, "outcome")


```

<span style="color: darkgreen;">**_TIP:_** As shown above, it is very important to comment your functions as you would do for the general programming. Bear in mind that a function's aim is to make a code ready to read, shorter and more efficient. Then one should be able to understand what the function does just by reading its name and should have more details reading the comments.</span>


A second option is to use this function in another one via a loop to make the process at once:

```{r}


for(var_to_tab in c("gender","age_cat",  "outcome")){
  
  print(proptab_multiple(linelist, var_to_tab))
  
}

```

A simpler way could be using the base R "apply" instead of a "for loop" as expressed below:

```{r, include= FALSE, eval=FALSE}

base::lapply(linelist[,c("gender","age_cat", "outcome")], table)

```


<span style="color: darkgreen;">**_TIP:_** R is often defined as a functional programming language and almost anytime you run a line of code you are using some built-in functions. A good habit to be more comfortable with writing functions is to often have an internal look at how the basic functions you are using daily are built. The shortcut to do so is selecting the function name and then clicking on`Ctrl+F2` or `fn+F2` or `Cmd+F2` (depending on your computer) .</span>

## Using **purrr**: writing functions that can be iteratively applied

### Modify class of multiple columns in a dataset {.unnumbered}  

Let's say many character variables in the original `linelist` data need to be changes to "factor" for analysis and plotting purposes. Instead of repeating the step several times, we can just use `lapply()` to do the transformation of all variables concerned on a single line of code.


<span style="color: orange;">**_CAUTION:_** `lapply()` returns a list, thus its use may require an additional modification as a last step.</span>


```{r, include=FALSE}

linelist_factor1 <- linelist %>%
      lapply(
          function(x) if(is.character(x)) as.factor(x) else x) %>%
      as.data.frame() %>% 
      glimpse()

```


The same step can be done using `map_if()` function from the **purrr** package

```{r}

linelist_factor2 <- linelist %>%
  purrr::map_if(is.character, as.factor)


linelist_factor2 %>%
        glimpse()

```


### Iteratively produce graphs for different levels of a variable {.unnumbered}

We will produce here pie chart to look at the distribution of patient's outcome in China during the H7N9 outbreak for each province. Instead of repeating the code for each of them, we will just apply a function that we will create.

```{r}

#precising options for the use of highchart
options(highcharter.theme =   highcharter::hc_theme_smpl(tooltip = list(valueDecimals = 2)))


#create a function called "chart_outcome_province" that takes as argument the dataset and the name of the province for which to plot the distribution of the outcome.

chart_outcome_province <- function(data_used, prov){
  
  tab_prov <- data_used %>% 
    filter(province == prov,
           !is.na(outcome))%>% 
    group_by(outcome) %>% 
    count() %>%
    adorn_totals(where = "row") %>% 
    adorn_percentages(denominator = "col", )%>%
    mutate(
        perc_outcome= round(n*100,2))
  
  
  tab_prov %>%
    filter(outcome != "Total") %>% 
  highcharter::hchart(
    "pie", hcaes(x = outcome, y = perc_outcome),
    name = paste0("Distibution of the outcome in:", prov)
    )
  
}

chart_outcome_province(flu_china, "Shanghai")
chart_outcome_province(flu_china,"Zhejiang")
chart_outcome_province(flu_china,"Jiangsu")


```



### Iteratively produce tables for different levels of a variable {.unnumbered}

Here we will create three indicators to summarize in a table and we would like to produce this table for each of the provinces. Our indicators are the delay between onset and hospitalization, the percentage of recovery and the median age of cases.

```{r}


indic_1 <- flu_china %>% 
  group_by(province) %>% 
  mutate(
    date_hosp= strptime(date_of_hospitalisation, format = "%m/%d/%Y"),
    date_ons= strptime(date_of_onset, format = "%m/%d/%Y"), 
    delay_onset_hosp= as.numeric(date_hosp - date_ons)/86400,
    mean_delay_onset_hosp = round(mean(delay_onset_hosp, na.rm=TRUE ), 0)) %>%
  select(province, mean_delay_onset_hosp)  %>% 
  distinct()
     

indic_2 <-  flu_china %>% 
            filter(!is.na(outcome)) %>% 
            group_by(province, outcome) %>% 
            count() %>%
            pivot_wider(names_from = outcome, values_from = n) %>% 
    adorn_totals(where = "col") %>% 
    mutate(
        perc_recovery= round((Recover/Total)*100,2))%>% 
  select(province, perc_recovery)
    
    
    
indic_3 <-  flu_china %>% 
            group_by(province) %>% 
            mutate(
                    median_age_cases = median(as.numeric(age), na.rm = TRUE)
            ) %>% 
  select(province, median_age_cases)  %>% 
  distinct()

#join the three indicator datasets

table_indic_all <- indic_1 %>% 
  dplyr::left_join(indic_2, by = "province") %>% 
        left_join(indic_3, by = "province")


#print the indicators in a flextable


print_indic_prov <-  function(table_used, prov){
  
  #first transform a bit the dataframe for printing ease
  indic_prov <- table_used %>%
    filter(province==prov) %>%
    pivot_longer(names_to = "Indicateurs", cols = 2:4) %>% 
   mutate( indic_label = factor(Indicateurs,
   levels= c("mean_delay_onset_hosp","perc_recovery","median_age_cases"),
   labels=c("Mean delay onset-hosp","Percentage of recovery", "Median age of the cases"))
   ) %>% 
    ungroup(province) %>% 
    select(indic_label, value)
  

    tab_print <- flextable(indic_prov)  %>%
    theme_vanilla() %>% 
    flextable::fontsize(part = "body", size = 10) 
    
    
     tab_print <- tab_print %>% 
                  autofit()   %>%
                  set_header_labels( 
                indic_label= "Indicateurs", value= "Estimation") %>%
    flextable::bg( bg = "darkblue", part = "header") %>%
    flextable::bold(part = "header") %>%
    flextable::color(color = "white", part = "header") %>% 
    add_header_lines(values = paste0("Indicateurs pour la province de: ", prov)) %>% 
bold(part = "header")
 
 tab_print <- set_formatter_type(tab_print,
   fmt_double = "%.2f",
   na_str = "-")

tab_print 
    
}




print_indic_prov(table_indic_all, "Shanghai")
print_indic_prov(table_indic_all, "Jiangsu")


```


## Tips and best Practices for well functioning functions

Functional programming is meant to ease code and facilitates its reading. It should produce the contrary. The tips below will help you having a clean code and easy to read code. 


### Naming and syntax {.unnumbered}

- Avoid using character that could have been easily already taken by other functions already existing in your environment

- It is recommended for the function name to be short and straightforward to understand for another reader

- It is preferred to use verbs as the function name and nouns for the argument names.


### Column names and tidy evaluation {.unnumbered}  

If you want to know how to reference *column names* that are provided to your code as arguments, read this [tidyverse programming guidance](https://dplyr.tidyverse.org/articles/programming.html). Among the topics covered are *tidy evaluation* and use of the *embrace* `{{ }}` "double braces"

For example, here is a quick skeleton template code from page tutorial mentioned just above:  

```{r, eval=F}

var_summary <- function(data, var) {
  data %>%
    summarise(n = n(), min = min({{ var }}), max = max({{ var }}))
}
mtcars %>% 
  group_by(cyl) %>% 
  var_summary(mpg)

```


### Testing and Error handling {.unnumbered}

The more complicated a function's task the higher the possibility of errors. Thus it is sometimes necessary to add some verification within the funtion to help quickly understand where the error is from and find a way t fix it.

- It can be more than recommended to introduce a check on the missingness of one argument using `missing(argument)`. This simple check can return "TRUE" or "FALSE" value.

```{r , error=TRUE}

contain_covid19_missing <- function(barrier_gest, wear_mask, get_vaccine){
  
  if (missing(barrier_gest)) (print("please provide arg1"))
  if (missing(wear_mask)) print("please provide arg2")
  if (missing(get_vaccine)) print("please provide arg3")


  if (!barrier_gest == "yes" | wear_mask =="yes" | get_vaccine == "yes" ) 
       
       return ("you can do better")
  
  else("please make sure all are yes, this pandemic has to end!")
}


contain_covid19_missing(get_vaccine = "yes")

```


- Use `stop()` for more detectable errors.

```{r, error=TRUE}

contain_covid19_stop <- function(barrier_gest, wear_mask, get_vaccine){
  
  if(!is.character(barrier_gest)) (stop("arg1 should be a character, please enter the value with `yes`, `no` or `sometimes"))
  
  if (barrier_gest == "yes" & wear_mask =="yes" & get_vaccine == "yes" ) 
       
       return ("success")
  
  else("please make sure all are yes, this pandemic has to end!")
}


contain_covid19_stop(barrier_gest=1, wear_mask="yes", get_vaccine = "no")

```

- As we see when we run most of the built-in functions, there are messages and warnings that can pop-up in certain conditions. We can integrate those in our written functions by using the functions `message()` and `warning()`.

- We can handle errors also by using `safely()` which takes one function as an argument and executes it in a safe way. In fact the function will execute without stopping if it encounters an error. `safely()` returns as output a **list** with two objects which are the results and the error it "skipped".

We can verify by first running the `mean()` as  function, then run it with `safely()`.


```{r, warning=FALSE}

map(linelist, mean)
```


```{r, warning=FALSE}
safe_mean <- safely(mean)
linelist %>% 
  map(safe_mean)

```


As said previously, well commenting our codes is already a good way for having documentation in our work.  


<!-- ======================================================= -->
## Resources


[R for Data Science link](https://r4ds.had.co.nz/functions.html)   

[Cheatsheet advance R programming](https://www.rstudio.com/wp-content/uploads/2016/02/advancedR.pdf)

[Cheatsheet purr Package](https://purrr.tidyverse.org/)

[Video-ACM talk by Hadley Wickham: The joy of functional programming (how does map_dbl work)](https://youtube.videoken.com/embed/bzUmK0Y07ck)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/writing_functions.Rmd-->


# Tương tác với thư mục làm việc {#directories}  

In this page we cover common scenarios where you create, interact with, save, and import with directories (folders).  


## Preparation  

### **fs** package {.unnumbered}  

The **fs** package is a **tidyverse** package that facilitate directory interactions, improving on some of the **base** R functions. In the sections below we will often use functions from **fs**.  

```{r}
pacman::p_load(
  fs,             # file/directory interactions
  rio,            # import/export
  here,           # relative file pathways
  tidyverse)      # data management and visualization
```


### Print directory as a dendrogram tree {.unnumbered}  

Use the function `dir_tree()` from **fs**.  

Provide the folder filepath to `path = ` and decide whether you want to show only one level (`recurse = FALSE`) or all files in all sub-levels (`recurse = TRUE`). Below we use `here()` as shorthand for the R project and specify its sub-folder "data", which contains all the data used for this R handbook. We set it to display all files within "data" and its sub-folders (e.g. "cache", "epidemic models", "population", "shp", and "weather").  


```{r}
fs::dir_tree(path = here("data"), recurse = TRUE)
```


## List files in a directory  

To list just the file names in a directory you can use `dir()` from **base** R. For example, this command lists the file names of the files in the "population" subfolder of the "data" folder in an R project. The relative filepath is provided using `here()` (which you can read about more in the [Import and export] page).  

```{r}
# file names
dir(here("data", "gis", "population"))
```

To list the full file paths of the directory's files, you can use you can use `dir_ls()` from **fs**. A **base** R alternative is `list.files()`.  

```{r}
# file paths
dir_ls(here("data", "gis", "population"))
```

To get all the metadata information about each file in a directory, (e.g. path, modification date, etc.) you can use `dir_info()` from **fs**.  

This can be particularly useful if you want to extract the last modification time of the file, for example if you want to import the most recent version of a file. For an example of this, see the [Import and export] page.     

```{r, eval=F}
# file info
dir_info(here("data", "gis", "population"))
```

Here is the data frame returned. Scroll to the right to see all the columns.  

```{r, echo=F}
DT::datatable(dir_info(here("data", "gis", "population")), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

## File information  

To extract metadata information about a specific file, you can use `file_info()` from **fs** (or `file.info()` from **base** R).  

```{r, eval=F}
file_info(here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, echo=F}
DT::datatable(file_info(here("data", "case_linelists", "linelist_cleaned.rds")), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Here we use the `$` to index the result and return only the `modification_time` value.  

```{r}
file_info(here("data", "case_linelists", "linelist_cleaned.rds"))$modification_time
```




## Check if exists  

### R objects {.unnumbered}  

You can use `exists()` from **base** R to check whether an R object exists *within* R (supply the object name in quotes).  

```{r}
exists("linelist")
```

Note that some **base** R packages use generic object names like "data" behind the scenes, that will appear as TRUE unless `inherit = FALSE` is specified. This is one reason to not name your dataset "data".  

```{r}
exists("data")
exists("data", inherit = FALSE)
```

If you are writing a function, you should use `missing()` from **base** R to check if an argument is present or not, instead of `exists()`.  



### Directories {.unnumbered}  

To check whether a directory exists, provide the file path (and file name) to `is_dir()` from **fs**. Scroll to the right to see that `TRUE` is printed.    

```{r}
is_dir(here("data"))
```

An alternative is `file.exists()` from **base** R.  


### Files {.unnumbered}  

To check if a specific file exists, use `is_file()` from **fs**. Scroll to the right to see that `TRUE` is printed.  

```{r}
is_file(here("data", "case_linelists", "linelist_cleaned.rds"))
```

A **base** R alternative is `file.exists()`.  



## Create  

### Directories {.unnumbered}  

To create a new directory (folder) you can use `dir_create()` from **fs**. If the directory already exists, it will not be overwritten and no error will be returned. 

```{r, eval=F}
dir_create(here("data", "test"))
```

An alternative is `dir.create()` from **base** R, which will show an error if the directory already exists. In contrast, `dir_create()` in this scenario will be silent.  

### Files {.unnumbered}  

You can create an (empty) file with `file_create()` from **fs**. If the file already exists, it will not be over-written or changed.  

```{r, eval=F}
file_create(here("data", "test.rds"))
```

A **base** R alternative is `file.create()`. But if the file already exists, this option will truncate it. If you use `file_create()` the file will be left unchanged.  


### Create if does not exists {.unnumbered}  

UNDER CONSTRUCTION  


## Delete

### R objects {.unnumbered}  

Use `rm()` from **base** R to remove an R object.  

### Directories {.unnumbered}  

Use `dir_delete()` from **fs**. 


### Files {.unnumbered}  

You can delete files with `file_delete()` from **fs**.  



## Running other files  

### `source()` {.unnumbered}  

To run one R script from another R script, you can use the `source()` command (from **base** R).

```{r, eval=F}
source(here("scripts", "cleaning_scripts", "clean_testing_data.R"))
```

This is equivalent to viewing the above R script and clicking the "Source" button in the upper-right of the script. This will execute the script but will do it silently (no output to the R console) unless specifically intended. See the page on [Interactive console] for examples of using `source()` to interact with a user via the R console in question-and-answer mode.  

```{r, fig.align = "center", out.height = '300%', echo=F}
knitr::include_graphics(here::here("images", "source_button.png"))
```


### `render()` {.unnumbered}  

`render()` is a variation on `source()` most often used for R markdown scripts. You provide the `input = ` which is the R markdown file, and also the `output_format = ` (typically either "html_document", "pdf_document", "word_document", "") 

See the page on [Reports with R Markdown] for more details. Also see the documentation for `render()` [here](https://rmarkdown.rstudio.com/docs/reference/render.html) or by entering `?render`.  



### Run files in a directory {.unnumbered}

You can create a *for loop* and use it to `source()` every file in a directory, as identified with `dir()`. 

```{r, eval=F}
for(script in dir(here("scripts"), pattern = ".R$")) {   # for each script name in the R Project's "scripts" folder (with .R extension)
  source(here("scripts", script))                        # source the file with the matching name that exists in the scripts folder
}
```

If you only want to run certain scripts, you can identify them by name like this:  

```{r, eval=F}

scripts_to_run <- c(
     "epicurves.R",
     "demographic_tables.R",
     "survival_curves.R"
)

for(script in scripts_to_run) {
  source(here("scripts", script))
}

```



Here is a [comparison](https://cran.r-project.org/web/packages/fs/vignettes/function-comparisons.html) of the **fs** and **base** R functions.  

### Import files in a directory  {.unnumbered}

See the page on [Import and export] for importing and exporting individual files.  

Also see the [Import and export] page for methods to automatically import the most recent file, based on a date in the file name *or* by looking at the file meta-data.  

See the page on [Iteration, loops, and lists] for an example with the package **purrr** demonstrating:  

* Splitting a data frame and saving it out as multiple CSV files  
* Splitting a data frame and saving each part as a separate sheet within one Excel workbook  
* Importing multiple CSV files and combining them into one dataframe  
* Importing an Excel workbook with multiple sheets and combining them into one dataframe  




## **base** R  

See below the functions `list.files()` and `dir()`, which perform the same operation of listing files within a specified directory. You can specify `ignore.case =` or a specific pattern to look for. 

```{r, eval=F}
list.files(path = here("data"))

list.files(path = here("data"), pattern = ".csv")
# dir(path = here("data"), pattern = ".csv")

list.files(path = here("data"), pattern = "evd", ignore.case = TRUE)

```

If a file is currently "open", it will display in your folder with a tilde in front, like "~$hospital_linelists.xlsx".  


<!-- ======================================================= -->
## Resources {  }

https://cran.r-project.org/web/packages/fs/vignettes/function-comparisons.html



```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/directories.Rmd-->

# Version control với Git và Github {#collaboration}

This chapter presents an overview of using Git to collaborate with others.
More extensive tutorials can be
found at the bottom in the Resources section.

## What is Git?

Git is a **version control** software that allows tracking changes in a
folder. It can be used like the "track change" option in Word, LibreOffice or
Google docs, but for all types of files. It is one of the most powerful
and most used options for version control.

**Why have I never heard of it? -** While people with a developer
background routinely learn to use version control software (Git,
Mercurial, Subversion or others), few of us from
quantitative disciplines are taught these skills. Consequently, most epidemiologists never
hear of it during their studies, and have to learn it on the fly.

**Wait, I heard of Github, is it the same?** - Not exactly, but you
often use them together, and we will show you how to. In short:

-   **Git** is the version control system, a piece of software. You can use it
    locally on your computer or to synchronize a folder with a
    host **website**. By default, one uses a terminal to give Git
    instructions in command-line.

-   You can use a **Git client/interface** to avoid the command-line and
    perform the same actions (at least for the simple, super common
    ones).

-   If you want to store your folder in a **host website** to
    collaborate with others, you may create an account at Github,
    Gitlab, Bitbucket or others.

So you could use the client/interface **Github Desktop**, which uses
**Git** in the background to manage your files, both locally on your
computer, and remotely on a **Github** server.

## Why use the combo Git and Github?

Using **Git** facilitates:

1)  Archiving documented versions with incremental changes so that you
    can easily revert backwards to any previous state
2)  Having parallel *branches*, i.e. developing/"working" versions with
    structured ways to integrate the changes after review

This can be done locally on your computer, even if you don't collaborate
with other people. Have you ever:

-   regretted having deleted a section of code, only to realize two
    months later that you actually needed it?


-   come back on a project that had been on pause and attempted to
    remember whether you had made that tricky modification in one of the
    models?

-   had a *file model_1.R* and another file *model_1\_test.R* and a file
    *model_1\_not_working.R* to try things out?

-   had a file *report.Rmd*, a file *report_full.Rmd*, a file
    *report_true_final.Rmd*, a file *report_final_20210304.Rmd*, a file
    *report_final_20210402.Rmd* and cursed your archiving skills?

Git will help with all that, and is worth to learn for that alone.


However, it becomes even more powerful when used with a online repository
such as Github to support **collaborative projects**. This facilitates:

-   Collaboration: others can review, comment on, and
    accept/decline changes

-   Sharing your code, data, and outputs, and invite feedback
    from the public (or privately, with your team)

and avoids:

-   "Oops, I forgot to send the last version and now you need to
    redo two days worth of work on this new file"

-   Mina, Henry and Oumar all worked at the same time on one script and
    need to manually merge their changes

-   Two people try to modify the same file on Dropbox and Sharepoint 
    and this creates a synchronization error.

### This sounds complicated, I am not a programmer {-}

It can be. Examples of advanced uses can be quite scary. However, much
like R, or even Excel, you don't need to become an expert to reap the
benefits of the tool. Learning a *small number of functions and notions*
lets you track your changes, synchronize your files on a online
repository and collaborate with your colleagues in a very short amount
of time.

Due to the learning curve, emergency context may not be the best of time
to learn these tools. But learning can be achieved by steps. Once you acquire 
a couple of notions, your workflow can be quite efficient and fast.
If you are not working on a project where collaborating with people
through Git is a necessity, **it is actually a good time to get
confident using it** in solo before diving in collaboration.

## Setup

### Install Git {.unnumbered}

*Git* is the engine behind the scenes on your computer, which tracks
changes, branches (versions), merges, and reverting. **You must first
install *Git* from <https://git-scm.com/downloads>.**

### Install an interface (optional but recommended) {.unnumbered}

Git has its own language of commands, which can be typed into a command
line terminal. However, there are many clients/interfaces and as non-developpers, in your
day-to-day use, you will rarely _need_ to interact with Git directly and 
interface usually provide nice visualisation tools for file modifications or branches. 

Many options exist, on all OS, from beginner friendly to more complex ones. 
Good options for beginners include the RStudio Git pane and 
[Github Desktop](https://desktop.github.com/), which we will showcase in 
this chapter.
Intermediate (more powerfull, but more complex) options include Source Tree, 
Gitkracken, Smart Git and others.

Quick explanation on [Git clients](-%09https:/happygitwithr.com/git-client.html#git-client).

*Note: since interfaces actually all use Git internally, you can try several of
them, switch from one to another on a given project, use the console punctually 
for an action your interface does not support, or even perform any number of 
actions online on Github.*

As noted below, you may occasionally have to write Git commands into a
terminal such as the RStudio terminal pane (a tab adjacent to the R
Console) or the Git Bash terminal.


### Github account {.unnumbered}

Sign-up for a free account at [github.com](github.com).

You may be offered to set-up two-factor authentication with an app on
your phone. Read more in the Github [help
documents](https://docs.github.com/en/github/authenticating-to-github/securing-your-account-with-two-factor-authentication-2fa).

If you use Github Desktop, you can enter your Gitub credentials after
installation following these
[steps](https://docs.github.com/en/desktop/installing-and-configuring-github-desktop/authenticating-to-github).
If you don't do it know, credentials will be asked later when you try to
clone a project from Github.

## Vocabulary, concepts and basic functions

As when learning R, there is a bit of vocabulary to remember to
understand Git. Here are the [basics to get you
going](https://www.freecodecamp.org/news/an-introduction-to-git-for-absolute-beginners-86fa1d32ff71/)
/ [interactive tutorial](learngitbranching.js.org). In the next
sections, we will show how to use interfaces, but it is good
to have the vocabulary and concepts in mind, to build your mental model,
and as you'll need them when using interfaces anyway.

### Repository {.unnumbered}

A Git *repository* ("*repo*") is a folder that contains all the
sub-folders and files for your project (data, code, images, etc.) and
their revision histories. When you begin tracking changes in the
repository with it, Git will create a hidden folder that contains
all tracking information. A typical Git repository is
your *R Project* folder (see handbook page on [R projects]).

We will show how to create (_initialize_) a Git repository 
from Github, Github Desktop or Rstudio in the next
sections.

### Commits {.unnumbered}

A *commit* is a **snapshot** of the project at a given time. 
When you make a change to the project, you will make a new commit
to track the changes (the delta) made to your
files. For example, perhaps you edited some lines of code and updated a
related dataset. Once your changes are saved, you can bundle these
changes together into one "commit".

Each commit has a unique ID (a *hash*). For version control purposes,
you can revert your project back in time based on commits, so it is best
to keep them relatively small and coherent. You will also attach a brief
description of the changes called the "commit message".

*Staged changes*? To stage changes is to add them to the *staging area*
in preparation for the next commit. The idea is that you can finely
decide which changes to include in a given commit. For example, if you
worked on model specification in one script, and later on a figure in
another script, it would make sense to have two different commits (it would be easier
in case you wanted to revert the changes on the figure but not the model).


### Branches {.unnumbered}

A branch represents an *independent line* of changes in your repo, a
parallel, alternate version of your project files. 


Branches are useful to test changes before they are incorporated into
the *main* branch, which is usually the primary/final/"live" version of
your project. When you are done experimenting on a branch, you can bring
the changes into your *main* branch, by *merging* it, or delete it, if
the changes were not so successful.

*Note: you do not have to collaborate with other people to use branches,
nor need to have a remote online repository.*



### Local and remote repositories {.unnumbered}

To *clone* is to create a copy of a Git repository in another place.

For example, you can *clone* a online repository _from_ Github locally on
your computer, or begin with a local repository and clone
it online _to_ Github.

When you have cloned a repository, the project files exist in
two places:

-   the *LOCAL* repository on your physical computer. This
    is where you make the actual changes to the files/code.

-   the *REMOTE*, online repository: the versions of your project files
    in the Github repository (or on any other web
    host).

To synchronize these repositories, we will use more functions. Indeed,
unlike Sharepoint, Dropbox or other synchronizing software, Git does
not automatically update your local repository based or what's online,
or vice-versa. You get to choose when and how to synchronize.

-   `git fetch` downloads the new changes from the remote repository but does not 
change your local repository. Think of it as checking the state of the remote repository.

-   `git pull` downloads the new changes from the remote repositories
    and update your local repository.

-   When you have made one or several commits locally, you can
    `git push` the commits to the remote repository. This sends your
    changes on Github so that other people can see and pull them if
    they want to.


## Get started: create a new repository

There are many ways to create new repositories. You can do it from the
console, from Github, from an interface.

Two general approaches to set-up are:


-   Create a new R Project from an existing or new Github repository
    (*preferred for beginners*), or
-   Create a Github repository for an existing R project


### Start-up files {.unnumbered}

When you create a new repository, you can optionally create 
all of the below files, or you can add them to your repository at a later stage.
They would typically live in the "root" folder of the repository.

-   A *README* file is a file that someone can read to understand why
    your project exists and what else they should know to use it. It
    will be empty at first, but you should complete it later.

-   A *.gitignore* file is a text file where each line would contain
    folders or files that Git should ignore (not track changes). Read
    more about it and see examples
    [here](https://www.freecodecamp.org/news/gitignore-what-is-it-and-how-to-add-to-repo/).

-   You can choose a *license* for your work, so that other people
    know under which conditions they can use or reproduce your work. For more
    information, see the [Creative Commons
    licenses](https://creativecommons.org/licenses/).

### Create a new repository in Github {.unnumbered}

To create a new repository, log into Github and look for the green
button to create a new repository. This now empty repository can be
cloned locally to your computer (see next section).

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_new.png"))
```

You must choose if you want your repository to be **public** (visible to
everyone on the internet) or **private** (only visible to those with
permission). This has important implications if your data are sensitive.
If your repository is private you will encounter some quotas in advanced
special circumstances, such as if you are using Github *actions* to
automatically run your code in the cloud.
 
### Clone from a Github repository {.unnumbered}

You can *clone* an existing Github repository to create
a new local R project on your computer.

The Github repository could be one that already exists and contains
content, or could be an empty repository that you just created. In this
latter case you are essentially creating the Github repo and local R
project at the same time (see instructions above).

_Note_: if you do not have contributing rights on a Github repository, 
it is possible to first _fork_ the repository to your profile, and then
proceed with the other actions. Forking is explained at the end of this 
chapter, but we recommend that you read the other sections first.

Step 1: Navigate in Github to the repository, click on the green "**Code**"
button and copy the **HTTPS clone URL** (see image below)

```{r echo=F, out.width = '100%', out.height='100%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_clone.png"))
```

The next step can be performed in any interface. We will illustrate with
Rstudio and Github desktop.

#### In Rstudio {.unnumbered}

In RStudio, start a new R project by clicking *File \> New Project \>
Version Control \> Git*

-   When prompted for the "Repository URL", paste the HTTPS URL from
    Github\
-   Assign the R project a short, informative name\
-   Choose where the new R Project will be saved locally\
-   Check "Open in new session" and click "Create project"


You are now in a new, local, RStudio project that is a clone of the
Github repository. This local project and the Github repository are now
linked.

#### In Github Desktop {.unnumbered}

-   Click on *File \> Clone a repository*

-   Select the URL tab

-   Paste the HTTPS URL from Github in the first box

-   Select the folder in which you want to have your local repository

-   Click "CLONE"

```{r echo=F, out.width = '100%', out.height='100%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_clone_desktop.png"))
```

### New Github repo from existing R project {.unnumbered}

An alternative setup scenario is that you have an existing R project
with content, and you want to create a Github repository for it.

1)  Create a new, empty Github repository for the project (see
    instructions above)\
2)  Clone this repository locally (see HTTPS instructions above)\
3)  Copy all the content from your pre-existing R
    project (codes, data, etc.) into this new empty, local, repository (e.g. use copy and paste).\
4)  Open your new project in RStudio, and go to the Git pane. The new files should
    register as file changes, now tracked by Git. Therefore, you can
    bundle these changes as a *commit* and *push* them up to Github.
    Once *pushed*, the repository on Github will reflect all the files.
    
See the Github workflow section below for details on this process.

### What does it look like now? {.unnumbered}

#### In RStudio {-}

Once you have cloned a Github repository to a new R project, 
you now see in RStudio a "Git" tab. This tab appears in the same RStudio pane
as your R Environment:

```{r echo=F, out.width = "75%", out.height="75%", fig.align = "center"}
knitr::include_graphics(here::here("images", "Git_console.png"))
```

Please note the buttons circled in the image above, as they will be
referenced later (from left to right):

-   Button to *commit* the saved file changes to the local
    branch (this will open a new window)
-   Blue arrow to *pull* (update your local version of the branch with
    any changes made on the remote/Github version of that branch)
-   Green arrow to *push* (send any commits/changes for your local
    version of the branch to the remote/Github version of that branch)
-   The Git tab in RStudio
-   Button to create a NEW branch using whichever local branch is shown
    to the right as the base. *You almost always want to branch off from
    the main branch (after you first pull to update the main branch)*
-   The branch you are currently working in
-   Changes you made to code or other files will appear below

#### In Github Desktop {-}

Github Desktop is an independent application that allows you to manage
all your repositories. When you open it, the interface allows you to
choose the repository you want to work on, and then to perform basic Git
actions from there.

```{r echo=F, out.width = "75%", out.height="75%", fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_interface.png"))
```


## Git + Github workflow

### Process overview {.unnumbered}

Once you have completed the setup (described above), you will have a
Github repo that is connected (*cloned*) to a local R project. The
*main* branch (created by default) is the so-called "live" version of
*all* the files. When you want to make modifications, it is a good
practice to create a *new branch* from the *main* branch (like "Make a
Copy"). This is a typical workflow in Git because creating a branch is
easy and fast.


A typical workflow is as follow:

1.  Make sure that your local repository is up-to-date, update it if
    not

2.  Go to the branch you were working on previously, or create a new
    branch to try out some things


3.  Work on the files locally on your computer, make one or several
    commits to this branch

4.  Update the remote version of the branch with your changes (push)

5.  When you are satisfied with your branch, you can merge the online
    version of the working branch into the online "main" branch to
    transfer the changes

Other team members may be doing the same thing with their own branches,
or perhaps contributing commits into your working branch as well. 

We go through the above process step-by-step in more detail below.
Here is a schematic we've developed - it's in the format of a two-way
table so it should help epidemiologists understand.

```{r echo=F, out.height='150%', out.width='100%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_table.png"))
```

Here's [another diagram](https://build5nines.com/introduction-to-git-version-control-workflow/).

*Note: until recently, the term "master" branch was used, but it is now
referred to as "main" branch.*

```{r echo=F, out.width = '100%', out.height='100%', fig.align = "center"}
knitr::include_graphics(here::here("images", "GitHub-Flow.png"))
```

Image
[source](https://build5nines.com/introduction-to-git-version-control-workflow/)

## Create a new branch

When you select a branch to work on, **Git resets your working directory
the way it was the last time you were on this branch**.

### In Rstudio Git pane {.unnumbered}

Ensure you are in the "main" branch, and then click on the purple icon to
create a new branch (see image above).

-   You will be prompted to name your branch with a one-word descriptive
    name (can use underscores if needed).
-   You will see that locally, you are still in the same R project, but
    you are no longer working on the "main" branch.
-   Once created, the new branch will also appear in the Github website
    as a branch.
    
You can visualize branches in the Git Pane in Rstudio after clicking on "History"

```{r echo=F, out.width = '100%', out.height='100%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_rstudio_branchs.png"))
```


### In Github Desktop {.unnumbered}

The process is very much similar, you are prompted to give your branch
a name. After, you will be prompted to "Publish you branch to Github" to
make the new branch appear in the remote repo as well.


```{r echo=F, out.width = '100%', out.height='100%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_new_branch.png"))
```

### In console {.unnumbered}

What is actually happening behind the scenes is that you create a new
branch with `git branch`, then go to the branch with
`git checkout` (_i.e._ tell Git that your next commits will occur there). 
From your git repository:

```{bash, eval = FALSE}
git branch my-new-branch  # Create the new branch branch
git checkout my-new-branch # Go to the branch
git checkout -b my-new-branch # Both at once (shortcut)
```


For more information about using the console, see the section on
Git commands at the end.

## Commit changes

Now you can edit code, add new files, update datasets, etc.


Every one of your changes is tracked, *once the respective file is
saved*. Changed files will appear in the RStudio Git tab, in Github
Desktop, or using the command `git status` in the terminal (see below).

Whenever you make substantial changes (e.g. adding or updating a section of
code), pause and *commit* those changes. Think of a commit as a "batch"
of changes related to a common purpose. You can always continue to
revise a file after having committed changes on it.

*Advice on commits*: generally, it is better to make small commits, that
can be easily reverted if a problem arises, to commit together
modifications related to a common purpose. To achieve this, you will
find that *you should commit often*. At the beginning, you'll probably
forget to commit often, but then the habit kicks in.

### In Rstudio {.unnumbered}

The example below shows that, since the last commit, the R Markdown script "collaboration.Rmd" has changed, 
and several PNG images were added.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_tracking2.png"))
```

You might be wondering what the yellow, blue, green, and red squares next to
the file names represent. Here is a snapshot from the [RStudio
cheatsheet](https://www.rstudio.com/wp-content/uploads/2016/01/rstudio-IDE-cheatsheet.pdf)
that explains their meaning. Note that changes with yellow "?" can still
be staged, committed, and pushed.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_tracking.png"))
```

-   Press the "Commit" button in the Git tab, which will open a new
    window (shown below)

-   Click on a file name in the upper-left box

-   Review the changes you made to that file (highlighted below in green
    or red)

-   "Stage" the file, which will include those changes in the commit. Do
    this by checking the box next to the file name. Alternatively, you
    can highlight multiple file names and then click "Stage"

-   Write a commit message that is short but descriptive (required)

-   Press the "Commit" button. A pop-up box will appear showing success
    or an error message.


Now you can make more changes and more commits, as many times as you would like

```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_commit.png"))
```

### In Github Desktop {.unnumbered}

You can see the list of the files that were changed on the left. If
you select a text file, you will see a summary of the modifications that were made
in the right pane (the view will not work on more complex files like .docs or .xlsx).

To stage the changes, just tick the little box near file names. When you
have selected the files you want to add to this commit, give the commit
a name, optionally a description and then click on the **commit**
button.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_commit.png"))
```

### In console {.unnumbered}

The two functions used behind the scenes are `git add` to select/stage
files and `git commit` to actually do the commit.

```{bash, eval = FALSE}
git status # see the changes 

git add new_pages/collaboration.Rmd  # select files to commit (= stage the changes)

git commit -m "Describe commit from Github Desktop" # commit the changes with a message

git log  # view information on past commits
```


### Amend a previous commit {.unnumbered}

What happens if you commit some changes, carry on working, and realize
that you made changes that should "belong" to the past commit (in your opinion). 
Fear not! You can append these changes to your previous commit.

In Rstudio, it should be pretty obvious as there is a "Amend previous commit" 
box on the same line as the COMMIT button. 

For some unclear reason, the functionality has not been implemented 
as such in Github Desktop, but there is a (conceptually awkward but easy)
way around. If you have committed **but not pushed** your changes yet, 
an "UNDO" button appears just under the COMMIT button. Click on it and 
it will revert your commit (but keep your staged files and your commit message). 
Save your changes, add new files to the commit if necessary and commit again.

In the console:  

```{bash, eval = FALSE}
git add [YOUR FILES] # Stage your new changes

git commit --amend  # Amend the previous commit

git commit --amend -m "An updated commit message"  # Amend the previous commit AND update the commit message
```


_Note: think before modifying commits that are already public and shared with your collaborators_.


## Pull and push changes up to Github

"First PULL, then PUSH"

It is good practice to *fetch* and *pull* before you begin working on
your project, to update the branch version on your local computer with
any changes that have been made to it in the remote/Github version.

PULL often. Don't hesitate. *Always pull before pushing*.

When your changes are made and committed and you are happy with the 
state of your project, you can *push* your commits up
to the remote/Github version of your branch.


Rince and repeat while you are working on the repository.

**Note:** it is much easier to revert changes that were committed but not 
pushed (i.e. are still local) than to revert changes that were pushed to the
remote repository (and perhaps already pulled by someone else), so it is better 
to push when you are done with introducing changes on the task that 
you were working on.


#### In Rstudio {.unnumbered}

*PULL* - First, click the "Pull" icon (downward arrow) which fetches and
pulls at the same time.

*PUSH* - Clicking the green "Pull" icon (upward arrow). You may be asked
to enter your Github username and password. The first time you are
asked, you may need to enter two Git command lines into the *Terminal*:

-   **git config --global user.email
    "[you\@example.com](mailto:you@example.com){.email}"** (your Github
    email address), and\
-   **git config --global user.name "Your Github username"**

To learn more about how to enter these commands, see the section below
on Git commands.

***TIP:*** Asked to provide your password too often? See these chapters
10 & 11 of this
[tutorial](https://happygitwithr.com/credential-caching.html#credential-caching)
to connect to a repository using a SSH key (more
complicated)  


#### In Github Desktop {.unnumbered}

Click on the "Fetch origin" button to check if there are new commits on
the remote repository.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_fetch_button.png"))
```

If Git finds new commits on the remote repository, the button will
change into a "Pull" button. Because the same button is used to push and
pull, you cannot push your changes if you don't pull before.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_pull_button.png"))
```

You can go to the "History" tab (near the "Changes" tab) to see all
commits (yours and others). This is a nice way of acquainting yourself
with what your collaborators did. You can read the commit message, the
description if there is one, and compare the code of the two files using
the *diff* pane.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_history.png"))
```

Once all remote changes have been pulled, and at least one local change
has been committed, you can push by clicking on the same button.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_push_button.png"))
```

#### Console {.unnumbered}

Without surprise, the commands are *fetch*, *pull* and *push*.

```{bash, eval = FALSE}
git fetch  # are there new commits in the remote directory?
git pull   # Bring remote commits into your local branch
git push   # Puch local commits of this branch to the remote branch
```


### I want to pull but I have local work {.unnumbered}

This can happen sometimes: 
you made some changes on your local repository, but the remote
repository has commits that you didn't pull. 


Git will refuse to pull because it might overwrite your changes. 
There are several strategies to keep your changes, 
well described in [Happy Git with R](https://happygitwithr.com/pull-tricky.html), 
among which the two main ones are:
- commit your changes, fetch remote changes, pull them in, resolve conflicts 
if needed (see section below), and push everything online
- `stash` your changes, which sort of stores them aside, pull, unstash 
(restore), and then commit, solve any conflicts, and push. 

If the files concerned by the remote changes and the files concerned 
by your local changes do not overlap, Git may solve conflicts automatically.

In Github Desktop, this can be done with buttons. To stash, go to _Branch > Stash all changes_.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_stash.png"))
```



## Merge branch into Main 

If you have finished making changes, you can begin the process of
merging those changes into the main branch. Depending on your situation,
this may be fast, or you may have deliberate review and approval
steps involving teammates.

### Locally in Github Desktop {.unnumbered}

One can merge branches locally using Github Desktop. First, go to
(checkout) the branch that will be the recipient of the commits, in other words, the
branch you want to update. Then go to the menu *Branch \> Merge into
current branch* and click. A box will allow you to select the branch you
want to import from.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_merge.png"))
```

### In console {.unnumbered}

First move back to the branch that will be the recipient of the changes.
This is usually *master*, but it could be another branch. Then merge your
working branch into master.

```{bash, eval = FALSE}
git checkout master  # Go back to master (or to the branch you want to move your )
git merge this_fancy_new_branch
```

[This
page](https://git-scm.com/book/en/v2/Git-Branching-Basic-Branching-and-Merging)
shows a more advanced example of branching and explains a bit what is
happening behind the scenes.

### In Github: submitting pull requests {.unnumbered}


While it is totally possible to merge two branches locally, or without
informing anybody, a merge may be discussed or investigated by several
people before being integrated to the master branch. To help with the
process, Github offers some discussion features around the merge: the
**pull request**.

A pull request (a "PR") is a request to merge one branch into another 
(in other words, a request that _your working branch be pulled into the "main" branch_). 
A pull request typically involves multiple commits. A pull request usually begins a conversation and review 
process before it is accepted and the branch is merged. For example, 
you can read pull request discussions on [dplyr's
github](https://github.com/tidyverse/dplyr/pulls).


You can submit a pull request (PR) directly form the website (as
illustrated bellow) or from Github Desktop.

-   Go to Github repository (online)
-   View the tab "Pull Requests" and click the "New pull request" button
-   Select from the drop-down menu to merge your branch into main
-   Write a detailed Pull Request comment and click "Create Pull
    Request".

In the image below, the branch "forests" has been selected to be merged
into "main":

```{r echo=F, out.width = '100%', out.height='150%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_pull_request2.png"))
```

Now you should be able to see the pull request (example image below):

-   Review the tab "Files changed" to see how the "main" branch would
    change if the branch were merged.\
-   On the right, you can request a review from members of your team by
    tagging their Github ID. If you like, you can set the repository
    settings to require one approving review in order to merge into
    main.\
-   Once the pull request is approved, a button to
    "Merge pull request" will become active. Click this.\
-   Once completed, delete your branch as explained below.

```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_pull_request.png"))
```

### Resolving conflicts {.unnumbered}

When two people modified the same line(s) at the same time, a
merge conflict arises. Indeed, Git refuses to make a decision about
which version to keep, but it helps you find where the
conflict is. **DO NOT PANIC**. Most of the time, it is pretty straightforward
to resolve.

For example, on Github:

```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_conflict2.png"))
```


After the merge raised a conflict, open the file in your favorite editor.
The conflict will be indicated by series of characters:

```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_conflict3.png"))
```

The text between *\<\<\<\<\<\<\< HEAD* and *=======* comes from your
local repository, and the one between *=======* and *\>\>\>\>\>\>\>* from the
the other branch (which may be origin, master or any branch of
your choice).

You need to decide which version of the code you prefer (or even write a
third, including changes from both sides if pertinent), delete the rest
and remove all the marks that Git added *(\<\<\<\<\<\<\< HEAD, =======,
\>\>\>\>\>\>\> origin/master/your_branch_name*). 

Then, save the file, stage it and commit it : this is the commit 
that makes the merged version "official". Do not forget to push afterwards.

The more often you and your collaborators pull and push, the smaller the
conflicts will be.


*Note: If you feel at ease with the console, there are more [advanced
merging
options](https://git-scm.com/book/en/v2/Git-Tools-Advanced-Merging)
(e.g. ignoring whitespace, giving a collaborator priority etc.).*

### Delete your branch {.unnumbered}

Once a branch was merged into master and is no longer needed, you can
delete it.

#### Github + Rstudio

Go to the repository on Github and click the button to view all the
branches (next to the drop-down to select branches). Now find your
branch and click the trash icon next to it. Read more detail on deleting
a branch
[here](https://docs.github.com/en/free-pro-team@latest/github/collaborating-with-issues-and-pull-requests/creating-and-deleting-branches-within-your-repository#deleting-a-branch).

Be sure to also delete the branch locally on your computer. This will
not happen automatically.

-   From RStudio, make sure you are in the Main branch
-   Switch to typing Git commands in the RStudio "Terminal" (the tab
    adjacent to the R console), and type: **git branch -d
    branch_name**, where "branch_name" is the name of your branch to be
    deleted
-   Refresh your Git tab and the branch should be gone


#### In Github Desktop

Just checkout the branch you want to delete, and go to the menu
*Branch \> Delete*.


### Forking {.unnumbered}

You can fork a project if you would like to contribute to it but 
do not have the rights to do so, or if you just 
want to modify it for your personal use. A 
short description of forking can be found [here](https://guides.github.com/activities/forking/).

On Github, click on the "Fork" button:  

```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_fork_1.png"))
```

This will clone the original repository, but in your own profile. So now, 
there are two versions of the repository **on Github**: the original one,
that you cannot modify, and the cloned version in your profile.

Then, you can proceed to clone your version of the online repository locally 
on your computer, using any of the methods described in previous sections. 
Then, you can create a new branch, make changes, commit and push them 
_to your remote repository_.

Once you are happy with the result you can create a Pull Request 
from Github or Github Desktop to begin the conversation with the 
owners/maintainers of the original repository.


**What if you need some newer commits from the official repository?**

Imagine that someone makes a critical modification to the official repository,
which you want to include to your cloned version.
It is possible to synchronize your fork with the official repository. 
It involves using the terminal, but it is not too complicated. 
You mostly need to remember that:
- _upstream_ = the official repository, the one that you could not modify
- _origin_ = your version of the repository on your Github profile


You can read [this tutorial](https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/syncing-a-fork) or follow along below: 


First, type in your Git terminal (inside your repo):  

```{bash, eval = FALSE}
git remote -v
```
 
If you have not yet configured the upstream repository you should 
see two lines, beginning by _origin_. They show the remote repo 
that `fetch` and `push` point to. Remember, _origin_ is the conventional
nickname for your own version of the repository on Github. For example:  

```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_fork_2.png"))
```

Now, add a new remote repository:  

```{bash, eval = FALSE}
git remote add upstream https://github.com/epirhandbook/Epi_R_handbook.git
```
 
Here the address is the address that Github generates when you clone
a repository (see section on cloning). Now you will have four remote pointers:

```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_fork_3.png"))
```

Now that the setup is done, whenever you want to get the changes from 
the original (_upstream_) repository, you just have to go (_checkout_) to 
the branch you want to update and type:

```{bash, eval = FALSE}
git fetch upstream # Get the new commits from the remote repository
git checkout the_branch_you_want_to_update
git merge upstream/the_branch_you_want_to_update  # Merge the upstream branch into your branch.
git push # Update your own version of the remote repo
```

If there are conflicts, you will have to solve them, as explained 
in the Resolving conflicts section. 


**Summary**: forking is cloning, but on the Github server side. 
The rest of the actions are typical collaboration workflow actions 
(clone, push, pull, commit, merge, submit pull requests...).

_Note: while forking is a concept, not a Git command, it also exist on other Web hosts, like [Bitbucket](https://www.atlassian.com/git/tutorials/comparing-workflows/forking-workflow)._


```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_fork_4.png"))
```


## What we learned

You have learned how to:  

- setup Git to track modifications in your folders,  
- connect your local repository to a remote online repository,  
- commit changes,  
- synchronize your local and remote repositories.  

All this should get you going and be enough for most of your needs as 
epidemiologists. We usually do not have as advanced usage as developers. 

However, know that should you want (or need) to go further, Git offers more power to simplify 
commit histories, revert one or several commits, cherry-pick commits, etc. 
Some of it may sound like pure wizardry, but now that you have the basics, 
it is easier to build on it.


Note that while the Git pane in Rstudio and Github Desktop are good for 
beginners / day-to-day usage in our line of work, they do not offer an 
interface to some of the intermediate / advanced Git functions. 
Some more complete interfaces allows you to do more with point-and-click 
(usually at the cost of a more complex layout). 

Remember that since you can use any tool at any point to track your repository, 
you can very easily install an interface to try it out sometimes, 
or to perform some less common complex task occasionally, 
while preferring a simplified interface for the rest of time (e.g. using 
Github Desktop most of the time, and switching to SourceTree or Gitbash for some specific tasks).


## Git commands {#git}


### Recommended learning {.unnumbered}

To learn Git commands in an interactive tutorial, see [this
website](https://learngitbranching.js.org/).

### Where to enter commands {.unnumbered}

You enter commands in a Git shell.

*Option 1* You can open a new Terminal in RStudio. This tab is next to
the R Console. If you cannot type any text in it, click on the
drop-down menu below "Terminal" and select "New terminal". Type the
commands at the blinking space in front of the dollar sign "\$".

```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_terminal.png"))
```

*Option 2* You can also open a *shell* (a terminal to enter commands) by
clicking the blue "gears" icon in the Git tab (near the RStudio
Environment). Select "Shell" from the drop-down menu. A new window will
open where you can type the commands after the dollar sign "\$".

*Option 3* Right click to open "Git Bash here" which will open the same
sort of terminal, or open *Git Bash* form your application list.
[More beginner-friendly informations on Git Bash](https://happygitwithr.com/shell.html), 
how to find it and some bash commands you will need.

### Sample commands {.unnumbered}

Below we present a few common git commands. When you use them, keep in mind
which branch is active (checked-out), as that will change the action!

In the commands below, <name> represents a branch name. 
<commit_hash> represents the hash ID of a specific
commit. <num> represents a number. Do not type the
\< or \> symbols.

| Git command              | Action                                                                   |
|--------------------------|--------------------------------------------------------------------------|
| `git branch <name>`      | Create a new branch with the name <name>                                 |
| `git checkout <name>`    | Switch current branch to <name>                                          |
| `git checkout -b <name>` | Shortcut to create new branch *and* switch to it                         |
| `git status`             | See untracked changes                                                    |
| `git add <file>`         | Stage a file                                                             |
| `git commit -m <message>`| Commit currently staged changes to current branch with message |
| `git fetch`              | Fetch commits from remote repository                                     |
| `git pull`               | Pull commits from remote repository in current branch                    |
| `git push`               | Push local commits to remote directory                          |
| `git switch`             | An alternative to `git checkout` that is being phased in to Git |
| `git merge <name>`       | Merge <name> branch into current branch                         |
| `git rebase <name>`      | Append commits from current branch on to <name> branch          |



<!-- ======================================================= -->

## Resources

Much of this page was informed by [this "Happy Git with R"
website](https://happygitwithr.com/) by Jenny Bryan. There is a very helpful
section of this website that helps you troubleshoot common Git and
R-related errors.

The [Github.com documentation and start
guide](https://docs.github.com/en/github).

The RStudio ["IDE"
cheatsheet](https://www.rstudio.com/wp-content/uploads/2016/01/rstudio-IDE-cheatsheet.pdf)
which includes tips on Git with RStudio.

<https://ohi-science.org/news/github-going-back-in-time>

**Git commands for beginners**

An [interactive
tutorial](learngitbranching.js.org) to learn
Git commands.

<https://www.freecodecamp.org/news/an-introduction-to-git-for-absolute-beginners-86fa1d32ff71/>:
good for learning the absolute basics to track changes in one folder on
you own computer.

Nice schematics to understand branches:
<https://speakerdeck.com/alicebartlett/git-for-humans>


**Tutorials covering both basic and more advanced subjects**

<https://tutorialzine.com/2016/06/learn-git-in-30-minutes>

<https://dzone.com/articles/git-tutorial-commands-and-operations-in-git>
<https://swcarpentry.github.io/git-novice/> (short course)
<https://rsjakob.gitbooks.io/git/content/chapter1.html>

The [Pro Git book](https://git-scm.com/book/en/v2) is considered an official reference. 
While some chapters are ok, it is usually a bit _technical_. It is probably a good resource 
once you have used Git a bit and want to learn a bit more precisely 
what happens and how to go further.
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/collaboration.Rmd-->


# Các lỗi thường gặp {#errors}  

Chương này bao gồm một danh sách các lỗi phổ biến và các giải pháp đề xuất để khắc phục chúng.  


## Phiên giải thông báo lỗi  

Các lỗi trong R đôi khi có thể khó hiểu, vì vậy Google là bạn của bạn. Tìm kiếm thông báo lỗi với “R” và tìm các bài đăng gần đây trong [StackExchange.com](StackExchange.com), [stackoverflow.com](stackoverflow.com), [community.rstudio.com](community.rstudio.com), twitter (#rstats), và các diễn đàn khác được lập trình viên sử dụng để gửi câu hỏi và câu trả lời. Hãy cố gắng tìm các bài đăng gần đây đã giải quyết các vấn đề tương tự.  

Nếu sau nhiều lần tìm kiếm, bạn không thể tìm thấy câu trả lời cho vấn đề của mình, hãy cân nhắc tạo một *ví dụ có thể tái tạo* ("reprex") và tự đăng câu hỏi. Xem chương [Nhờ sự trợ giúp] để biết các mẹo về cách tạo và đăng một ví dụ có thể tái tạo lên diễn đàn. 


## Các lỗi thường gặp 

Dưới đây, chúng tôi liệt kê một số lỗi phổ biến và các giải thích/giải pháp tiềm năng. Một số trong số này được mượn từ Noam Ross, người đã phân tích các bài đăng phổ biến nhất trên diễn đàn Stack Overflow về các thông báo lỗi trên R (xem bài phân tích [tại đây](https://github.com/noamross/zero-dependency-problems/blob/master/misc/stack-overflow-common-r-errors.md))  


### Lỗi đánh máy {.unnumbered}  

```
Error: unexpected symbol in:
"  geom_histogram(stat = "identity")+
  tidyquant::geom_ma(n=7, size = 2, color = "red" lty"
```
Nếu bạn thấy lỗi "unexpected symbol", kiểm tra xem có thiếu dấu phẩy không  



### Các lỗi liên quan đến Package {.unnumbered}  

```
could not find function "x"...
```
Điều này có thể có nghĩa là bạn đã nhập sai tên hàm hoặc quên cài đặt hoặc gọi một package.


```
Error in select(data, var) : unused argument (var)
```
Bạn nghĩ rằng bạn đang sử dụng `dplyr::select()` nhưng thực tế là hàm `select()` đã bị đè bởi hàm `MASS::select()` - hãy ghi rõ `dplyr::` hoặc sắp xếp lại thứ tự các package được gọi để dplyr đứng sau tất cả các package khác.

Các lỗi hàm bị đè phổ biến khác bắt nguồn từ: `plyr::summarise()` và `stats::filter()`. Cân nhắc sử dụng [**conflicted** package](https://www.tidyverse.org/blog/2018/06/conflicted/).




```
Error in install.packages : ERROR: failed to lock directory ‘C:\Users\Name\Documents\R\win-library\4.0’ for modifying
Try removing ‘C:\Users\Name\Documents\R\win-library\4.0/00LOCK’
```

Nếu bạn gặp lỗi thông báo rằng bạn cần xóa tệp "00LOCK", go to your "R" library in your computer directory (e.g. R/win-library/) and look for a folder called "00LOCK". Delete this manually, and try installing the package again. A previous install process was probably interrupted, which led to this.  




### Các lỗi liên quan tới đối tượng {.unnumbered}  

```
No such file or directory:
```
Nếu bạn gặp lỗi như thế này khi cố gắng xuất hoặc nhập: Hãy kiểm tra lỗi chính tả của tệp và đường dẫn tệp và nếu đường dẫn chứa dấu gạch chéo, hãy đảm bảo rằng bạn đang dùng dấu gạch chéo xuôi `/` chứ không phải dấu gạch chéo ngược `\`. Ngoài ra, hãy đảm bảo rằng bạn đã sử dụng đúng phần mở rộng tệp (ví dụ: .csv, .xlsx).  


```
object 'x' not found 
```
Điều này có nghĩa là một đối tượng bạn đang tham chiếu không tồn tại. Có lẽ code trên đã không chạy đúng cách? 


```
Error in 'x': subscript out of bounds
```
Điều này có nghĩa là bạn đã cố gắng truy cập vào thứ gì đó (một phần tử của vectơ hoặc danh sách) không có ở đó. 




### Các lỗi liên quan tới cú pháp hàm {.unnumbered}

```
# ran recode without re-stating the x variable in mutate(x = recode(x, OLD = NEW)
Error: Problem with `mutate()` input `hospital`.
x argument ".x" is missing, with no default
i Input `hospital` is `recode(...)`.
```
Lỗi bên trên (`argument .x is missing, with no default`) thường gặp với hàm `mutate()` nếu bạn đang cung cấp một hàm như `recode()` hoặc `replace_na()` trong đó nó yêu cầu bạn cung cấp tên cột làm đối số đầu tiên. Điều này rất dễ quên.  



### Các lỗi logic {.unnumbered}  

```
Error in if
```

Điều này có thể có nghĩa là một mệnh đề `if` đã được áp dụng cho một cái gì đó không phải là TRUE hoặc FALSE.  


### Các lỗi liên quan tới Factor {.unnumbered}  

```
#Tried to add a value ("Missing") to a factor (with replace_na operating on a factor)
Problem with `mutate()` input `age_cat`.
i invalid factor level, NA generated
i Input `age_cat` is `replace_na(age_cat, "Missing")`.invalid factor level, NA generated
```
Nếu bạn nhìn thấy lỗi liên quan tới thứ bậc kiểu factor không hợp lệ, có thể bạn đang có một cột kiểu Factor(chứa các thứ bậc đã được xác định) và bạn đang cố gắng thêm một giá trị mới vào nó. Chuyển nó thành kiểu ký tự trước khi thêm giá trị mới.  


### Lỗi khi vẽ biểu đồ {.unnumbered}  

`Error: Insufficient values in manual scale. 3 needed but only 2 provided.`
Đây có thể là lỗi khi bạn vẽ biểu đồ bằng ggplot, sử dụng scale_fill_manual() values = c("orange", "purple"), trong đó bạn chưa cung cấp đủ số lượng màu cho hàm. Nếu cột này dạng factor, hãy cân nhắc liệu NA có phải là một bậc của factor hay không.

```
Can't add x object
```
Bạn có thể thừa dấu `+` ở cuối lệnh ggplot mà bạn cần xóa.


### Lỗi R Markdown {.unnumbered}  

Nếu thông báo lỗi là `Error in options[[sprintf("fig.%s", i)]]`, kiểm tra xem các tùy chọn của knitr ở đầu mỗi đoạn code có sử dụng chính xác `out.width = ` hoặc `out.height = ` và *không phải* `fig.width=` và `fig.height=` hay không.

### Tổng hợp {.unnumbered}  

Cân nhắc liệu bạn có đã sắp xếp lại các động từ **dplyr** đã được pipe và đã không thay thế một pipe ở giữa, hoặc đã không xóa một pipe ở cuối sau khi sắp xếp lại.

 


<!-- ======================================================= -->
## Nguồn { }

Đây là một bài đăng trên blog khác liệt kê [Các lỗi lập trình R phổ biến mà người mới bắt đầu gặp phải](https://www.r-bloggers.com/2016/06/common-r-programming-errors-faced-by-beginners/)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/errors.Rmd-->


# Nhờ sự trợ giúp {#help}

This page covers how to get help by posting a Github issue or by posting a reproducible example ("reprex") to an online forum.  




## Github issues  

Many R packages and projects have their code hosted on the website Github.com. You can communicate directly with authors via this website by posting an "Issue".  

Read more about how to store your work on Github in the page [Collaboration and Github]. 

On Github, each project is contained within a *repository*. Each repository contains code, data, outputs, help documentation, etc. There is also a vehicle to communicate with the authors called "Issues".  

See below the Github page for the **incidence2** package (used to make epidemic curves). You can see the "Issues" tab highlighted in yellow. You can see that there are 5 open issues.  

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "errors_Github_issues.png"))
```

Once in the Issues tab, you can see the open issues. Review them to ensure your problem is not already being addressed. You can open a new issue by clicking the green button on the right. You will need a Github account to do this. 

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "errors_Github_issues2.png"))
```


  
In your issue, follow the instructions below to provide a minimal, reproducible example. And please be courteous! Most people developing R packages and projects are doing so in their spare time (like this handbook!).  

To read more advanced materials about handling issues in your own Github repository, check out the Github [documentation on Issues](https://guides.github.com/features/issues/).  



## Reproducible example  

Providing a reproducible example ("reprex") is key to getting help when posting in a forum or in a Github issue. People want to help you, but you have to give them an example that they can work with on their own computer. The example should:  

* Demonstrate the problem you encountered  
* Be *minimal*, in that it includes only the data and code required to reproduce your problem  
* Be *reproducible*, such that all objects (e.g. data), package calls (e.g. `library()` or `p_load()`) are included

*Also, be sure you do not post any sensitive data with the reprex!* You can create example data frames, or use one of the data frames built into R (enter `data()` to open a list of these datasets).  



### The **reprex** package {.unnumbered}  

The **reprex** package can assist you with making a reproducible example:  

1) **reprex** is installed with **tidyverse**, so load either package  

```{r, eval=F}
# install/load tidyverse (which includes reprex)
pacman::p_load(tidyverse)
```

2) Begin an R script that creates your problem, step-by-step, starting from loading packages and data.  

```{r, eval=F}
# load packages
pacman::p_load(
     tidyverse,  # data mgmt and vizualization
     outbreaks)  # example outbreak datasets

# flu epidemic case linelist
outbreak_raw <- outbreaks::fluH7N9_china_2013  # retrieve dataset from outbreaks package

# Clean dataset
outbreak <- outbreak_raw %>% 
     mutate(across(contains("date"), as.Date))

# Plot epidemic

ggplot(data = outbreak)+
     geom_histogram(
          mapping = aes(x = date_of_onset),
          binwidth = 7
     )+
  scale_x_date(
    date_format = "%d %m"
  )

```
*Copy* all the code to your clipboard, and run the following command:  

```{r, eval=F}
reprex::reprex()
```

You will see an HTML output appear in the RStudio Viewer pane. It will contain all your code and any warnings, errors, or plot outputs. This output is also copied to your clipboard, so you can post it directly into a Github issue or a forum post.  

```{r, out.width=c('100%', '100%'), warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "errors_reprex_RStudio1.png"))
```


* If you set `session_info = TRUE` the output of `sessioninfo::session_info()` with your R and R package versions will be included  
* You can provide a working directory to `wd = `  
* You can read more about the arguments and possible variations at the [documentation]() or by entering `?reprex`

In the example above, the `ggplot()` command did not run because the arguemnt `date_format =` is not correct - it should be `date_labels = `.  


### Minimal data {.unnumbered}  

The helpers need to be able to use your data - ideally they need to be able to create it *with code*.  

To create a minumal dataset, consider anonymising and using only a subset of the observations. 

UNDER CONSTRUCTION - you can also use the function `dput()` to create minimal dataset.  




## Posting to a forum  

Read lots of forum posts. Get an understanding for which posts are well-written, and which ones are not.  

1) First, decide whether to ask the question at all. Have you *thoroughly* reviewed the forum website, trying various search terms, to see if your question has already been asked?  

2) Give your question an informative title (not "Help! this isn't working").  

3) Write your question:  

* Introduce your situation and problem  
* Link to posts of similar issues and explain how they do not answer your question  
* Include any relevant information to help someone who does not know the context of your work  
* Give a minimal reproducible example with your R session information  
* Use proper spelling, grammar, punctuation, and break your question into paragraphs so that it is easier to read  

4) Monitor your question once posted to respond to any requests for clarification. Be courteous and gracious - often the people answering are volunteering their time to help you. If you have a follow-up question consider whether it should be a separate posted question.  

5) Mark the question as answered, *if* you get an answer that meets the *original* request. This helps others later quickly recognize the solution.  


Read these posts about [how to ask a good question](https://stackoverflow.com/help/how-to-ask) the [Stack overflow code of conduct](https://stackoverflow.com/conduct).  


<!-- ======================================================= -->
## Resources { }


Tidyverse page on how to [get help!](https://www.tidyverse.org/help/#:~:text=When%20you%20want%20to%20make,to%20load%20the%20reprex%20package.&text=Enter%20reprex()%20in%20the,preview%20of%20your%20rendered%20reprex.)

Tips on [producing a minimal dataset](https://xiangxing98.github.io/R_Learning/R_Reproducible.nb.html#producing-a-minimal-dataset)

Documentation for the [dput function](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/dput)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/help.Rmd-->


# R trên ổ cứng mạng {#network-drives}  

 


<!-- ======================================================= -->
## Overview {  }

Using R on network or "company" shared drives can present additional challenges. This page contains approaches, common errors, and suggestions on troubleshooting gained from our experience working through these issues. These include tips for the particularly delicate situations involving R Markdown.  


**Using R on Network Drives: Overarching principles**  

1) You must get administrator access for your computer. Setup RStudio specifically to run as administrator.  
2) Save packages to a library on a lettered drive (e.g. "C:") when possible. Use a package library whose path begins with "\\\" as little as possible.  
3) the **rmarkdown** package must **not** be in a "\\\" package library, as then it can't connect to TinyTex or Pandoc.  




## RStudio as administrator  

When you click the RStudio icon to open RStudio, do so with a right-click. Depending on your machine, you may see an option to "Run as Administrator". Otherwise, you may see an option to select Properties (then there should appear a window with the option "Compatibility", and you can select a checkbox "Run as Administrator").  




## Useful commands 

Below are some useful commands when trying to troubleshoot issues using R on network drives.  

You can return the path(s) to package libraries that R is using. They will be listed in the order that R is using to install/load/search for packages. Thus, if you want R to use a different default library, you can switch the order of these paths (see below).  

```{r, eval=F}
# Find libraries
.libPaths()                   # Your library paths, listed in order that R installs/searches. 
                              # Note: all libraries will be listed, but to install to some (e.g. C:) you 
                              # may need to be running RStudio as an administrator (it won't appear in the 
                              # install packages library drop-down menu) 
```

You may want to switch the order of the package libraries used by R. For example if R is picking up a library location that begins with "\\\" and one that begins with a letter e.g. "D:". You can adjust the order of `.libPaths()` with the following code.  

````{r, eval=F}
# Switch order of libraries
# this can effect the priority of R finding a package. E.g. you may want your C: library to be listed first
myPaths <- .libPaths() # get the paths
myPaths <- c(myPaths[2], myPaths[1]) # switch them
.libPaths(myPaths) # reassign them
```

If you are having difficulties with R Markdown connecting to Pandoc, begin with this code to find out where RStudio thinks your Pandoc installation is.  

```{r, eval=F}
# Find Pandoc
Sys.getenv("RSTUDIO_PANDOC")  # Find where RStudio thinks your Pandoc installation is
```

If you want to see which library a package is loading from, try the below code:  

```{r, eval=F}
# Find a package
# gives first location of package (note order of your libraries)
find.package("rmarkdown", lib.loc = NULL, quiet = FALSE, verbose = getOption("verbose")) 
```



<!-- ======================================================= -->
## Troubleshooting common errors {  }


**"Failed to compile...tex in rmarkdown"**  

* Check the installation of TinyTex, or install TinyTex to C: location. See the [R basics] page on how to install TinyTex.  

```{r, eval=F}
# check/install tinytex, to C: location
tinytex::install_tinytex()
tinytex:::is_tinytex() # should return TRUE (note three colons)
```


**Internet routines cannot be loaded**  

For example, `Error in tools::startDynamicHelp() : internet routines cannot be loaded`  

* Try selecting 32-bit version from RStudio via Tools/Global Options.  
  * note: if 32-bit version does not appear in menu, make sure you are not using RStudio v1.2.  
* Alternatively, try uninstalling R and re-installing with different bit version (32 instead of 64)


**C: library does not appear as an option when I try to install packages manually**

* Run RStudio as an administrator, then this option will appear.  
* To set-up RStudio to always run as administrator (advantageous when using an Rproject where you don't click RStudio icon to open)... right-click the Rstudio icon 

The image below shows how you can manually select the library to install a package to. This window appears when you open the Packages RStudio pane and click "Install".  

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "network_install.png"))
```

**Pandoc 1 error**  

If you are getting "pandoc error 1" when knitting R Markdowns scripts on network drives:  

* Of multiple library locations, have the one with a lettered drive listed first (see codes above)  
* The above solution worked when knitting on local drive but while on a networked internet connection  
* See more tips here: https://ciser.cornell.edu/rmarkdown-knit-to-html-word-pdf/  

**Pandoc Error 83**  

The error will look something like this: `can't find file...rmarkdown...lua...`. This means that it was unable to find this file.  

See https://stackoverflow.com/questions/58830927/rmarkdown-unable-to-locate-lua-filter-when-knitting-to-word  

Possibilities:  

1) Rmarkdown package is not installed  
2) Rmarkdown package is not findable  
3) An admin rights issue.  

It is possible that R is not able to find the **rmarkdown** package file, so check which library the **rmarkdown** package lives (see code above). If the package is installed to a library that in inaccessible (e.g. starts with "\\\") consider manually moving it to C: or other named drive library. Be aware that the **rmarkdown** package has to be able to connect to TinyTex installation, so can not live in a library on a network drive.


**Pandoc Error 61**  

For example: `Error: pandoc document conversion failed with error 61`  or `Could not fetch...`  

* Try running RStudio as administrator (right click icon, select run as admin, see above instructions)  
* Also see if the specific package that was unable to be reached can be moved to C: library.

**LaTex error (see below)**

An error like: `! Package pdftex.def Error: File 'cict_qm2_2020-06-29_files/figure-latex/unnamed-chunk-5-1.png' not found: using draft setting.` or `Error: LaTeX failed to compile file_name.tex.`  

* See https://yihui.org/tinytex/r/#debugging for debugging tips.  
* See file_name.log for more info.


**Pandoc Error 127**  

This could be a RAM (space) issue. Re-start your R session and try again. 


**Mapping network drives**

Mapping a network drive can be risky. Consult with your IT department before attempting this.  

A tip borrowed from this [forum discussion](https://stackoverflow.com/questions/48161177/r-markdown-openbinaryfile-does-not-exist-no-such-file-or-directory/55616529?noredirect=1#comment97966859_55616529): 

How does one open a file "through a mapped network drive"?  

* First, you'll need to know the network location you're trying to access.  
* Next, in the Windows file manager, you will need to right click on "This PC" on the right hand pane, and select "Map a network drive".  
* Go through the dialogue to define the network location from earlier as a lettered drive.  
* Now you have two ways to get to the file you're opening. Using the drive-letter path should work.  


**Error in install.packages()**  

If you get an error that includes mention of a "lock" directory, for example: `Error in install.packages : ERROR: failed to lock directory...`

Look in your package library and you will see a folder whose name begins with "00LOCK". Try the following tips:  

* Manually delete the "00LOCK" folder directory from your package library. Try installing the package again.  
* You can also try the command `pacman::p_unlock()` (you can also put this command in the Rprofile so it runs every time project opens.). Then try installing the package again. It may take several tries.  
* Try running RStudio in Administrator mode, and try installing the packages one-by-one.  
* If all else fails, install the package to another library or folder (e.g. Temp) and then manually copy the package's folder over to the desired library.  






```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/network_drives.Rmd-->


# Data Table {#data-table}  
     
The handbook focusses on the **dplyr** “verb” functions and the **magrittr** pipe operator `%>%` as a method to clean and group data, but the **data.table** package offers an alternative method that you may encounter in your R career.  



<!-- ======================================================= -->
## Intro to data tables {  }

A data table is a 2-dimensional data structure like a data frame that allows complex grouping operations to be performed. The data.table syntax is structured so that operations can be performed on rows, columns and groups. 

The structure is **DT[i, j, by]**, separated by 3 parts; the **i, j** and **by** arguments. The **i** argument allows for subsetting of required rows, the **j** argument allows you to operate on columns and the **by** argument allows you operate on columns by groups.
  
This page will address the following topics:  

* Importing data and use of `fread()` and `fwrite()`
* Selecting and filtering rows using the **i** argument
* Using helper functions `%like%`, `%chin%`, `%between%` 
* Selecting and computing on columns using the **j** argument
* Computing by groups using the **by** argument
* Adding and updating data to data tables using `:=`

<!-- ======================================================= -->
## Load packages and import data { }

### Load packages {.unnumbered}  

Using the `p_load()` function from **pacman**, we load (and install if necessary) packages required for this analysis.
     
     
```{r}
pacman::p_load(
  rio,        # to import data
  data.table, # to group and clean data
  tidyverse,  # allows use of pipe (%>%) function in this chapter
  here 
  ) 
```


### Import data {.unnumbered}

This page will explore some of the core functions of **data.table** using the case linelist referenced throughout the handbook.

We import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow step-by-step, see instructions in the [Download book and data] page. The dataset is imported using the `import()` function from the **rio** package. See the page on [Import and export] for various ways to import data. From here we use `data.table()` to convert the data frame to a data table.

```{r}
linelist <- rio::import(here("data", "linelist_cleaned.xlsx")) %>% data.table()
```

The `fread()` function is used to directly import regular delimited files, such as .csv files, directly to a data table format. This function, and its counterpart, `fwrite()`, used for writing data.tables as regular delimited files are very fast and computationally efficient options for large databases.


The first 20 rows of `linelist`:  

```{r message=FALSE, echo=F, eval=FALSE}
DT::datatable(head(linelist,20), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Base R commands such as `dim()` that are used for data frames can also be used for data tables

```{r}
dim(linelist) #gives the number of rows and columns in the data table
```


<!-- ======================================================= -->
## The i argument: selecting and filtering rows{ }
     
Recalling the **DT[i, j, by]** structure, we can filter rows using either row numbers or logical expressions. The i argument is first; therefore, the syntax **DT[i]** or **DT[i,]** can be used. 

The first example retrieves the first 5 rows of the data table, the second example subsets cases are 18 years or over, and the third example subsets cases 18 years old or over but not diagnosed at the Central Hospital:


```{r, eval=F}
linelist[1:5] #returns the 1st to 5th row
linelist[age >= 18] #subsets cases are equal to or over 18 years
linelist[age >= 18 & hospital != "Central Hospital"] #subsets cases equal to or over 18 years old but not diagnosed at the Central Hospital

```

Using .N in the i argument represents the total number of rows in the data table. This can be used to subset on the row numbers: 

```{r, eval=F}
linelist[.N] #returns the last row
linelist[15:.N] #returns the 15th to the last row
```


### Using helper functions for filtering {.unnumbered}  

Data table uses helper functions that make subsetting rows easy. The `%like%` function is used to match a pattern in a column, `%chin%` is used to match a specific character, and the `%between%` helper function is used to match numeric columns within a prespecified range.

In the following examples we:
*  filter rows where the hospital variable contains “Hospital”
*  filter rows where the outcome is “Recover” or “Death”
*  filter rows in the age range 40-60

```{r, eval=F}
linelist[hospital %like% "Hospital"] #filter rows where the hospital variable contains “Hospital”
linelist[outcome %chin% c("Recover", "Death")] #filter rows where the outcome is “Recover” or “Death”
linelist[age %between% c(40, 60)] #filter rows in the age range 40-60

#%between% must take a vector of length 2, whereas %chin% can take vectors of length >= 1

```

## The j argument: selecting and computing on columns{ }

Using the **DT[i, j, by]** structure, we can select columns using numbers or names. The **j** argument is second; therefore, the syntax **DT[, j]** is used. To facilitate computations on the **j** argument, the column is wrapped using either `list()` or `.()`. 


### Selecting columns {.unnumbered} 

The first example retrieves the first, third and fifth columns of the data table, the second example selects all columns except the height, weight and gender columns. The third example uses the `.()` wrap to select the **case_id** and **outcome** columns.


```{r, eval=F}
linelist[ , c(1,3,5)]
linelist[ , -c("gender", "age", "wt_kg", "ht_cm")]
linelist[ , list(case_id, outcome)] #linelist[ , .(case_id, outcome)] works just as well

```

### Computing on columns {.unnumbered} 

By combining the **i** and **j** arguments it is possible to filter rows and compute on the columns. Using **.N** in the **j** argument also represents the total number of rows in the data table and can be useful to return the number of rows after row filtering.

In the following examples we:
* Count the number of cases that stayed over 7 days in hospital
* Calculate the mean age of the cases that died at the military hospital
* Calculate the standard deviation, median, mean age of the cases that recovered at the central hospital

```{r}
linelist[days_onset_hosp > 7 , .N]
linelist[hospital %like% "Military" & outcome %chin% "Death", .(mean(age, na.rm = T))] #na.rm = T removes N/A values
linelist[hospital == "Central Hospital" & outcome == "Recover", 
                 .(mean_age = mean(age, na.rm = T),
                   median_age = median(age, na.rm = T),
                   sd_age = sd(age, na.rm = T))] #this syntax does not use the helper functions but works just as well

```

Remember using the .() wrap in the j argument facilitates computation, returns a data table and allows for column naming.

## The by argument: computing by groups{ }

The **by** argument is the third argument in the **DT[i, j, by]** structure. The **by** argument accepts both a character vector and the `list()` or `.()` syntax. Using the `.()` syntax in the **by** argument allows column renaming on the fly.

In the following examples we:	
* group the number of cases by hospital
* in cases 18 years old or over, calculate the mean height and weight of cases according to gender and whether they recovered or died
* in admissions that lasted over 7 days, count the number of cases according to the month they were admitted and the hospital they were admitted to


````{r}
linelist[, .N, .(hospital)] #the number of cases by hospital
linelist[age > 18, .(mean_wt = mean(wt_kg, na.rm = T),
                             mean_ht = mean(ht_cm, na.rm = T)), .(gender, outcome)] #NAs represent the categories where the data is missing
linelist[days_onset_hosp > 7, .N, .(month = month(date_hospitalisation), hospital)]

```

Data.table also allows the chaining expressions as follows:

````{r}

linelist[, .N, .(hospital)][order(-N)][1:3] #1st selects all cases by hospital, 2nd orders the cases in descending order, 3rd subsets the 3 hospitals with the largest caseload


```

In these examples we are following the assumption that a row in the data table is equal to a new case, and so we can use the **.N** to represent the number of rows in the data table. Another useful function to represent the number of unique cases is `uniqueN()`, which returns the number of unique values in a given input. This is illustrated here:

````{r}

linelist[, .(uniqueN(gender))] #remember .() in the j argument returns a data table

```

The answer is 3, as the unique values in the gender column are m, f and N/A. Compare with the base R function `unique()`, which returns all the unique values in a given input:

````{r}

linelist[, .(unique(gender))]
```

To find the number of unique cases in a given month we would write the following:

````{r}

linelist[, .(uniqueN(case_id)), .(month = month(date_hospitalisation))]

```

## Adding and updating to data tables { }

The `:=` operator is used to add or update data in a data table. Adding columns to your data table can be done in the following ways:

````{r}

linelist[, adult := age >= 18] #adds one column
linelist[, c("child", "wt_lbs") := .(age < 18, wt_kg*2.204)] #to add multiple columns requires c("") and list() or .() syntax
linelist[, `:=` (bmi_in_range = (bmi > 16 & bmi < 40),
                         no_infector_source_data = is.na(infector) | is.na(source))] #this method uses := as a functional operator `:=`
linelist[, adult := NULL] #deletes the column

```


Further complex aggregations are beyond the scope of this introductory chapter, but the idea is to provide a popular and viable alternative to **dplyr** for grouping and cleaning data. The **data.table** package is a great package that allows for neat and readable code.


## Resources {  }

Here are some useful resources for more information:
* https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html
* https://github.com/Rdatatable/data.table
* https://s3.amazonaws.com/assets.datacamp.com/img/blog/data+table+cheat+sheet.pdf
* https://www.machinelearningplus.com/data-manipulation/datatable-in-r-complete-guide/
* https://www.datacamp.com/community/tutorials/data-table-r-tutorial

You can perform any summary function on grouped data; see the Cheat Sheet here for more info:
https://s3.amazonaws.com/assets.datacamp.com/blog_assets/datatable_Cheat_Sheet_R.pdf




```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/data_table.Rmd-->

